################################################################################
                      [1m Learning iteration 0/2000 [0m                       

                       Computation: 10666 steps/s (collection: 8.909s, learning 0.307s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 31.2464
                       Mean reward: 0.00
               Mean episode length: 21.40
    Episode_Reward/reaching_object: 0.0006
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0001
        Episode_Reward/action_rate: -0.0002
          Episode_Reward/joint_vel: -0.0003
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 9.22s
                      Time elapsed: 00:00:09
                               ETA: 05:07:12

################################################################################
                      [1m Learning iteration 1/2000 [0m                       

                       Computation: 14381 steps/s (collection: 6.699s, learning 0.136s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 31.3398
                       Mean reward: 0.00
               Mean episode length: 45.09
    Episode_Reward/reaching_object: 0.0019
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0003
        Episode_Reward/action_rate: -0.0006
          Episode_Reward/joint_vel: -0.0009
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 6.84s
                      Time elapsed: 00:00:16
                               ETA: 04:27:23

################################################################################
                      [1m Learning iteration 2/2000 [0m                       

                       Computation: 14468 steps/s (collection: 6.657s, learning 0.137s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 31.4200
                       Mean reward: 0.01
               Mean episode length: 69.30
    Episode_Reward/reaching_object: 0.0034
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0005
        Episode_Reward/action_rate: -0.0011
          Episode_Reward/joint_vel: -0.0014
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 6.79s
                      Time elapsed: 00:00:22
                               ETA: 04:13:35

################################################################################
                      [1m Learning iteration 3/2000 [0m                       

                       Computation: 13763 steps/s (collection: 6.980s, learning 0.163s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 31.4580
                       Mean reward: 0.02
               Mean episode length: 93.06
    Episode_Reward/reaching_object: 0.0053
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0007
        Episode_Reward/action_rate: -0.0015
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 7.14s
                      Time elapsed: 00:00:29
                               ETA: 04:09:31

################################################################################
                      [1m Learning iteration 4/2000 [0m                       

                       Computation: 14332 steps/s (collection: 6.732s, learning 0.127s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 31.5033
                       Mean reward: 0.02
               Mean episode length: 117.24
    Episode_Reward/reaching_object: 0.0071
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 6.86s
                      Time elapsed: 00:00:36
                               ETA: 04:05:09

################################################################################
                      [1m Learning iteration 5/2000 [0m                       

                       Computation: 14397 steps/s (collection: 6.657s, learning 0.171s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 31.5397
                       Mean reward: 0.02
               Mean episode length: 141.16
    Episode_Reward/reaching_object: 0.0089
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0032
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 6.83s
                      Time elapsed: 00:00:43
                               ETA: 04:02:01

################################################################################
                      [1m Learning iteration 6/2000 [0m                       

                       Computation: 14200 steps/s (collection: 6.783s, learning 0.140s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 31.5905
                       Mean reward: 0.03
               Mean episode length: 165.69
    Episode_Reward/reaching_object: 0.0117
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0037
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 6.92s
                      Time elapsed: 00:00:50
                               ETA: 04:00:13

################################################################################
                      [1m Learning iteration 7/2000 [0m                       

                       Computation: 14445 steps/s (collection: 6.662s, learning 0.144s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 31.6263
                       Mean reward: 0.05
               Mean episode length: 189.43
    Episode_Reward/reaching_object: 0.0156
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0044
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 6.81s
                      Time elapsed: 00:00:57
                               ETA: 03:58:20

################################################################################
                      [1m Learning iteration 8/2000 [0m                       

                       Computation: 17173 steps/s (collection: 5.606s, learning 0.118s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 31.6363
                       Mean reward: 0.06
               Mean episode length: 213.24
    Episode_Reward/reaching_object: 0.0175
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0049
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 5.72s
                      Time elapsed: 00:01:03
                               ETA: 03:52:52

################################################################################
                      [1m Learning iteration 9/2000 [0m                       

                       Computation: 59416 steps/s (collection: 1.567s, learning 0.088s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 31.6856
                       Mean reward: 0.10
               Mean episode length: 237.77
    Episode_Reward/reaching_object: 0.0240
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0055
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 1.65s
                      Time elapsed: 00:01:04
                               ETA: 03:34:58

################################################################################
                      [1m Learning iteration 10/2000 [0m                      

                       Computation: 59434 steps/s (collection: 1.564s, learning 0.090s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 31.7041
                       Mean reward: 0.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0283
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0060
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 1.65s
                      Time elapsed: 00:01:06
                               ETA: 03:20:18

################################################################################
                      [1m Learning iteration 11/2000 [0m                      

                       Computation: 59043 steps/s (collection: 1.562s, learning 0.103s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 31.7190
                       Mean reward: 0.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0346
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0061
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 1.66s
                      Time elapsed: 00:01:08
                               ETA: 03:08:07

################################################################################
                      [1m Learning iteration 12/2000 [0m                      

                       Computation: 59276 steps/s (collection: 1.575s, learning 0.083s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 31.7399
                       Mean reward: 0.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0382
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0061
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 1.66s
                      Time elapsed: 00:01:09
                               ETA: 02:57:47

################################################################################
                      [1m Learning iteration 13/2000 [0m                      

                       Computation: 57760 steps/s (collection: 1.609s, learning 0.093s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 31.7709
                       Mean reward: 0.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0446
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0061
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 1.70s
                      Time elapsed: 00:01:11
                               ETA: 02:49:02

################################################################################
                      [1m Learning iteration 14/2000 [0m                      

                       Computation: 56831 steps/s (collection: 1.620s, learning 0.110s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 31.7857
                       Mean reward: 0.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0545
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0061
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 1.73s
                      Time elapsed: 00:01:13
                               ETA: 02:41:30

################################################################################
                      [1m Learning iteration 15/2000 [0m                      

                       Computation: 56082 steps/s (collection: 1.663s, learning 0.090s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.0005
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 31.8171
                       Mean reward: 0.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0611
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0061
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 1.75s
                      Time elapsed: 00:01:14
                               ETA: 02:34:57

################################################################################
                      [1m Learning iteration 16/2000 [0m                      

                       Computation: 52880 steps/s (collection: 1.775s, learning 0.084s)
             Mean action noise std: 1.04
          Mean value_function loss: 0.5956
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 31.9629
                       Mean reward: 0.39
               Mean episode length: 249.28
    Episode_Reward/reaching_object: 0.0867
     Episode_Reward/lifting_object: -0.0212
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0061
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 1.86s
                      Time elapsed: 00:01:16
                               ETA: 02:29:23

################################################################################
                      [1m Learning iteration 17/2000 [0m                      

                       Computation: 54936 steps/s (collection: 1.704s, learning 0.085s)
             Mean action noise std: 1.04
          Mean value_function loss: 0.7702
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 32.0520
                       Mean reward: 0.40
               Mean episode length: 249.05
    Episode_Reward/reaching_object: 0.1128
     Episode_Reward/lifting_object: -0.1324
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0061
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 1.79s
                      Time elapsed: 00:01:18
                               ETA: 02:24:18

################################################################################
                      [1m Learning iteration 18/2000 [0m                      

                       Computation: 53735 steps/s (collection: 1.735s, learning 0.094s)
             Mean action noise std: 1.05
          Mean value_function loss: 1.0301
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 32.2642
                       Mean reward: -0.22
               Mean episode length: 247.59
    Episode_Reward/reaching_object: 0.1184
     Episode_Reward/lifting_object: -0.3252
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0061
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 1.83s
                      Time elapsed: 00:01:20
                               ETA: 02:19:49

################################################################################
                      [1m Learning iteration 19/2000 [0m                      

                       Computation: 54217 steps/s (collection: 1.693s, learning 0.121s)
             Mean action noise std: 1.06
          Mean value_function loss: 1.5215
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 32.4142
                       Mean reward: 0.68
               Mean episode length: 247.19
    Episode_Reward/reaching_object: 0.1452
     Episode_Reward/lifting_object: -0.2810
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0061
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 1.81s
                      Time elapsed: 00:01:22
                               ETA: 02:15:45

################################################################################
                      [1m Learning iteration 20/2000 [0m                      

                       Computation: 54900 steps/s (collection: 1.692s, learning 0.099s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.0798
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 32.6367
                       Mean reward: 0.65
               Mean episode length: 247.76
    Episode_Reward/reaching_object: 0.1664
     Episode_Reward/lifting_object: -0.0895
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0062
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 1.79s
                      Time elapsed: 00:01:24
                               ETA: 02:12:02

################################################################################
                      [1m Learning iteration 21/2000 [0m                      

                       Computation: 51837 steps/s (collection: 1.793s, learning 0.103s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.1232
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 32.7874
                       Mean reward: 0.48
               Mean episode length: 247.77
    Episode_Reward/reaching_object: 0.1830
     Episode_Reward/lifting_object: -0.0588
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0062
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 1.90s
                      Time elapsed: 00:01:25
                               ETA: 02:08:49

################################################################################
                      [1m Learning iteration 22/2000 [0m                      

                       Computation: 54795 steps/s (collection: 1.702s, learning 0.092s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.1153
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 32.9519
                       Mean reward: 0.49
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.1891
     Episode_Reward/lifting_object: -0.0222
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0063
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 1.79s
                      Time elapsed: 00:01:27
                               ETA: 02:05:43

################################################################################
                      [1m Learning iteration 23/2000 [0m                      

                       Computation: 54192 steps/s (collection: 1.725s, learning 0.089s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0288
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 33.0803
                       Mean reward: 0.83
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.1875
     Episode_Reward/lifting_object: -0.0139
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0064
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 1.81s
                      Time elapsed: 00:01:29
                               ETA: 02:02:54

################################################################################
                      [1m Learning iteration 24/2000 [0m                      

                       Computation: 54278 steps/s (collection: 1.723s, learning 0.088s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0592
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 33.2750
                       Mean reward: 0.62
               Mean episode length: 249.16
    Episode_Reward/reaching_object: 0.1770
     Episode_Reward/lifting_object: -0.0093
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0064
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 1.81s
                      Time elapsed: 00:01:31
                               ETA: 02:00:19

################################################################################
                      [1m Learning iteration 25/2000 [0m                      

                       Computation: 53616 steps/s (collection: 1.721s, learning 0.112s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0049
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 33.3557
                       Mean reward: 0.73
               Mean episode length: 249.53
    Episode_Reward/reaching_object: 0.1725
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0065
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 1.83s
                      Time elapsed: 00:01:33
                               ETA: 01:57:57

################################################################################
                      [1m Learning iteration 26/2000 [0m                      

                       Computation: 54644 steps/s (collection: 1.705s, learning 0.093s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0007
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 33.5265
                       Mean reward: 0.74
               Mean episode length: 249.73
    Episode_Reward/reaching_object: 0.1676
     Episode_Reward/lifting_object: -0.0167
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0066
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 1.80s
                      Time elapsed: 00:01:34
                               ETA: 01:55:43

################################################################################
                      [1m Learning iteration 27/2000 [0m                      

                       Computation: 54174 steps/s (collection: 1.728s, learning 0.086s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.1165
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 33.4953
                       Mean reward: 0.73
               Mean episode length: 249.43
    Episode_Reward/reaching_object: 0.1570
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0067
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 1.81s
                      Time elapsed: 00:01:36
                               ETA: 01:53:40

################################################################################
                      [1m Learning iteration 28/2000 [0m                      

                       Computation: 52416 steps/s (collection: 1.764s, learning 0.111s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.0306
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 33.5439
                       Mean reward: 0.75
               Mean episode length: 249.32
    Episode_Reward/reaching_object: 0.1500
     Episode_Reward/lifting_object: -0.0152
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0068
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 1.88s
                      Time elapsed: 00:01:38
                               ETA: 01:51:49

################################################################################
                      [1m Learning iteration 29/2000 [0m                      

                       Computation: 50933 steps/s (collection: 1.797s, learning 0.133s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.4678
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 33.7567
                       Mean reward: 0.46
               Mean episode length: 248.71
    Episode_Reward/reaching_object: 0.1421
     Episode_Reward/lifting_object: -0.0800
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 1.93s
                      Time elapsed: 00:01:40
                               ETA: 01:50:08

################################################################################
                      [1m Learning iteration 30/2000 [0m                      

                       Computation: 52285 steps/s (collection: 1.758s, learning 0.123s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.1640
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 33.8369
                       Mean reward: 0.36
               Mean episode length: 249.32
    Episode_Reward/reaching_object: 0.1620
     Episode_Reward/lifting_object: -0.1128
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 1.88s
                      Time elapsed: 00:01:42
                               ETA: 01:48:31

################################################################################
                      [1m Learning iteration 31/2000 [0m                      

                       Computation: 52879 steps/s (collection: 1.735s, learning 0.124s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.1015
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 34.0083
                       Mean reward: 0.39
               Mean episode length: 248.79
    Episode_Reward/reaching_object: 0.1702
     Episode_Reward/lifting_object: -0.0133
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0070
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 1.86s
                      Time elapsed: 00:01:44
                               ETA: 01:46:59

################################################################################
                      [1m Learning iteration 32/2000 [0m                      

                       Computation: 51896 steps/s (collection: 1.747s, learning 0.147s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.1658
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 34.1101
                       Mean reward: 0.30
               Mean episode length: 247.33
    Episode_Reward/reaching_object: 0.1906
     Episode_Reward/lifting_object: -0.0391
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0071
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 1.89s
                      Time elapsed: 00:01:46
                               ETA: 01:45:34

################################################################################
                      [1m Learning iteration 33/2000 [0m                      

                       Computation: 46033 steps/s (collection: 2.014s, learning 0.122s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.3838
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.2106
                       Mean reward: 0.81
               Mean episode length: 247.40
    Episode_Reward/reaching_object: 0.2002
     Episode_Reward/lifting_object: -0.0787
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0072
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 2.14s
                      Time elapsed: 00:01:48
                               ETA: 01:44:29

################################################################################
                      [1m Learning iteration 34/2000 [0m                      

                       Computation: 49788 steps/s (collection: 1.848s, learning 0.127s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.4653
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 34.3090
                       Mean reward: 1.02
               Mean episode length: 245.26
    Episode_Reward/reaching_object: 0.2054
     Episode_Reward/lifting_object: -0.0822
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0072
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 1.97s
                      Time elapsed: 00:01:50
                               ETA: 01:43:17

################################################################################
                      [1m Learning iteration 35/2000 [0m                      

                       Computation: 50933 steps/s (collection: 1.815s, learning 0.115s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.3071
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 34.4192
                       Mean reward: 0.97
               Mean episode length: 244.92
    Episode_Reward/reaching_object: 0.2267
     Episode_Reward/lifting_object: -0.0868
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0073
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 1.93s
                      Time elapsed: 00:01:52
                               ETA: 01:42:07

################################################################################
                      [1m Learning iteration 36/2000 [0m                      

                       Computation: 52185 steps/s (collection: 1.762s, learning 0.122s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.0756
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 34.5116
                       Mean reward: 1.21
               Mean episode length: 246.95
    Episode_Reward/reaching_object: 0.2419
     Episode_Reward/lifting_object: -0.0346
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0074
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 1.88s
                      Time elapsed: 00:01:54
                               ETA: 01:40:59

################################################################################
                      [1m Learning iteration 37/2000 [0m                      

                       Computation: 49998 steps/s (collection: 1.875s, learning 0.092s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.0016
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 34.6664
                       Mean reward: 1.16
               Mean episode length: 247.36
    Episode_Reward/reaching_object: 0.2512
     Episode_Reward/lifting_object: -0.0093
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 1.97s
                      Time elapsed: 00:01:56
                               ETA: 01:39:58

################################################################################
                      [1m Learning iteration 38/2000 [0m                      

                       Computation: 51846 steps/s (collection: 1.779s, learning 0.118s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.2688
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 34.6337
                       Mean reward: 0.78
               Mean episode length: 247.22
    Episode_Reward/reaching_object: 0.2512
     Episode_Reward/lifting_object: -0.0376
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 1.90s
                      Time elapsed: 00:01:58
                               ETA: 01:38:56

################################################################################
                      [1m Learning iteration 39/2000 [0m                      

                       Computation: 50715 steps/s (collection: 1.834s, learning 0.105s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.1398
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 34.6640
                       Mean reward: 0.55
               Mean episode length: 247.46
    Episode_Reward/reaching_object: 0.2398
     Episode_Reward/lifting_object: -0.0446
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 1.94s
                      Time elapsed: 00:01:59
                               ETA: 01:38:00

################################################################################
                      [1m Learning iteration 40/2000 [0m                      

                       Computation: 52403 steps/s (collection: 1.780s, learning 0.096s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.5156
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 34.7654
                       Mean reward: 0.93
               Mean episode length: 244.75
    Episode_Reward/reaching_object: 0.2443
     Episode_Reward/lifting_object: -0.0450
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 1.88s
                      Time elapsed: 00:02:01
                               ETA: 01:37:03

################################################################################
                      [1m Learning iteration 41/2000 [0m                      

                       Computation: 50223 steps/s (collection: 1.868s, learning 0.090s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0064
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 34.8225
                       Mean reward: 1.16
               Mean episode length: 244.26
    Episode_Reward/reaching_object: 0.2367
     Episode_Reward/lifting_object: -0.0392
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 1.96s
                      Time elapsed: 00:02:03
                               ETA: 01:36:13

################################################################################
                      [1m Learning iteration 42/2000 [0m                      

                       Computation: 51031 steps/s (collection: 1.836s, learning 0.090s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.3933
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 34.9235
                       Mean reward: 0.47
               Mean episode length: 239.18
    Episode_Reward/reaching_object: 0.2512
     Episode_Reward/lifting_object: -0.0587
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 1.93s
                      Time elapsed: 00:02:05
                               ETA: 01:35:24

################################################################################
                      [1m Learning iteration 43/2000 [0m                      

                       Computation: 47636 steps/s (collection: 1.976s, learning 0.088s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.4292
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.9576
                       Mean reward: 0.47
               Mean episode length: 240.46
    Episode_Reward/reaching_object: 0.2587
     Episode_Reward/lifting_object: -0.0582
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 2.06s
                      Time elapsed: 00:02:07
                               ETA: 01:34:43

################################################################################
                      [1m Learning iteration 44/2000 [0m                      

                       Computation: 51058 steps/s (collection: 1.823s, learning 0.102s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 35.0137
                       Mean reward: 1.37
               Mean episode length: 233.37
    Episode_Reward/reaching_object: 0.2703
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 1.93s
                      Time elapsed: 00:02:09
                               ETA: 01:33:57

################################################################################
                      [1m Learning iteration 45/2000 [0m                      

                       Computation: 48841 steps/s (collection: 1.912s, learning 0.101s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.0575
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.1150
                       Mean reward: 1.28
               Mean episode length: 232.00
    Episode_Reward/reaching_object: 0.2701
     Episode_Reward/lifting_object: -0.0344
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 2.01s
                      Time elapsed: 00:02:11
                               ETA: 01:33:17

################################################################################
                      [1m Learning iteration 46/2000 [0m                      

                       Computation: 50714 steps/s (collection: 1.848s, learning 0.090s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.1167
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.1578
                       Mean reward: 1.18
               Mean episode length: 231.99
    Episode_Reward/reaching_object: 0.2817
     Episode_Reward/lifting_object: -0.0179
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 1.94s
                      Time elapsed: 00:02:13
                               ETA: 01:32:36

################################################################################
                      [1m Learning iteration 47/2000 [0m                      

                       Computation: 48461 steps/s (collection: 1.895s, learning 0.134s)
             Mean action noise std: 1.21
          Mean value_function loss: 1.0283
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.3040
                       Mean reward: 1.35
               Mean episode length: 224.19
    Episode_Reward/reaching_object: 0.2855
     Episode_Reward/lifting_object: -0.0709
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 2.03s
                      Time elapsed: 00:02:15
                               ETA: 01:32:00

################################################################################
                      [1m Learning iteration 48/2000 [0m                      

                       Computation: 50532 steps/s (collection: 1.851s, learning 0.094s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.2111
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.3485
                       Mean reward: 1.32
               Mean episode length: 223.62
    Episode_Reward/reaching_object: 0.2811
     Episode_Reward/lifting_object: -0.0762
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 1.95s
                      Time elapsed: 00:02:17
                               ETA: 01:31:22

################################################################################
                      [1m Learning iteration 49/2000 [0m                      

                       Computation: 50114 steps/s (collection: 1.874s, learning 0.088s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.3988
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 35.4078
                       Mean reward: 1.43
               Mean episode length: 215.52
    Episode_Reward/reaching_object: 0.2887
     Episode_Reward/lifting_object: -0.0578
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.8333
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 1.96s
                      Time elapsed: 00:02:19
                               ETA: 01:30:46

################################################################################
                      [1m Learning iteration 50/2000 [0m                      

                       Computation: 51999 steps/s (collection: 1.806s, learning 0.084s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.1061
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.5607
                       Mean reward: 1.09
               Mean episode length: 221.91
    Episode_Reward/reaching_object: 0.2929
     Episode_Reward/lifting_object: -0.0539
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 1.89s
                      Time elapsed: 00:02:21
                               ETA: 01:30:09

################################################################################
                      [1m Learning iteration 51/2000 [0m                      

                       Computation: 47951 steps/s (collection: 1.910s, learning 0.140s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.2153
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 35.7762
                       Mean reward: 1.45
               Mean episode length: 215.44
    Episode_Reward/reaching_object: 0.3012
     Episode_Reward/lifting_object: -0.0346
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.7917
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 2.05s
                      Time elapsed: 00:02:23
                               ETA: 01:29:39

################################################################################
                      [1m Learning iteration 52/2000 [0m                      

                       Computation: 48033 steps/s (collection: 1.873s, learning 0.174s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.0472
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.8170
                       Mean reward: 1.53
               Mean episode length: 218.65
    Episode_Reward/reaching_object: 0.3058
     Episode_Reward/lifting_object: -0.0368
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.7917
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 2.05s
                      Time elapsed: 00:02:25
                               ETA: 01:29:10

################################################################################
                      [1m Learning iteration 53/2000 [0m                      

                       Computation: 48629 steps/s (collection: 1.886s, learning 0.135s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.1303
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 35.9139
                       Mean reward: 1.52
               Mean episode length: 216.89
    Episode_Reward/reaching_object: 0.3049
     Episode_Reward/lifting_object: -0.0329
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 5.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 2.02s
                      Time elapsed: 00:02:27
                               ETA: 01:28:41

################################################################################
                      [1m Learning iteration 54/2000 [0m                      

                       Computation: 48845 steps/s (collection: 1.846s, learning 0.166s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.2743
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.9713
                       Mean reward: 1.54
               Mean episode length: 219.35
    Episode_Reward/reaching_object: 0.3178
     Episode_Reward/lifting_object: -0.0072
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0081
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 2.01s
                      Time elapsed: 00:02:29
                               ETA: 01:28:13

################################################################################
                      [1m Learning iteration 55/2000 [0m                      

                       Computation: 51807 steps/s (collection: 1.798s, learning 0.100s)
             Mean action noise std: 1.25
          Mean value_function loss: 1.6549
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 36.0063
                       Mean reward: -0.59
               Mean episode length: 216.44
    Episode_Reward/reaching_object: 0.3188
     Episode_Reward/lifting_object: -0.3128
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0082
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 1.90s
                      Time elapsed: 00:02:31
                               ETA: 01:27:42

################################################################################
                      [1m Learning iteration 56/2000 [0m                      

                       Computation: 50618 steps/s (collection: 1.850s, learning 0.092s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.4845
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 36.0517
                       Mean reward: 1.42
               Mean episode length: 222.05
    Episode_Reward/reaching_object: 0.3308
     Episode_Reward/lifting_object: -0.0921
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0083
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.2917
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 1.94s
                      Time elapsed: 00:02:33
                               ETA: 01:27:13

################################################################################
                      [1m Learning iteration 57/2000 [0m                      

                       Computation: 50653 steps/s (collection: 1.843s, learning 0.098s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.3010
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.0853
                       Mean reward: 1.42
               Mean episode length: 218.83
    Episode_Reward/reaching_object: 0.3330
     Episode_Reward/lifting_object: -0.1282
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0084
      Episode_Termination/time_out: 5.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.5417
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 1.94s
                      Time elapsed: 00:02:35
                               ETA: 01:26:45

################################################################################
                      [1m Learning iteration 58/2000 [0m                      

                       Computation: 49832 steps/s (collection: 1.830s, learning 0.143s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.4717
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.1284
                       Mean reward: 0.42
               Mean episode length: 222.18
    Episode_Reward/reaching_object: 0.3167
     Episode_Reward/lifting_object: -0.0804
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0084
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.2500
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 1.97s
                      Time elapsed: 00:02:37
                               ETA: 01:26:19

################################################################################
                      [1m Learning iteration 59/2000 [0m                      

                       Computation: 49659 steps/s (collection: 1.888s, learning 0.092s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.1180
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.1918
                       Mean reward: 1.49
               Mean episode length: 229.61
    Episode_Reward/reaching_object: 0.3189
     Episode_Reward/lifting_object: -0.0263
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 1.98s
                      Time elapsed: 00:02:39
                               ETA: 01:25:54

################################################################################
                      [1m Learning iteration 60/2000 [0m                      

                       Computation: 48043 steps/s (collection: 1.929s, learning 0.117s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0566
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.3474
                       Mean reward: 1.38
               Mean episode length: 225.61
    Episode_Reward/reaching_object: 0.2940
     Episode_Reward/lifting_object: -0.0150
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 2.05s
                      Time elapsed: 00:02:41
                               ETA: 01:25:32

################################################################################
                      [1m Learning iteration 61/2000 [0m                      

                       Computation: 50735 steps/s (collection: 1.830s, learning 0.108s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.5766
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.3887
                       Mean reward: 1.20
               Mean episode length: 220.03
    Episode_Reward/reaching_object: 0.2970
     Episode_Reward/lifting_object: -0.0985
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 1.94s
                      Time elapsed: 00:02:43
                               ETA: 01:25:07

################################################################################
                      [1m Learning iteration 62/2000 [0m                      

                       Computation: 50955 steps/s (collection: 1.822s, learning 0.108s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.4996
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 36.4196
                       Mean reward: 0.57
               Mean episode length: 223.20
    Episode_Reward/reaching_object: 0.2815
     Episode_Reward/lifting_object: -0.0544
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 1.93s
                      Time elapsed: 00:02:45
                               ETA: 01:24:43

################################################################################
                      [1m Learning iteration 63/2000 [0m                      

                       Computation: 50042 steps/s (collection: 1.844s, learning 0.121s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0783
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 36.4801
                       Mean reward: 0.88
               Mean episode length: 225.30
    Episode_Reward/reaching_object: 0.2819
     Episode_Reward/lifting_object: -0.0565
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 1.96s
                      Time elapsed: 00:02:47
                               ETA: 01:24:20

################################################################################
                      [1m Learning iteration 64/2000 [0m                      

                       Computation: 50912 steps/s (collection: 1.826s, learning 0.105s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.0418
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 36.6126
                       Mean reward: 1.35
               Mean episode length: 230.41
    Episode_Reward/reaching_object: 0.2772
     Episode_Reward/lifting_object: -0.0145
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 1.93s
                      Time elapsed: 00:02:49
                               ETA: 01:23:57

################################################################################
                      [1m Learning iteration 65/2000 [0m                      

                       Computation: 50714 steps/s (collection: 1.849s, learning 0.089s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.6568
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.6995
                       Mean reward: 0.73
               Mean episode length: 228.48
    Episode_Reward/reaching_object: 0.2804
     Episode_Reward/lifting_object: -0.1069
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 1.94s
                      Time elapsed: 00:02:51
                               ETA: 01:23:35

################################################################################
                      [1m Learning iteration 66/2000 [0m                      

                       Computation: 51540 steps/s (collection: 1.819s, learning 0.088s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 36.7301
                       Mean reward: 1.36
               Mean episode length: 228.49
    Episode_Reward/reaching_object: 0.2766
     Episode_Reward/lifting_object: 0.0059
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 1.91s
                      Time elapsed: 00:02:52
                               ETA: 01:23:13

################################################################################
                      [1m Learning iteration 67/2000 [0m                      

                       Computation: 50807 steps/s (collection: 1.845s, learning 0.090s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.1453
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.7923
                       Mean reward: 0.98
               Mean episode length: 226.47
    Episode_Reward/reaching_object: 0.2747
     Episode_Reward/lifting_object: -0.0152
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 1.93s
                      Time elapsed: 00:02:54
                               ETA: 01:22:52

################################################################################
                      [1m Learning iteration 68/2000 [0m                      

                       Computation: 51714 steps/s (collection: 1.814s, learning 0.087s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.1867
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 36.8215
                       Mean reward: 0.64
               Mean episode length: 228.14
    Episode_Reward/reaching_object: 0.2805
     Episode_Reward/lifting_object: -0.0559
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 1.90s
                      Time elapsed: 00:02:56
                               ETA: 01:22:31

################################################################################
                      [1m Learning iteration 69/2000 [0m                      

                       Computation: 51001 steps/s (collection: 1.826s, learning 0.102s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.0043
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 36.8638
                       Mean reward: 1.29
               Mean episode length: 227.48
    Episode_Reward/reaching_object: 0.2948
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 1.93s
                      Time elapsed: 00:02:58
                               ETA: 01:22:11

################################################################################
                      [1m Learning iteration 70/2000 [0m                      

                       Computation: 50557 steps/s (collection: 1.825s, learning 0.120s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.1113
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 36.9248
                       Mean reward: 1.12
               Mean episode length: 222.06
    Episode_Reward/reaching_object: 0.2746
     Episode_Reward/lifting_object: -0.0293
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 1.94s
                      Time elapsed: 00:03:00
                               ETA: 01:21:52

################################################################################
                      [1m Learning iteration 71/2000 [0m                      

                       Computation: 51134 steps/s (collection: 1.823s, learning 0.100s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.3225
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 36.9877
                       Mean reward: 0.70
               Mean episode length: 215.40
    Episode_Reward/reaching_object: 0.2868
     Episode_Reward/lifting_object: -0.0790
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 1.92s
                      Time elapsed: 00:03:02
                               ETA: 01:21:32

################################################################################
                      [1m Learning iteration 72/2000 [0m                      

                       Computation: 49888 steps/s (collection: 1.834s, learning 0.136s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.6090
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.0178
                       Mean reward: 1.00
               Mean episode length: 212.59
    Episode_Reward/reaching_object: 0.2829
     Episode_Reward/lifting_object: -0.0822
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 1.97s
                      Time elapsed: 00:03:04
                               ETA: 01:21:15

################################################################################
                      [1m Learning iteration 73/2000 [0m                      

                       Computation: 50723 steps/s (collection: 1.838s, learning 0.100s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.5533
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 37.0596
                       Mean reward: 1.47
               Mean episode length: 220.04
    Episode_Reward/reaching_object: 0.2983
     Episode_Reward/lifting_object: -0.1529
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 1.94s
                      Time elapsed: 00:03:06
                               ETA: 01:20:57

################################################################################
                      [1m Learning iteration 74/2000 [0m                      

                       Computation: 51813 steps/s (collection: 1.805s, learning 0.092s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.1326
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.1113
                       Mean reward: 1.12
               Mean episode length: 218.18
    Episode_Reward/reaching_object: 0.3090
     Episode_Reward/lifting_object: -0.0455
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 1.90s
                      Time elapsed: 00:03:08
                               ETA: 01:20:38

################################################################################
                      [1m Learning iteration 75/2000 [0m                      

                       Computation: 51354 steps/s (collection: 1.821s, learning 0.093s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.0345
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 37.2689
                       Mean reward: 1.52
               Mean episode length: 212.34
    Episode_Reward/reaching_object: 0.3160
     Episode_Reward/lifting_object: -0.0062
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 1.91s
                      Time elapsed: 00:03:10
                               ETA: 01:20:21

################################################################################
                      [1m Learning iteration 76/2000 [0m                      

                       Computation: 50779 steps/s (collection: 1.836s, learning 0.100s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.0459
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 37.4233
                       Mean reward: 1.35
               Mean episode length: 219.30
    Episode_Reward/reaching_object: 0.3245
     Episode_Reward/lifting_object: -0.0157
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.6667
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 1.94s
                      Time elapsed: 00:03:12
                               ETA: 01:20:04

################################################################################
                      [1m Learning iteration 77/2000 [0m                      

                       Computation: 48353 steps/s (collection: 1.928s, learning 0.106s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.2798
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.4667
                       Mean reward: 0.61
               Mean episode length: 208.40
    Episode_Reward/reaching_object: 0.3170
     Episode_Reward/lifting_object: -0.0622
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 2.03s
                      Time elapsed: 00:03:14
                               ETA: 01:19:50

################################################################################
                      [1m Learning iteration 78/2000 [0m                      

                       Computation: 50845 steps/s (collection: 1.828s, learning 0.105s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.0332
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 37.4960
                       Mean reward: 1.40
               Mean episode length: 212.04
    Episode_Reward/reaching_object: 0.3226
     Episode_Reward/lifting_object: -0.0119
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 1.93s
                      Time elapsed: 00:03:16
                               ETA: 01:19:34

################################################################################
                      [1m Learning iteration 79/2000 [0m                      

                       Computation: 49521 steps/s (collection: 1.871s, learning 0.114s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.2364
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 37.5837
                       Mean reward: 1.66
               Mean episode length: 216.83
    Episode_Reward/reaching_object: 0.3388
     Episode_Reward/lifting_object: -0.0488
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 6.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 1.99s
                      Time elapsed: 00:03:18
                               ETA: 01:19:20

################################################################################
                      [1m Learning iteration 80/2000 [0m                      

                       Computation: 49527 steps/s (collection: 1.865s, learning 0.120s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.6333
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 37.6115
                       Mean reward: 0.88
               Mean episode length: 219.67
    Episode_Reward/reaching_object: 0.3344
     Episode_Reward/lifting_object: -0.0837
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 1.98s
                      Time elapsed: 00:03:20
                               ETA: 01:19:05

################################################################################
                      [1m Learning iteration 81/2000 [0m                      

                       Computation: 50242 steps/s (collection: 1.856s, learning 0.101s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 37.6370
                       Mean reward: 1.67
               Mean episode length: 226.20
    Episode_Reward/reaching_object: 0.3538
     Episode_Reward/lifting_object: -0.0023
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 1.96s
                      Time elapsed: 00:03:22
                               ETA: 01:18:51

################################################################################
                      [1m Learning iteration 82/2000 [0m                      

                       Computation: 49341 steps/s (collection: 1.876s, learning 0.116s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.2136
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 37.6787
                       Mean reward: 1.74
               Mean episode length: 226.48
    Episode_Reward/reaching_object: 0.3479
     Episode_Reward/lifting_object: -0.0417
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 1.99s
                      Time elapsed: 00:03:24
                               ETA: 01:18:37

################################################################################
                      [1m Learning iteration 83/2000 [0m                      

                       Computation: 50698 steps/s (collection: 1.847s, learning 0.092s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0238
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 37.7001
                       Mean reward: 1.73
               Mean episode length: 234.04
    Episode_Reward/reaching_object: 0.3685
     Episode_Reward/lifting_object: -0.0083
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 1.94s
                      Time elapsed: 00:03:26
                               ETA: 01:18:23

################################################################################
                      [1m Learning iteration 84/2000 [0m                      

                       Computation: 50901 steps/s (collection: 1.823s, learning 0.108s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.1973
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.7556
                       Mean reward: 1.23
               Mean episode length: 232.74
    Episode_Reward/reaching_object: 0.3675
     Episode_Reward/lifting_object: -0.0287
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 1.93s
                      Time elapsed: 00:03:28
                               ETA: 01:18:09

################################################################################
                      [1m Learning iteration 85/2000 [0m                      

                       Computation: 51685 steps/s (collection: 1.810s, learning 0.092s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0530
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 37.7793
                       Mean reward: 1.72
               Mean episode length: 242.00
    Episode_Reward/reaching_object: 0.3715
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 1.90s
                      Time elapsed: 00:03:29
                               ETA: 01:17:54

################################################################################
                      [1m Learning iteration 86/2000 [0m                      

                       Computation: 51492 steps/s (collection: 1.794s, learning 0.115s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0246
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 37.8636
                       Mean reward: 1.65
               Mean episode length: 239.72
    Episode_Reward/reaching_object: 0.3795
     Episode_Reward/lifting_object: -0.0523
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 1.91s
                      Time elapsed: 00:03:31
                               ETA: 01:17:40

################################################################################
                      [1m Learning iteration 87/2000 [0m                      

                       Computation: 46962 steps/s (collection: 1.984s, learning 0.109s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 37.9558
                       Mean reward: 1.95
               Mean episode length: 241.46
    Episode_Reward/reaching_object: 0.3970
     Episode_Reward/lifting_object: 0.0042
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 2.09s
                      Time elapsed: 00:03:33
                               ETA: 01:17:30

################################################################################
                      [1m Learning iteration 88/2000 [0m                      

                       Computation: 51493 steps/s (collection: 1.795s, learning 0.114s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0337
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.9832
                       Mean reward: 1.96
               Mean episode length: 243.38
    Episode_Reward/reaching_object: 0.3721
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 1.91s
                      Time elapsed: 00:03:35
                               ETA: 01:17:17

################################################################################
                      [1m Learning iteration 89/2000 [0m                      

                       Computation: 49784 steps/s (collection: 1.820s, learning 0.154s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0390
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 38.0348
                       Mean reward: 1.91
               Mean episode length: 235.84
    Episode_Reward/reaching_object: 0.3851
     Episode_Reward/lifting_object: -0.0104
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 1.97s
                      Time elapsed: 00:03:37
                               ETA: 01:17:05

################################################################################
                      [1m Learning iteration 90/2000 [0m                      

                       Computation: 48230 steps/s (collection: 1.891s, learning 0.148s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.2180
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.0978
                       Mean reward: 1.70
               Mean episode length: 230.87
    Episode_Reward/reaching_object: 0.3894
     Episode_Reward/lifting_object: -0.0148
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 2.04s
                      Time elapsed: 00:03:39
                               ETA: 01:16:54

################################################################################
                      [1m Learning iteration 91/2000 [0m                      

                       Computation: 48052 steps/s (collection: 1.859s, learning 0.187s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0342
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 38.1185
                       Mean reward: 1.91
               Mean episode length: 226.07
    Episode_Reward/reaching_object: 0.3970
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 2.05s
                      Time elapsed: 00:03:41
                               ETA: 01:16:44

################################################################################
                      [1m Learning iteration 92/2000 [0m                      

                       Computation: 46673 steps/s (collection: 1.948s, learning 0.159s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0213
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 38.1763
                       Mean reward: 2.22
               Mean episode length: 228.19
    Episode_Reward/reaching_object: 0.4382
     Episode_Reward/lifting_object: -0.0150
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 2.11s
                      Time elapsed: 00:03:44
                               ETA: 01:16:35

################################################################################
                      [1m Learning iteration 93/2000 [0m                      

                       Computation: 48211 steps/s (collection: 1.908s, learning 0.131s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.0425
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 38.2373
                       Mean reward: 2.29
               Mean episode length: 239.89
    Episode_Reward/reaching_object: 0.4652
     Episode_Reward/lifting_object: -0.0023
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 2.04s
                      Time elapsed: 00:03:46
                               ETA: 01:16:25

################################################################################
                      [1m Learning iteration 94/2000 [0m                      

                       Computation: 49861 steps/s (collection: 1.873s, learning 0.099s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.0589
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 38.2956
                       Mean reward: 2.50
               Mean episode length: 237.42
    Episode_Reward/reaching_object: 0.4981
     Episode_Reward/lifting_object: -0.0032
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 1.97s
                      Time elapsed: 00:03:48
                               ETA: 01:16:14

################################################################################
                      [1m Learning iteration 95/2000 [0m                      

                       Computation: 48282 steps/s (collection: 1.941s, learning 0.095s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.4670
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 38.3664
                       Mean reward: 2.25
               Mean episode length: 226.08
    Episode_Reward/reaching_object: 0.5091
     Episode_Reward/lifting_object: -0.0409
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 2.04s
                      Time elapsed: 00:03:50
                               ETA: 01:16:05

################################################################################
                      [1m Learning iteration 96/2000 [0m                      

                       Computation: 50375 steps/s (collection: 1.861s, learning 0.091s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.2018
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 38.3965
                       Mean reward: 2.54
               Mean episode length: 236.66
    Episode_Reward/reaching_object: 0.5564
     Episode_Reward/lifting_object: -0.0156
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 1.95s
                      Time elapsed: 00:03:52
                               ETA: 01:15:54

################################################################################
                      [1m Learning iteration 97/2000 [0m                      

                       Computation: 49478 steps/s (collection: 1.885s, learning 0.102s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.1956
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.4370
                       Mean reward: 2.46
               Mean episode length: 231.35
    Episode_Reward/reaching_object: 0.5745
     Episode_Reward/lifting_object: -0.0523
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 1.99s
                      Time elapsed: 00:03:53
                               ETA: 01:15:43

################################################################################
                      [1m Learning iteration 98/2000 [0m                      

                       Computation: 50811 steps/s (collection: 1.839s, learning 0.096s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.4668
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.4751
                       Mean reward: 2.95
               Mean episode length: 244.03
    Episode_Reward/reaching_object: 0.5833
     Episode_Reward/lifting_object: -0.0204
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 1.93s
                      Time elapsed: 00:03:55
                               ETA: 01:15:32

################################################################################
                      [1m Learning iteration 99/2000 [0m                      

                       Computation: 49889 steps/s (collection: 1.880s, learning 0.090s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.3474
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.4972
                       Mean reward: 3.31
               Mean episode length: 239.17
    Episode_Reward/reaching_object: 0.6323
     Episode_Reward/lifting_object: 0.0250
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 1.97s
                      Time elapsed: 00:03:57
                               ETA: 01:15:22

################################################################################
                     [1m Learning iteration 100/2000 [0m                      

                       Computation: 48952 steps/s (collection: 1.878s, learning 0.131s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0773
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 38.5147
                       Mean reward: 2.50
               Mean episode length: 233.03
    Episode_Reward/reaching_object: 0.6304
     Episode_Reward/lifting_object: -0.0135
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 2.01s
                      Time elapsed: 00:03:59
                               ETA: 01:15:13

################################################################################
                     [1m Learning iteration 101/2000 [0m                      

                       Computation: 47712 steps/s (collection: 1.942s, learning 0.118s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.3112
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.5488
                       Mean reward: 3.38
               Mean episode length: 239.14
    Episode_Reward/reaching_object: 0.6614
     Episode_Reward/lifting_object: -0.0334
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 2.06s
                      Time elapsed: 00:04:01
                               ETA: 01:15:04

################################################################################
                     [1m Learning iteration 102/2000 [0m                      

                       Computation: 49796 steps/s (collection: 1.875s, learning 0.099s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.6362
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 38.5766
                       Mean reward: 2.52
               Mean episode length: 228.38
    Episode_Reward/reaching_object: 0.6322
     Episode_Reward/lifting_object: -0.0881
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 1.97s
                      Time elapsed: 00:04:03
                               ETA: 01:14:55

################################################################################
                     [1m Learning iteration 103/2000 [0m                      

                       Computation: 49497 steps/s (collection: 1.879s, learning 0.107s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.9913
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.5921
                       Mean reward: 2.62
               Mean episode length: 231.36
    Episode_Reward/reaching_object: 0.6570
     Episode_Reward/lifting_object: -0.0890
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 1.99s
                      Time elapsed: 00:04:05
                               ETA: 01:14:45

################################################################################
                     [1m Learning iteration 104/2000 [0m                      

                       Computation: 49456 steps/s (collection: 1.840s, learning 0.148s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.1168
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.6139
                       Mean reward: 2.88
               Mean episode length: 246.27
    Episode_Reward/reaching_object: 0.7085
     Episode_Reward/lifting_object: -0.0239
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0128
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 1.99s
                      Time elapsed: 00:04:07
                               ETA: 01:14:36

################################################################################
                     [1m Learning iteration 105/2000 [0m                      

                       Computation: 47828 steps/s (collection: 1.894s, learning 0.161s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.1204
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 38.6637
                       Mean reward: 2.67
               Mean episode length: 244.65
    Episode_Reward/reaching_object: 0.7040
     Episode_Reward/lifting_object: -0.0274
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0131
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 2.06s
                      Time elapsed: 00:04:09
                               ETA: 01:14:28

################################################################################
                     [1m Learning iteration 106/2000 [0m                      

                       Computation: 51239 steps/s (collection: 1.799s, learning 0.120s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0049
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 38.6725
                       Mean reward: 3.59
               Mean episode length: 241.15
    Episode_Reward/reaching_object: 0.7164
     Episode_Reward/lifting_object: 0.0071
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0131
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 1.92s
                      Time elapsed: 00:04:11
                               ETA: 01:14:18

################################################################################
                     [1m Learning iteration 107/2000 [0m                      

                       Computation: 49298 steps/s (collection: 1.899s, learning 0.095s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.0515
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.6860
                       Mean reward: 3.41
               Mean episode length: 235.43
    Episode_Reward/reaching_object: 0.6859
     Episode_Reward/lifting_object: -0.0221
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0131
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 1.99s
                      Time elapsed: 00:04:13
                               ETA: 01:14:10

################################################################################
                     [1m Learning iteration 108/2000 [0m                      

                       Computation: 49921 steps/s (collection: 1.877s, learning 0.092s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.0361
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 38.7169
                       Mean reward: 3.25
               Mean episode length: 235.16
    Episode_Reward/reaching_object: 0.6789
     Episode_Reward/lifting_object: -0.0561
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 1.97s
                      Time elapsed: 00:04:15
                               ETA: 01:14:01

################################################################################
                     [1m Learning iteration 109/2000 [0m                      

                       Computation: 48763 steps/s (collection: 1.891s, learning 0.125s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.2750
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.7628
                       Mean reward: 3.21
               Mean episode length: 228.38
    Episode_Reward/reaching_object: 0.6808
     Episode_Reward/lifting_object: -0.0393
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0128
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 2.02s
                      Time elapsed: 00:04:17
                               ETA: 01:13:53

################################################################################
                     [1m Learning iteration 110/2000 [0m                      

                       Computation: 45774 steps/s (collection: 2.042s, learning 0.105s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.3064
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.7797
                       Mean reward: 3.41
               Mean episode length: 236.42
    Episode_Reward/reaching_object: 0.6625
     Episode_Reward/lifting_object: -0.0040
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0128
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 2.15s
                      Time elapsed: 00:04:20
                               ETA: 01:13:47

################################################################################
                     [1m Learning iteration 111/2000 [0m                      

                       Computation: 49566 steps/s (collection: 1.893s, learning 0.090s)
             Mean action noise std: 1.41
          Mean value_function loss: 2.0208
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.8205
                       Mean reward: 2.19
               Mean episode length: 236.14
    Episode_Reward/reaching_object: 0.7011
     Episode_Reward/lifting_object: -0.0912
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 1.98s
                      Time elapsed: 00:04:22
                               ETA: 01:13:38

################################################################################
                     [1m Learning iteration 112/2000 [0m                      

                       Computation: 47531 steps/s (collection: 1.939s, learning 0.129s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.1291
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.8389
                       Mean reward: 3.60
               Mean episode length: 235.68
    Episode_Reward/reaching_object: 0.7022
     Episode_Reward/lifting_object: -0.0162
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0133
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 2.07s
                      Time elapsed: 00:04:24
                               ETA: 01:13:32

################################################################################
                     [1m Learning iteration 113/2000 [0m                      

                       Computation: 49639 steps/s (collection: 1.887s, learning 0.093s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0304
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 38.8855
                       Mean reward: 3.54
               Mean episode length: 231.86
    Episode_Reward/reaching_object: 0.7126
     Episode_Reward/lifting_object: 0.0227
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 1.98s
                      Time elapsed: 00:04:26
                               ETA: 01:13:23

################################################################################
                     [1m Learning iteration 114/2000 [0m                      

                       Computation: 49334 steps/s (collection: 1.877s, learning 0.116s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0889
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.9542
                       Mean reward: 2.90
               Mean episode length: 232.18
    Episode_Reward/reaching_object: 0.7149
     Episode_Reward/lifting_object: -0.0267
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0133
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 1.99s
                      Time elapsed: 00:04:28
                               ETA: 01:13:15

################################################################################
                     [1m Learning iteration 115/2000 [0m                      

                       Computation: 49898 steps/s (collection: 1.847s, learning 0.123s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.0649
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 39.0268
                       Mean reward: 3.91
               Mean episode length: 244.02
    Episode_Reward/reaching_object: 0.7429
     Episode_Reward/lifting_object: -0.0181
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 1.97s
                      Time elapsed: 00:04:30
                               ETA: 01:13:07

################################################################################
                     [1m Learning iteration 116/2000 [0m                      

                       Computation: 49559 steps/s (collection: 1.872s, learning 0.112s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.3840
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 39.0840
                       Mean reward: 3.09
               Mean episode length: 230.84
    Episode_Reward/reaching_object: 0.7421
     Episode_Reward/lifting_object: -0.1108
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 1.98s
                      Time elapsed: 00:04:31
                               ETA: 01:12:59

################################################################################
                     [1m Learning iteration 117/2000 [0m                      

                       Computation: 49538 steps/s (collection: 1.862s, learning 0.122s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.0404
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 39.0962
                       Mean reward: 3.64
               Mean episode length: 235.10
    Episode_Reward/reaching_object: 0.7277
     Episode_Reward/lifting_object: 0.0116
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 1.98s
                      Time elapsed: 00:04:33
                               ETA: 01:12:52

################################################################################
                     [1m Learning iteration 118/2000 [0m                      

                       Computation: 50042 steps/s (collection: 1.862s, learning 0.103s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.0798
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 39.1296
                       Mean reward: 3.65
               Mean episode length: 233.46
    Episode_Reward/reaching_object: 0.7262
     Episode_Reward/lifting_object: 0.0049
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 1.96s
                      Time elapsed: 00:04:35
                               ETA: 01:12:44

################################################################################
                     [1m Learning iteration 119/2000 [0m                      

                       Computation: 50045 steps/s (collection: 1.863s, learning 0.102s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.9658
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.1798
                       Mean reward: 3.66
               Mean episode length: 235.70
    Episode_Reward/reaching_object: 0.7664
     Episode_Reward/lifting_object: -0.0022
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 1.96s
                      Time elapsed: 00:04:37
                               ETA: 01:12:36

################################################################################
                     [1m Learning iteration 120/2000 [0m                      

                       Computation: 46413 steps/s (collection: 2.017s, learning 0.101s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.0849
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 39.1941
                       Mean reward: 3.95
               Mean episode length: 242.85
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 0.0035
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 2.12s
                      Time elapsed: 00:04:40
                               ETA: 01:12:30

################################################################################
                     [1m Learning iteration 121/2000 [0m                      

                       Computation: 49106 steps/s (collection: 1.898s, learning 0.104s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.1490
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 39.2355
                       Mean reward: 3.61
               Mean episode length: 241.78
    Episode_Reward/reaching_object: 0.7852
     Episode_Reward/lifting_object: -0.0455
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 2.00s
                      Time elapsed: 00:04:42
                               ETA: 01:12:23

################################################################################
                     [1m Learning iteration 122/2000 [0m                      

                       Computation: 49383 steps/s (collection: 1.890s, learning 0.101s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.0356
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 39.2940
                       Mean reward: 3.92
               Mean episode length: 244.32
    Episode_Reward/reaching_object: 0.7717
     Episode_Reward/lifting_object: 0.0318
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 1.99s
                      Time elapsed: 00:04:44
                               ETA: 01:12:16

################################################################################
                     [1m Learning iteration 123/2000 [0m                      

                       Computation: 50140 steps/s (collection: 1.873s, learning 0.088s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.1085
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 39.3571
                       Mean reward: 3.85
               Mean episode length: 243.00
    Episode_Reward/reaching_object: 0.7660
     Episode_Reward/lifting_object: -0.0246
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 1.96s
                      Time elapsed: 00:04:45
                               ETA: 01:12:08

################################################################################
                     [1m Learning iteration 124/2000 [0m                      

                       Computation: 49160 steps/s (collection: 1.869s, learning 0.131s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.1371
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 39.4042
                       Mean reward: 3.78
               Mean episode length: 228.74
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 0.0117
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 2.00s
                      Time elapsed: 00:04:47
                               ETA: 01:12:02

################################################################################
                     [1m Learning iteration 125/2000 [0m                      

                       Computation: 47840 steps/s (collection: 1.908s, learning 0.147s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.4567
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 39.4360
                       Mean reward: 4.43
               Mean episode length: 239.89
    Episode_Reward/reaching_object: 0.7834
     Episode_Reward/lifting_object: 0.0553
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 2.05s
                      Time elapsed: 00:04:50
                               ETA: 01:11:56

################################################################################
                     [1m Learning iteration 126/2000 [0m                      

                       Computation: 48036 steps/s (collection: 1.937s, learning 0.109s)
             Mean action noise std: 1.46
          Mean value_function loss: 1.0639
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.4474
                       Mean reward: 4.06
               Mean episode length: 232.45
    Episode_Reward/reaching_object: 0.7722
     Episode_Reward/lifting_object: 0.0317
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 2.05s
                      Time elapsed: 00:04:52
                               ETA: 01:11:49

################################################################################
                     [1m Learning iteration 127/2000 [0m                      

                       Computation: 46364 steps/s (collection: 1.951s, learning 0.170s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.4371
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.4601
                       Mean reward: 3.37
               Mean episode length: 230.96
    Episode_Reward/reaching_object: 0.7551
     Episode_Reward/lifting_object: -0.0308
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 2.12s
                      Time elapsed: 00:04:54
                               ETA: 01:11:45

################################################################################
                     [1m Learning iteration 128/2000 [0m                      

                       Computation: 47823 steps/s (collection: 1.966s, learning 0.090s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.3465
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 39.4780
                       Mean reward: 4.34
               Mean episode length: 233.86
    Episode_Reward/reaching_object: 0.7968
     Episode_Reward/lifting_object: -0.0021
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 2.06s
                      Time elapsed: 00:04:56
                               ETA: 01:11:39

################################################################################
                     [1m Learning iteration 129/2000 [0m                      

                       Computation: 49417 steps/s (collection: 1.903s, learning 0.087s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.0873
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 39.5241
                       Mean reward: 3.83
               Mean episode length: 244.93
    Episode_Reward/reaching_object: 0.8104
     Episode_Reward/lifting_object: -0.0269
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 1.99s
                      Time elapsed: 00:04:58
                               ETA: 01:11:32

################################################################################
                     [1m Learning iteration 130/2000 [0m                      

                       Computation: 49851 steps/s (collection: 1.884s, learning 0.088s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.0411
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.6006
                       Mean reward: 4.15
               Mean episode length: 244.47
    Episode_Reward/reaching_object: 0.8132
     Episode_Reward/lifting_object: -0.0209
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 1.97s
                      Time elapsed: 00:05:00
                               ETA: 01:11:25

################################################################################
                     [1m Learning iteration 131/2000 [0m                      

                       Computation: 48872 steps/s (collection: 1.911s, learning 0.101s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.2246
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 39.6747
                       Mean reward: 3.71
               Mean episode length: 236.34
    Episode_Reward/reaching_object: 0.8080
     Episode_Reward/lifting_object: -0.0297
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 2.01s
                      Time elapsed: 00:05:02
                               ETA: 01:11:19

################################################################################
                     [1m Learning iteration 132/2000 [0m                      

                       Computation: 48375 steps/s (collection: 1.921s, learning 0.112s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.3139
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 39.7152
                       Mean reward: 3.86
               Mean episode length: 239.37
    Episode_Reward/reaching_object: 0.8169
     Episode_Reward/lifting_object: 0.0085
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 2.03s
                      Time elapsed: 00:05:04
                               ETA: 01:11:13

################################################################################
                     [1m Learning iteration 133/2000 [0m                      

                       Computation: 48401 steps/s (collection: 1.942s, learning 0.089s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.0872
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.7283
                       Mean reward: 3.81
               Mean episode length: 242.44
    Episode_Reward/reaching_object: 0.8317
     Episode_Reward/lifting_object: -0.0034
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 2.03s
                      Time elapsed: 00:05:06
                               ETA: 01:11:07

################################################################################
                     [1m Learning iteration 134/2000 [0m                      

                       Computation: 49065 steps/s (collection: 1.910s, learning 0.094s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.1547
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 39.7602
                       Mean reward: 3.75
               Mean episode length: 237.22
    Episode_Reward/reaching_object: 0.8096
     Episode_Reward/lifting_object: -0.0171
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 2.00s
                      Time elapsed: 00:05:08
                               ETA: 01:11:01

################################################################################
                     [1m Learning iteration 135/2000 [0m                      

                       Computation: 47182 steps/s (collection: 1.983s, learning 0.100s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.3234
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 39.8052
                       Mean reward: 4.05
               Mean episode length: 242.18
    Episode_Reward/reaching_object: 0.8202
     Episode_Reward/lifting_object: -0.0242
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 2.08s
                      Time elapsed: 00:05:10
                               ETA: 01:10:56

################################################################################
                     [1m Learning iteration 136/2000 [0m                      

                       Computation: 48378 steps/s (collection: 1.934s, learning 0.098s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.1857
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 39.8473
                       Mean reward: 4.14
               Mean episode length: 241.60
    Episode_Reward/reaching_object: 0.8142
     Episode_Reward/lifting_object: 0.0426
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 2.03s
                      Time elapsed: 00:05:12
                               ETA: 01:10:50

################################################################################
                     [1m Learning iteration 137/2000 [0m                      

                       Computation: 48696 steps/s (collection: 1.917s, learning 0.102s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.0389
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 39.9164
                       Mean reward: 4.07
               Mean episode length: 241.50
    Episode_Reward/reaching_object: 0.7925
     Episode_Reward/lifting_object: 0.0070
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 2.02s
                      Time elapsed: 00:05:14
                               ETA: 01:10:44

################################################################################
                     [1m Learning iteration 138/2000 [0m                      

                       Computation: 48523 steps/s (collection: 1.926s, learning 0.100s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.3849
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.9534
                       Mean reward: 3.49
               Mean episode length: 244.46
    Episode_Reward/reaching_object: 0.8082
     Episode_Reward/lifting_object: 0.0223
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 2.03s
                      Time elapsed: 00:05:16
                               ETA: 01:10:39

################################################################################
                     [1m Learning iteration 139/2000 [0m                      

                       Computation: 49639 steps/s (collection: 1.887s, learning 0.094s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.2640
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 39.9696
                       Mean reward: 3.69
               Mean episode length: 238.29
    Episode_Reward/reaching_object: 0.8035
     Episode_Reward/lifting_object: 0.0254
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 1.98s
                      Time elapsed: 00:05:18
                               ETA: 01:10:32

################################################################################
                     [1m Learning iteration 140/2000 [0m                      

                       Computation: 49913 steps/s (collection: 1.885s, learning 0.084s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.7619
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 39.9994
                       Mean reward: 2.83
               Mean episode length: 239.82
    Episode_Reward/reaching_object: 0.8223
     Episode_Reward/lifting_object: -0.0649
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 1.97s
                      Time elapsed: 00:05:20
                               ETA: 01:10:26

################################################################################
                     [1m Learning iteration 141/2000 [0m                      

                       Computation: 50077 steps/s (collection: 1.877s, learning 0.087s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.3199
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 40.0092
                       Mean reward: 4.03
               Mean episode length: 242.03
    Episode_Reward/reaching_object: 0.8332
     Episode_Reward/lifting_object: 0.0072
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 1.96s
                      Time elapsed: 00:05:22
                               ETA: 01:10:20

################################################################################
                     [1m Learning iteration 142/2000 [0m                      

                       Computation: 49308 steps/s (collection: 1.897s, learning 0.097s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.2695
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 40.0298
                       Mean reward: 4.29
               Mean episode length: 243.32
    Episode_Reward/reaching_object: 0.8450
     Episode_Reward/lifting_object: 0.0026
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 1.99s
                      Time elapsed: 00:05:24
                               ETA: 01:10:14

################################################################################
                     [1m Learning iteration 143/2000 [0m                      

                       Computation: 49661 steps/s (collection: 1.890s, learning 0.090s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.3840
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 40.0690
                       Mean reward: 4.35
               Mean episode length: 245.95
    Episode_Reward/reaching_object: 0.8769
     Episode_Reward/lifting_object: -0.0054
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 1.98s
                      Time elapsed: 00:05:26
                               ETA: 01:10:08

################################################################################
                     [1m Learning iteration 144/2000 [0m                      

                       Computation: 50396 steps/s (collection: 1.861s, learning 0.090s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.2586
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.0887
                       Mean reward: 4.25
               Mean episode length: 246.51
    Episode_Reward/reaching_object: 0.8523
     Episode_Reward/lifting_object: 0.0318
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 1.95s
                      Time elapsed: 00:05:28
                               ETA: 01:10:02

################################################################################
                     [1m Learning iteration 145/2000 [0m                      

                       Computation: 49358 steps/s (collection: 1.896s, learning 0.096s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.2449
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.1018
                       Mean reward: 4.05
               Mean episode length: 246.08
    Episode_Reward/reaching_object: 0.8578
     Episode_Reward/lifting_object: -0.0106
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 1.99s
                      Time elapsed: 00:05:30
                               ETA: 01:09:56

################################################################################
                     [1m Learning iteration 146/2000 [0m                      

                       Computation: 49670 steps/s (collection: 1.858s, learning 0.121s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.0934
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 40.1425
                       Mean reward: 3.78
               Mean episode length: 236.23
    Episode_Reward/reaching_object: 0.8114
     Episode_Reward/lifting_object: 0.0406
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 1.98s
                      Time elapsed: 00:05:32
                               ETA: 01:09:50

################################################################################
                     [1m Learning iteration 147/2000 [0m                      

                       Computation: 50104 steps/s (collection: 1.858s, learning 0.104s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.1451
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 40.2115
                       Mean reward: 4.34
               Mean episode length: 238.83
    Episode_Reward/reaching_object: 0.8243
     Episode_Reward/lifting_object: 0.0276
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 1.96s
                      Time elapsed: 00:05:34
                               ETA: 01:09:44

################################################################################
                     [1m Learning iteration 148/2000 [0m                      

                       Computation: 50218 steps/s (collection: 1.850s, learning 0.108s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.0901
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.2633
                       Mean reward: 3.64
               Mean episode length: 239.75
    Episode_Reward/reaching_object: 0.8162
     Episode_Reward/lifting_object: -0.0001
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 1.96s
                      Time elapsed: 00:05:36
                               ETA: 01:09:38

################################################################################
                     [1m Learning iteration 149/2000 [0m                      

                       Computation: 49831 steps/s (collection: 1.883s, learning 0.090s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.7575
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 40.2986
                       Mean reward: 4.47
               Mean episode length: 246.54
    Episode_Reward/reaching_object: 0.8162
     Episode_Reward/lifting_object: 0.0673
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 1.97s
                      Time elapsed: 00:05:38
                               ETA: 01:09:32

################################################################################
                     [1m Learning iteration 150/2000 [0m                      

                       Computation: 49286 steps/s (collection: 1.896s, learning 0.099s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.5087
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 40.3091
                       Mean reward: 3.67
               Mean episode length: 239.60
    Episode_Reward/reaching_object: 0.8378
     Episode_Reward/lifting_object: 0.0186
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 1.99s
                      Time elapsed: 00:05:40
                               ETA: 01:09:27

################################################################################
                     [1m Learning iteration 151/2000 [0m                      

                       Computation: 47844 steps/s (collection: 1.948s, learning 0.107s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.5219
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.3352
                       Mean reward: 4.42
               Mean episode length: 238.37
    Episode_Reward/reaching_object: 0.8358
     Episode_Reward/lifting_object: 0.0037
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 2.05s
                      Time elapsed: 00:05:42
                               ETA: 01:09:22

################################################################################
                     [1m Learning iteration 152/2000 [0m                      

                       Computation: 49473 steps/s (collection: 1.885s, learning 0.102s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.1224
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.3510
                       Mean reward: 4.09
               Mean episode length: 237.99
    Episode_Reward/reaching_object: 0.8412
     Episode_Reward/lifting_object: 0.0296
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 1.99s
                      Time elapsed: 00:05:44
                               ETA: 01:09:17

################################################################################
                     [1m Learning iteration 153/2000 [0m                      

                       Computation: 50485 steps/s (collection: 1.850s, learning 0.097s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.3393
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 40.3936
                       Mean reward: 4.44
               Mean episode length: 246.84
    Episode_Reward/reaching_object: 0.8276
     Episode_Reward/lifting_object: 0.0310
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 1.95s
                      Time elapsed: 00:05:46
                               ETA: 01:09:11

################################################################################
                     [1m Learning iteration 154/2000 [0m                      

                       Computation: 48139 steps/s (collection: 1.914s, learning 0.128s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.4282
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 40.4363
                       Mean reward: 4.37
               Mean episode length: 246.50
    Episode_Reward/reaching_object: 0.8311
     Episode_Reward/lifting_object: 0.0014
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 2.04s
                      Time elapsed: 00:05:48
                               ETA: 01:09:06

################################################################################
                     [1m Learning iteration 155/2000 [0m                      

                       Computation: 48764 steps/s (collection: 1.924s, learning 0.092s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.3369
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 40.4641
                       Mean reward: 3.78
               Mean episode length: 238.12
    Episode_Reward/reaching_object: 0.8200
     Episode_Reward/lifting_object: 0.0164
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 2.02s
                      Time elapsed: 00:05:50
                               ETA: 01:09:01

################################################################################
                     [1m Learning iteration 156/2000 [0m                      

                       Computation: 49472 steps/s (collection: 1.900s, learning 0.087s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.5014
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.4876
                       Mean reward: 4.52
               Mean episode length: 245.24
    Episode_Reward/reaching_object: 0.8296
     Episode_Reward/lifting_object: 0.0269
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 1.99s
                      Time elapsed: 00:05:52
                               ETA: 01:08:56

################################################################################
                     [1m Learning iteration 157/2000 [0m                      

                       Computation: 48700 steps/s (collection: 1.928s, learning 0.091s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.1737
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 40.5097
                       Mean reward: 4.71
               Mean episode length: 246.48
    Episode_Reward/reaching_object: 0.8119
     Episode_Reward/lifting_object: 0.0109
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 2.02s
                      Time elapsed: 00:05:54
                               ETA: 01:08:51

################################################################################
                     [1m Learning iteration 158/2000 [0m                      

                       Computation: 49360 steps/s (collection: 1.895s, learning 0.096s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.1371
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 40.5723
                       Mean reward: 4.44
               Mean episode length: 240.19
    Episode_Reward/reaching_object: 0.8168
     Episode_Reward/lifting_object: 0.0559
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 1.99s
                      Time elapsed: 00:05:56
                               ETA: 01:08:46

################################################################################
                     [1m Learning iteration 159/2000 [0m                      

                       Computation: 50258 steps/s (collection: 1.865s, learning 0.091s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.6805
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 40.6218
                       Mean reward: 3.55
               Mean episode length: 241.02
    Episode_Reward/reaching_object: 0.8176
     Episode_Reward/lifting_object: 0.0467
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 1.96s
                      Time elapsed: 00:05:58
                               ETA: 01:08:40

################################################################################
                     [1m Learning iteration 160/2000 [0m                      

                       Computation: 49356 steps/s (collection: 1.896s, learning 0.096s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.5412
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.6337
                       Mean reward: 4.85
               Mean episode length: 244.23
    Episode_Reward/reaching_object: 0.8434
     Episode_Reward/lifting_object: 0.0584
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 1.99s
                      Time elapsed: 00:06:00
                               ETA: 01:08:35

################################################################################
                     [1m Learning iteration 161/2000 [0m                      

                       Computation: 49783 steps/s (collection: 1.876s, learning 0.099s)
             Mean action noise std: 1.54
          Mean value_function loss: 1.1905
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.6672
                       Mean reward: 4.32
               Mean episode length: 241.23
    Episode_Reward/reaching_object: 0.8064
     Episode_Reward/lifting_object: 0.0687
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 1.97s
                      Time elapsed: 00:06:02
                               ETA: 01:08:30

################################################################################
                     [1m Learning iteration 162/2000 [0m                      

                       Computation: 47753 steps/s (collection: 1.946s, learning 0.112s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.2038
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 40.6925
                       Mean reward: 4.41
               Mean episode length: 243.59
    Episode_Reward/reaching_object: 0.8164
     Episode_Reward/lifting_object: 0.1156
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 2.06s
                      Time elapsed: 00:06:04
                               ETA: 01:08:26

################################################################################
                     [1m Learning iteration 163/2000 [0m                      

                       Computation: 50044 steps/s (collection: 1.876s, learning 0.089s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.1353
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 40.7382
                       Mean reward: 4.30
               Mean episode length: 244.30
    Episode_Reward/reaching_object: 0.8325
     Episode_Reward/lifting_object: -0.0193
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 1.96s
                      Time elapsed: 00:06:06
                               ETA: 01:08:21

################################################################################
                     [1m Learning iteration 164/2000 [0m                      

                       Computation: 49661 steps/s (collection: 1.887s, learning 0.093s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.2463
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 40.7943
                       Mean reward: 4.28
               Mean episode length: 242.88
    Episode_Reward/reaching_object: 0.8246
     Episode_Reward/lifting_object: 0.0683
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 1.98s
                      Time elapsed: 00:06:08
                               ETA: 01:08:16

################################################################################
                     [1m Learning iteration 165/2000 [0m                      

                       Computation: 46410 steps/s (collection: 2.028s, learning 0.090s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.2825
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 40.8405
                       Mean reward: 3.47
               Mean episode length: 240.74
    Episode_Reward/reaching_object: 0.7910
     Episode_Reward/lifting_object: -0.0214
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 2.12s
                      Time elapsed: 00:06:10
                               ETA: 01:08:12

################################################################################
                     [1m Learning iteration 166/2000 [0m                      

                       Computation: 48582 steps/s (collection: 1.937s, learning 0.086s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.1919
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 40.8655
                       Mean reward: 4.69
               Mean episode length: 239.86
    Episode_Reward/reaching_object: 0.8347
     Episode_Reward/lifting_object: 0.0754
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 2.02s
                      Time elapsed: 00:06:12
                               ETA: 01:08:08

################################################################################
                     [1m Learning iteration 167/2000 [0m                      

                       Computation: 44736 steps/s (collection: 1.996s, learning 0.202s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.2485
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 40.9070
                       Mean reward: 4.36
               Mean episode length: 236.88
    Episode_Reward/reaching_object: 0.7969
     Episode_Reward/lifting_object: 0.1042
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 2.20s
                      Time elapsed: 00:06:14
                               ETA: 01:08:05

################################################################################
                     [1m Learning iteration 168/2000 [0m                      

                       Computation: 48185 steps/s (collection: 1.916s, learning 0.125s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.1920
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 40.9431
                       Mean reward: 4.05
               Mean episode length: 245.29
    Episode_Reward/reaching_object: 0.8143
     Episode_Reward/lifting_object: 0.1023
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 2.04s
                      Time elapsed: 00:06:16
                               ETA: 01:08:01

################################################################################
                     [1m Learning iteration 169/2000 [0m                      

                       Computation: 48437 steps/s (collection: 1.890s, learning 0.140s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.7751
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 40.9894
                       Mean reward: 4.15
               Mean episode length: 240.39
    Episode_Reward/reaching_object: 0.7921
     Episode_Reward/lifting_object: 0.0775
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 2.03s
                      Time elapsed: 00:06:18
                               ETA: 01:07:56

################################################################################
                     [1m Learning iteration 170/2000 [0m                      

                       Computation: 47714 steps/s (collection: 1.941s, learning 0.119s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.8031
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 41.0213
                       Mean reward: 3.58
               Mean episode length: 242.33
    Episode_Reward/reaching_object: 0.8045
     Episode_Reward/lifting_object: -0.0262
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 2.06s
                      Time elapsed: 00:06:20
                               ETA: 01:07:52

################################################################################
                     [1m Learning iteration 171/2000 [0m                      

                       Computation: 48836 steps/s (collection: 1.893s, learning 0.120s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.1331
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 41.0508
                       Mean reward: 3.91
               Mean episode length: 243.21
    Episode_Reward/reaching_object: 0.7881
     Episode_Reward/lifting_object: 0.0728
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 2.01s
                      Time elapsed: 00:06:22
                               ETA: 01:07:48

################################################################################
                     [1m Learning iteration 172/2000 [0m                      

                       Computation: 46931 steps/s (collection: 1.986s, learning 0.109s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.2058
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 41.1010
                       Mean reward: 4.45
               Mean episode length: 242.67
    Episode_Reward/reaching_object: 0.8219
     Episode_Reward/lifting_object: 0.0945
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 2.09s
                      Time elapsed: 00:06:24
                               ETA: 01:07:44

################################################################################
                     [1m Learning iteration 173/2000 [0m                      

                       Computation: 48346 steps/s (collection: 1.918s, learning 0.116s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.4292
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 41.1441
                       Mean reward: 4.03
               Mean episode length: 242.79
    Episode_Reward/reaching_object: 0.8014
     Episode_Reward/lifting_object: 0.1044
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 2.03s
                      Time elapsed: 00:06:26
                               ETA: 01:07:40

################################################################################
                     [1m Learning iteration 174/2000 [0m                      

                       Computation: 47212 steps/s (collection: 1.919s, learning 0.164s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.3923
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 41.1849
                       Mean reward: 4.51
               Mean episode length: 244.17
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 0.0959
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 2.08s
                      Time elapsed: 00:06:28
                               ETA: 01:07:36

################################################################################
                     [1m Learning iteration 175/2000 [0m                      

                       Computation: 48699 steps/s (collection: 1.927s, learning 0.092s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.2801
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 41.2354
                       Mean reward: 4.50
               Mean episode length: 236.68
    Episode_Reward/reaching_object: 0.8011
     Episode_Reward/lifting_object: 0.1410
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 2.02s
                      Time elapsed: 00:06:30
                               ETA: 01:07:32

################################################################################
                     [1m Learning iteration 176/2000 [0m                      

                       Computation: 48983 steps/s (collection: 1.911s, learning 0.096s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.2465
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 41.2901
                       Mean reward: 4.06
               Mean episode length: 236.50
    Episode_Reward/reaching_object: 0.8070
     Episode_Reward/lifting_object: 0.0916
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 2.01s
                      Time elapsed: 00:06:32
                               ETA: 01:07:28

################################################################################
                     [1m Learning iteration 177/2000 [0m                      

                       Computation: 48941 steps/s (collection: 1.914s, learning 0.095s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.2044
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 41.3346
                       Mean reward: 3.72
               Mean episode length: 242.98
    Episode_Reward/reaching_object: 0.7583
     Episode_Reward/lifting_object: 0.0794
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 2.01s
                      Time elapsed: 00:06:34
                               ETA: 01:07:23

################################################################################
                     [1m Learning iteration 178/2000 [0m                      

                       Computation: 49797 steps/s (collection: 1.877s, learning 0.098s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.4564
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.3821
                       Mean reward: 4.56
               Mean episode length: 240.85
    Episode_Reward/reaching_object: 0.7639
     Episode_Reward/lifting_object: 0.0318
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 1.97s
                      Time elapsed: 00:06:36
                               ETA: 01:07:19

################################################################################
                     [1m Learning iteration 179/2000 [0m                      

                       Computation: 48499 steps/s (collection: 1.902s, learning 0.125s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.4076
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 41.3982
                       Mean reward: 3.91
               Mean episode length: 237.99
    Episode_Reward/reaching_object: 0.7555
     Episode_Reward/lifting_object: 0.0940
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 2.03s
                      Time elapsed: 00:06:38
                               ETA: 01:07:15

################################################################################
                     [1m Learning iteration 180/2000 [0m                      

                       Computation: 47971 steps/s (collection: 1.957s, learning 0.093s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.3299
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 41.4393
                       Mean reward: 4.04
               Mean episode length: 236.60
    Episode_Reward/reaching_object: 0.7328
     Episode_Reward/lifting_object: 0.0508
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 2.05s
                      Time elapsed: 00:06:40
                               ETA: 01:07:11

################################################################################
                     [1m Learning iteration 181/2000 [0m                      

                       Computation: 47513 steps/s (collection: 1.975s, learning 0.094s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.2936
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 41.4976
                       Mean reward: 4.36
               Mean episode length: 238.29
    Episode_Reward/reaching_object: 0.7405
     Episode_Reward/lifting_object: 0.1210
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 2.07s
                      Time elapsed: 00:06:42
                               ETA: 01:07:07

################################################################################
                     [1m Learning iteration 182/2000 [0m                      

                       Computation: 49300 steps/s (collection: 1.880s, learning 0.114s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.1342
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 41.5454
                       Mean reward: 4.06
               Mean episode length: 242.91
    Episode_Reward/reaching_object: 0.7337
     Episode_Reward/lifting_object: 0.0479
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 1.99s
                      Time elapsed: 00:06:44
                               ETA: 01:07:03

################################################################################
                     [1m Learning iteration 183/2000 [0m                      

                       Computation: 49193 steps/s (collection: 1.901s, learning 0.098s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.5224
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 41.5895
                       Mean reward: 4.37
               Mean episode length: 239.55
    Episode_Reward/reaching_object: 0.7396
     Episode_Reward/lifting_object: 0.1368
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 2.00s
                      Time elapsed: 00:06:46
                               ETA: 01:06:58

################################################################################
                     [1m Learning iteration 184/2000 [0m                      

                       Computation: 46236 steps/s (collection: 1.998s, learning 0.128s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.5975
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 41.6032
                       Mean reward: 3.87
               Mean episode length: 244.50
    Episode_Reward/reaching_object: 0.7424
     Episode_Reward/lifting_object: 0.0985
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 2.13s
                      Time elapsed: 00:06:49
                               ETA: 01:06:55

################################################################################
                     [1m Learning iteration 185/2000 [0m                      

                       Computation: 48939 steps/s (collection: 1.901s, learning 0.108s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.5131
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 41.6494
                       Mean reward: 4.15
               Mean episode length: 239.10
    Episode_Reward/reaching_object: 0.7657
     Episode_Reward/lifting_object: 0.1065
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 2.01s
                      Time elapsed: 00:06:51
                               ETA: 01:06:51

################################################################################
                     [1m Learning iteration 186/2000 [0m                      

                       Computation: 48938 steps/s (collection: 1.920s, learning 0.089s)
             Mean action noise std: 1.61
          Mean value_function loss: 1.7115
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.7061
                       Mean reward: 4.51
               Mean episode length: 237.58
    Episode_Reward/reaching_object: 0.7476
     Episode_Reward/lifting_object: 0.1503
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 2.01s
                      Time elapsed: 00:06:53
                               ETA: 01:06:47

################################################################################
                     [1m Learning iteration 187/2000 [0m                      

                       Computation: 45926 steps/s (collection: 2.005s, learning 0.136s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.5928
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 41.7193
                       Mean reward: 3.48
               Mean episode length: 240.96
    Episode_Reward/reaching_object: 0.7411
     Episode_Reward/lifting_object: -0.0705
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 2.14s
                      Time elapsed: 00:06:55
                               ETA: 01:06:44

################################################################################
                     [1m Learning iteration 188/2000 [0m                      

                       Computation: 46840 steps/s (collection: 1.989s, learning 0.110s)
             Mean action noise std: 1.62
          Mean value_function loss: 1.0809
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 41.7464
                       Mean reward: 4.55
               Mean episode length: 239.83
    Episode_Reward/reaching_object: 0.7796
     Episode_Reward/lifting_object: 0.1608
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 2.10s
                      Time elapsed: 00:06:57
                               ETA: 01:06:41

################################################################################
                     [1m Learning iteration 189/2000 [0m                      

                       Computation: 45815 steps/s (collection: 2.057s, learning 0.089s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.9391
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 41.7727
                       Mean reward: 4.38
               Mean episode length: 239.60
    Episode_Reward/reaching_object: 0.7466
     Episode_Reward/lifting_object: 0.1403
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 2.15s
                      Time elapsed: 00:06:59
                               ETA: 01:06:38

################################################################################
                     [1m Learning iteration 190/2000 [0m                      

                       Computation: 46110 steps/s (collection: 2.009s, learning 0.123s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.5846
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 41.8209
                       Mean reward: 5.12
               Mean episode length: 242.91
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 0.0647
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 2.13s
                      Time elapsed: 00:07:01
                               ETA: 01:06:35

################################################################################
                     [1m Learning iteration 191/2000 [0m                      

                       Computation: 46143 steps/s (collection: 1.989s, learning 0.142s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.2959
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 41.8687
                       Mean reward: 4.43
               Mean episode length: 234.54
    Episode_Reward/reaching_object: 0.7404
     Episode_Reward/lifting_object: 0.0949
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 2.13s
                      Time elapsed: 00:07:03
                               ETA: 01:06:32

################################################################################
                     [1m Learning iteration 192/2000 [0m                      

                       Computation: 47982 steps/s (collection: 1.949s, learning 0.100s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.2399
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 41.9217
                       Mean reward: 4.42
               Mean episode length: 239.87
    Episode_Reward/reaching_object: 0.7683
     Episode_Reward/lifting_object: -0.0474
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 2.05s
                      Time elapsed: 00:07:05
                               ETA: 01:06:28

################################################################################
                     [1m Learning iteration 193/2000 [0m                      

                       Computation: 48671 steps/s (collection: 1.911s, learning 0.109s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.2824
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 41.9783
                       Mean reward: 4.30
               Mean episode length: 242.83
    Episode_Reward/reaching_object: 0.7630
     Episode_Reward/lifting_object: 0.1197
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 2.02s
                      Time elapsed: 00:07:07
                               ETA: 01:06:24

################################################################################
                     [1m Learning iteration 194/2000 [0m                      

                       Computation: 48370 steps/s (collection: 1.927s, learning 0.105s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.3065
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 42.0300
                       Mean reward: 4.50
               Mean episode length: 239.47
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 0.1558
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 2.03s
                      Time elapsed: 00:07:09
                               ETA: 01:06:21

################################################################################
                     [1m Learning iteration 195/2000 [0m                      

                       Computation: 49734 steps/s (collection: 1.875s, learning 0.102s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.2108
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 42.1071
                       Mean reward: 3.92
               Mean episode length: 242.97
    Episode_Reward/reaching_object: 0.7842
     Episode_Reward/lifting_object: 0.0852
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 1.98s
                      Time elapsed: 00:07:11
                               ETA: 01:06:16

################################################################################
                     [1m Learning iteration 196/2000 [0m                      

                       Computation: 48808 steps/s (collection: 1.886s, learning 0.129s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.2079
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 42.1930
                       Mean reward: 3.81
               Mean episode length: 237.49
    Episode_Reward/reaching_object: 0.7424
     Episode_Reward/lifting_object: 0.1298
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 2.01s
                      Time elapsed: 00:07:13
                               ETA: 01:06:12

################################################################################
                     [1m Learning iteration 197/2000 [0m                      

                       Computation: 49119 steps/s (collection: 1.881s, learning 0.120s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.8175
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 42.2434
                       Mean reward: 4.28
               Mean episode length: 235.42
    Episode_Reward/reaching_object: 0.7627
     Episode_Reward/lifting_object: 0.0712
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 2.00s
                      Time elapsed: 00:07:15
                               ETA: 01:06:08

################################################################################
                     [1m Learning iteration 198/2000 [0m                      

                       Computation: 49280 steps/s (collection: 1.884s, learning 0.111s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.5105
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 42.2607
                       Mean reward: 4.53
               Mean episode length: 242.13
    Episode_Reward/reaching_object: 0.7450
     Episode_Reward/lifting_object: 0.1444
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 1.99s
                      Time elapsed: 00:07:17
                               ETA: 01:06:04

################################################################################
                     [1m Learning iteration 199/2000 [0m                      

                       Computation: 48440 steps/s (collection: 1.924s, learning 0.105s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.2337
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 42.3081
                       Mean reward: 4.07
               Mean episode length: 241.97
    Episode_Reward/reaching_object: 0.7729
     Episode_Reward/lifting_object: 0.0582
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 2.03s
                      Time elapsed: 00:07:19
                               ETA: 01:06:00

################################################################################
                     [1m Learning iteration 200/2000 [0m                      

                       Computation: 49295 steps/s (collection: 1.888s, learning 0.106s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.5529
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 42.3738
                       Mean reward: 4.48
               Mean episode length: 242.08
    Episode_Reward/reaching_object: 0.7787
     Episode_Reward/lifting_object: 0.1332
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 1.99s
                      Time elapsed: 00:07:21
                               ETA: 01:05:56

################################################################################
                     [1m Learning iteration 201/2000 [0m                      

                       Computation: 49302 steps/s (collection: 1.888s, learning 0.106s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.1692
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 42.4438
                       Mean reward: 4.43
               Mean episode length: 238.15
    Episode_Reward/reaching_object: 0.7548
     Episode_Reward/lifting_object: 0.2297
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 1.99s
                      Time elapsed: 00:07:23
                               ETA: 01:05:52

################################################################################
                     [1m Learning iteration 202/2000 [0m                      

                       Computation: 50222 steps/s (collection: 1.863s, learning 0.094s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.3338
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 42.5054
                       Mean reward: 5.50
               Mean episode length: 242.07
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 0.1893
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 1.96s
                      Time elapsed: 00:07:25
                               ETA: 01:05:48

################################################################################
                     [1m Learning iteration 203/2000 [0m                      

                       Computation: 48968 steps/s (collection: 1.886s, learning 0.121s)
             Mean action noise std: 1.68
          Mean value_function loss: 1.2677
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 42.5376
                       Mean reward: 4.28
               Mean episode length: 245.17
    Episode_Reward/reaching_object: 0.7925
     Episode_Reward/lifting_object: 0.1267
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 2.01s
                      Time elapsed: 00:07:27
                               ETA: 01:05:44

################################################################################
                     [1m Learning iteration 204/2000 [0m                      

                       Computation: 48900 steps/s (collection: 1.870s, learning 0.140s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.4295
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 42.5499
                       Mean reward: 3.19
               Mean episode length: 237.56
    Episode_Reward/reaching_object: 0.7362
     Episode_Reward/lifting_object: 0.1323
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 2.01s
                      Time elapsed: 00:07:29
                               ETA: 01:05:40

################################################################################
                     [1m Learning iteration 205/2000 [0m                      

                       Computation: 49615 steps/s (collection: 1.875s, learning 0.106s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.4095
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 42.5868
                       Mean reward: 2.91
               Mean episode length: 238.41
    Episode_Reward/reaching_object: 0.7778
     Episode_Reward/lifting_object: 0.1112
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 1.98s
                      Time elapsed: 00:07:31
                               ETA: 01:05:36

################################################################################
                     [1m Learning iteration 206/2000 [0m                      

                       Computation: 48816 steps/s (collection: 1.880s, learning 0.134s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.5848
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 42.6352
                       Mean reward: 4.08
               Mean episode length: 245.42
    Episode_Reward/reaching_object: 0.7746
     Episode_Reward/lifting_object: 0.1328
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 2.01s
                      Time elapsed: 00:07:33
                               ETA: 01:05:33

################################################################################
                     [1m Learning iteration 207/2000 [0m                      

                       Computation: 48960 steps/s (collection: 1.877s, learning 0.131s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.8125
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 42.6803
                       Mean reward: 4.64
               Mean episode length: 231.99
    Episode_Reward/reaching_object: 0.7473
     Episode_Reward/lifting_object: 0.1698
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 2.01s
                      Time elapsed: 00:07:35
                               ETA: 01:05:29

################################################################################
                     [1m Learning iteration 208/2000 [0m                      

                       Computation: 50979 steps/s (collection: 1.829s, learning 0.099s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.3545
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 42.7349
                       Mean reward: 4.46
               Mean episode length: 240.82
    Episode_Reward/reaching_object: 0.7363
     Episode_Reward/lifting_object: 0.2304
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 1.93s
                      Time elapsed: 00:07:37
                               ETA: 01:05:24

################################################################################
                     [1m Learning iteration 209/2000 [0m                      

                       Computation: 49882 steps/s (collection: 1.865s, learning 0.106s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.3123
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 42.8048
                       Mean reward: 4.67
               Mean episode length: 240.48
    Episode_Reward/reaching_object: 0.7554
     Episode_Reward/lifting_object: 0.2014
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 1.97s
                      Time elapsed: 00:07:39
                               ETA: 01:05:20

################################################################################
                     [1m Learning iteration 210/2000 [0m                      

                       Computation: 47844 steps/s (collection: 1.903s, learning 0.151s)
             Mean action noise std: 1.70
          Mean value_function loss: 1.0474
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 42.8579
                       Mean reward: 4.88
               Mean episode length: 228.93
    Episode_Reward/reaching_object: 0.7326
     Episode_Reward/lifting_object: 0.2637
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 2.05s
                      Time elapsed: 00:07:41
                               ETA: 01:05:17

################################################################################
                     [1m Learning iteration 211/2000 [0m                      

                       Computation: 49458 steps/s (collection: 1.896s, learning 0.091s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.5465
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 42.8779
                       Mean reward: 5.00
               Mean episode length: 232.59
    Episode_Reward/reaching_object: 0.7355
     Episode_Reward/lifting_object: 0.0944
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 1.99s
                      Time elapsed: 00:07:43
                               ETA: 01:05:13

################################################################################
                     [1m Learning iteration 212/2000 [0m                      

                       Computation: 48844 steps/s (collection: 1.897s, learning 0.116s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.7873
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 42.9294
                       Mean reward: 4.25
               Mean episode length: 233.95
    Episode_Reward/reaching_object: 0.7484
     Episode_Reward/lifting_object: 0.2725
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0253
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 2.01s
                      Time elapsed: 00:07:45
                               ETA: 01:05:09

################################################################################
                     [1m Learning iteration 213/2000 [0m                      

                       Computation: 50422 steps/s (collection: 1.861s, learning 0.089s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.9161
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 42.9760
                       Mean reward: 4.50
               Mean episode length: 240.61
    Episode_Reward/reaching_object: 0.7439
     Episode_Reward/lifting_object: 0.2456
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 1.95s
                      Time elapsed: 00:07:47
                               ETA: 01:05:05

################################################################################
                     [1m Learning iteration 214/2000 [0m                      

                       Computation: 49636 steps/s (collection: 1.875s, learning 0.106s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.4814
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 43.0297
                       Mean reward: 5.05
               Mean episode length: 237.07
    Episode_Reward/reaching_object: 0.7446
     Episode_Reward/lifting_object: 0.1824
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 1.98s
                      Time elapsed: 00:07:49
                               ETA: 01:05:01

################################################################################
                     [1m Learning iteration 215/2000 [0m                      

                       Computation: 49055 steps/s (collection: 1.884s, learning 0.120s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.6532
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 43.0984
                       Mean reward: 4.90
               Mean episode length: 241.73
    Episode_Reward/reaching_object: 0.7513
     Episode_Reward/lifting_object: 0.2106
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 2.00s
                      Time elapsed: 00:07:51
                               ETA: 01:04:58

################################################################################
                     [1m Learning iteration 216/2000 [0m                      

                       Computation: 43151 steps/s (collection: 2.136s, learning 0.142s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.4646
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 43.1458
                       Mean reward: 5.12
               Mean episode length: 239.30
    Episode_Reward/reaching_object: 0.7332
     Episode_Reward/lifting_object: 0.2860
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 2.28s
                      Time elapsed: 00:07:53
                               ETA: 01:04:56

################################################################################
                     [1m Learning iteration 217/2000 [0m                      

                       Computation: 41556 steps/s (collection: 2.264s, learning 0.101s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.9590
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 43.1971
                       Mean reward: 4.65
               Mean episode length: 235.23
    Episode_Reward/reaching_object: 0.7250
     Episode_Reward/lifting_object: 0.1471
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 2.37s
                      Time elapsed: 00:07:56
                               ETA: 01:04:56

################################################################################
                     [1m Learning iteration 218/2000 [0m                      

                       Computation: 48419 steps/s (collection: 1.931s, learning 0.100s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.9960
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 43.2474
                       Mean reward: 4.01
               Mean episode length: 236.15
    Episode_Reward/reaching_object: 0.7168
     Episode_Reward/lifting_object: 0.2415
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 2.03s
                      Time elapsed: 00:07:58
                               ETA: 01:04:52

################################################################################
                     [1m Learning iteration 219/2000 [0m                      

                       Computation: 50065 steps/s (collection: 1.852s, learning 0.112s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.6423
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 43.3184
                       Mean reward: 3.96
               Mean episode length: 233.80
    Episode_Reward/reaching_object: 0.7336
     Episode_Reward/lifting_object: 0.3413
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 1.96s
                      Time elapsed: 00:08:00
                               ETA: 01:04:48

################################################################################
                     [1m Learning iteration 220/2000 [0m                      

                       Computation: 50642 steps/s (collection: 1.837s, learning 0.105s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.8705
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 43.3671
                       Mean reward: 4.46
               Mean episode length: 237.50
    Episode_Reward/reaching_object: 0.7102
     Episode_Reward/lifting_object: 0.4146
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 1.94s
                      Time elapsed: 00:08:02
                               ETA: 01:04:44

################################################################################
                     [1m Learning iteration 221/2000 [0m                      

                       Computation: 51115 steps/s (collection: 1.828s, learning 0.096s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.8629
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 43.3971
                       Mean reward: 5.49
               Mean episode length: 235.11
    Episode_Reward/reaching_object: 0.7292
     Episode_Reward/lifting_object: 0.3713
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 1.92s
                      Time elapsed: 00:08:04
                               ETA: 01:04:40

################################################################################
                     [1m Learning iteration 222/2000 [0m                      

                       Computation: 50449 steps/s (collection: 1.858s, learning 0.091s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.8965
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 43.4337
                       Mean reward: 5.33
               Mean episode length: 232.53
    Episode_Reward/reaching_object: 0.7393
     Episode_Reward/lifting_object: 0.4226
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 1.95s
                      Time elapsed: 00:08:06
                               ETA: 01:04:36

################################################################################
                     [1m Learning iteration 223/2000 [0m                      

                       Computation: 47732 steps/s (collection: 1.897s, learning 0.162s)
             Mean action noise std: 1.75
          Mean value_function loss: 1.1334
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 43.4684
                       Mean reward: 5.04
               Mean episode length: 237.17
    Episode_Reward/reaching_object: 0.7099
     Episode_Reward/lifting_object: 0.2962
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 2.06s
                      Time elapsed: 00:08:08
                               ETA: 01:04:33

################################################################################
                     [1m Learning iteration 224/2000 [0m                      

                       Computation: 45929 steps/s (collection: 2.023s, learning 0.118s)
             Mean action noise std: 1.75
          Mean value_function loss: 1.0634
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 43.5112
                       Mean reward: 5.31
               Mean episode length: 235.31
    Episode_Reward/reaching_object: 0.7272
     Episode_Reward/lifting_object: 0.4399
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 2.14s
                      Time elapsed: 00:08:10
                               ETA: 01:04:30

################################################################################
                     [1m Learning iteration 225/2000 [0m                      

                       Computation: 46854 steps/s (collection: 1.956s, learning 0.143s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.8782
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 43.5766
                       Mean reward: 5.08
               Mean episode length: 233.11
    Episode_Reward/reaching_object: 0.7101
     Episode_Reward/lifting_object: 0.3790
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 2.10s
                      Time elapsed: 00:08:12
                               ETA: 01:04:27

################################################################################
                     [1m Learning iteration 226/2000 [0m                      

                       Computation: 49381 steps/s (collection: 1.888s, learning 0.103s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.6997
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 43.6414
                       Mean reward: 5.12
               Mean episode length: 234.65
    Episode_Reward/reaching_object: 0.7192
     Episode_Reward/lifting_object: 0.4034
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 1.99s
                      Time elapsed: 00:08:14
                               ETA: 01:04:24

################################################################################
                     [1m Learning iteration 227/2000 [0m                      

                       Computation: 49228 steps/s (collection: 1.896s, learning 0.101s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.8210
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 43.6932
                       Mean reward: 5.33
               Mean episode length: 225.17
    Episode_Reward/reaching_object: 0.6991
     Episode_Reward/lifting_object: 0.4047
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 2.00s
                      Time elapsed: 00:08:16
                               ETA: 01:04:20

################################################################################
                     [1m Learning iteration 228/2000 [0m                      

                       Computation: 49400 steps/s (collection: 1.881s, learning 0.109s)
             Mean action noise std: 1.77
          Mean value_function loss: 1.1347
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 43.7579
                       Mean reward: 5.18
               Mean episode length: 231.20
    Episode_Reward/reaching_object: 0.6873
     Episode_Reward/lifting_object: 0.3317
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 1.99s
                      Time elapsed: 00:08:18
                               ETA: 01:04:16

################################################################################
                     [1m Learning iteration 229/2000 [0m                      

                       Computation: 49042 steps/s (collection: 1.893s, learning 0.111s)
             Mean action noise std: 1.78
          Mean value_function loss: 4.3213
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 43.8148
                       Mean reward: 4.57
               Mean episode length: 232.92
    Episode_Reward/reaching_object: 0.7019
     Episode_Reward/lifting_object: 0.4191
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 2.00s
                      Time elapsed: 00:08:20
                               ETA: 01:04:13

################################################################################
                     [1m Learning iteration 230/2000 [0m                      

                       Computation: 50023 steps/s (collection: 1.860s, learning 0.105s)
             Mean action noise std: 1.78
          Mean value_function loss: 1.7206
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 43.8303
                       Mean reward: 4.21
               Mean episode length: 226.28
    Episode_Reward/reaching_object: 0.6749
     Episode_Reward/lifting_object: 0.0051
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 1.97s
                      Time elapsed: 00:08:22
                               ETA: 01:04:09

################################################################################
                     [1m Learning iteration 231/2000 [0m                      

                       Computation: 50611 steps/s (collection: 1.843s, learning 0.099s)
             Mean action noise std: 1.78
          Mean value_function loss: 1.1368
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 43.8789
                       Mean reward: 5.79
               Mean episode length: 228.16
    Episode_Reward/reaching_object: 0.6833
     Episode_Reward/lifting_object: 0.4981
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 1.94s
                      Time elapsed: 00:08:24
                               ETA: 01:04:05

################################################################################
                     [1m Learning iteration 232/2000 [0m                      

                       Computation: 50936 steps/s (collection: 1.833s, learning 0.097s)
             Mean action noise std: 1.79
          Mean value_function loss: 1.1159
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 43.9328
                       Mean reward: 4.20
               Mean episode length: 230.86
    Episode_Reward/reaching_object: 0.6646
     Episode_Reward/lifting_object: 0.3743
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 1.93s
                      Time elapsed: 00:08:26
                               ETA: 01:04:01

################################################################################
                     [1m Learning iteration 233/2000 [0m                      

                       Computation: 49008 steps/s (collection: 1.907s, learning 0.099s)
             Mean action noise std: 1.79
          Mean value_function loss: 1.6553
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 43.9619
                       Mean reward: 4.27
               Mean episode length: 223.49
    Episode_Reward/reaching_object: 0.6614
     Episode_Reward/lifting_object: 0.3438
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 2.01s
                      Time elapsed: 00:08:28
                               ETA: 01:03:58

################################################################################
                     [1m Learning iteration 234/2000 [0m                      

                       Computation: 49567 steps/s (collection: 1.883s, learning 0.100s)
             Mean action noise std: 1.79
          Mean value_function loss: 0.9931
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 43.9704
                       Mean reward: 4.25
               Mean episode length: 224.24
    Episode_Reward/reaching_object: 0.6636
     Episode_Reward/lifting_object: 0.4046
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 1.98s
                      Time elapsed: 00:08:30
                               ETA: 01:03:54

################################################################################
                     [1m Learning iteration 235/2000 [0m                      

                       Computation: 49480 steps/s (collection: 1.870s, learning 0.117s)
             Mean action noise std: 1.79
          Mean value_function loss: 2.5551
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 43.9932
                       Mean reward: 6.53
               Mean episode length: 226.83
    Episode_Reward/reaching_object: 0.6576
     Episode_Reward/lifting_object: 0.4220
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 1.99s
                      Time elapsed: 00:08:32
                               ETA: 01:03:51

################################################################################
                     [1m Learning iteration 236/2000 [0m                      

                       Computation: 48774 steps/s (collection: 1.900s, learning 0.115s)
             Mean action noise std: 1.79
          Mean value_function loss: 1.3251
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 44.0145
                       Mean reward: 5.42
               Mean episode length: 220.06
    Episode_Reward/reaching_object: 0.6434
     Episode_Reward/lifting_object: 0.4802
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 2.02s
                      Time elapsed: 00:08:34
                               ETA: 01:03:47

################################################################################
                     [1m Learning iteration 237/2000 [0m                      

                       Computation: 49398 steps/s (collection: 1.887s, learning 0.103s)
             Mean action noise std: 1.80
          Mean value_function loss: 1.1389
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 44.0526
                       Mean reward: 5.66
               Mean episode length: 219.21
    Episode_Reward/reaching_object: 0.6085
     Episode_Reward/lifting_object: 0.5767
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 1.99s
                      Time elapsed: 00:08:36
                               ETA: 01:03:44

################################################################################
                     [1m Learning iteration 238/2000 [0m                      

                       Computation: 49798 steps/s (collection: 1.886s, learning 0.088s)
             Mean action noise std: 1.80
          Mean value_function loss: 1.4868
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 44.0987
                       Mean reward: 6.55
               Mean episode length: 222.25
    Episode_Reward/reaching_object: 0.6504
     Episode_Reward/lifting_object: 0.5295
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 1.97s
                      Time elapsed: 00:08:38
                               ETA: 01:03:40

################################################################################
                     [1m Learning iteration 239/2000 [0m                      

                       Computation: 49376 steps/s (collection: 1.894s, learning 0.097s)
             Mean action noise std: 1.81
          Mean value_function loss: 1.3199
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 44.1630
                       Mean reward: 5.73
               Mean episode length: 206.44
    Episode_Reward/reaching_object: 0.6298
     Episode_Reward/lifting_object: 0.5364
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 1.99s
                      Time elapsed: 00:08:40
                               ETA: 01:03:37

################################################################################
                     [1m Learning iteration 240/2000 [0m                      

                       Computation: 47831 steps/s (collection: 1.958s, learning 0.098s)
             Mean action noise std: 1.81
          Mean value_function loss: 1.4613
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 44.2154
                       Mean reward: 5.69
               Mean episode length: 221.54
    Episode_Reward/reaching_object: 0.6146
     Episode_Reward/lifting_object: 0.6303
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 2.06s
                      Time elapsed: 00:08:42
                               ETA: 01:03:34

################################################################################
                     [1m Learning iteration 241/2000 [0m                      

                       Computation: 49288 steps/s (collection: 1.886s, learning 0.109s)
             Mean action noise std: 1.81
          Mean value_function loss: 1.7201
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 44.2676
                       Mean reward: 5.14
               Mean episode length: 208.63
    Episode_Reward/reaching_object: 0.6234
     Episode_Reward/lifting_object: 0.6201
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 1.99s
                      Time elapsed: 00:08:44
                               ETA: 01:03:30

################################################################################
                     [1m Learning iteration 242/2000 [0m                      

                       Computation: 49370 steps/s (collection: 1.886s, learning 0.106s)
             Mean action noise std: 1.82
          Mean value_function loss: 1.6312
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 44.3002
                       Mean reward: 5.61
               Mean episode length: 201.44
    Episode_Reward/reaching_object: 0.5780
     Episode_Reward/lifting_object: 0.6250
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0253
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 1.99s
                      Time elapsed: 00:08:46
                               ETA: 01:03:27

################################################################################
                     [1m Learning iteration 243/2000 [0m                      

                       Computation: 48653 steps/s (collection: 1.887s, learning 0.133s)
             Mean action noise std: 1.82
          Mean value_function loss: 2.2121
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 44.3306
                       Mean reward: 6.92
               Mean episode length: 210.37
    Episode_Reward/reaching_object: 0.5994
     Episode_Reward/lifting_object: 0.5984
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 2.02s
                      Time elapsed: 00:08:48
                               ETA: 01:03:24

################################################################################
                     [1m Learning iteration 244/2000 [0m                      

                       Computation: 47862 steps/s (collection: 1.937s, learning 0.117s)
             Mean action noise std: 1.82
          Mean value_function loss: 2.1293
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 44.3710
                       Mean reward: 7.78
               Mean episode length: 213.63
    Episode_Reward/reaching_object: 0.6094
     Episode_Reward/lifting_object: 0.6059
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 2.05s
                      Time elapsed: 00:08:50
                               ETA: 01:03:21

################################################################################
                     [1m Learning iteration 245/2000 [0m                      

                       Computation: 49622 steps/s (collection: 1.883s, learning 0.098s)
             Mean action noise std: 1.82
          Mean value_function loss: 2.5946
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 44.3950
                       Mean reward: 5.21
               Mean episode length: 209.50
    Episode_Reward/reaching_object: 0.5872
     Episode_Reward/lifting_object: 0.6291
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 1.98s
                      Time elapsed: 00:08:52
                               ETA: 01:03:17

################################################################################
                     [1m Learning iteration 246/2000 [0m                      

                       Computation: 49022 steps/s (collection: 1.886s, learning 0.119s)
             Mean action noise std: 1.83
          Mean value_function loss: 1.3133
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 44.4277
                       Mean reward: 6.20
               Mean episode length: 206.31
    Episode_Reward/reaching_object: 0.5861
     Episode_Reward/lifting_object: 0.6910
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 2.01s
                      Time elapsed: 00:08:54
                               ETA: 01:03:14

################################################################################
                     [1m Learning iteration 247/2000 [0m                      

                       Computation: 50452 steps/s (collection: 1.853s, learning 0.096s)
             Mean action noise std: 1.83
          Mean value_function loss: 2.1557
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 44.4594
                       Mean reward: 6.05
               Mean episode length: 218.56
    Episode_Reward/reaching_object: 0.5809
     Episode_Reward/lifting_object: 0.6943
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 1.95s
                      Time elapsed: 00:08:56
                               ETA: 01:03:10

################################################################################
                     [1m Learning iteration 248/2000 [0m                      

                       Computation: 50568 steps/s (collection: 1.856s, learning 0.088s)
             Mean action noise std: 1.83
          Mean value_function loss: 1.5210
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 44.4977
                       Mean reward: 6.85
               Mean episode length: 205.72
    Episode_Reward/reaching_object: 0.5710
     Episode_Reward/lifting_object: 0.7604
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 1.94s
                      Time elapsed: 00:08:58
                               ETA: 01:03:07

################################################################################
                     [1m Learning iteration 249/2000 [0m                      

                       Computation: 50071 steps/s (collection: 1.860s, learning 0.103s)
             Mean action noise std: 1.84
          Mean value_function loss: 2.0932
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 44.5373
                       Mean reward: 7.56
               Mean episode length: 208.72
    Episode_Reward/reaching_object: 0.5713
     Episode_Reward/lifting_object: 0.8481
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 1.96s
                      Time elapsed: 00:09:00
                               ETA: 01:03:03

################################################################################
                     [1m Learning iteration 250/2000 [0m                      

                       Computation: 50500 steps/s (collection: 1.843s, learning 0.104s)
             Mean action noise std: 1.84
          Mean value_function loss: 3.7049
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 44.5939
                       Mean reward: 7.92
               Mean episode length: 214.09
    Episode_Reward/reaching_object: 0.5818
     Episode_Reward/lifting_object: 0.8941
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 1.95s
                      Time elapsed: 00:09:02
                               ETA: 01:02:59

################################################################################
                     [1m Learning iteration 251/2000 [0m                      

                       Computation: 50403 steps/s (collection: 1.840s, learning 0.110s)
             Mean action noise std: 1.85
          Mean value_function loss: 2.3418
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 44.6464
                       Mean reward: 5.37
               Mean episode length: 200.14
    Episode_Reward/reaching_object: 0.5576
     Episode_Reward/lifting_object: 0.7284
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 1.95s
                      Time elapsed: 00:09:04
                               ETA: 01:02:56

################################################################################
                     [1m Learning iteration 252/2000 [0m                      

                       Computation: 49402 steps/s (collection: 1.868s, learning 0.122s)
             Mean action noise std: 1.85
          Mean value_function loss: 1.9185
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 44.6887
                       Mean reward: 4.84
               Mean episode length: 203.15
    Episode_Reward/reaching_object: 0.5716
     Episode_Reward/lifting_object: 0.5659
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 1.99s
                      Time elapsed: 00:09:06
                               ETA: 01:02:52

################################################################################
                     [1m Learning iteration 253/2000 [0m                      

                       Computation: 48498 steps/s (collection: 1.898s, learning 0.129s)
             Mean action noise std: 1.85
          Mean value_function loss: 1.9280
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 44.7308
                       Mean reward: 6.15
               Mean episode length: 203.11
    Episode_Reward/reaching_object: 0.5597
     Episode_Reward/lifting_object: 0.7615
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 2.03s
                      Time elapsed: 00:09:08
                               ETA: 01:02:49

################################################################################
                     [1m Learning iteration 254/2000 [0m                      

                       Computation: 50005 steps/s (collection: 1.864s, learning 0.102s)
             Mean action noise std: 1.86
          Mean value_function loss: 2.3013
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 44.7647
                       Mean reward: 6.43
               Mean episode length: 196.45
    Episode_Reward/reaching_object: 0.5548
     Episode_Reward/lifting_object: 0.8101
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 1.97s
                      Time elapsed: 00:09:10
                               ETA: 01:02:46

################################################################################
                     [1m Learning iteration 255/2000 [0m                      

                       Computation: 49728 steps/s (collection: 1.879s, learning 0.098s)
             Mean action noise std: 1.86
          Mean value_function loss: 2.5688
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 44.8096
                       Mean reward: 6.86
               Mean episode length: 200.16
    Episode_Reward/reaching_object: 0.5392
     Episode_Reward/lifting_object: 0.9191
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 1.98s
                      Time elapsed: 00:09:12
                               ETA: 01:02:42

################################################################################
                     [1m Learning iteration 256/2000 [0m                      

                       Computation: 49962 steps/s (collection: 1.867s, learning 0.100s)
             Mean action noise std: 1.86
          Mean value_function loss: 5.2426
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 44.8481
                       Mean reward: 6.57
               Mean episode length: 202.19
    Episode_Reward/reaching_object: 0.5313
     Episode_Reward/lifting_object: 0.7539
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 1.97s
                      Time elapsed: 00:09:14
                               ETA: 01:02:39

################################################################################
                     [1m Learning iteration 257/2000 [0m                      

                       Computation: 49690 steps/s (collection: 1.881s, learning 0.098s)
             Mean action noise std: 1.86
          Mean value_function loss: 2.0366
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 44.8797
                       Mean reward: 6.09
               Mean episode length: 192.92
    Episode_Reward/reaching_object: 0.5374
     Episode_Reward/lifting_object: 0.9414
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 1.98s
                      Time elapsed: 00:09:15
                               ETA: 01:02:36

################################################################################
                     [1m Learning iteration 258/2000 [0m                      

                       Computation: 49937 steps/s (collection: 1.882s, learning 0.086s)
             Mean action noise std: 1.87
          Mean value_function loss: 2.7105
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 44.9102
                       Mean reward: 5.28
               Mean episode length: 189.11
    Episode_Reward/reaching_object: 0.5352
     Episode_Reward/lifting_object: 0.7767
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 1.97s
                      Time elapsed: 00:09:17
                               ETA: 01:02:32

################################################################################
                     [1m Learning iteration 259/2000 [0m                      

                       Computation: 49227 steps/s (collection: 1.908s, learning 0.089s)
             Mean action noise std: 1.87
          Mean value_function loss: 2.4197
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 44.9446
                       Mean reward: 7.01
               Mean episode length: 196.24
    Episode_Reward/reaching_object: 0.5243
     Episode_Reward/lifting_object: 0.9167
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 2.00s
                      Time elapsed: 00:09:19
                               ETA: 01:02:29

################################################################################
                     [1m Learning iteration 260/2000 [0m                      

                       Computation: 47136 steps/s (collection: 1.987s, learning 0.099s)
             Mean action noise std: 1.87
          Mean value_function loss: 2.5370
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 44.9833
                       Mean reward: 6.81
               Mean episode length: 189.10
    Episode_Reward/reaching_object: 0.5146
     Episode_Reward/lifting_object: 0.8269
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 2.09s
                      Time elapsed: 00:09:22
                               ETA: 01:02:26

################################################################################
                     [1m Learning iteration 261/2000 [0m                      

                       Computation: 46138 steps/s (collection: 2.022s, learning 0.108s)
             Mean action noise std: 1.88
          Mean value_function loss: 2.5474
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 45.0213
                       Mean reward: 7.60
               Mean episode length: 200.09
    Episode_Reward/reaching_object: 0.5052
     Episode_Reward/lifting_object: 1.0389
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 2.13s
                      Time elapsed: 00:09:24
                               ETA: 01:02:24

################################################################################
                     [1m Learning iteration 262/2000 [0m                      

                       Computation: 45244 steps/s (collection: 2.041s, learning 0.131s)
             Mean action noise std: 1.88
          Mean value_function loss: 2.9517
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 45.0617
                       Mean reward: 6.55
               Mean episode length: 182.91
    Episode_Reward/reaching_object: 0.5245
     Episode_Reward/lifting_object: 1.0029
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 2.17s
                      Time elapsed: 00:09:26
                               ETA: 01:02:22

################################################################################
                     [1m Learning iteration 263/2000 [0m                      

                       Computation: 45753 steps/s (collection: 2.042s, learning 0.107s)
             Mean action noise std: 1.88
          Mean value_function loss: 3.2681
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 45.0953
                       Mean reward: 7.38
               Mean episode length: 193.23
    Episode_Reward/reaching_object: 0.4949
     Episode_Reward/lifting_object: 1.0040
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 2.15s
                      Time elapsed: 00:09:28
                               ETA: 01:02:20

################################################################################
                     [1m Learning iteration 264/2000 [0m                      

                       Computation: 45988 steps/s (collection: 1.999s, learning 0.139s)
             Mean action noise std: 1.89
          Mean value_function loss: 3.0716
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 45.1292
                       Mean reward: 8.60
               Mean episode length: 196.96
    Episode_Reward/reaching_object: 0.5097
     Episode_Reward/lifting_object: 0.9793
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 2.14s
                      Time elapsed: 00:09:30
                               ETA: 01:02:18

################################################################################
                     [1m Learning iteration 265/2000 [0m                      

                       Computation: 46546 steps/s (collection: 2.008s, learning 0.104s)
             Mean action noise std: 1.89
          Mean value_function loss: 5.5976
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 45.1583
                       Mean reward: 7.34
               Mean episode length: 185.94
    Episode_Reward/reaching_object: 0.4943
     Episode_Reward/lifting_object: 0.9449
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 15.0833
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 2.11s
                      Time elapsed: 00:09:32
                               ETA: 01:02:15

################################################################################
                     [1m Learning iteration 266/2000 [0m                      

                       Computation: 48378 steps/s (collection: 1.931s, learning 0.101s)
             Mean action noise std: 1.89
          Mean value_function loss: 2.6976
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 45.2018
                       Mean reward: 6.87
               Mean episode length: 182.87
    Episode_Reward/reaching_object: 0.4837
     Episode_Reward/lifting_object: 0.8652
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0253
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 15.7083
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 2.03s
                      Time elapsed: 00:09:34
                               ETA: 01:02:12

################################################################################
                     [1m Learning iteration 267/2000 [0m                      

                       Computation: 45667 steps/s (collection: 1.975s, learning 0.178s)
             Mean action noise std: 1.90
          Mean value_function loss: 2.7679
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 45.2522
                       Mean reward: 7.18
               Mean episode length: 165.86
    Episode_Reward/reaching_object: 0.4750
     Episode_Reward/lifting_object: 0.9353
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 16.2500
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 2.15s
                      Time elapsed: 00:09:36
                               ETA: 01:02:10

################################################################################
                     [1m Learning iteration 268/2000 [0m                      

                       Computation: 45394 steps/s (collection: 2.073s, learning 0.093s)
             Mean action noise std: 1.90
          Mean value_function loss: 3.3911
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 45.3022
                       Mean reward: 6.95
               Mean episode length: 174.60
    Episode_Reward/reaching_object: 0.4772
     Episode_Reward/lifting_object: 1.0937
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 16.5833
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 2.17s
                      Time elapsed: 00:09:39
                               ETA: 01:02:08

################################################################################
                     [1m Learning iteration 269/2000 [0m                      

                       Computation: 45729 steps/s (collection: 2.057s, learning 0.093s)
             Mean action noise std: 1.90
          Mean value_function loss: 2.6950
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 45.3284
                       Mean reward: 9.94
               Mean episode length: 181.94
    Episode_Reward/reaching_object: 0.4984
     Episode_Reward/lifting_object: 1.4182
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 14.0417
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 2.15s
                      Time elapsed: 00:09:41
                               ETA: 01:02:06

################################################################################
                     [1m Learning iteration 270/2000 [0m                      

                       Computation: 46943 steps/s (collection: 1.944s, learning 0.150s)
             Mean action noise std: 1.91
          Mean value_function loss: 4.3401
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 45.3528
                       Mean reward: 8.73
               Mean episode length: 170.23
    Episode_Reward/reaching_object: 0.4600
     Episode_Reward/lifting_object: 1.1708
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 15.0833
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 2.09s
                      Time elapsed: 00:09:43
                               ETA: 01:02:03

################################################################################
                     [1m Learning iteration 271/2000 [0m                      

                       Computation: 44383 steps/s (collection: 2.058s, learning 0.157s)
             Mean action noise std: 1.91
          Mean value_function loss: 3.8805
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 45.3757
                       Mean reward: 7.20
               Mean episode length: 167.10
    Episode_Reward/reaching_object: 0.4754
     Episode_Reward/lifting_object: 1.1654
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 2.21s
                      Time elapsed: 00:09:45
                               ETA: 01:02:02

################################################################################
                     [1m Learning iteration 272/2000 [0m                      

                       Computation: 43681 steps/s (collection: 2.100s, learning 0.150s)
             Mean action noise std: 1.91
          Mean value_function loss: 3.6979
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 45.4130
                       Mean reward: 9.08
               Mean episode length: 178.87
    Episode_Reward/reaching_object: 0.4647
     Episode_Reward/lifting_object: 1.1158
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 17.0833
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 2.25s
                      Time elapsed: 00:09:47
                               ETA: 01:02:00

################################################################################
                     [1m Learning iteration 273/2000 [0m                      

                       Computation: 49364 steps/s (collection: 1.892s, learning 0.100s)
             Mean action noise std: 1.91
          Mean value_function loss: 4.1082
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 45.4494
                       Mean reward: 7.74
               Mean episode length: 178.06
    Episode_Reward/reaching_object: 0.4681
     Episode_Reward/lifting_object: 1.0731
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0253
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 17.4583
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 1.99s
                      Time elapsed: 00:09:49
                               ETA: 01:01:57

################################################################################
                     [1m Learning iteration 274/2000 [0m                      

                       Computation: 49608 steps/s (collection: 1.893s, learning 0.089s)
             Mean action noise std: 1.92
          Mean value_function loss: 3.3938
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 45.4757
                       Mean reward: 7.12
               Mean episode length: 170.19
    Episode_Reward/reaching_object: 0.4726
     Episode_Reward/lifting_object: 1.2234
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 19.5833
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 1.98s
                      Time elapsed: 00:09:51
                               ETA: 01:01:54

################################################################################
                     [1m Learning iteration 275/2000 [0m                      

                       Computation: 49404 steps/s (collection: 1.893s, learning 0.097s)
             Mean action noise std: 1.92
          Mean value_function loss: 3.3797
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 45.5047
                       Mean reward: 7.96
               Mean episode length: 177.00
    Episode_Reward/reaching_object: 0.4611
     Episode_Reward/lifting_object: 1.3431
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 6.3750
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 19.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 1.99s
                      Time elapsed: 00:09:53
                               ETA: 01:01:51

################################################################################
                     [1m Learning iteration 276/2000 [0m                      

                       Computation: 48043 steps/s (collection: 1.957s, learning 0.089s)
             Mean action noise std: 1.92
          Mean value_function loss: 3.1027
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 45.5366
                       Mean reward: 8.71
               Mean episode length: 167.40
    Episode_Reward/reaching_object: 0.4476
     Episode_Reward/lifting_object: 1.2912
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 19.4583
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 2.05s
                      Time elapsed: 00:09:55
                               ETA: 01:01:48

################################################################################
                     [1m Learning iteration 277/2000 [0m                      

                       Computation: 49449 steps/s (collection: 1.900s, learning 0.088s)
             Mean action noise std: 1.92
          Mean value_function loss: 4.0285
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 45.5626
                       Mean reward: 8.45
               Mean episode length: 173.01
    Episode_Reward/reaching_object: 0.4483
     Episode_Reward/lifting_object: 1.0373
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 19.7500
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 1.99s
                      Time elapsed: 00:09:57
                               ETA: 01:01:45

################################################################################
                     [1m Learning iteration 278/2000 [0m                      

                       Computation: 48731 steps/s (collection: 1.927s, learning 0.090s)
             Mean action noise std: 1.93
          Mean value_function loss: 6.9870
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 45.5975
                       Mean reward: 9.26
               Mean episode length: 164.83
    Episode_Reward/reaching_object: 0.4412
     Episode_Reward/lifting_object: 1.2065
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 18.5000
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 2.02s
                      Time elapsed: 00:09:59
                               ETA: 01:01:42

################################################################################
                     [1m Learning iteration 279/2000 [0m                      

                       Computation: 48732 steps/s (collection: 1.922s, learning 0.095s)
             Mean action noise std: 1.93
          Mean value_function loss: 5.2569
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 45.6314
                       Mean reward: 9.32
               Mean episode length: 153.47
    Episode_Reward/reaching_object: 0.4312
     Episode_Reward/lifting_object: 1.2509
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 4.2917
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 19.0417
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 2.02s
                      Time elapsed: 00:10:01
                               ETA: 01:01:39

################################################################################
                     [1m Learning iteration 280/2000 [0m                      

                       Computation: 48221 steps/s (collection: 1.923s, learning 0.116s)
             Mean action noise std: 1.93
          Mean value_function loss: 10.9654
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 45.6686
                       Mean reward: 6.09
               Mean episode length: 156.58
    Episode_Reward/reaching_object: 0.4305
     Episode_Reward/lifting_object: 1.0988
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 3.8750
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 19.7917
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 2.04s
                      Time elapsed: 00:10:03
                               ETA: 01:01:36

################################################################################
                     [1m Learning iteration 281/2000 [0m                      

                       Computation: 48451 steps/s (collection: 1.929s, learning 0.100s)
             Mean action noise std: 1.94
          Mean value_function loss: 10.9369
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 45.7193
                       Mean reward: 7.59
               Mean episode length: 163.78
    Episode_Reward/reaching_object: 0.4208
     Episode_Reward/lifting_object: 0.9176
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 4.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 20.9583
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 2.03s
                      Time elapsed: 00:10:05
                               ETA: 01:01:33

################################################################################
                     [1m Learning iteration 282/2000 [0m                      

                       Computation: 44662 steps/s (collection: 2.052s, learning 0.149s)
             Mean action noise std: 1.94
          Mean value_function loss: 3.7060
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 45.7276
                       Mean reward: 7.73
               Mean episode length: 151.83
    Episode_Reward/reaching_object: 0.4249
     Episode_Reward/lifting_object: 1.1586
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 3.8750
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 26.5000
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 2.20s
                      Time elapsed: 00:10:08
                               ETA: 01:01:31

################################################################################
                     [1m Learning iteration 283/2000 [0m                      

                       Computation: 47259 steps/s (collection: 1.974s, learning 0.107s)
             Mean action noise std: 1.94
          Mean value_function loss: 4.2941
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 45.7395
                       Mean reward: 9.61
               Mean episode length: 152.86
    Episode_Reward/reaching_object: 0.4170
     Episode_Reward/lifting_object: 1.3013
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 3.8750
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 26.1667
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 2.08s
                      Time elapsed: 00:10:10
                               ETA: 01:01:29

################################################################################
                     [1m Learning iteration 284/2000 [0m                      

                       Computation: 45667 steps/s (collection: 1.971s, learning 0.182s)
             Mean action noise std: 1.94
          Mean value_function loss: 4.8211
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 45.7560
                       Mean reward: 8.81
               Mean episode length: 147.78
    Episode_Reward/reaching_object: 0.4205
     Episode_Reward/lifting_object: 1.1531
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 3.7083
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 24.8333
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 2.15s
                      Time elapsed: 00:10:12
                               ETA: 01:01:26

################################################################################
                     [1m Learning iteration 285/2000 [0m                      

                       Computation: 45219 steps/s (collection: 2.061s, learning 0.113s)
             Mean action noise std: 1.94
          Mean value_function loss: 6.3180
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 45.7759
                       Mean reward: 10.19
               Mean episode length: 134.19
    Episode_Reward/reaching_object: 0.3984
     Episode_Reward/lifting_object: 1.4053
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 29.1250
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 2.17s
                      Time elapsed: 00:10:14
                               ETA: 01:01:24

################################################################################
                     [1m Learning iteration 286/2000 [0m                      

                       Computation: 47092 steps/s (collection: 1.965s, learning 0.123s)
             Mean action noise std: 1.94
          Mean value_function loss: 6.0833
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 45.8003
                       Mean reward: 7.36
               Mean episode length: 141.93
    Episode_Reward/reaching_object: 0.4070
     Episode_Reward/lifting_object: 1.4455
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 27.9167
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 2.09s
                      Time elapsed: 00:10:16
                               ETA: 01:01:22

################################################################################
                     [1m Learning iteration 287/2000 [0m                      

                       Computation: 42276 steps/s (collection: 2.219s, learning 0.107s)
             Mean action noise std: 1.95
          Mean value_function loss: 5.4145
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 45.8241
                       Mean reward: 11.35
               Mean episode length: 132.37
    Episode_Reward/reaching_object: 0.3925
     Episode_Reward/lifting_object: 1.4659
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 27.3750
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 2.33s
                      Time elapsed: 00:10:18
                               ETA: 01:01:21

################################################################################
                     [1m Learning iteration 288/2000 [0m                      

                       Computation: 46356 steps/s (collection: 2.018s, learning 0.103s)
             Mean action noise std: 1.95
          Mean value_function loss: 9.3121
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 45.8405
                       Mean reward: 9.73
               Mean episode length: 133.13
    Episode_Reward/reaching_object: 0.3986
     Episode_Reward/lifting_object: 1.4990
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 27.2083
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 2.12s
                      Time elapsed: 00:10:21
                               ETA: 01:01:18

################################################################################
                     [1m Learning iteration 289/2000 [0m                      

                       Computation: 44198 steps/s (collection: 2.043s, learning 0.181s)
             Mean action noise std: 1.95
          Mean value_function loss: 7.1678
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 45.8712
                       Mean reward: 9.04
               Mean episode length: 140.00
    Episode_Reward/reaching_object: 0.3863
     Episode_Reward/lifting_object: 1.5279
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 26.1667
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 2.22s
                      Time elapsed: 00:10:23
                               ETA: 01:01:17

################################################################################
                     [1m Learning iteration 290/2000 [0m                      

                       Computation: 46300 steps/s (collection: 1.994s, learning 0.129s)
             Mean action noise std: 1.95
          Mean value_function loss: 6.7065
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 45.9062
                       Mean reward: 11.26
               Mean episode length: 142.70
    Episode_Reward/reaching_object: 0.4042
     Episode_Reward/lifting_object: 1.6921
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 24.4167
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 2.12s
                      Time elapsed: 00:10:25
                               ETA: 01:01:14

################################################################################
                     [1m Learning iteration 291/2000 [0m                      

                       Computation: 47266 steps/s (collection: 1.940s, learning 0.140s)
             Mean action noise std: 1.96
          Mean value_function loss: 6.6819
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 45.9292
                       Mean reward: 12.68
               Mean episode length: 141.37
    Episode_Reward/reaching_object: 0.4025
     Episode_Reward/lifting_object: 1.8094
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 24.5833
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 2.08s
                      Time elapsed: 00:10:27
                               ETA: 01:01:12

################################################################################
                     [1m Learning iteration 292/2000 [0m                      

                       Computation: 48130 steps/s (collection: 1.918s, learning 0.125s)
             Mean action noise std: 1.96
          Mean value_function loss: 5.2287
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 45.9562
                       Mean reward: 11.89
               Mean episode length: 138.90
    Episode_Reward/reaching_object: 0.3956
     Episode_Reward/lifting_object: 1.8064
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 24.7500
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 2.04s
                      Time elapsed: 00:10:29
                               ETA: 01:01:09

################################################################################
                     [1m Learning iteration 293/2000 [0m                      

                       Computation: 48527 steps/s (collection: 1.935s, learning 0.091s)
             Mean action noise std: 1.96
          Mean value_function loss: 5.6889
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 45.9764
                       Mean reward: 11.65
               Mean episode length: 145.98
    Episode_Reward/reaching_object: 0.4063
     Episode_Reward/lifting_object: 1.7025
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 25.8750
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 2.03s
                      Time elapsed: 00:10:31
                               ETA: 01:01:06

################################################################################
                     [1m Learning iteration 294/2000 [0m                      

                       Computation: 47662 steps/s (collection: 1.954s, learning 0.108s)
             Mean action noise std: 1.96
          Mean value_function loss: 7.3148
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 46.0073
                       Mean reward: 8.49
               Mean episode length: 150.65
    Episode_Reward/reaching_object: 0.4223
     Episode_Reward/lifting_object: 1.7507
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 24.0417
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 2.06s
                      Time elapsed: 00:10:33
                               ETA: 01:01:04

################################################################################
                     [1m Learning iteration 295/2000 [0m                      

                       Computation: 49366 steps/s (collection: 1.902s, learning 0.089s)
             Mean action noise std: 1.97
          Mean value_function loss: 9.0955
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 46.0356
                       Mean reward: 11.67
               Mean episode length: 153.37
    Episode_Reward/reaching_object: 0.4131
     Episode_Reward/lifting_object: 1.4517
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 25.8750
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 1.99s
                      Time elapsed: 00:10:35
                               ETA: 01:01:01

################################################################################
                     [1m Learning iteration 296/2000 [0m                      

                       Computation: 47360 steps/s (collection: 1.924s, learning 0.152s)
             Mean action noise std: 1.97
          Mean value_function loss: 9.5727
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 46.0581
                       Mean reward: 11.54
               Mean episode length: 154.02
    Episode_Reward/reaching_object: 0.4242
     Episode_Reward/lifting_object: 1.8094
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 25.7917
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 2.08s
                      Time elapsed: 00:10:37
                               ETA: 01:00:58

################################################################################
                     [1m Learning iteration 297/2000 [0m                      

                       Computation: 48020 steps/s (collection: 1.939s, learning 0.109s)
             Mean action noise std: 1.97
          Mean value_function loss: 7.5042
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 46.0773
                       Mean reward: 12.78
               Mean episode length: 138.25
    Episode_Reward/reaching_object: 0.4157
     Episode_Reward/lifting_object: 1.9599
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 27.7500
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 2.05s
                      Time elapsed: 00:10:39
                               ETA: 01:00:55

################################################################################
                     [1m Learning iteration 298/2000 [0m                      

                       Computation: 47351 steps/s (collection: 1.970s, learning 0.106s)
             Mean action noise std: 1.97
          Mean value_function loss: 10.2972
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 46.1071
                       Mean reward: 9.53
               Mean episode length: 145.24
    Episode_Reward/reaching_object: 0.3965
     Episode_Reward/lifting_object: 1.7084
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 26.1250
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 2.08s
                      Time elapsed: 00:10:41
                               ETA: 01:00:53

################################################################################
                     [1m Learning iteration 299/2000 [0m                      

                       Computation: 48804 steps/s (collection: 1.923s, learning 0.092s)
             Mean action noise std: 1.98
          Mean value_function loss: 8.7394
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 46.1427
                       Mean reward: 10.98
               Mean episode length: 139.77
    Episode_Reward/reaching_object: 0.4150
     Episode_Reward/lifting_object: 1.8913
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 25.5833
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 2.01s
                      Time elapsed: 00:10:43
                               ETA: 01:00:50

################################################################################
                     [1m Learning iteration 300/2000 [0m                      

                       Computation: 47658 steps/s (collection: 1.951s, learning 0.112s)
             Mean action noise std: 1.98
          Mean value_function loss: 7.0703
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 46.1686
                       Mean reward: 11.10
               Mean episode length: 147.09
    Episode_Reward/reaching_object: 0.4060
     Episode_Reward/lifting_object: 2.1088
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 27.8750
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 2.06s
                      Time elapsed: 00:10:45
                               ETA: 01:00:47

################################################################################
                     [1m Learning iteration 301/2000 [0m                      

                       Computation: 44285 steps/s (collection: 2.056s, learning 0.164s)
             Mean action noise std: 1.98
          Mean value_function loss: 7.6645
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 46.1969
                       Mean reward: 13.43
               Mean episode length: 137.73
    Episode_Reward/reaching_object: 0.3967
     Episode_Reward/lifting_object: 1.9818
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 27.7917
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 2.22s
                      Time elapsed: 00:10:48
                               ETA: 01:00:46

################################################################################
                     [1m Learning iteration 302/2000 [0m                      

                       Computation: 47374 steps/s (collection: 1.979s, learning 0.096s)
             Mean action noise std: 1.98
          Mean value_function loss: 7.0228
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 46.2195
                       Mean reward: 12.93
               Mean episode length: 145.63
    Episode_Reward/reaching_object: 0.3925
     Episode_Reward/lifting_object: 1.9741
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 26.9167
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 2.08s
                      Time elapsed: 00:10:50
                               ETA: 01:00:43

################################################################################
                     [1m Learning iteration 303/2000 [0m                      

                       Computation: 48162 steps/s (collection: 1.946s, learning 0.096s)
             Mean action noise std: 1.98
          Mean value_function loss: 6.8591
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 46.2340
                       Mean reward: 13.19
               Mean episode length: 139.40
    Episode_Reward/reaching_object: 0.3966
     Episode_Reward/lifting_object: 2.2546
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 29.8333
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 2.04s
                      Time elapsed: 00:10:52
                               ETA: 01:00:40

################################################################################
                     [1m Learning iteration 304/2000 [0m                      

                       Computation: 47823 steps/s (collection: 1.955s, learning 0.101s)
             Mean action noise std: 1.98
          Mean value_function loss: 9.0412
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 46.2439
                       Mean reward: 11.12
               Mean episode length: 127.20
    Episode_Reward/reaching_object: 0.3845
     Episode_Reward/lifting_object: 1.8391
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 29.7083
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 2.06s
                      Time elapsed: 00:10:54
                               ETA: 01:00:38

################################################################################
                     [1m Learning iteration 305/2000 [0m                      

                       Computation: 47752 steps/s (collection: 1.971s, learning 0.088s)
             Mean action noise std: 1.99
          Mean value_function loss: 10.2340
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 46.2519
                       Mean reward: 12.59
               Mean episode length: 141.54
    Episode_Reward/reaching_object: 0.3944
     Episode_Reward/lifting_object: 2.1427
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 28.9167
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 2.06s
                      Time elapsed: 00:10:56
                               ETA: 01:00:35

################################################################################
                     [1m Learning iteration 306/2000 [0m                      

                       Computation: 43805 steps/s (collection: 2.122s, learning 0.122s)
             Mean action noise std: 1.99
          Mean value_function loss: 9.0092
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 46.2699
                       Mean reward: 10.64
               Mean episode length: 136.36
    Episode_Reward/reaching_object: 0.3858
     Episode_Reward/lifting_object: 2.0585
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 29.1667
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 2.24s
                      Time elapsed: 00:10:58
                               ETA: 01:00:33

################################################################################
                     [1m Learning iteration 307/2000 [0m                      

                       Computation: 46218 steps/s (collection: 2.012s, learning 0.115s)
             Mean action noise std: 1.99
          Mean value_function loss: 7.6595
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 46.2888
                       Mean reward: 13.55
               Mean episode length: 130.18
    Episode_Reward/reaching_object: 0.3709
     Episode_Reward/lifting_object: 2.2671
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 28.8333
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 2.13s
                      Time elapsed: 00:11:00
                               ETA: 01:00:31

################################################################################
                     [1m Learning iteration 308/2000 [0m                      

                       Computation: 46162 steps/s (collection: 2.023s, learning 0.107s)
             Mean action noise std: 1.99
          Mean value_function loss: 7.8196
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 46.3061
                       Mean reward: 11.48
               Mean episode length: 126.09
    Episode_Reward/reaching_object: 0.3707
     Episode_Reward/lifting_object: 2.1968
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 30.6667
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 2.13s
                      Time elapsed: 00:11:02
                               ETA: 01:00:29

################################################################################
                     [1m Learning iteration 309/2000 [0m                      

                       Computation: 42133 steps/s (collection: 2.204s, learning 0.130s)
             Mean action noise std: 1.99
          Mean value_function loss: 10.3341
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 46.3241
                       Mean reward: 10.11
               Mean episode length: 129.27
    Episode_Reward/reaching_object: 0.3619
     Episode_Reward/lifting_object: 2.0570
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 31.4583
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 2.33s
                      Time elapsed: 00:11:05
                               ETA: 01:00:28

################################################################################
                     [1m Learning iteration 310/2000 [0m                      

                       Computation: 46555 steps/s (collection: 2.012s, learning 0.100s)
             Mean action noise std: 1.99
          Mean value_function loss: 8.8147
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 46.3392
                       Mean reward: 13.81
               Mean episode length: 134.34
    Episode_Reward/reaching_object: 0.3629
     Episode_Reward/lifting_object: 2.0521
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 33.9583
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 2.11s
                      Time elapsed: 00:11:07
                               ETA: 01:00:25

################################################################################
                     [1m Learning iteration 311/2000 [0m                      

                       Computation: 47895 steps/s (collection: 1.959s, learning 0.093s)
             Mean action noise std: 1.99
          Mean value_function loss: 11.0645
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 46.3523
                       Mean reward: 11.30
               Mean episode length: 119.98
    Episode_Reward/reaching_object: 0.3573
     Episode_Reward/lifting_object: 2.1085
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 30.9583
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 2.05s
                      Time elapsed: 00:11:09
                               ETA: 01:00:23

################################################################################
                     [1m Learning iteration 312/2000 [0m                      

                       Computation: 46822 steps/s (collection: 1.999s, learning 0.101s)
             Mean action noise std: 2.00
          Mean value_function loss: 10.2152
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 46.3708
                       Mean reward: 10.29
               Mean episode length: 126.55
    Episode_Reward/reaching_object: 0.3712
     Episode_Reward/lifting_object: 2.0631
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 31.4167
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 2.10s
                      Time elapsed: 00:11:11
                               ETA: 01:00:20

################################################################################
                     [1m Learning iteration 313/2000 [0m                      

                       Computation: 42998 steps/s (collection: 2.174s, learning 0.113s)
             Mean action noise std: 2.00
          Mean value_function loss: 12.0945
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 46.3912
                       Mean reward: 13.14
               Mean episode length: 127.35
    Episode_Reward/reaching_object: 0.3619
     Episode_Reward/lifting_object: 2.0612
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 31.5000
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 2.29s
                      Time elapsed: 00:11:13
                               ETA: 01:00:19

################################################################################
                     [1m Learning iteration 314/2000 [0m                      

                       Computation: 45157 steps/s (collection: 2.056s, learning 0.121s)
             Mean action noise std: 2.00
          Mean value_function loss: 12.6740
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 46.4093
                       Mean reward: 14.50
               Mean episode length: 128.45
    Episode_Reward/reaching_object: 0.3473
     Episode_Reward/lifting_object: 1.9086
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 30.3750
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 2.18s
                      Time elapsed: 00:11:15
                               ETA: 01:00:17

################################################################################
                     [1m Learning iteration 315/2000 [0m                      

                       Computation: 42556 steps/s (collection: 2.186s, learning 0.124s)
             Mean action noise std: 2.00
          Mean value_function loss: 8.8140
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 46.4225
                       Mean reward: 10.11
               Mean episode length: 118.81
    Episode_Reward/reaching_object: 0.3517
     Episode_Reward/lifting_object: 2.2379
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 33.4583
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 2.31s
                      Time elapsed: 00:11:18
                               ETA: 01:00:16

################################################################################
                     [1m Learning iteration 316/2000 [0m                      

                       Computation: 47689 steps/s (collection: 1.969s, learning 0.093s)
             Mean action noise std: 2.00
          Mean value_function loss: 9.9665
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 46.4385
                       Mean reward: 9.07
               Mean episode length: 126.97
    Episode_Reward/reaching_object: 0.3730
     Episode_Reward/lifting_object: 2.4321
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 33.4583
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 2.06s
                      Time elapsed: 00:11:20
                               ETA: 01:00:13

################################################################################
                     [1m Learning iteration 317/2000 [0m                      

                       Computation: 46479 steps/s (collection: 2.022s, learning 0.093s)
             Mean action noise std: 2.00
          Mean value_function loss: 9.5897
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 46.4494
                       Mean reward: 11.92
               Mean episode length: 117.03
    Episode_Reward/reaching_object: 0.3580
     Episode_Reward/lifting_object: 2.2878
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 32.7500
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 2.12s
                      Time elapsed: 00:11:22
                               ETA: 01:00:11

################################################################################
                     [1m Learning iteration 318/2000 [0m                      

                       Computation: 44100 steps/s (collection: 2.079s, learning 0.150s)
             Mean action noise std: 2.00
          Mean value_function loss: 18.4889
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 46.4602
                       Mean reward: 12.97
               Mean episode length: 118.72
    Episode_Reward/reaching_object: 0.3551
     Episode_Reward/lifting_object: 2.2581
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 33.7917
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 2.23s
                      Time elapsed: 00:11:24
                               ETA: 01:00:09

################################################################################
                     [1m Learning iteration 319/2000 [0m                      

                       Computation: 45135 steps/s (collection: 2.067s, learning 0.111s)
             Mean action noise std: 2.01
          Mean value_function loss: 11.2916
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 46.4792
                       Mean reward: 10.10
               Mean episode length: 126.75
    Episode_Reward/reaching_object: 0.3582
     Episode_Reward/lifting_object: 2.3996
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 31.5417
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 2.18s
                      Time elapsed: 00:11:26
                               ETA: 01:00:07

################################################################################
                     [1m Learning iteration 320/2000 [0m                      

                       Computation: 46745 steps/s (collection: 1.979s, learning 0.124s)
             Mean action noise std: 2.01
          Mean value_function loss: 11.9221
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 46.4985
                       Mean reward: 13.08
               Mean episode length: 127.86
    Episode_Reward/reaching_object: 0.3609
     Episode_Reward/lifting_object: 2.4443
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 34.5417
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 2.10s
                      Time elapsed: 00:11:28
                               ETA: 01:00:05

################################################################################
                     [1m Learning iteration 321/2000 [0m                      

                       Computation: 46759 steps/s (collection: 1.981s, learning 0.122s)
             Mean action noise std: 2.01
          Mean value_function loss: 12.2144
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 46.5101
                       Mean reward: 13.78
               Mean episode length: 113.27
    Episode_Reward/reaching_object: 0.3545
     Episode_Reward/lifting_object: 2.6477
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 33.7083
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 2.10s
                      Time elapsed: 00:11:30
                               ETA: 01:00:02

################################################################################
                     [1m Learning iteration 322/2000 [0m                      

                       Computation: 47295 steps/s (collection: 1.972s, learning 0.106s)
             Mean action noise std: 2.01
          Mean value_function loss: 10.3245
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 46.5234
                       Mean reward: 16.40
               Mean episode length: 109.01
    Episode_Reward/reaching_object: 0.3491
     Episode_Reward/lifting_object: 2.7545
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 34.9167
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 2.08s
                      Time elapsed: 00:11:33
                               ETA: 01:00:00

################################################################################
                     [1m Learning iteration 323/2000 [0m                      

                       Computation: 46321 steps/s (collection: 1.999s, learning 0.123s)
             Mean action noise std: 2.01
          Mean value_function loss: 13.1642
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 46.5386
                       Mean reward: 12.78
               Mean episode length: 111.41
    Episode_Reward/reaching_object: 0.3497
     Episode_Reward/lifting_object: 2.4765
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 35.9583
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 2.12s
                      Time elapsed: 00:11:35
                               ETA: 00:59:58

################################################################################
                     [1m Learning iteration 324/2000 [0m                      

                       Computation: 46849 steps/s (collection: 2.006s, learning 0.092s)
             Mean action noise std: 2.01
          Mean value_function loss: 14.0594
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 46.5564
                       Mean reward: 14.44
               Mean episode length: 106.69
    Episode_Reward/reaching_object: 0.3407
     Episode_Reward/lifting_object: 2.6269
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 35.9167
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 2.10s
                      Time elapsed: 00:11:37
                               ETA: 00:59:55

################################################################################
                     [1m Learning iteration 325/2000 [0m                      

                       Computation: 46753 steps/s (collection: 2.003s, learning 0.100s)
             Mean action noise std: 2.02
          Mean value_function loss: 14.6302
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 46.5722
                       Mean reward: 12.16
               Mean episode length: 114.24
    Episode_Reward/reaching_object: 0.3561
     Episode_Reward/lifting_object: 2.8316
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 37.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 2.10s
                      Time elapsed: 00:11:39
                               ETA: 00:59:53

################################################################################
                     [1m Learning iteration 326/2000 [0m                      

                       Computation: 46069 steps/s (collection: 2.037s, learning 0.097s)
             Mean action noise std: 2.02
          Mean value_function loss: 12.8615
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 46.5889
                       Mean reward: 15.05
               Mean episode length: 111.06
    Episode_Reward/reaching_object: 0.3168
     Episode_Reward/lifting_object: 2.5775
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 34.7917
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 2.13s
                      Time elapsed: 00:11:41
                               ETA: 00:59:51

################################################################################
                     [1m Learning iteration 327/2000 [0m                      

                       Computation: 46930 steps/s (collection: 1.997s, learning 0.098s)
             Mean action noise std: 2.02
          Mean value_function loss: 12.0227
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 46.6074
                       Mean reward: 15.13
               Mean episode length: 107.49
    Episode_Reward/reaching_object: 0.3189
     Episode_Reward/lifting_object: 2.6207
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 37.2500
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 2.09s
                      Time elapsed: 00:11:43
                               ETA: 00:59:48

################################################################################
                     [1m Learning iteration 328/2000 [0m                      

                       Computation: 46320 steps/s (collection: 2.027s, learning 0.095s)
             Mean action noise std: 2.02
          Mean value_function loss: 14.5776
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 46.6216
                       Mean reward: 17.08
               Mean episode length: 107.78
    Episode_Reward/reaching_object: 0.3220
     Episode_Reward/lifting_object: 2.8001
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 38.7500
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 2.12s
                      Time elapsed: 00:11:45
                               ETA: 00:59:46

################################################################################
                     [1m Learning iteration 329/2000 [0m                      

                       Computation: 43774 steps/s (collection: 2.114s, learning 0.132s)
             Mean action noise std: 2.02
          Mean value_function loss: 13.2057
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 46.6432
                       Mean reward: 17.54
               Mean episode length: 111.49
    Episode_Reward/reaching_object: 0.3206
     Episode_Reward/lifting_object: 2.7529
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 38.6250
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 2.25s
                      Time elapsed: 00:11:47
                               ETA: 00:59:44

################################################################################
                     [1m Learning iteration 330/2000 [0m                      

                       Computation: 46664 steps/s (collection: 2.016s, learning 0.091s)
             Mean action noise std: 2.02
          Mean value_function loss: 14.9032
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 46.6634
                       Mean reward: 14.51
               Mean episode length: 102.06
    Episode_Reward/reaching_object: 0.3055
     Episode_Reward/lifting_object: 2.6286
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 37.8333
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 2.11s
                      Time elapsed: 00:11:50
                               ETA: 00:59:42

################################################################################
                     [1m Learning iteration 331/2000 [0m                      

                       Computation: 44747 steps/s (collection: 2.050s, learning 0.147s)
             Mean action noise std: 2.03
          Mean value_function loss: 21.8992
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 46.6792
                       Mean reward: 14.06
               Mean episode length: 103.73
    Episode_Reward/reaching_object: 0.3048
     Episode_Reward/lifting_object: 2.6773
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 36.8333
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 2.20s
                      Time elapsed: 00:11:52
                               ETA: 00:59:40

################################################################################
                     [1m Learning iteration 332/2000 [0m                      

                       Computation: 45475 steps/s (collection: 2.049s, learning 0.113s)
             Mean action noise std: 2.03
          Mean value_function loss: 19.7955
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 46.6946
                       Mean reward: 14.92
               Mean episode length: 105.76
    Episode_Reward/reaching_object: 0.3171
     Episode_Reward/lifting_object: 2.4441
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 35.5833
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 2.16s
                      Time elapsed: 00:11:54
                               ETA: 00:59:38

################################################################################
                     [1m Learning iteration 333/2000 [0m                      

                       Computation: 18890 steps/s (collection: 5.089s, learning 0.115s)
             Mean action noise std: 2.03
          Mean value_function loss: 14.0720
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 46.7093
                       Mean reward: 19.58
               Mean episode length: 116.17
    Episode_Reward/reaching_object: 0.3128
     Episode_Reward/lifting_object: 2.6079
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 36.7083
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 5.20s
                      Time elapsed: 00:11:59
                               ETA: 00:59:51

################################################################################
                     [1m Learning iteration 334/2000 [0m                      

                       Computation: 13938 steps/s (collection: 6.918s, learning 0.135s)
             Mean action noise std: 2.03
          Mean value_function loss: 20.8792
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 46.7194
                       Mean reward: 15.43
               Mean episode length: 106.79
    Episode_Reward/reaching_object: 0.3144
     Episode_Reward/lifting_object: 2.7074
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 38.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 7.05s
                      Time elapsed: 00:12:06
                               ETA: 01:00:13

################################################################################
                     [1m Learning iteration 335/2000 [0m                      

                       Computation: 14148 steps/s (collection: 6.832s, learning 0.116s)
             Mean action noise std: 2.03
          Mean value_function loss: 16.5567
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 46.7301
                       Mean reward: 18.92
               Mean episode length: 119.08
    Episode_Reward/reaching_object: 0.3211
     Episode_Reward/lifting_object: 2.9067
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 37.9167
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 6.95s
                      Time elapsed: 00:12:13
                               ETA: 01:00:35

################################################################################
                     [1m Learning iteration 336/2000 [0m                      

                       Computation: 14437 steps/s (collection: 6.672s, learning 0.137s)
             Mean action noise std: 2.03
          Mean value_function loss: 16.4165
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 46.7355
                       Mean reward: 13.91
               Mean episode length: 107.61
    Episode_Reward/reaching_object: 0.3186
     Episode_Reward/lifting_object: 2.8545
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 35.3333
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 6.81s
                      Time elapsed: 00:12:20
                               ETA: 01:00:56

################################################################################
                     [1m Learning iteration 337/2000 [0m                      

                       Computation: 14082 steps/s (collection: 6.854s, learning 0.127s)
             Mean action noise std: 2.03
          Mean value_function loss: 19.7540
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 46.7439
                       Mean reward: 17.61
               Mean episode length: 107.78
    Episode_Reward/reaching_object: 0.3210
     Episode_Reward/lifting_object: 3.2586
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 35.5417
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 6.98s
                      Time elapsed: 00:12:27
                               ETA: 01:01:17

################################################################################
                     [1m Learning iteration 338/2000 [0m                      

                       Computation: 14073 steps/s (collection: 6.844s, learning 0.141s)
             Mean action noise std: 2.03
          Mean value_function loss: 15.3034
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 46.7474
                       Mean reward: 13.34
               Mean episode length: 113.54
    Episode_Reward/reaching_object: 0.3228
     Episode_Reward/lifting_object: 2.8519
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 35.5417
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 6.99s
                      Time elapsed: 00:12:34
                               ETA: 01:01:38

################################################################################
                     [1m Learning iteration 339/2000 [0m                      

                       Computation: 14150 steps/s (collection: 6.827s, learning 0.120s)
             Mean action noise std: 2.03
          Mean value_function loss: 18.6907
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 46.7523
                       Mean reward: 15.52
               Mean episode length: 108.59
    Episode_Reward/reaching_object: 0.3241
     Episode_Reward/lifting_object: 2.9224
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 33.8750
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 6.95s
                      Time elapsed: 00:12:41
                               ETA: 01:01:59

################################################################################
                     [1m Learning iteration 340/2000 [0m                      

                       Computation: 14267 steps/s (collection: 6.761s, learning 0.130s)
             Mean action noise std: 2.03
          Mean value_function loss: 13.8178
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 46.7591
                       Mean reward: 16.34
               Mean episode length: 105.42
    Episode_Reward/reaching_object: 0.3288
     Episode_Reward/lifting_object: 3.0794
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 37.7917
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 6.89s
                      Time elapsed: 00:12:48
                               ETA: 01:02:19

################################################################################
                     [1m Learning iteration 341/2000 [0m                      

                       Computation: 12944 steps/s (collection: 7.479s, learning 0.116s)
             Mean action noise std: 2.04
          Mean value_function loss: 16.8650
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 46.7744
                       Mean reward: 16.65
               Mean episode length: 113.46
    Episode_Reward/reaching_object: 0.3354
     Episode_Reward/lifting_object: 3.0925
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 36.1250
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 7.59s
                      Time elapsed: 00:12:55
                               ETA: 01:02:43

################################################################################
                     [1m Learning iteration 342/2000 [0m                      

                       Computation: 47518 steps/s (collection: 1.968s, learning 0.101s)
             Mean action noise std: 2.04
          Mean value_function loss: 15.1788
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 46.7876
                       Mean reward: 13.60
               Mean episode length: 116.58
    Episode_Reward/reaching_object: 0.3381
     Episode_Reward/lifting_object: 3.2946
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 37.9167
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 2.07s
                      Time elapsed: 00:12:57
                               ETA: 01:02:40

################################################################################
                     [1m Learning iteration 343/2000 [0m                      

                       Computation: 49949 steps/s (collection: 1.874s, learning 0.094s)
             Mean action noise std: 2.04
          Mean value_function loss: 17.1181
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 46.7905
                       Mean reward: 22.06
               Mean episode length: 111.98
    Episode_Reward/reaching_object: 0.3342
     Episode_Reward/lifting_object: 3.5213
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 37.1250
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 1.97s
                      Time elapsed: 00:12:59
                               ETA: 01:02:36

################################################################################
                     [1m Learning iteration 344/2000 [0m                      

                       Computation: 48168 steps/s (collection: 1.933s, learning 0.108s)
             Mean action noise std: 2.04
          Mean value_function loss: 15.0546
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 46.7916
                       Mean reward: 14.95
               Mean episode length: 102.64
    Episode_Reward/reaching_object: 0.3245
     Episode_Reward/lifting_object: 3.2175
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 38.5000
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 2.04s
                      Time elapsed: 00:13:01
                               ETA: 01:02:33

################################################################################
                     [1m Learning iteration 345/2000 [0m                      

                       Computation: 48964 steps/s (collection: 1.918s, learning 0.090s)
             Mean action noise std: 2.04
          Mean value_function loss: 15.9546
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 46.7948
                       Mean reward: 20.84
               Mean episode length: 107.28
    Episode_Reward/reaching_object: 0.3336
     Episode_Reward/lifting_object: 3.3806
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 39.8750
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 2.01s
                      Time elapsed: 00:13:03
                               ETA: 01:02:29

################################################################################
                     [1m Learning iteration 346/2000 [0m                      

                       Computation: 49720 steps/s (collection: 1.892s, learning 0.085s)
             Mean action noise std: 2.04
          Mean value_function loss: 16.6519
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 46.8010
                       Mean reward: 21.49
               Mean episode length: 107.77
    Episode_Reward/reaching_object: 0.3250
     Episode_Reward/lifting_object: 3.4719
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 41.2500
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 1.98s
                      Time elapsed: 00:13:05
                               ETA: 01:02:26

################################################################################
                     [1m Learning iteration 347/2000 [0m                      

                       Computation: 49125 steps/s (collection: 1.898s, learning 0.104s)
             Mean action noise std: 2.04
          Mean value_function loss: 23.2934
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 46.8091
                       Mean reward: 17.93
               Mean episode length: 105.91
    Episode_Reward/reaching_object: 0.3185
     Episode_Reward/lifting_object: 3.3151
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 40.0417
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 2.00s
                      Time elapsed: 00:13:07
                               ETA: 01:02:22

################################################################################
                     [1m Learning iteration 348/2000 [0m                      

                       Computation: 49860 steps/s (collection: 1.875s, learning 0.097s)
             Mean action noise std: 2.04
          Mean value_function loss: 21.1253
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 46.8191
                       Mean reward: 14.70
               Mean episode length: 100.84
    Episode_Reward/reaching_object: 0.3107
     Episode_Reward/lifting_object: 3.1050
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 40.5833
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 1.97s
                      Time elapsed: 00:13:09
                               ETA: 01:02:18

################################################################################
                     [1m Learning iteration 349/2000 [0m                      

                       Computation: 49284 steps/s (collection: 1.882s, learning 0.113s)
             Mean action noise std: 2.04
          Mean value_function loss: 19.6319
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 46.8296
                       Mean reward: 18.73
               Mean episode length: 97.62
    Episode_Reward/reaching_object: 0.3089
     Episode_Reward/lifting_object: 3.3200
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 40.2500
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 1.99s
                      Time elapsed: 00:13:11
                               ETA: 01:02:15

################################################################################
                     [1m Learning iteration 350/2000 [0m                      

                       Computation: 46644 steps/s (collection: 2.003s, learning 0.104s)
             Mean action noise std: 2.04
          Mean value_function loss: 38.2571
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 46.8400
                       Mean reward: 17.55
               Mean episode length: 100.63
    Episode_Reward/reaching_object: 0.3064
     Episode_Reward/lifting_object: 3.2597
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 39.2083
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 2.11s
                      Time elapsed: 00:13:13
                               ETA: 01:02:12

################################################################################
                     [1m Learning iteration 351/2000 [0m                      

                       Computation: 49448 steps/s (collection: 1.890s, learning 0.098s)
             Mean action noise std: 2.04
          Mean value_function loss: 17.9106
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 46.8544
                       Mean reward: 21.81
               Mean episode length: 101.92
    Episode_Reward/reaching_object: 0.3057
     Episode_Reward/lifting_object: 3.0324
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 41.0833
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 1.99s
                      Time elapsed: 00:13:15
                               ETA: 01:02:08

################################################################################
                     [1m Learning iteration 352/2000 [0m                      

                       Computation: 49206 steps/s (collection: 1.902s, learning 0.096s)
             Mean action noise std: 2.04
          Mean value_function loss: 20.6695
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 46.8624
                       Mean reward: 18.56
               Mean episode length: 95.39
    Episode_Reward/reaching_object: 0.3027
     Episode_Reward/lifting_object: 3.2978
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 39.7500
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 2.00s
                      Time elapsed: 00:13:17
                               ETA: 01:02:05

################################################################################
                     [1m Learning iteration 353/2000 [0m                      

                       Computation: 49240 steps/s (collection: 1.910s, learning 0.086s)
             Mean action noise std: 2.04
          Mean value_function loss: 19.9650
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 46.8665
                       Mean reward: 21.00
               Mean episode length: 100.21
    Episode_Reward/reaching_object: 0.2992
     Episode_Reward/lifting_object: 3.2270
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 42.4167
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 2.00s
                      Time elapsed: 00:13:19
                               ETA: 01:02:01

################################################################################
                     [1m Learning iteration 354/2000 [0m                      

                       Computation: 49297 steps/s (collection: 1.908s, learning 0.087s)
             Mean action noise std: 2.04
          Mean value_function loss: 17.7401
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 46.8729
                       Mean reward: 20.05
               Mean episode length: 100.90
    Episode_Reward/reaching_object: 0.3034
     Episode_Reward/lifting_object: 3.5302
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 41.8750
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 1.99s
                      Time elapsed: 00:13:21
                               ETA: 01:01:58

################################################################################
                     [1m Learning iteration 355/2000 [0m                      

                       Computation: 48118 steps/s (collection: 1.951s, learning 0.092s)
             Mean action noise std: 2.05
          Mean value_function loss: 17.8589
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 46.8825
                       Mean reward: 16.21
               Mean episode length: 98.68
    Episode_Reward/reaching_object: 0.2929
     Episode_Reward/lifting_object: 3.2756
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 42.7083
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 2.04s
                      Time elapsed: 00:13:24
                               ETA: 01:01:55

################################################################################
                     [1m Learning iteration 356/2000 [0m                      

                       Computation: 49200 steps/s (collection: 1.909s, learning 0.089s)
             Mean action noise std: 2.05
          Mean value_function loss: 18.6850
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 46.8880
                       Mean reward: 16.87
               Mean episode length: 90.80
    Episode_Reward/reaching_object: 0.2870
     Episode_Reward/lifting_object: 3.2181
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 41.3750
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 2.00s
                      Time elapsed: 00:13:26
                               ETA: 01:01:51

################################################################################
                     [1m Learning iteration 357/2000 [0m                      

                       Computation: 48883 steps/s (collection: 1.923s, learning 0.088s)
             Mean action noise std: 2.05
          Mean value_function loss: 30.4151
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 46.8931
                       Mean reward: 20.82
               Mean episode length: 102.69
    Episode_Reward/reaching_object: 0.2881
     Episode_Reward/lifting_object: 3.3538
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 44.5833
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 2.01s
                      Time elapsed: 00:13:28
                               ETA: 01:01:48

################################################################################
                     [1m Learning iteration 358/2000 [0m                      

                       Computation: 47760 steps/s (collection: 1.963s, learning 0.095s)
             Mean action noise std: 2.05
          Mean value_function loss: 24.3927
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 46.9030
                       Mean reward: 14.79
               Mean episode length: 92.84
    Episode_Reward/reaching_object: 0.2869
     Episode_Reward/lifting_object: 3.1010
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 41.0417
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 2.06s
                      Time elapsed: 00:13:30
                               ETA: 01:01:45

################################################################################
                     [1m Learning iteration 359/2000 [0m                      

                       Computation: 47921 steps/s (collection: 1.943s, learning 0.108s)
             Mean action noise std: 2.05
          Mean value_function loss: 25.4998
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 46.9145
                       Mean reward: 23.63
               Mean episode length: 94.28
    Episode_Reward/reaching_object: 0.2928
     Episode_Reward/lifting_object: 3.7048
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 42.6250
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 2.05s
                      Time elapsed: 00:13:32
                               ETA: 01:01:41

################################################################################
                     [1m Learning iteration 360/2000 [0m                      

                       Computation: 46585 steps/s (collection: 2.020s, learning 0.090s)
             Mean action noise std: 2.05
          Mean value_function loss: 20.9808
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 46.9261
                       Mean reward: 20.21
               Mean episode length: 99.62
    Episode_Reward/reaching_object: 0.2874
     Episode_Reward/lifting_object: 3.5422
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 42.5417
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 2.11s
                      Time elapsed: 00:13:34
                               ETA: 01:01:39

################################################################################
                     [1m Learning iteration 361/2000 [0m                      

                       Computation: 48092 steps/s (collection: 1.959s, learning 0.086s)
             Mean action noise std: 2.05
          Mean value_function loss: 24.0684
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 46.9354
                       Mean reward: 16.35
               Mean episode length: 96.87
    Episode_Reward/reaching_object: 0.2887
     Episode_Reward/lifting_object: 3.4122
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 42.7917
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 2.04s
                      Time elapsed: 00:13:36
                               ETA: 01:01:35

################################################################################
                     [1m Learning iteration 362/2000 [0m                      

                       Computation: 47884 steps/s (collection: 1.960s, learning 0.093s)
             Mean action noise std: 2.05
          Mean value_function loss: 20.5977
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 46.9435
                       Mean reward: 20.07
               Mean episode length: 99.91
    Episode_Reward/reaching_object: 0.2907
     Episode_Reward/lifting_object: 3.2201
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 41.7917
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 2.05s
                      Time elapsed: 00:13:38
                               ETA: 01:01:32

################################################################################
                     [1m Learning iteration 363/2000 [0m                      

                       Computation: 45737 steps/s (collection: 2.024s, learning 0.126s)
             Mean action noise std: 2.05
          Mean value_function loss: 28.1794
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 46.9478
                       Mean reward: 23.05
               Mean episode length: 92.56
    Episode_Reward/reaching_object: 0.2969
     Episode_Reward/lifting_object: 3.7972
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 43.8333
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 2.15s
                      Time elapsed: 00:13:40
                               ETA: 01:01:29

################################################################################
                     [1m Learning iteration 364/2000 [0m                      

                       Computation: 44787 steps/s (collection: 2.049s, learning 0.146s)
             Mean action noise std: 2.05
          Mean value_function loss: 19.4920
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 46.9494
                       Mean reward: 23.31
               Mean episode length: 94.85
    Episode_Reward/reaching_object: 0.2848
     Episode_Reward/lifting_object: 3.5854
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 40.8333
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 2.19s
                      Time elapsed: 00:13:42
                               ETA: 01:01:27

################################################################################
                     [1m Learning iteration 365/2000 [0m                      

                       Computation: 47564 steps/s (collection: 1.972s, learning 0.095s)
             Mean action noise std: 2.05
          Mean value_function loss: 25.3655
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 46.9499
                       Mean reward: 14.93
               Mean episode length: 98.93
    Episode_Reward/reaching_object: 0.2986
     Episode_Reward/lifting_object: 3.5496
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 44.3333
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 2.07s
                      Time elapsed: 00:13:44
                               ETA: 01:01:24

################################################################################
                     [1m Learning iteration 366/2000 [0m                      

                       Computation: 44540 steps/s (collection: 2.114s, learning 0.093s)
             Mean action noise std: 2.05
          Mean value_function loss: 18.7706
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 46.9502
                       Mean reward: 19.15
               Mean episode length: 91.22
    Episode_Reward/reaching_object: 0.2901
     Episode_Reward/lifting_object: 3.8876
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 43.0417
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 2.21s
                      Time elapsed: 00:13:46
                               ETA: 01:01:21

################################################################################
                     [1m Learning iteration 367/2000 [0m                      

                       Computation: 48350 steps/s (collection: 1.929s, learning 0.104s)
             Mean action noise std: 2.05
          Mean value_function loss: 24.2369
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 46.9518
                       Mean reward: 18.40
               Mean episode length: 90.70
    Episode_Reward/reaching_object: 0.2929
     Episode_Reward/lifting_object: 3.6328
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 43.7917
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 2.03s
                      Time elapsed: 00:13:48
                               ETA: 01:01:18

################################################################################
                     [1m Learning iteration 368/2000 [0m                      

                       Computation: 49184 steps/s (collection: 1.908s, learning 0.091s)
             Mean action noise std: 2.05
          Mean value_function loss: 28.8891
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 46.9547
                       Mean reward: 20.46
               Mean episode length: 92.90
    Episode_Reward/reaching_object: 0.2852
     Episode_Reward/lifting_object: 3.6488
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 46.4583
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 2.00s
                      Time elapsed: 00:13:50
                               ETA: 01:01:15

################################################################################
                     [1m Learning iteration 369/2000 [0m                      

                       Computation: 44811 steps/s (collection: 2.081s, learning 0.113s)
             Mean action noise std: 2.05
          Mean value_function loss: 26.7449
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 46.9564
                       Mean reward: 17.21
               Mean episode length: 83.16
    Episode_Reward/reaching_object: 0.2843
     Episode_Reward/lifting_object: 3.5796
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 43.8750
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 2.19s
                      Time elapsed: 00:13:53
                               ETA: 01:01:12

################################################################################
                     [1m Learning iteration 370/2000 [0m                      

                       Computation: 46431 steps/s (collection: 1.940s, learning 0.177s)
             Mean action noise std: 2.05
          Mean value_function loss: 27.4705
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 46.9601
                       Mean reward: 20.49
               Mean episode length: 90.45
    Episode_Reward/reaching_object: 0.2918
     Episode_Reward/lifting_object: 3.7338
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 44.2083
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 2.12s
                      Time elapsed: 00:13:55
                               ETA: 01:01:09

################################################################################
                     [1m Learning iteration 371/2000 [0m                      

                       Computation: 45813 steps/s (collection: 2.049s, learning 0.097s)
             Mean action noise std: 2.05
          Mean value_function loss: 21.7355
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 46.9645
                       Mean reward: 17.03
               Mean episode length: 89.25
    Episode_Reward/reaching_object: 0.2881
     Episode_Reward/lifting_object: 3.5745
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 44.3333
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 2.15s
                      Time elapsed: 00:13:57
                               ETA: 01:01:07

################################################################################
                     [1m Learning iteration 372/2000 [0m                      

                       Computation: 45266 steps/s (collection: 1.989s, learning 0.182s)
             Mean action noise std: 2.05
          Mean value_function loss: 28.5202
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 46.9717
                       Mean reward: 23.23
               Mean episode length: 94.07
    Episode_Reward/reaching_object: 0.2916
     Episode_Reward/lifting_object: 3.9263
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 44.8333
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 2.17s
                      Time elapsed: 00:13:59
                               ETA: 01:01:04

################################################################################
                     [1m Learning iteration 373/2000 [0m                      

                       Computation: 47486 steps/s (collection: 1.973s, learning 0.097s)
             Mean action noise std: 2.05
          Mean value_function loss: 30.5091
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 46.9802
                       Mean reward: 25.33
               Mean episode length: 94.48
    Episode_Reward/reaching_object: 0.3012
     Episode_Reward/lifting_object: 4.0196
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 44.0417
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 2.07s
                      Time elapsed: 00:14:01
                               ETA: 01:01:01

################################################################################
                     [1m Learning iteration 374/2000 [0m                      

                       Computation: 47928 steps/s (collection: 1.952s, learning 0.100s)
             Mean action noise std: 2.06
          Mean value_function loss: 27.6117
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 46.9843
                       Mean reward: 23.55
               Mean episode length: 96.12
    Episode_Reward/reaching_object: 0.2971
     Episode_Reward/lifting_object: 3.9260
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 43.2083
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 2.05s
                      Time elapsed: 00:14:03
                               ETA: 01:00:58

################################################################################
                     [1m Learning iteration 375/2000 [0m                      

                       Computation: 47384 steps/s (collection: 1.961s, learning 0.114s)
             Mean action noise std: 2.06
          Mean value_function loss: 23.1601
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 46.9938
                       Mean reward: 19.30
               Mean episode length: 88.08
    Episode_Reward/reaching_object: 0.3001
     Episode_Reward/lifting_object: 3.7753
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 42.3333
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 2.07s
                      Time elapsed: 00:14:05
                               ETA: 01:00:55

################################################################################
                     [1m Learning iteration 376/2000 [0m                      

                       Computation: 45852 steps/s (collection: 2.051s, learning 0.093s)
             Mean action noise std: 2.06
          Mean value_function loss: 32.2508
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 47.0058
                       Mean reward: 18.51
               Mean episode length: 86.11
    Episode_Reward/reaching_object: 0.3070
     Episode_Reward/lifting_object: 4.0067
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 44.3333
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 2.14s
                      Time elapsed: 00:14:07
                               ETA: 01:00:52

################################################################################
                     [1m Learning iteration 377/2000 [0m                      

                       Computation: 48445 steps/s (collection: 1.941s, learning 0.089s)
             Mean action noise std: 2.06
          Mean value_function loss: 23.2019
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 47.0137
                       Mean reward: 21.59
               Mean episode length: 86.80
    Episode_Reward/reaching_object: 0.3067
     Episode_Reward/lifting_object: 4.0493
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 45.1667
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 2.03s
                      Time elapsed: 00:14:09
                               ETA: 01:00:49

################################################################################
                     [1m Learning iteration 378/2000 [0m                      

                       Computation: 45434 steps/s (collection: 2.008s, learning 0.156s)
             Mean action noise std: 2.06
          Mean value_function loss: 23.8300
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 47.0199
                       Mean reward: 24.17
               Mean episode length: 93.97
    Episode_Reward/reaching_object: 0.3112
     Episode_Reward/lifting_object: 4.3262
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 42.6667
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 2.16s
                      Time elapsed: 00:14:12
                               ETA: 01:00:46

################################################################################
                     [1m Learning iteration 379/2000 [0m                      

                       Computation: 45230 steps/s (collection: 2.059s, learning 0.115s)
             Mean action noise std: 2.06
          Mean value_function loss: 21.2518
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.0240
                       Mean reward: 22.00
               Mean episode length: 92.93
    Episode_Reward/reaching_object: 0.3164
     Episode_Reward/lifting_object: 4.3887
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 43.5833
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 2.17s
                      Time elapsed: 00:14:14
                               ETA: 01:00:44

################################################################################
                     [1m Learning iteration 380/2000 [0m                      

                       Computation: 47790 steps/s (collection: 1.945s, learning 0.112s)
             Mean action noise std: 2.06
          Mean value_function loss: 26.0201
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 47.0301
                       Mean reward: 22.52
               Mean episode length: 95.15
    Episode_Reward/reaching_object: 0.3109
     Episode_Reward/lifting_object: 4.3974
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 43.2083
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 2.06s
                      Time elapsed: 00:14:16
                               ETA: 01:00:41

################################################################################
                     [1m Learning iteration 381/2000 [0m                      

                       Computation: 44062 steps/s (collection: 2.122s, learning 0.109s)
             Mean action noise std: 2.06
          Mean value_function loss: 29.3454
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 47.0351
                       Mean reward: 23.10
               Mean episode length: 95.64
    Episode_Reward/reaching_object: 0.3098
     Episode_Reward/lifting_object: 4.3632
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 44.7500
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 2.23s
                      Time elapsed: 00:14:18
                               ETA: 01:00:38

################################################################################
                     [1m Learning iteration 382/2000 [0m                      

                       Computation: 46963 steps/s (collection: 1.959s, learning 0.134s)
             Mean action noise std: 2.06
          Mean value_function loss: 26.0932
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.0405
                       Mean reward: 24.15
               Mean episode length: 93.68
    Episode_Reward/reaching_object: 0.3104
     Episode_Reward/lifting_object: 4.4228
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 46.2500
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 2.09s
                      Time elapsed: 00:14:20
                               ETA: 01:00:36

################################################################################
                     [1m Learning iteration 383/2000 [0m                      

                       Computation: 46531 steps/s (collection: 2.024s, learning 0.089s)
             Mean action noise std: 2.06
          Mean value_function loss: 32.2702
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 47.0465
                       Mean reward: 22.41
               Mean episode length: 90.43
    Episode_Reward/reaching_object: 0.2986
     Episode_Reward/lifting_object: 4.3152
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 42.9167
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 2.11s
                      Time elapsed: 00:14:22
                               ETA: 01:00:33

################################################################################
                     [1m Learning iteration 384/2000 [0m                      

                       Computation: 45500 steps/s (collection: 2.030s, learning 0.131s)
             Mean action noise std: 2.06
          Mean value_function loss: 25.7748
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 47.0490
                       Mean reward: 24.93
               Mean episode length: 96.05
    Episode_Reward/reaching_object: 0.3087
     Episode_Reward/lifting_object: 4.4951
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 43.3333
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 2.16s
                      Time elapsed: 00:14:24
                               ETA: 01:00:30

################################################################################
                     [1m Learning iteration 385/2000 [0m                      

                       Computation: 46348 steps/s (collection: 2.034s, learning 0.087s)
             Mean action noise std: 2.06
          Mean value_function loss: 26.5095
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 47.0502
                       Mean reward: 25.97
               Mean episode length: 86.59
    Episode_Reward/reaching_object: 0.3009
     Episode_Reward/lifting_object: 4.6093
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 44.3750
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 2.12s
                      Time elapsed: 00:14:27
                               ETA: 01:00:27

################################################################################
                     [1m Learning iteration 386/2000 [0m                      

                       Computation: 46607 steps/s (collection: 2.016s, learning 0.094s)
             Mean action noise std: 2.06
          Mean value_function loss: 30.6184
               Mean surrogate loss: 0.0102
                 Mean entropy loss: 47.0509
                       Mean reward: 22.55
               Mean episode length: 91.97
    Episode_Reward/reaching_object: 0.3039
     Episode_Reward/lifting_object: 4.5178
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 47.0417
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 2.11s
                      Time elapsed: 00:14:29
                               ETA: 01:00:25

################################################################################
                     [1m Learning iteration 387/2000 [0m                      

                       Computation: 47974 steps/s (collection: 1.952s, learning 0.097s)
             Mean action noise std: 2.06
          Mean value_function loss: 34.5595
               Mean surrogate loss: 0.0100
                 Mean entropy loss: 47.0510
                       Mean reward: 28.71
               Mean episode length: 92.90
    Episode_Reward/reaching_object: 0.2999
     Episode_Reward/lifting_object: 4.7192
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 46.6250
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 2.05s
                      Time elapsed: 00:14:31
                               ETA: 01:00:21

################################################################################
                     [1m Learning iteration 388/2000 [0m                      

                       Computation: 46421 steps/s (collection: 2.006s, learning 0.112s)
             Mean action noise std: 2.06
          Mean value_function loss: 25.6933
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 47.0512
                       Mean reward: 21.26
               Mean episode length: 91.18
    Episode_Reward/reaching_object: 0.2941
     Episode_Reward/lifting_object: 4.5152
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 44.2917
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 2.12s
                      Time elapsed: 00:14:33
                               ETA: 01:00:19

################################################################################
                     [1m Learning iteration 389/2000 [0m                      

                       Computation: 47408 steps/s (collection: 1.982s, learning 0.092s)
             Mean action noise std: 2.06
          Mean value_function loss: 36.8691
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 47.0536
                       Mean reward: 25.92
               Mean episode length: 89.18
    Episode_Reward/reaching_object: 0.2982
     Episode_Reward/lifting_object: 4.5683
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 47.0833
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 2.07s
                      Time elapsed: 00:14:35
                               ETA: 01:00:16

################################################################################
                     [1m Learning iteration 390/2000 [0m                      

                       Computation: 47573 steps/s (collection: 1.970s, learning 0.097s)
             Mean action noise std: 2.06
          Mean value_function loss: 27.7042
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 47.0619
                       Mean reward: 24.31
               Mean episode length: 83.37
    Episode_Reward/reaching_object: 0.2982
     Episode_Reward/lifting_object: 4.5921
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 44.2917
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 2.07s
                      Time elapsed: 00:14:37
                               ETA: 01:00:13

################################################################################
                     [1m Learning iteration 391/2000 [0m                      

                       Computation: 45920 steps/s (collection: 2.023s, learning 0.118s)
             Mean action noise std: 2.06
          Mean value_function loss: 31.7250
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 47.0715
                       Mean reward: 21.48
               Mean episode length: 87.76
    Episode_Reward/reaching_object: 0.2966
     Episode_Reward/lifting_object: 4.6611
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 44.3750
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 2.14s
                      Time elapsed: 00:14:39
                               ETA: 01:00:10

################################################################################
                     [1m Learning iteration 392/2000 [0m                      

                       Computation: 45521 steps/s (collection: 2.033s, learning 0.126s)
             Mean action noise std: 2.06
          Mean value_function loss: 25.6734
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 47.0781
                       Mean reward: 23.25
               Mean episode length: 83.29
    Episode_Reward/reaching_object: 0.2868
     Episode_Reward/lifting_object: 4.2490
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 44.9583
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 2.16s
                      Time elapsed: 00:14:41
                               ETA: 01:00:08

################################################################################
                     [1m Learning iteration 393/2000 [0m                      

                       Computation: 46271 steps/s (collection: 1.987s, learning 0.138s)
             Mean action noise std: 2.07
          Mean value_function loss: 41.1349
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 47.0853
                       Mean reward: 20.26
               Mean episode length: 86.46
    Episode_Reward/reaching_object: 0.2987
     Episode_Reward/lifting_object: 4.6510
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 45.9583
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 2.12s
                      Time elapsed: 00:14:43
                               ETA: 01:00:05

################################################################################
                     [1m Learning iteration 394/2000 [0m                      

                       Computation: 46292 steps/s (collection: 1.991s, learning 0.132s)
             Mean action noise std: 2.07
          Mean value_function loss: 31.5601
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 47.0947
                       Mean reward: 25.52
               Mean episode length: 86.20
    Episode_Reward/reaching_object: 0.2943
     Episode_Reward/lifting_object: 4.6698
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 46.1667
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 2.12s
                      Time elapsed: 00:14:46
                               ETA: 01:00:02

################################################################################
                     [1m Learning iteration 395/2000 [0m                      

                       Computation: 47596 steps/s (collection: 1.973s, learning 0.093s)
             Mean action noise std: 2.07
          Mean value_function loss: 45.1383
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 47.0985
                       Mean reward: 23.56
               Mean episode length: 89.37
    Episode_Reward/reaching_object: 0.3007
     Episode_Reward/lifting_object: 4.9727
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 44.1250
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 2.07s
                      Time elapsed: 00:14:48
                               ETA: 00:59:59

################################################################################
                     [1m Learning iteration 396/2000 [0m                      

                       Computation: 46532 steps/s (collection: 1.998s, learning 0.115s)
             Mean action noise std: 2.07
          Mean value_function loss: 39.0125
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 47.1035
                       Mean reward: 25.43
               Mean episode length: 83.69
    Episode_Reward/reaching_object: 0.2856
     Episode_Reward/lifting_object: 4.6790
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 46.5000
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 2.11s
                      Time elapsed: 00:14:50
                               ETA: 00:59:56

################################################################################
                     [1m Learning iteration 397/2000 [0m                      

                       Computation: 47173 steps/s (collection: 1.986s, learning 0.098s)
             Mean action noise std: 2.07
          Mean value_function loss: 39.9700
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 47.1117
                       Mean reward: 28.72
               Mean episode length: 94.67
    Episode_Reward/reaching_object: 0.2975
     Episode_Reward/lifting_object: 4.9937
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 45.3333
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 2.08s
                      Time elapsed: 00:14:52
                               ETA: 00:59:53

################################################################################
                     [1m Learning iteration 398/2000 [0m                      

                       Computation: 46745 steps/s (collection: 1.995s, learning 0.108s)
             Mean action noise std: 2.07
          Mean value_function loss: 36.5432
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 47.1217
                       Mean reward: 22.93
               Mean episode length: 93.74
    Episode_Reward/reaching_object: 0.2808
     Episode_Reward/lifting_object: 4.9061
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 45.5417
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 2.10s
                      Time elapsed: 00:14:54
                               ETA: 00:59:51

################################################################################
                     [1m Learning iteration 399/2000 [0m                      

                       Computation: 46371 steps/s (collection: 2.029s, learning 0.091s)
             Mean action noise std: 2.07
          Mean value_function loss: 35.8662
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 47.1301
                       Mean reward: 24.38
               Mean episode length: 85.35
    Episode_Reward/reaching_object: 0.2889
     Episode_Reward/lifting_object: 4.7759
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 45.6667
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 2.12s
                      Time elapsed: 00:14:56
                               ETA: 00:59:48

################################################################################
                     [1m Learning iteration 400/2000 [0m                      

                       Computation: 46244 steps/s (collection: 1.997s, learning 0.129s)
             Mean action noise std: 2.07
          Mean value_function loss: 34.2471
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 47.1359
                       Mean reward: 25.91
               Mean episode length: 86.19
    Episode_Reward/reaching_object: 0.2850
     Episode_Reward/lifting_object: 4.8852
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 46.1667
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 2.13s
                      Time elapsed: 00:14:58
                               ETA: 00:59:45

################################################################################
                     [1m Learning iteration 401/2000 [0m                      

                       Computation: 45619 steps/s (collection: 2.037s, learning 0.118s)
             Mean action noise std: 2.07
          Mean value_function loss: 31.3761
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 47.1394
                       Mean reward: 25.21
               Mean episode length: 84.16
    Episode_Reward/reaching_object: 0.2782
     Episode_Reward/lifting_object: 4.7665
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 47.3750
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 2.15s
                      Time elapsed: 00:15:00
                               ETA: 00:59:43

################################################################################
                     [1m Learning iteration 402/2000 [0m                      

                       Computation: 47060 steps/s (collection: 1.961s, learning 0.128s)
             Mean action noise std: 2.07
          Mean value_function loss: 32.6056
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 47.1412
                       Mean reward: 24.40
               Mean episode length: 86.64
    Episode_Reward/reaching_object: 0.2801
     Episode_Reward/lifting_object: 5.1163
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 45.6250
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 2.09s
                      Time elapsed: 00:15:02
                               ETA: 00:59:40

################################################################################
                     [1m Learning iteration 403/2000 [0m                      

                       Computation: 47476 steps/s (collection: 1.971s, learning 0.100s)
             Mean action noise std: 2.07
          Mean value_function loss: 35.1259
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 47.1423
                       Mean reward: 26.19
               Mean episode length: 86.26
    Episode_Reward/reaching_object: 0.2747
     Episode_Reward/lifting_object: 4.7992
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 49.0417
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 2.07s
                      Time elapsed: 00:15:04
                               ETA: 00:59:37

################################################################################
                     [1m Learning iteration 404/2000 [0m                      

                       Computation: 46946 steps/s (collection: 2.007s, learning 0.087s)
             Mean action noise std: 2.07
          Mean value_function loss: 32.7714
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 47.1434
                       Mean reward: 24.72
               Mean episode length: 88.98
    Episode_Reward/reaching_object: 0.2765
     Episode_Reward/lifting_object: 4.7555
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 45.2083
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 2.09s
                      Time elapsed: 00:15:07
                               ETA: 00:59:34

################################################################################
                     [1m Learning iteration 405/2000 [0m                      

                       Computation: 47224 steps/s (collection: 1.993s, learning 0.089s)
             Mean action noise std: 2.07
          Mean value_function loss: 41.0913
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 47.1463
                       Mean reward: 30.82
               Mean episode length: 82.95
    Episode_Reward/reaching_object: 0.2779
     Episode_Reward/lifting_object: 4.9668
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 46.8333
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 2.08s
                      Time elapsed: 00:15:09
                               ETA: 00:59:31

################################################################################
                     [1m Learning iteration 406/2000 [0m                      

                       Computation: 48449 steps/s (collection: 1.939s, learning 0.090s)
             Mean action noise std: 2.07
          Mean value_function loss: 37.1724
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 47.1514
                       Mean reward: 14.60
               Mean episode length: 84.63
    Episode_Reward/reaching_object: 0.2890
     Episode_Reward/lifting_object: 5.1512
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 44.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 2.03s
                      Time elapsed: 00:15:11
                               ETA: 00:59:28

################################################################################
                     [1m Learning iteration 407/2000 [0m                      

                       Computation: 47916 steps/s (collection: 1.957s, learning 0.095s)
             Mean action noise std: 2.07
          Mean value_function loss: 33.7581
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 47.1550
                       Mean reward: 30.53
               Mean episode length: 99.19
    Episode_Reward/reaching_object: 0.2885
     Episode_Reward/lifting_object: 5.3426
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 48.3333
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 2.05s
                      Time elapsed: 00:15:13
                               ETA: 00:59:25

################################################################################
                     [1m Learning iteration 408/2000 [0m                      

                       Computation: 46731 steps/s (collection: 2.004s, learning 0.100s)
             Mean action noise std: 2.07
          Mean value_function loss: 41.1479
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 47.1606
                       Mean reward: 27.70
               Mean episode length: 84.72
    Episode_Reward/reaching_object: 0.2997
     Episode_Reward/lifting_object: 5.5658
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 43.8750
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 2.10s
                      Time elapsed: 00:15:15
                               ETA: 00:59:22

################################################################################
                     [1m Learning iteration 409/2000 [0m                      

                       Computation: 47034 steps/s (collection: 2.002s, learning 0.088s)
             Mean action noise std: 2.07
          Mean value_function loss: 39.7263
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 47.1671
                       Mean reward: 25.06
               Mean episode length: 82.36
    Episode_Reward/reaching_object: 0.2942
     Episode_Reward/lifting_object: 5.3431
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 47.9583
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 2.09s
                      Time elapsed: 00:15:17
                               ETA: 00:59:20

################################################################################
                     [1m Learning iteration 410/2000 [0m                      

                       Computation: 47109 steps/s (collection: 1.978s, learning 0.109s)
             Mean action noise std: 2.07
          Mean value_function loss: 39.7607
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 47.1739
                       Mean reward: 29.07
               Mean episode length: 82.72
    Episode_Reward/reaching_object: 0.2983
     Episode_Reward/lifting_object: 5.4582
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 48.8750
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 2.09s
                      Time elapsed: 00:15:19
                               ETA: 00:59:17

################################################################################
                     [1m Learning iteration 411/2000 [0m                      

                       Computation: 47650 steps/s (collection: 1.977s, learning 0.086s)
             Mean action noise std: 2.07
          Mean value_function loss: 52.5917
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 47.1834
                       Mean reward: 29.88
               Mean episode length: 87.00
    Episode_Reward/reaching_object: 0.2924
     Episode_Reward/lifting_object: 5.5261
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 49.2083
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 2.06s
                      Time elapsed: 00:15:21
                               ETA: 00:59:14

################################################################################
                     [1m Learning iteration 412/2000 [0m                      

                       Computation: 47636 steps/s (collection: 1.975s, learning 0.089s)
             Mean action noise std: 2.08
          Mean value_function loss: 81.5251
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 47.1930
                       Mean reward: 27.29
               Mean episode length: 87.48
    Episode_Reward/reaching_object: 0.2924
     Episode_Reward/lifting_object: 5.5122
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 46.7083
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 2.06s
                      Time elapsed: 00:15:23
                               ETA: 00:59:11

################################################################################
                     [1m Learning iteration 413/2000 [0m                      

                       Computation: 47775 steps/s (collection: 1.971s, learning 0.087s)
             Mean action noise std: 2.08
          Mean value_function loss: 121.8015
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 47.2046
                       Mean reward: 29.40
               Mean episode length: 86.93
    Episode_Reward/reaching_object: 0.2949
     Episode_Reward/lifting_object: 5.5595
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 49.3333
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 2.06s
                      Time elapsed: 00:15:25
                               ETA: 00:59:08

################################################################################
                     [1m Learning iteration 414/2000 [0m                      

                       Computation: 47320 steps/s (collection: 1.953s, learning 0.125s)
             Mean action noise std: 2.08
          Mean value_function loss: 46.9412
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 47.2158
                       Mean reward: 33.95
               Mean episode length: 91.16
    Episode_Reward/reaching_object: 0.2985
     Episode_Reward/lifting_object: 5.9674
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 48.7083
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 2.08s
                      Time elapsed: 00:15:27
                               ETA: 00:59:05

################################################################################
                     [1m Learning iteration 415/2000 [0m                      

                       Computation: 46600 steps/s (collection: 1.958s, learning 0.152s)
             Mean action noise std: 2.08
          Mean value_function loss: 46.9518
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 47.2224
                       Mean reward: 26.10
               Mean episode length: 88.19
    Episode_Reward/reaching_object: 0.2900
     Episode_Reward/lifting_object: 5.7345
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 48.6667
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 2.11s
                      Time elapsed: 00:15:29
                               ETA: 00:59:02

################################################################################
                     [1m Learning iteration 416/2000 [0m                      

                       Computation: 46674 steps/s (collection: 2.010s, learning 0.096s)
             Mean action noise std: 2.08
          Mean value_function loss: 44.4291
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 47.2266
                       Mean reward: 32.17
               Mean episode length: 89.36
    Episode_Reward/reaching_object: 0.2885
     Episode_Reward/lifting_object: 5.6975
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 50.6667
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 2.11s
                      Time elapsed: 00:15:31
                               ETA: 00:59:00

################################################################################
                     [1m Learning iteration 417/2000 [0m                      

                       Computation: 47098 steps/s (collection: 1.988s, learning 0.099s)
             Mean action noise std: 2.08
          Mean value_function loss: 40.0724
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 47.2311
                       Mean reward: 32.61
               Mean episode length: 85.23
    Episode_Reward/reaching_object: 0.2902
     Episode_Reward/lifting_object: 5.6897
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 50.2083
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 2.09s
                      Time elapsed: 00:15:34
                               ETA: 00:58:57

################################################################################
                     [1m Learning iteration 418/2000 [0m                      

                       Computation: 47872 steps/s (collection: 1.964s, learning 0.090s)
             Mean action noise std: 2.08
          Mean value_function loss: 53.0404
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 47.2344
                       Mean reward: 29.99
               Mean episode length: 75.52
    Episode_Reward/reaching_object: 0.2825
     Episode_Reward/lifting_object: 6.1890
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 51.8750
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 2.05s
                      Time elapsed: 00:15:36
                               ETA: 00:58:54

################################################################################
                     [1m Learning iteration 419/2000 [0m                      

                       Computation: 46513 steps/s (collection: 2.021s, learning 0.093s)
             Mean action noise std: 2.08
          Mean value_function loss: 50.7640
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 47.2367
                       Mean reward: 34.97
               Mean episode length: 81.67
    Episode_Reward/reaching_object: 0.2729
     Episode_Reward/lifting_object: 5.8676
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 49.9583
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 2.11s
                      Time elapsed: 00:15:38
                               ETA: 00:58:51

################################################################################
                     [1m Learning iteration 420/2000 [0m                      

                       Computation: 47206 steps/s (collection: 1.987s, learning 0.095s)
             Mean action noise std: 2.08
          Mean value_function loss: 51.4814
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 47.2440
                       Mean reward: 31.21
               Mean episode length: 78.31
    Episode_Reward/reaching_object: 0.2731
     Episode_Reward/lifting_object: 5.7607
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 51.8333
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 2.08s
                      Time elapsed: 00:15:40
                               ETA: 00:58:49

################################################################################
                     [1m Learning iteration 421/2000 [0m                      

                       Computation: 47579 steps/s (collection: 1.968s, learning 0.098s)
             Mean action noise std: 2.08
          Mean value_function loss: 65.2820
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 47.2507
                       Mean reward: 29.39
               Mean episode length: 72.83
    Episode_Reward/reaching_object: 0.2697
     Episode_Reward/lifting_object: 5.6711
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 49.7500
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 2.07s
                      Time elapsed: 00:15:42
                               ETA: 00:58:46

################################################################################
                     [1m Learning iteration 422/2000 [0m                      

                       Computation: 48441 steps/s (collection: 1.942s, learning 0.088s)
             Mean action noise std: 2.08
          Mean value_function loss: 49.8398
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 47.2525
                       Mean reward: 31.97
               Mean episode length: 81.30
    Episode_Reward/reaching_object: 0.2797
     Episode_Reward/lifting_object: 6.0701
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 51.7500
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 2.03s
                      Time elapsed: 00:15:44
                               ETA: 00:58:43

################################################################################
                     [1m Learning iteration 423/2000 [0m                      

                       Computation: 47744 steps/s (collection: 1.970s, learning 0.089s)
             Mean action noise std: 2.08
          Mean value_function loss: 43.5319
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 47.2573
                       Mean reward: 34.46
               Mean episode length: 84.09
    Episode_Reward/reaching_object: 0.2759
     Episode_Reward/lifting_object: 5.8242
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 51.0833
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 2.06s
                      Time elapsed: 00:15:46
                               ETA: 00:58:40

################################################################################
                     [1m Learning iteration 424/2000 [0m                      

                       Computation: 47374 steps/s (collection: 1.981s, learning 0.094s)
             Mean action noise std: 2.08
          Mean value_function loss: 49.4682
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 47.2624
                       Mean reward: 29.63
               Mean episode length: 81.23
    Episode_Reward/reaching_object: 0.2707
     Episode_Reward/lifting_object: 6.0267
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 50.3333
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 2.08s
                      Time elapsed: 00:15:48
                               ETA: 00:58:37

################################################################################
                     [1m Learning iteration 425/2000 [0m                      

                       Computation: 47883 steps/s (collection: 1.963s, learning 0.090s)
             Mean action noise std: 2.08
          Mean value_function loss: 42.0275
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 47.2656
                       Mean reward: 30.52
               Mean episode length: 81.92
    Episode_Reward/reaching_object: 0.2796
     Episode_Reward/lifting_object: 6.1160
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 51.3333
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 2.05s
                      Time elapsed: 00:15:50
                               ETA: 00:58:34

################################################################################
                     [1m Learning iteration 426/2000 [0m                      

                       Computation: 47421 steps/s (collection: 1.980s, learning 0.093s)
             Mean action noise std: 2.08
          Mean value_function loss: 53.9319
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 47.2683
                       Mean reward: 31.09
               Mean episode length: 78.25
    Episode_Reward/reaching_object: 0.2745
     Episode_Reward/lifting_object: 6.0357
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 52.1667
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 2.07s
                      Time elapsed: 00:15:52
                               ETA: 00:58:31

################################################################################
                     [1m Learning iteration 427/2000 [0m                      

                       Computation: 46442 steps/s (collection: 2.008s, learning 0.109s)
             Mean action noise std: 2.08
          Mean value_function loss: 61.2456
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 47.2725
                       Mean reward: 31.81
               Mean episode length: 80.08
    Episode_Reward/reaching_object: 0.2639
     Episode_Reward/lifting_object: 5.9906
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 48.6667
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 2.12s
                      Time elapsed: 00:15:54
                               ETA: 00:58:29

################################################################################
                     [1m Learning iteration 428/2000 [0m                      

                       Computation: 47611 steps/s (collection: 1.959s, learning 0.106s)
             Mean action noise std: 2.08
          Mean value_function loss: 46.5539
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 47.2755
                       Mean reward: 33.78
               Mean episode length: 76.46
    Episode_Reward/reaching_object: 0.2681
     Episode_Reward/lifting_object: 5.8784
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 51.5833
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 2.06s
                      Time elapsed: 00:15:56
                               ETA: 00:58:26

################################################################################
                     [1m Learning iteration 429/2000 [0m                      

                       Computation: 48024 steps/s (collection: 1.956s, learning 0.091s)
             Mean action noise std: 2.08
          Mean value_function loss: 48.1149
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 47.2765
                       Mean reward: 37.04
               Mean episode length: 83.47
    Episode_Reward/reaching_object: 0.2780
     Episode_Reward/lifting_object: 6.5804
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 52.0833
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 2.05s
                      Time elapsed: 00:15:58
                               ETA: 00:58:23

################################################################################
                     [1m Learning iteration 430/2000 [0m                      

                       Computation: 47755 steps/s (collection: 1.968s, learning 0.091s)
             Mean action noise std: 2.08
          Mean value_function loss: 56.1818
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 47.2767
                       Mean reward: 32.80
               Mean episode length: 84.58
    Episode_Reward/reaching_object: 0.2786
     Episode_Reward/lifting_object: 6.3137
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 51.7917
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 2.06s
                      Time elapsed: 00:16:00
                               ETA: 00:58:20

################################################################################
                     [1m Learning iteration 431/2000 [0m                      

                       Computation: 47868 steps/s (collection: 1.962s, learning 0.092s)
             Mean action noise std: 2.08
          Mean value_function loss: 52.2710
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 47.2768
                       Mean reward: 32.49
               Mean episode length: 78.25
    Episode_Reward/reaching_object: 0.2755
     Episode_Reward/lifting_object: 6.0389
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 49.9583
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 2.05s
                      Time elapsed: 00:16:03
                               ETA: 00:58:17

################################################################################
                     [1m Learning iteration 432/2000 [0m                      

                       Computation: 46584 steps/s (collection: 2.006s, learning 0.105s)
             Mean action noise std: 2.08
          Mean value_function loss: 47.7764
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 47.2769
                       Mean reward: 29.06
               Mean episode length: 76.14
    Episode_Reward/reaching_object: 0.2703
     Episode_Reward/lifting_object: 6.0776
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 53.2500
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 2.11s
                      Time elapsed: 00:16:05
                               ETA: 00:58:15

################################################################################
                     [1m Learning iteration 433/2000 [0m                      

                       Computation: 47824 steps/s (collection: 1.968s, learning 0.088s)
             Mean action noise std: 2.08
          Mean value_function loss: 59.7709
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 47.2770
                       Mean reward: 33.60
               Mean episode length: 83.90
    Episode_Reward/reaching_object: 0.2790
     Episode_Reward/lifting_object: 6.5422
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 51.3750
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 2.06s
                      Time elapsed: 00:16:07
                               ETA: 00:58:12

################################################################################
                     [1m Learning iteration 434/2000 [0m                      

                       Computation: 46106 steps/s (collection: 2.021s, learning 0.112s)
             Mean action noise std: 2.08
          Mean value_function loss: 74.6581
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 47.2772
                       Mean reward: 27.87
               Mean episode length: 72.93
    Episode_Reward/reaching_object: 0.2714
     Episode_Reward/lifting_object: 6.1831
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 51.0833
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 2.13s
                      Time elapsed: 00:16:09
                               ETA: 00:58:09

################################################################################
                     [1m Learning iteration 435/2000 [0m                      

                       Computation: 45166 steps/s (collection: 2.056s, learning 0.121s)
             Mean action noise std: 2.08
          Mean value_function loss: 57.6557
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 47.2774
                       Mean reward: 34.19
               Mean episode length: 77.13
    Episode_Reward/reaching_object: 0.2767
     Episode_Reward/lifting_object: 6.3802
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 51.4167
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 2.18s
                      Time elapsed: 00:16:11
                               ETA: 00:58:07

################################################################################
                     [1m Learning iteration 436/2000 [0m                      

                       Computation: 45110 steps/s (collection: 2.084s, learning 0.096s)
             Mean action noise std: 2.08
          Mean value_function loss: 57.6392
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.2796
                       Mean reward: 31.73
               Mean episode length: 86.09
    Episode_Reward/reaching_object: 0.2788
     Episode_Reward/lifting_object: 6.3126
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 50.2500
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 2.18s
                      Time elapsed: 00:16:13
                               ETA: 00:58:04

################################################################################
                     [1m Learning iteration 437/2000 [0m                      

                       Computation: 47304 steps/s (collection: 1.984s, learning 0.095s)
             Mean action noise std: 2.08
          Mean value_function loss: 49.5243
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 47.2843
                       Mean reward: 30.17
               Mean episode length: 79.54
    Episode_Reward/reaching_object: 0.2706
     Episode_Reward/lifting_object: 6.2669
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 52.1667
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 2.08s
                      Time elapsed: 00:16:15
                               ETA: 00:58:01

################################################################################
                     [1m Learning iteration 438/2000 [0m                      

                       Computation: 47503 steps/s (collection: 1.983s, learning 0.087s)
             Mean action noise std: 2.09
          Mean value_function loss: 58.5078
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 47.2874
                       Mean reward: 34.28
               Mean episode length: 79.39
    Episode_Reward/reaching_object: 0.2662
     Episode_Reward/lifting_object: 6.2146
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 50.0417
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 2.07s
                      Time elapsed: 00:16:17
                               ETA: 00:57:59

################################################################################
                     [1m Learning iteration 439/2000 [0m                      

                       Computation: 46524 steps/s (collection: 2.015s, learning 0.098s)
             Mean action noise std: 2.09
          Mean value_function loss: 54.8442
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 47.2924
                       Mean reward: 32.96
               Mean episode length: 79.29
    Episode_Reward/reaching_object: 0.2741
     Episode_Reward/lifting_object: 6.5155
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 51.4583
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 2.11s
                      Time elapsed: 00:16:19
                               ETA: 00:57:56

################################################################################
                     [1m Learning iteration 440/2000 [0m                      

                       Computation: 47110 steps/s (collection: 1.979s, learning 0.108s)
             Mean action noise std: 2.09
          Mean value_function loss: 49.7199
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 47.2965
                       Mean reward: 31.15
               Mean episode length: 81.33
    Episode_Reward/reaching_object: 0.2748
     Episode_Reward/lifting_object: 6.0688
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 51.3750
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 2.09s
                      Time elapsed: 00:16:22
                               ETA: 00:57:53

################################################################################
                     [1m Learning iteration 441/2000 [0m                      

                       Computation: 47320 steps/s (collection: 1.972s, learning 0.105s)
             Mean action noise std: 2.09
          Mean value_function loss: 53.3032
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 47.2984
                       Mean reward: 34.18
               Mean episode length: 76.21
    Episode_Reward/reaching_object: 0.2733
     Episode_Reward/lifting_object: 6.4398
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 50.3750
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 2.08s
                      Time elapsed: 00:16:24
                               ETA: 00:57:51

################################################################################
                     [1m Learning iteration 442/2000 [0m                      

                       Computation: 44951 steps/s (collection: 2.034s, learning 0.153s)
             Mean action noise std: 2.09
          Mean value_function loss: 93.2497
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 47.3037
                       Mean reward: 35.58
               Mean episode length: 80.09
    Episode_Reward/reaching_object: 0.2787
     Episode_Reward/lifting_object: 6.7359
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 51.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 2.19s
                      Time elapsed: 00:16:26
                               ETA: 00:57:48

################################################################################
                     [1m Learning iteration 443/2000 [0m                      

                       Computation: 44525 steps/s (collection: 2.064s, learning 0.144s)
             Mean action noise std: 2.09
          Mean value_function loss: 78.0376
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 47.3115
                       Mean reward: 32.26
               Mean episode length: 78.65
    Episode_Reward/reaching_object: 0.2710
     Episode_Reward/lifting_object: 6.1155
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 49.1250
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 2.21s
                      Time elapsed: 00:16:28
                               ETA: 00:57:46

################################################################################
                     [1m Learning iteration 444/2000 [0m                      

                       Computation: 47691 steps/s (collection: 1.935s, learning 0.126s)
             Mean action noise std: 2.09
          Mean value_function loss: 50.0541
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 47.3150
                       Mean reward: 34.99
               Mean episode length: 81.82
    Episode_Reward/reaching_object: 0.2816
     Episode_Reward/lifting_object: 6.7017
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 54.2083
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 2.06s
                      Time elapsed: 00:16:30
                               ETA: 00:57:43

################################################################################
                     [1m Learning iteration 445/2000 [0m                      

                       Computation: 45012 steps/s (collection: 2.094s, learning 0.090s)
             Mean action noise std: 2.09
          Mean value_function loss: 85.4340
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 47.3174
                       Mean reward: 36.64
               Mean episode length: 81.39
    Episode_Reward/reaching_object: 0.2798
     Episode_Reward/lifting_object: 6.2669
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 48.3750
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 2.18s
                      Time elapsed: 00:16:32
                               ETA: 00:57:41

################################################################################
                     [1m Learning iteration 446/2000 [0m                      

                       Computation: 44069 steps/s (collection: 2.136s, learning 0.095s)
             Mean action noise std: 2.09
          Mean value_function loss: 82.2352
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 47.3214
                       Mean reward: 32.25
               Mean episode length: 73.92
    Episode_Reward/reaching_object: 0.2861
     Episode_Reward/lifting_object: 6.5175
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 50.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 2.23s
                      Time elapsed: 00:16:34
                               ETA: 00:57:39

################################################################################
                     [1m Learning iteration 447/2000 [0m                      

                       Computation: 47846 steps/s (collection: 1.962s, learning 0.092s)
             Mean action noise std: 2.09
          Mean value_function loss: 56.8030
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 47.3270
                       Mean reward: 37.78
               Mean episode length: 81.92
    Episode_Reward/reaching_object: 0.2905
     Episode_Reward/lifting_object: 6.9556
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 49.4583
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 2.05s
                      Time elapsed: 00:16:37
                               ETA: 00:57:36

################################################################################
                     [1m Learning iteration 448/2000 [0m                      

                       Computation: 43975 steps/s (collection: 2.125s, learning 0.111s)
             Mean action noise std: 2.09
          Mean value_function loss: 47.1628
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 47.3298
                       Mean reward: 31.45
               Mean episode length: 76.79
    Episode_Reward/reaching_object: 0.2966
     Episode_Reward/lifting_object: 7.0206
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 48.3333
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 2.24s
                      Time elapsed: 00:16:39
                               ETA: 00:57:34

################################################################################
                     [1m Learning iteration 449/2000 [0m                      

                       Computation: 45623 steps/s (collection: 2.026s, learning 0.129s)
             Mean action noise std: 2.09
          Mean value_function loss: 47.8894
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 47.3311
                       Mean reward: 31.89
               Mean episode length: 78.74
    Episode_Reward/reaching_object: 0.2886
     Episode_Reward/lifting_object: 7.2628
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 49.2500
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 2.15s
                      Time elapsed: 00:16:41
                               ETA: 00:57:31

################################################################################
                     [1m Learning iteration 450/2000 [0m                      

                       Computation: 46041 steps/s (collection: 1.980s, learning 0.155s)
             Mean action noise std: 2.09
          Mean value_function loss: 61.1357
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 47.3330
                       Mean reward: 37.28
               Mean episode length: 81.91
    Episode_Reward/reaching_object: 0.2857
     Episode_Reward/lifting_object: 7.0311
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 51.6667
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 2.14s
                      Time elapsed: 00:16:43
                               ETA: 00:57:29

################################################################################
                     [1m Learning iteration 451/2000 [0m                      

                       Computation: 46606 steps/s (collection: 2.010s, learning 0.099s)
             Mean action noise std: 2.09
          Mean value_function loss: 48.3122
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 47.3359
                       Mean reward: 37.10
               Mean episode length: 79.26
    Episode_Reward/reaching_object: 0.2879
     Episode_Reward/lifting_object: 7.2773
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 50.9583
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 2.11s
                      Time elapsed: 00:16:45
                               ETA: 00:57:26

################################################################################
                     [1m Learning iteration 452/2000 [0m                      

                       Computation: 48064 steps/s (collection: 1.958s, learning 0.087s)
             Mean action noise std: 2.09
          Mean value_function loss: 53.5396
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 47.3388
                       Mean reward: 36.60
               Mean episode length: 79.78
    Episode_Reward/reaching_object: 0.2843
     Episode_Reward/lifting_object: 7.4105
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 49.5000
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 2.05s
                      Time elapsed: 00:16:47
                               ETA: 00:57:23

################################################################################
                     [1m Learning iteration 453/2000 [0m                      

                       Computation: 47125 steps/s (collection: 2.000s, learning 0.086s)
             Mean action noise std: 2.09
          Mean value_function loss: 51.2710
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 47.3418
                       Mean reward: 36.80
               Mean episode length: 81.28
    Episode_Reward/reaching_object: 0.2795
     Episode_Reward/lifting_object: 7.2567
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 51.5000
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 2.09s
                      Time elapsed: 00:16:49
                               ETA: 00:57:20

################################################################################
                     [1m Learning iteration 454/2000 [0m                      

                       Computation: 46202 steps/s (collection: 2.040s, learning 0.088s)
             Mean action noise std: 2.09
          Mean value_function loss: 64.7027
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 47.3439
                       Mean reward: 39.94
               Mean episode length: 85.57
    Episode_Reward/reaching_object: 0.2798
     Episode_Reward/lifting_object: 7.3386
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 51.9583
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 2.13s
                      Time elapsed: 00:16:51
                               ETA: 00:57:18

################################################################################
                     [1m Learning iteration 455/2000 [0m                      

                       Computation: 47636 steps/s (collection: 1.973s, learning 0.091s)
             Mean action noise std: 2.09
          Mean value_function loss: 61.3589
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 47.3473
                       Mean reward: 36.71
               Mean episode length: 85.01
    Episode_Reward/reaching_object: 0.2783
     Episode_Reward/lifting_object: 7.2337
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 52.7083
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 2.06s
                      Time elapsed: 00:16:53
                               ETA: 00:57:15

################################################################################
                     [1m Learning iteration 456/2000 [0m                      

                       Computation: 46920 steps/s (collection: 1.983s, learning 0.112s)
             Mean action noise std: 2.09
          Mean value_function loss: 63.4955
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 47.3512
                       Mean reward: 34.53
               Mean episode length: 76.65
    Episode_Reward/reaching_object: 0.2690
     Episode_Reward/lifting_object: 7.2612
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 52.2083
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 2.10s
                      Time elapsed: 00:16:56
                               ETA: 00:57:12

################################################################################
                     [1m Learning iteration 457/2000 [0m                      

                       Computation: 48329 steps/s (collection: 1.946s, learning 0.088s)
             Mean action noise std: 2.09
          Mean value_function loss: 49.1109
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 47.3537
                       Mean reward: 40.69
               Mean episode length: 73.03
    Episode_Reward/reaching_object: 0.2587
     Episode_Reward/lifting_object: 7.1137
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 53.2083
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 2.03s
                      Time elapsed: 00:16:58
                               ETA: 00:57:10

################################################################################
                     [1m Learning iteration 458/2000 [0m                      

                       Computation: 44223 steps/s (collection: 2.109s, learning 0.114s)
             Mean action noise std: 2.09
          Mean value_function loss: 56.1874
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 47.3558
                       Mean reward: 43.79
               Mean episode length: 77.31
    Episode_Reward/reaching_object: 0.2695
     Episode_Reward/lifting_object: 7.6761
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 50.4583
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 2.22s
                      Time elapsed: 00:17:00
                               ETA: 00:57:07

################################################################################
                     [1m Learning iteration 459/2000 [0m                      

                       Computation: 45166 steps/s (collection: 2.037s, learning 0.139s)
             Mean action noise std: 2.09
          Mean value_function loss: 65.3824
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.3581
                       Mean reward: 41.29
               Mean episode length: 75.76
    Episode_Reward/reaching_object: 0.2725
     Episode_Reward/lifting_object: 7.5414
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 52.2917
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 2.18s
                      Time elapsed: 00:17:02
                               ETA: 00:57:05

################################################################################
                     [1m Learning iteration 460/2000 [0m                      

                       Computation: 46138 steps/s (collection: 2.016s, learning 0.115s)
             Mean action noise std: 2.09
          Mean value_function loss: 57.3041
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 47.3636
                       Mean reward: 37.69
               Mean episode length: 74.41
    Episode_Reward/reaching_object: 0.2678
     Episode_Reward/lifting_object: 7.3319
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 53.1667
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 2.13s
                      Time elapsed: 00:17:04
                               ETA: 00:57:02

################################################################################
                     [1m Learning iteration 461/2000 [0m                      

                       Computation: 46700 steps/s (collection: 2.004s, learning 0.101s)
             Mean action noise std: 2.09
          Mean value_function loss: 77.2134
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 47.3663
                       Mean reward: 41.62
               Mean episode length: 74.74
    Episode_Reward/reaching_object: 0.2730
     Episode_Reward/lifting_object: 7.8711
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 55.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 2.10s
                      Time elapsed: 00:17:06
                               ETA: 00:57:00

################################################################################
                     [1m Learning iteration 462/2000 [0m                      

                       Computation: 47434 steps/s (collection: 1.982s, learning 0.091s)
             Mean action noise std: 2.09
          Mean value_function loss: 76.0453
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 47.3671
                       Mean reward: 37.07
               Mean episode length: 75.93
    Episode_Reward/reaching_object: 0.2631
     Episode_Reward/lifting_object: 7.3399
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 52.2917
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 2.07s
                      Time elapsed: 00:17:08
                               ETA: 00:56:57

################################################################################
                     [1m Learning iteration 463/2000 [0m                      

                       Computation: 44030 steps/s (collection: 2.078s, learning 0.155s)
             Mean action noise std: 2.09
          Mean value_function loss: 67.0783
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 47.3696
                       Mean reward: 40.90
               Mean episode length: 76.07
    Episode_Reward/reaching_object: 0.2710
     Episode_Reward/lifting_object: 7.4211
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 49.7500
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 2.23s
                      Time elapsed: 00:17:11
                               ETA: 00:56:55

################################################################################
                     [1m Learning iteration 464/2000 [0m                      

                       Computation: 43997 steps/s (collection: 2.143s, learning 0.092s)
             Mean action noise std: 2.09
          Mean value_function loss: 69.6682
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 47.3741
                       Mean reward: 34.97
               Mean episode length: 77.04
    Episode_Reward/reaching_object: 0.2751
     Episode_Reward/lifting_object: 7.4314
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 53.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 2.23s
                      Time elapsed: 00:17:13
                               ETA: 00:56:53

################################################################################
                     [1m Learning iteration 465/2000 [0m                      

                       Computation: 45725 steps/s (collection: 2.047s, learning 0.103s)
             Mean action noise std: 2.09
          Mean value_function loss: 107.5157
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 47.3758
                       Mean reward: 44.66
               Mean episode length: 81.60
    Episode_Reward/reaching_object: 0.2731
     Episode_Reward/lifting_object: 7.5200
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 51.9167
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 2.15s
                      Time elapsed: 00:17:15
                               ETA: 00:56:50

################################################################################
                     [1m Learning iteration 466/2000 [0m                      

                       Computation: 45955 steps/s (collection: 2.035s, learning 0.105s)
             Mean action noise std: 2.09
          Mean value_function loss: 60.4238
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 47.3761
                       Mean reward: 34.59
               Mean episode length: 74.09
    Episode_Reward/reaching_object: 0.2752
     Episode_Reward/lifting_object: 7.9186
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 51.1250
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 2.14s
                      Time elapsed: 00:17:17
                               ETA: 00:56:48

################################################################################
                     [1m Learning iteration 467/2000 [0m                      

                       Computation: 43856 steps/s (collection: 2.138s, learning 0.103s)
             Mean action noise std: 2.09
          Mean value_function loss: 67.9999
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 47.3765
                       Mean reward: 37.90
               Mean episode length: 69.06
    Episode_Reward/reaching_object: 0.2732
     Episode_Reward/lifting_object: 7.4481
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 52.6250
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 2.24s
                      Time elapsed: 00:17:19
                               ETA: 00:56:46

################################################################################
                     [1m Learning iteration 468/2000 [0m                      

                       Computation: 44279 steps/s (collection: 2.100s, learning 0.120s)
             Mean action noise std: 2.09
          Mean value_function loss: 79.0663
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 47.3767
                       Mean reward: 40.55
               Mean episode length: 82.66
    Episode_Reward/reaching_object: 0.2757
     Episode_Reward/lifting_object: 7.4516
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 51.9583
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 2.22s
                      Time elapsed: 00:17:22
                               ETA: 00:56:43

################################################################################
                     [1m Learning iteration 469/2000 [0m                      

                       Computation: 46753 steps/s (collection: 2.013s, learning 0.090s)
             Mean action noise std: 2.09
          Mean value_function loss: 62.8668
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 47.3768
                       Mean reward: 38.22
               Mean episode length: 76.71
    Episode_Reward/reaching_object: 0.2763
     Episode_Reward/lifting_object: 7.8502
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 54.5833
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 2.10s
                      Time elapsed: 00:17:24
                               ETA: 00:56:41

################################################################################
                     [1m Learning iteration 470/2000 [0m                      

                       Computation: 46710 steps/s (collection: 2.017s, learning 0.087s)
             Mean action noise std: 2.09
          Mean value_function loss: 63.7571
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 47.3769
                       Mean reward: 41.10
               Mean episode length: 72.51
    Episode_Reward/reaching_object: 0.2659
     Episode_Reward/lifting_object: 7.3772
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 52.3750
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 2.10s
                      Time elapsed: 00:17:26
                               ETA: 00:56:38

################################################################################
                     [1m Learning iteration 471/2000 [0m                      

                       Computation: 46851 steps/s (collection: 2.011s, learning 0.087s)
             Mean action noise std: 2.09
          Mean value_function loss: 135.3389
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 47.3770
                       Mean reward: 38.15
               Mean episode length: 74.89
    Episode_Reward/reaching_object: 0.2647
     Episode_Reward/lifting_object: 7.5072
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 52.7083
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 2.10s
                      Time elapsed: 00:17:28
                               ETA: 00:56:36

################################################################################
                     [1m Learning iteration 472/2000 [0m                      

                       Computation: 46939 steps/s (collection: 2.009s, learning 0.085s)
             Mean action noise std: 2.09
          Mean value_function loss: 119.2222
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 47.3770
                       Mean reward: 42.73
               Mean episode length: 81.57
    Episode_Reward/reaching_object: 0.2642
     Episode_Reward/lifting_object: 7.4568
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 52.0833
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 2.09s
                      Time elapsed: 00:17:30
                               ETA: 00:56:33

################################################################################
                     [1m Learning iteration 473/2000 [0m                      

                       Computation: 46810 steps/s (collection: 2.008s, learning 0.093s)
             Mean action noise std: 2.09
          Mean value_function loss: 71.1371
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 47.3772
                       Mean reward: 43.12
               Mean episode length: 78.96
    Episode_Reward/reaching_object: 0.2712
     Episode_Reward/lifting_object: 7.8383
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 52.8333
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 2.10s
                      Time elapsed: 00:17:32
                               ETA: 00:56:30

################################################################################
                     [1m Learning iteration 474/2000 [0m                      

                       Computation: 46380 steps/s (collection: 2.015s, learning 0.104s)
             Mean action noise std: 2.09
          Mean value_function loss: 68.4820
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 47.3779
                       Mean reward: 38.43
               Mean episode length: 77.97
    Episode_Reward/reaching_object: 0.2725
     Episode_Reward/lifting_object: 7.4751
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 53.4167
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 2.12s
                      Time elapsed: 00:17:34
                               ETA: 00:56:28

################################################################################
                     [1m Learning iteration 475/2000 [0m                      

                       Computation: 46942 steps/s (collection: 2.000s, learning 0.095s)
             Mean action noise std: 2.09
          Mean value_function loss: 91.7766
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 47.3799
                       Mean reward: 39.33
               Mean episode length: 72.24
    Episode_Reward/reaching_object: 0.2657
     Episode_Reward/lifting_object: 7.2635
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 52.8333
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 2.09s
                      Time elapsed: 00:17:36
                               ETA: 00:56:25

################################################################################
                     [1m Learning iteration 476/2000 [0m                      

                       Computation: 46806 steps/s (collection: 1.983s, learning 0.118s)
             Mean action noise std: 2.10
          Mean value_function loss: 76.6265
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 47.3849
                       Mean reward: 46.56
               Mean episode length: 77.10
    Episode_Reward/reaching_object: 0.2706
     Episode_Reward/lifting_object: 7.7061
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 55.4583
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 2.10s
                      Time elapsed: 00:17:38
                               ETA: 00:56:23

################################################################################
                     [1m Learning iteration 477/2000 [0m                      

                       Computation: 46408 steps/s (collection: 2.009s, learning 0.109s)
             Mean action noise std: 2.10
          Mean value_function loss: 60.8143
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 47.3911
                       Mean reward: 41.25
               Mean episode length: 78.57
    Episode_Reward/reaching_object: 0.2651
     Episode_Reward/lifting_object: 7.6927
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 56.2083
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 2.12s
                      Time elapsed: 00:17:40
                               ETA: 00:56:20

################################################################################
                     [1m Learning iteration 478/2000 [0m                      

                       Computation: 47064 steps/s (collection: 1.982s, learning 0.107s)
             Mean action noise std: 2.10
          Mean value_function loss: 64.5795
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 47.3945
                       Mean reward: 41.06
               Mean episode length: 75.81
    Episode_Reward/reaching_object: 0.2623
     Episode_Reward/lifting_object: 7.6378
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 53.5417
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 2.09s
                      Time elapsed: 00:17:43
                               ETA: 00:56:17

################################################################################
                     [1m Learning iteration 479/2000 [0m                      

                       Computation: 47730 steps/s (collection: 1.969s, learning 0.091s)
             Mean action noise std: 2.10
          Mean value_function loss: 62.0776
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 47.3971
                       Mean reward: 44.14
               Mean episode length: 76.73
    Episode_Reward/reaching_object: 0.2594
     Episode_Reward/lifting_object: 7.6575
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 55.2500
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 2.06s
                      Time elapsed: 00:17:45
                               ETA: 00:56:15

################################################################################
                     [1m Learning iteration 480/2000 [0m                      

                       Computation: 46141 steps/s (collection: 2.045s, learning 0.086s)
             Mean action noise std: 2.10
          Mean value_function loss: 71.7854
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 47.4009
                       Mean reward: 41.20
               Mean episode length: 74.53
    Episode_Reward/reaching_object: 0.2534
     Episode_Reward/lifting_object: 7.3978
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 53.5833
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 2.13s
                      Time elapsed: 00:17:47
                               ETA: 00:56:12

################################################################################
                     [1m Learning iteration 481/2000 [0m                      

                       Computation: 47520 steps/s (collection: 1.978s, learning 0.091s)
             Mean action noise std: 2.10
          Mean value_function loss: 73.3862
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 47.4037
                       Mean reward: 41.84
               Mean episode length: 71.91
    Episode_Reward/reaching_object: 0.2673
     Episode_Reward/lifting_object: 7.8469
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 55.6667
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 2.07s
                      Time elapsed: 00:17:49
                               ETA: 00:56:09

################################################################################
                     [1m Learning iteration 482/2000 [0m                      

                       Computation: 43388 steps/s (collection: 2.148s, learning 0.118s)
             Mean action noise std: 2.10
          Mean value_function loss: 69.6456
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 47.4061
                       Mean reward: 44.73
               Mean episode length: 72.62
    Episode_Reward/reaching_object: 0.2609
     Episode_Reward/lifting_object: 7.8012
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 53.4167
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 2.27s
                      Time elapsed: 00:17:51
                               ETA: 00:56:07

################################################################################
                     [1m Learning iteration 483/2000 [0m                      

                       Computation: 46129 steps/s (collection: 2.021s, learning 0.110s)
             Mean action noise std: 2.10
          Mean value_function loss: 86.9140
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 47.4087
                       Mean reward: 41.89
               Mean episode length: 73.37
    Episode_Reward/reaching_object: 0.2622
     Episode_Reward/lifting_object: 7.7510
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 54.8750
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 2.13s
                      Time elapsed: 00:17:53
                               ETA: 00:56:05

################################################################################
                     [1m Learning iteration 484/2000 [0m                      

                       Computation: 43445 steps/s (collection: 2.089s, learning 0.174s)
             Mean action noise std: 2.10
          Mean value_function loss: 66.5038
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 47.4103
                       Mean reward: 37.41
               Mean episode length: 74.79
    Episode_Reward/reaching_object: 0.2670
     Episode_Reward/lifting_object: 8.0184
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 55.8750
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 2.26s
                      Time elapsed: 00:17:55
                               ETA: 00:56:03

################################################################################
                     [1m Learning iteration 485/2000 [0m                      

                       Computation: 45504 steps/s (collection: 2.062s, learning 0.099s)
             Mean action noise std: 2.10
          Mean value_function loss: 82.5209
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 47.4122
                       Mean reward: 46.13
               Mean episode length: 71.95
    Episode_Reward/reaching_object: 0.2689
     Episode_Reward/lifting_object: 7.9823
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 53.9583
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 2.16s
                      Time elapsed: 00:17:58
                               ETA: 00:56:00

################################################################################
                     [1m Learning iteration 486/2000 [0m                      

                       Computation: 43981 steps/s (collection: 2.090s, learning 0.145s)
             Mean action noise std: 2.10
          Mean value_function loss: 71.3192
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 47.4145
                       Mean reward: 41.07
               Mean episode length: 70.34
    Episode_Reward/reaching_object: 0.2634
     Episode_Reward/lifting_object: 8.0519
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 55.9583
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 2.24s
                      Time elapsed: 00:18:00
                               ETA: 00:55:58

################################################################################
                     [1m Learning iteration 487/2000 [0m                      

                       Computation: 46584 steps/s (collection: 2.021s, learning 0.089s)
             Mean action noise std: 2.10
          Mean value_function loss: 78.2519
               Mean surrogate loss: 0.0093
                 Mean entropy loss: 47.4160
                       Mean reward: 41.19
               Mean episode length: 74.95
    Episode_Reward/reaching_object: 0.2725
     Episode_Reward/lifting_object: 8.4053
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 51.8750
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 2.11s
                      Time elapsed: 00:18:02
                               ETA: 00:55:56

################################################################################
                     [1m Learning iteration 488/2000 [0m                      

                       Computation: 47189 steps/s (collection: 1.992s, learning 0.092s)
             Mean action noise std: 2.10
          Mean value_function loss: 76.5501
               Mean surrogate loss: 0.0098
                 Mean entropy loss: 47.4168
                       Mean reward: 44.98
               Mean episode length: 71.55
    Episode_Reward/reaching_object: 0.2755
     Episode_Reward/lifting_object: 8.6157
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 55.4167
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 2.08s
                      Time elapsed: 00:18:04
                               ETA: 00:55:53

################################################################################
                     [1m Learning iteration 489/2000 [0m                      

                       Computation: 45382 steps/s (collection: 2.068s, learning 0.098s)
             Mean action noise std: 2.10
          Mean value_function loss: 94.4263
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 47.4169
                       Mean reward: 44.93
               Mean episode length: 69.15
    Episode_Reward/reaching_object: 0.2743
     Episode_Reward/lifting_object: 8.7574
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 54.1250
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 2.17s
                      Time elapsed: 00:18:06
                               ETA: 00:55:51

################################################################################
                     [1m Learning iteration 490/2000 [0m                      

                       Computation: 46047 steps/s (collection: 2.000s, learning 0.135s)
             Mean action noise std: 2.10
          Mean value_function loss: 78.4833
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 47.4171
                       Mean reward: 39.77
               Mean episode length: 75.08
    Episode_Reward/reaching_object: 0.2643
     Episode_Reward/lifting_object: 8.3596
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 55.1250
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 2.13s
                      Time elapsed: 00:18:08
                               ETA: 00:55:48

################################################################################
                     [1m Learning iteration 491/2000 [0m                      

                       Computation: 44066 steps/s (collection: 2.088s, learning 0.143s)
             Mean action noise std: 2.10
          Mean value_function loss: 85.4823
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 47.4175
                       Mean reward: 37.89
               Mean episode length: 72.29
    Episode_Reward/reaching_object: 0.2665
     Episode_Reward/lifting_object: 8.3110
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 54.2083
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 2.23s
                      Time elapsed: 00:18:11
                               ETA: 00:55:46

################################################################################
                     [1m Learning iteration 492/2000 [0m                      

                       Computation: 46956 steps/s (collection: 2.001s, learning 0.092s)
             Mean action noise std: 2.10
          Mean value_function loss: 77.6941
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 47.4184
                       Mean reward: 39.99
               Mean episode length: 70.60
    Episode_Reward/reaching_object: 0.2687
     Episode_Reward/lifting_object: 8.3290
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 55.5000
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 2.09s
                      Time elapsed: 00:18:13
                               ETA: 00:55:43

################################################################################
                     [1m Learning iteration 493/2000 [0m                      

                       Computation: 47238 steps/s (collection: 1.987s, learning 0.094s)
             Mean action noise std: 2.10
          Mean value_function loss: 71.9122
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 47.4195
                       Mean reward: 45.99
               Mean episode length: 73.06
    Episode_Reward/reaching_object: 0.2753
     Episode_Reward/lifting_object: 8.2452
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 56.6250
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 2.08s
                      Time elapsed: 00:18:15
                               ETA: 00:55:41

################################################################################
                     [1m Learning iteration 494/2000 [0m                      

                       Computation: 47349 steps/s (collection: 1.987s, learning 0.089s)
             Mean action noise std: 2.10
          Mean value_function loss: 75.4373
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 47.4211
                       Mean reward: 50.19
               Mean episode length: 74.98
    Episode_Reward/reaching_object: 0.2691
     Episode_Reward/lifting_object: 8.4896
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 56.9583
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 2.08s
                      Time elapsed: 00:18:17
                               ETA: 00:55:38

################################################################################
                     [1m Learning iteration 495/2000 [0m                      

                       Computation: 47117 steps/s (collection: 2.000s, learning 0.086s)
             Mean action noise std: 2.10
          Mean value_function loss: 69.9909
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 47.4217
                       Mean reward: 40.55
               Mean episode length: 77.34
    Episode_Reward/reaching_object: 0.2713
     Episode_Reward/lifting_object: 8.5637
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 54.2917
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 2.09s
                      Time elapsed: 00:18:19
                               ETA: 00:55:36

################################################################################
                     [1m Learning iteration 496/2000 [0m                      

                       Computation: 46842 steps/s (collection: 2.010s, learning 0.089s)
             Mean action noise std: 2.10
          Mean value_function loss: 164.5940
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 47.4229
                       Mean reward: 42.35
               Mean episode length: 77.65
    Episode_Reward/reaching_object: 0.2686
     Episode_Reward/lifting_object: 8.2320
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 54.4583
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 2.10s
                      Time elapsed: 00:18:21
                               ETA: 00:55:33

################################################################################
                     [1m Learning iteration 497/2000 [0m                      

                       Computation: 46662 steps/s (collection: 2.014s, learning 0.093s)
             Mean action noise std: 2.10
          Mean value_function loss: 106.6349
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 47.4259
                       Mean reward: 39.21
               Mean episode length: 74.24
    Episode_Reward/reaching_object: 0.2710
     Episode_Reward/lifting_object: 7.8575
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 57.7083
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 2.11s
                      Time elapsed: 00:18:23
                               ETA: 00:55:30

################################################################################
                     [1m Learning iteration 498/2000 [0m                      

                       Computation: 46827 steps/s (collection: 1.982s, learning 0.118s)
             Mean action noise std: 2.10
          Mean value_function loss: 79.0330
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 47.4294
                       Mean reward: 40.26
               Mean episode length: 73.14
    Episode_Reward/reaching_object: 0.2762
     Episode_Reward/lifting_object: 7.7438
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 55.1667
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 2.10s
                      Time elapsed: 00:18:25
                               ETA: 00:55:28

################################################################################
                     [1m Learning iteration 499/2000 [0m                      

                       Computation: 45264 steps/s (collection: 2.014s, learning 0.158s)
             Mean action noise std: 2.10
          Mean value_function loss: 100.8656
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 47.4342
                       Mean reward: 40.73
               Mean episode length: 77.06
    Episode_Reward/reaching_object: 0.2791
     Episode_Reward/lifting_object: 8.1502
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 54.2500
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 2.17s
                      Time elapsed: 00:18:27
                               ETA: 00:55:25

################################################################################
                     [1m Learning iteration 500/2000 [0m                      

                       Computation: 47879 steps/s (collection: 1.957s, learning 0.096s)
             Mean action noise std: 2.10
          Mean value_function loss: 88.0057
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 47.4378
                       Mean reward: 40.91
               Mean episode length: 74.81
    Episode_Reward/reaching_object: 0.2749
     Episode_Reward/lifting_object: 7.6892
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 56.9167
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 2.05s
                      Time elapsed: 00:18:29
                               ETA: 00:55:23

################################################################################
                     [1m Learning iteration 501/2000 [0m                      

                       Computation: 46575 steps/s (collection: 2.004s, learning 0.107s)
             Mean action noise std: 2.10
          Mean value_function loss: 109.4770
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 47.4386
                       Mean reward: 40.11
               Mean episode length: 74.06
    Episode_Reward/reaching_object: 0.2748
     Episode_Reward/lifting_object: 8.1831
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 55.2500
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 2.11s
                      Time elapsed: 00:18:32
                               ETA: 00:55:20

################################################################################
                     [1m Learning iteration 502/2000 [0m                      

                       Computation: 46134 steps/s (collection: 2.035s, learning 0.096s)
             Mean action noise std: 2.10
          Mean value_function loss: 165.0981
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 47.4396
                       Mean reward: 38.15
               Mean episode length: 72.46
    Episode_Reward/reaching_object: 0.2711
     Episode_Reward/lifting_object: 7.8656
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 55.5417
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 2.13s
                      Time elapsed: 00:18:34
                               ETA: 00:55:18

################################################################################
                     [1m Learning iteration 503/2000 [0m                      

                       Computation: 47361 steps/s (collection: 1.990s, learning 0.086s)
             Mean action noise std: 2.10
          Mean value_function loss: 143.6475
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 47.4438
                       Mean reward: 44.65
               Mean episode length: 71.93
    Episode_Reward/reaching_object: 0.2686
     Episode_Reward/lifting_object: 8.0494
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 55.1667
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 2.08s
                      Time elapsed: 00:18:36
                               ETA: 00:55:15

################################################################################
                     [1m Learning iteration 504/2000 [0m                      

                       Computation: 47025 steps/s (collection: 1.965s, learning 0.125s)
             Mean action noise std: 2.10
          Mean value_function loss: 82.2917
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 47.4501
                       Mean reward: 41.80
               Mean episode length: 73.33
    Episode_Reward/reaching_object: 0.2790
     Episode_Reward/lifting_object: 8.4622
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 51.7917
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 2.09s
                      Time elapsed: 00:18:38
                               ETA: 00:55:13

################################################################################
                     [1m Learning iteration 505/2000 [0m                      

                       Computation: 46476 steps/s (collection: 2.013s, learning 0.103s)
             Mean action noise std: 2.10
          Mean value_function loss: 86.7574
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 47.4546
                       Mean reward: 50.34
               Mean episode length: 72.91
    Episode_Reward/reaching_object: 0.2769
     Episode_Reward/lifting_object: 8.2515
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 56.4583
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 2.12s
                      Time elapsed: 00:18:40
                               ETA: 00:55:10

################################################################################
                     [1m Learning iteration 506/2000 [0m                      

                       Computation: 46118 steps/s (collection: 2.024s, learning 0.107s)
             Mean action noise std: 2.10
          Mean value_function loss: 93.9016
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.4571
                       Mean reward: 43.67
               Mean episode length: 74.72
    Episode_Reward/reaching_object: 0.2823
     Episode_Reward/lifting_object: 8.6091
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 53.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 2.13s
                      Time elapsed: 00:18:42
                               ETA: 00:55:08

################################################################################
                     [1m Learning iteration 507/2000 [0m                      

                       Computation: 46844 steps/s (collection: 2.015s, learning 0.084s)
             Mean action noise std: 2.10
          Mean value_function loss: 87.5210
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 47.4597
                       Mean reward: 47.53
               Mean episode length: 74.22
    Episode_Reward/reaching_object: 0.2895
     Episode_Reward/lifting_object: 9.0995
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 53.2917
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 2.10s
                      Time elapsed: 00:18:44
                               ETA: 00:55:05

################################################################################
                     [1m Learning iteration 508/2000 [0m                      

                       Computation: 43552 steps/s (collection: 2.149s, learning 0.109s)
             Mean action noise std: 2.10
          Mean value_function loss: 102.0967
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 47.4624
                       Mean reward: 37.35
               Mean episode length: 73.59
    Episode_Reward/reaching_object: 0.2794
     Episode_Reward/lifting_object: 8.3125
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 52.6667
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 2.26s
                      Time elapsed: 00:18:46
                               ETA: 00:55:03

################################################################################
                     [1m Learning iteration 509/2000 [0m                      

                       Computation: 45031 steps/s (collection: 2.069s, learning 0.114s)
             Mean action noise std: 2.10
          Mean value_function loss: 90.0126
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 47.4657
                       Mean reward: 40.31
               Mean episode length: 77.28
    Episode_Reward/reaching_object: 0.2932
     Episode_Reward/lifting_object: 8.7591
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 50.7917
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 2.18s
                      Time elapsed: 00:18:49
                               ETA: 00:55:01

################################################################################
                     [1m Learning iteration 510/2000 [0m                      

                       Computation: 46217 steps/s (collection: 2.005s, learning 0.122s)
             Mean action noise std: 2.10
          Mean value_function loss: 105.1789
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 47.4712
                       Mean reward: 46.36
               Mean episode length: 75.51
    Episode_Reward/reaching_object: 0.2867
     Episode_Reward/lifting_object: 9.2284
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 51.3333
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 2.13s
                      Time elapsed: 00:18:51
                               ETA: 00:54:58

################################################################################
                     [1m Learning iteration 511/2000 [0m                      

                       Computation: 45456 steps/s (collection: 2.077s, learning 0.086s)
             Mean action noise std: 2.10
          Mean value_function loss: 75.2663
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 47.4755
                       Mean reward: 46.31
               Mean episode length: 77.58
    Episode_Reward/reaching_object: 0.2945
     Episode_Reward/lifting_object: 9.1487
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 54.9167
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 2.16s
                      Time elapsed: 00:18:53
                               ETA: 00:54:56

################################################################################
                     [1m Learning iteration 512/2000 [0m                      

                       Computation: 43238 steps/s (collection: 2.124s, learning 0.150s)
             Mean action noise std: 2.10
          Mean value_function loss: 87.0372
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 47.4764
                       Mean reward: 48.53
               Mean episode length: 79.85
    Episode_Reward/reaching_object: 0.2929
     Episode_Reward/lifting_object: 9.2622
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 53.2917
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 2.27s
                      Time elapsed: 00:18:55
                               ETA: 00:54:54

################################################################################
                     [1m Learning iteration 513/2000 [0m                      

                       Computation: 45488 steps/s (collection: 2.072s, learning 0.089s)
             Mean action noise std: 2.10
          Mean value_function loss: 79.6586
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 47.4767
                       Mean reward: 38.42
               Mean episode length: 77.00
    Episode_Reward/reaching_object: 0.2865
     Episode_Reward/lifting_object: 8.6539
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 52.5417
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 2.16s
                      Time elapsed: 00:18:57
                               ETA: 00:54:51

################################################################################
                     [1m Learning iteration 514/2000 [0m                      

                       Computation: 45955 steps/s (collection: 2.034s, learning 0.105s)
             Mean action noise std: 2.10
          Mean value_function loss: 83.9081
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 47.4772
                       Mean reward: 51.69
               Mean episode length: 80.00
    Episode_Reward/reaching_object: 0.2905
     Episode_Reward/lifting_object: 9.0929
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 51.7500
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 2.14s
                      Time elapsed: 00:19:00
                               ETA: 00:54:49

################################################################################
                     [1m Learning iteration 515/2000 [0m                      

                       Computation: 47421 steps/s (collection: 1.980s, learning 0.093s)
             Mean action noise std: 2.11
          Mean value_function loss: 79.5480
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 47.4794
                       Mean reward: 39.54
               Mean episode length: 77.41
    Episode_Reward/reaching_object: 0.2914
     Episode_Reward/lifting_object: 8.9426
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 51.3750
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 2.07s
                      Time elapsed: 00:19:02
                               ETA: 00:54:46

################################################################################
                     [1m Learning iteration 516/2000 [0m                      

                       Computation: 46780 steps/s (collection: 2.016s, learning 0.086s)
             Mean action noise std: 2.11
          Mean value_function loss: 72.7073
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 47.4829
                       Mean reward: 45.80
               Mean episode length: 78.53
    Episode_Reward/reaching_object: 0.2941
     Episode_Reward/lifting_object: 9.5649
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 53.0417
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 2.10s
                      Time elapsed: 00:19:04
                               ETA: 00:54:44

################################################################################
                     [1m Learning iteration 517/2000 [0m                      

                       Computation: 45853 steps/s (collection: 2.052s, learning 0.092s)
             Mean action noise std: 2.11
          Mean value_function loss: 100.7406
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 47.4894
                       Mean reward: 49.68
               Mean episode length: 76.47
    Episode_Reward/reaching_object: 0.2957
     Episode_Reward/lifting_object: 9.7075
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 51.5417
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 2.14s
                      Time elapsed: 00:19:06
                               ETA: 00:54:41

################################################################################
                     [1m Learning iteration 518/2000 [0m                      

                       Computation: 46894 steps/s (collection: 2.005s, learning 0.092s)
             Mean action noise std: 2.11
          Mean value_function loss: 103.8061
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 47.4966
                       Mean reward: 49.08
               Mean episode length: 76.43
    Episode_Reward/reaching_object: 0.2898
     Episode_Reward/lifting_object: 9.3152
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 52.3333
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 2.10s
                      Time elapsed: 00:19:08
                               ETA: 00:54:39

################################################################################
                     [1m Learning iteration 519/2000 [0m                      

                       Computation: 46269 steps/s (collection: 2.030s, learning 0.095s)
             Mean action noise std: 2.11
          Mean value_function loss: 82.3568
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 47.4989
                       Mean reward: 49.13
               Mean episode length: 77.98
    Episode_Reward/reaching_object: 0.2844
     Episode_Reward/lifting_object: 9.1560
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 54.3750
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 2.12s
                      Time elapsed: 00:19:10
                               ETA: 00:54:36

################################################################################
                     [1m Learning iteration 520/2000 [0m                      

                       Computation: 45455 steps/s (collection: 2.055s, learning 0.108s)
             Mean action noise std: 2.11
          Mean value_function loss: 86.4060
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 47.5009
                       Mean reward: 54.63
               Mean episode length: 80.33
    Episode_Reward/reaching_object: 0.2926
     Episode_Reward/lifting_object: 9.7575
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 53.0833
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 2.16s
                      Time elapsed: 00:19:12
                               ETA: 00:54:34

################################################################################
                     [1m Learning iteration 521/2000 [0m                      

                       Computation: 47152 steps/s (collection: 1.986s, learning 0.099s)
             Mean action noise std: 2.11
          Mean value_function loss: 86.3363
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 47.5026
                       Mean reward: 45.35
               Mean episode length: 75.11
    Episode_Reward/reaching_object: 0.2907
     Episode_Reward/lifting_object: 9.5166
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 53.8333
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 2.08s
                      Time elapsed: 00:19:14
                               ETA: 00:54:31

################################################################################
                     [1m Learning iteration 522/2000 [0m                      

                       Computation: 46211 steps/s (collection: 2.033s, learning 0.094s)
             Mean action noise std: 2.11
          Mean value_function loss: 101.6969
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 47.5040
                       Mean reward: 53.34
               Mean episode length: 75.41
    Episode_Reward/reaching_object: 0.2863
     Episode_Reward/lifting_object: 9.4478
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 53.7083
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 2.13s
                      Time elapsed: 00:19:16
                               ETA: 00:54:29

################################################################################
                     [1m Learning iteration 523/2000 [0m                      

                       Computation: 45457 steps/s (collection: 2.032s, learning 0.131s)
             Mean action noise std: 2.11
          Mean value_function loss: 92.7035
               Mean surrogate loss: 0.0141
                 Mean entropy loss: 47.5050
                       Mean reward: 50.25
               Mean episode length: 79.55
    Episode_Reward/reaching_object: 0.2862
     Episode_Reward/lifting_object: 9.3834
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 55.5833
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 2.16s
                      Time elapsed: 00:19:19
                               ETA: 00:54:27

################################################################################
                     [1m Learning iteration 524/2000 [0m                      

                       Computation: 45933 steps/s (collection: 2.045s, learning 0.096s)
             Mean action noise std: 2.11
          Mean value_function loss: 101.3793
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 47.5054
                       Mean reward: 55.68
               Mean episode length: 77.70
    Episode_Reward/reaching_object: 0.2871
     Episode_Reward/lifting_object: 9.8766
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 53.6250
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 2.14s
                      Time elapsed: 00:19:21
                               ETA: 00:54:24

################################################################################
                     [1m Learning iteration 525/2000 [0m                      

                       Computation: 46530 steps/s (collection: 2.013s, learning 0.100s)
             Mean action noise std: 2.11
          Mean value_function loss: 96.3378
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 47.5070
                       Mean reward: 50.14
               Mean episode length: 71.53
    Episode_Reward/reaching_object: 0.2838
     Episode_Reward/lifting_object: 10.0655
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 54.1250
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 2.11s
                      Time elapsed: 00:19:23
                               ETA: 00:54:22

################################################################################
                     [1m Learning iteration 526/2000 [0m                      

                       Computation: 46298 steps/s (collection: 2.028s, learning 0.095s)
             Mean action noise std: 2.11
          Mean value_function loss: 89.9281
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 47.5082
                       Mean reward: 48.22
               Mean episode length: 68.00
    Episode_Reward/reaching_object: 0.2770
     Episode_Reward/lifting_object: 9.7937
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 53.7917
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 2.12s
                      Time elapsed: 00:19:25
                               ETA: 00:54:19

################################################################################
                     [1m Learning iteration 527/2000 [0m                      

                       Computation: 45761 steps/s (collection: 2.045s, learning 0.103s)
             Mean action noise std: 2.11
          Mean value_function loss: 96.5102
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 47.5092
                       Mean reward: 50.67
               Mean episode length: 73.42
    Episode_Reward/reaching_object: 0.2773
     Episode_Reward/lifting_object: 9.8419
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 54.4583
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 2.15s
                      Time elapsed: 00:19:27
                               ETA: 00:54:17

################################################################################
                     [1m Learning iteration 528/2000 [0m                      

                       Computation: 45418 steps/s (collection: 2.062s, learning 0.103s)
             Mean action noise std: 2.11
          Mean value_function loss: 107.1082
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 47.5107
                       Mean reward: 49.08
               Mean episode length: 68.80
    Episode_Reward/reaching_object: 0.2739
     Episode_Reward/lifting_object: 9.5580
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 54.5417
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 2.16s
                      Time elapsed: 00:19:29
                               ETA: 00:54:15

################################################################################
                     [1m Learning iteration 529/2000 [0m                      

                       Computation: 46151 steps/s (collection: 2.041s, learning 0.089s)
             Mean action noise std: 2.11
          Mean value_function loss: 97.4705
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 47.5127
                       Mean reward: 52.28
               Mean episode length: 75.26
    Episode_Reward/reaching_object: 0.2776
     Episode_Reward/lifting_object: 9.9668
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 55.2083
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 2.13s
                      Time elapsed: 00:19:31
                               ETA: 00:54:12

################################################################################
                     [1m Learning iteration 530/2000 [0m                      

                       Computation: 46302 steps/s (collection: 2.029s, learning 0.095s)
             Mean action noise std: 2.11
          Mean value_function loss: 94.1055
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 47.5147
                       Mean reward: 54.55
               Mean episode length: 72.93
    Episode_Reward/reaching_object: 0.2779
     Episode_Reward/lifting_object: 10.3359
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 54.6667
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 2.12s
                      Time elapsed: 00:19:34
                               ETA: 00:54:10

################################################################################
                     [1m Learning iteration 531/2000 [0m                      

                       Computation: 45614 steps/s (collection: 2.002s, learning 0.153s)
             Mean action noise std: 2.11
          Mean value_function loss: 118.5389
               Mean surrogate loss: 0.0107
                 Mean entropy loss: 47.5161
                       Mean reward: 47.18
               Mean episode length: 68.88
    Episode_Reward/reaching_object: 0.2809
     Episode_Reward/lifting_object: 10.4532
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 54.5000
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 2.16s
                      Time elapsed: 00:19:36
                               ETA: 00:54:07

################################################################################
                     [1m Learning iteration 532/2000 [0m                      

                       Computation: 43241 steps/s (collection: 2.090s, learning 0.183s)
             Mean action noise std: 2.11
          Mean value_function loss: 100.7375
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 47.5163
                       Mean reward: 45.78
               Mean episode length: 70.06
    Episode_Reward/reaching_object: 0.2806
     Episode_Reward/lifting_object: 9.8989
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 55.2917
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 2.27s
                      Time elapsed: 00:19:38
                               ETA: 00:54:05

################################################################################
                     [1m Learning iteration 533/2000 [0m                      

                       Computation: 46753 steps/s (collection: 2.015s, learning 0.088s)
             Mean action noise std: 2.11
          Mean value_function loss: 107.0023
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 47.5174
                       Mean reward: 54.29
               Mean episode length: 75.15
    Episode_Reward/reaching_object: 0.2856
     Episode_Reward/lifting_object: 10.4648
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 54.8750
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 2.10s
                      Time elapsed: 00:19:40
                               ETA: 00:54:03

################################################################################
                     [1m Learning iteration 534/2000 [0m                      

                       Computation: 46536 steps/s (collection: 1.999s, learning 0.113s)
             Mean action noise std: 2.11
          Mean value_function loss: 113.7631
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 47.5190
                       Mean reward: 51.84
               Mean episode length: 73.47
    Episode_Reward/reaching_object: 0.2923
     Episode_Reward/lifting_object: 10.5759
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 53.0417
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 2.11s
                      Time elapsed: 00:19:42
                               ETA: 00:54:00

################################################################################
                     [1m Learning iteration 535/2000 [0m                      

                       Computation: 46061 steps/s (collection: 2.041s, learning 0.093s)
             Mean action noise std: 2.11
          Mean value_function loss: 108.4120
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 47.5197
                       Mean reward: 50.85
               Mean episode length: 73.16
    Episode_Reward/reaching_object: 0.2930
     Episode_Reward/lifting_object: 10.4511
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 56.5000
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 2.13s
                      Time elapsed: 00:19:44
                               ETA: 00:53:58

################################################################################
                     [1m Learning iteration 536/2000 [0m                      

                       Computation: 45248 steps/s (collection: 2.052s, learning 0.120s)
             Mean action noise std: 2.11
          Mean value_function loss: 116.7457
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 47.5215
                       Mean reward: 50.45
               Mean episode length: 75.31
    Episode_Reward/reaching_object: 0.2910
     Episode_Reward/lifting_object: 10.3427
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 57.7083
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 2.17s
                      Time elapsed: 00:19:46
                               ETA: 00:53:56

################################################################################
                     [1m Learning iteration 537/2000 [0m                      

                       Computation: 46010 steps/s (collection: 2.015s, learning 0.122s)
             Mean action noise std: 2.11
          Mean value_function loss: 130.9077
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 47.5242
                       Mean reward: 48.52
               Mean episode length: 69.98
    Episode_Reward/reaching_object: 0.2819
     Episode_Reward/lifting_object: 9.9957
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 55.1250
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 2.14s
                      Time elapsed: 00:19:49
                               ETA: 00:53:53

################################################################################
                     [1m Learning iteration 538/2000 [0m                      

                       Computation: 45041 steps/s (collection: 2.017s, learning 0.165s)
             Mean action noise std: 2.11
          Mean value_function loss: 108.1921
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 47.5264
                       Mean reward: 51.58
               Mean episode length: 69.36
    Episode_Reward/reaching_object: 0.2849
     Episode_Reward/lifting_object: 10.1399
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 55.7500
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 2.18s
                      Time elapsed: 00:19:51
                               ETA: 00:53:51

################################################################################
                     [1m Learning iteration 539/2000 [0m                      

                       Computation: 46713 steps/s (collection: 2.010s, learning 0.094s)
             Mean action noise std: 2.11
          Mean value_function loss: 103.8177
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 47.5277
                       Mean reward: 48.94
               Mean episode length: 67.22
    Episode_Reward/reaching_object: 0.2835
     Episode_Reward/lifting_object: 10.4331
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 55.7917
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 2.10s
                      Time elapsed: 00:19:53
                               ETA: 00:53:48

################################################################################
                     [1m Learning iteration 540/2000 [0m                      

                       Computation: 46995 steps/s (collection: 1.994s, learning 0.098s)
             Mean action noise std: 2.11
          Mean value_function loss: 100.8437
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 47.5292
                       Mean reward: 52.76
               Mean episode length: 70.55
    Episode_Reward/reaching_object: 0.2885
     Episode_Reward/lifting_object: 10.5361
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 58.1250
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 2.09s
                      Time elapsed: 00:19:55
                               ETA: 00:53:46

################################################################################
                     [1m Learning iteration 541/2000 [0m                      

                       Computation: 46600 steps/s (collection: 2.005s, learning 0.104s)
             Mean action noise std: 2.11
          Mean value_function loss: 107.4669
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 47.5305
                       Mean reward: 52.47
               Mean episode length: 68.93
    Episode_Reward/reaching_object: 0.2812
     Episode_Reward/lifting_object: 10.4025
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 54.0833
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 2.11s
                      Time elapsed: 00:19:57
                               ETA: 00:53:43

################################################################################
                     [1m Learning iteration 542/2000 [0m                      

                       Computation: 45726 steps/s (collection: 2.058s, learning 0.092s)
             Mean action noise std: 2.11
          Mean value_function loss: 123.0742
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 47.5332
                       Mean reward: 51.20
               Mean episode length: 70.90
    Episode_Reward/reaching_object: 0.2873
     Episode_Reward/lifting_object: 10.6512
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 55.6250
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 2.15s
                      Time elapsed: 00:19:59
                               ETA: 00:53:41

################################################################################
                     [1m Learning iteration 543/2000 [0m                      

                       Computation: 45707 steps/s (collection: 2.065s, learning 0.086s)
             Mean action noise std: 2.11
          Mean value_function loss: 107.0778
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 47.5370
                       Mean reward: 57.51
               Mean episode length: 75.36
    Episode_Reward/reaching_object: 0.2874
     Episode_Reward/lifting_object: 10.9130
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 54.5000
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 2.15s
                      Time elapsed: 00:20:01
                               ETA: 00:53:39

################################################################################
                     [1m Learning iteration 544/2000 [0m                      

                       Computation: 46130 steps/s (collection: 2.034s, learning 0.097s)
             Mean action noise std: 2.11
          Mean value_function loss: 112.1351
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 47.5390
                       Mean reward: 60.15
               Mean episode length: 76.05
    Episode_Reward/reaching_object: 0.2872
     Episode_Reward/lifting_object: 11.0090
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 56.5000
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 2.13s
                      Time elapsed: 00:20:04
                               ETA: 00:53:36

################################################################################
                     [1m Learning iteration 545/2000 [0m                      

                       Computation: 45716 steps/s (collection: 2.058s, learning 0.093s)
             Mean action noise std: 2.11
          Mean value_function loss: 117.8186
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 47.5401
                       Mean reward: 59.55
               Mean episode length: 76.15
    Episode_Reward/reaching_object: 0.2946
     Episode_Reward/lifting_object: 10.9578
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 53.1667
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 2.15s
                      Time elapsed: 00:20:06
                               ETA: 00:53:34

################################################################################
                     [1m Learning iteration 546/2000 [0m                      

                       Computation: 44638 steps/s (collection: 2.065s, learning 0.137s)
             Mean action noise std: 2.11
          Mean value_function loss: 127.6547
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 47.5412
                       Mean reward: 54.47
               Mean episode length: 73.93
    Episode_Reward/reaching_object: 0.2911
     Episode_Reward/lifting_object: 10.7033
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 56.2083
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 2.20s
                      Time elapsed: 00:20:08
                               ETA: 00:53:32

################################################################################
                     [1m Learning iteration 547/2000 [0m                      

                       Computation: 43270 steps/s (collection: 2.127s, learning 0.145s)
             Mean action noise std: 2.11
          Mean value_function loss: 113.9426
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 47.5424
                       Mean reward: 59.60
               Mean episode length: 75.44
    Episode_Reward/reaching_object: 0.2880
     Episode_Reward/lifting_object: 10.2761
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 55.5000
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 2.27s
                      Time elapsed: 00:20:10
                               ETA: 00:53:30

################################################################################
                     [1m Learning iteration 548/2000 [0m                      

                       Computation: 43373 steps/s (collection: 2.168s, learning 0.099s)
             Mean action noise std: 2.11
          Mean value_function loss: 118.3012
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 47.5434
                       Mean reward: 54.81
               Mean episode length: 72.24
    Episode_Reward/reaching_object: 0.2932
     Episode_Reward/lifting_object: 11.0497
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 53.3750
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 2.27s
                      Time elapsed: 00:20:12
                               ETA: 00:53:27

################################################################################
                     [1m Learning iteration 549/2000 [0m                      

                       Computation: 42945 steps/s (collection: 2.165s, learning 0.124s)
             Mean action noise std: 2.11
          Mean value_function loss: 136.4301
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 47.5453
                       Mean reward: 57.27
               Mean episode length: 77.84
    Episode_Reward/reaching_object: 0.2921
     Episode_Reward/lifting_object: 10.7940
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 54.4167
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 2.29s
                      Time elapsed: 00:20:15
                               ETA: 00:53:25

################################################################################
                     [1m Learning iteration 550/2000 [0m                      

                       Computation: 44883 steps/s (collection: 2.072s, learning 0.119s)
             Mean action noise std: 2.11
          Mean value_function loss: 115.4278
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 47.5469
                       Mean reward: 56.92
               Mean episode length: 74.65
    Episode_Reward/reaching_object: 0.2935
     Episode_Reward/lifting_object: 10.7426
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 54.4583
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 2.19s
                      Time elapsed: 00:20:17
                               ETA: 00:53:23

################################################################################
                     [1m Learning iteration 551/2000 [0m                      

                       Computation: 44983 steps/s (collection: 2.093s, learning 0.092s)
             Mean action noise std: 2.11
          Mean value_function loss: 134.2991
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 47.5483
                       Mean reward: 57.33
               Mean episode length: 74.12
    Episode_Reward/reaching_object: 0.2952
     Episode_Reward/lifting_object: 11.2917
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 53.7500
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 2.19s
                      Time elapsed: 00:20:19
                               ETA: 00:53:21

################################################################################
                     [1m Learning iteration 552/2000 [0m                      

                       Computation: 45017 steps/s (collection: 2.071s, learning 0.113s)
             Mean action noise std: 2.11
          Mean value_function loss: 127.2522
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 47.5490
                       Mean reward: 55.00
               Mean episode length: 70.73
    Episode_Reward/reaching_object: 0.2967
     Episode_Reward/lifting_object: 11.0380
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 56.5417
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 2.18s
                      Time elapsed: 00:20:21
                               ETA: 00:53:19

################################################################################
                     [1m Learning iteration 553/2000 [0m                      

                       Computation: 44870 steps/s (collection: 2.078s, learning 0.113s)
             Mean action noise std: 2.11
          Mean value_function loss: 110.6154
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 47.5498
                       Mean reward: 58.10
               Mean episode length: 77.45
    Episode_Reward/reaching_object: 0.2930
     Episode_Reward/lifting_object: 10.7968
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 56.5417
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 2.19s
                      Time elapsed: 00:20:23
                               ETA: 00:53:16

################################################################################
                     [1m Learning iteration 554/2000 [0m                      

                       Computation: 45373 steps/s (collection: 2.077s, learning 0.090s)
             Mean action noise std: 2.11
          Mean value_function loss: 94.6414
               Mean surrogate loss: 0.0209
                 Mean entropy loss: 47.5506
                       Mean reward: 51.70
               Mean episode length: 70.53
    Episode_Reward/reaching_object: 0.2945
     Episode_Reward/lifting_object: 10.8735
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 57.0417
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 2.17s
                      Time elapsed: 00:20:26
                               ETA: 00:53:14

################################################################################
                     [1m Learning iteration 555/2000 [0m                      

                       Computation: 44915 steps/s (collection: 2.037s, learning 0.152s)
             Mean action noise std: 2.11
          Mean value_function loss: 115.8093
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 47.5507
                       Mean reward: 55.96
               Mean episode length: 70.75
    Episode_Reward/reaching_object: 0.2925
     Episode_Reward/lifting_object: 10.7522
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 54.4583
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 2.19s
                      Time elapsed: 00:20:28
                               ETA: 00:53:12

################################################################################
                     [1m Learning iteration 556/2000 [0m                      

                       Computation: 45045 steps/s (collection: 2.057s, learning 0.126s)
             Mean action noise std: 2.11
          Mean value_function loss: 125.4800
               Mean surrogate loss: 0.0131
                 Mean entropy loss: 47.5509
                       Mean reward: 58.16
               Mean episode length: 72.55
    Episode_Reward/reaching_object: 0.2904
     Episode_Reward/lifting_object: 10.4843
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 53.8750
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 2.18s
                      Time elapsed: 00:20:30
                               ETA: 00:53:10

################################################################################
                     [1m Learning iteration 557/2000 [0m                      

                       Computation: 46036 steps/s (collection: 2.042s, learning 0.093s)
             Mean action noise std: 2.11
          Mean value_function loss: 116.4648
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 47.5511
                       Mean reward: 62.69
               Mean episode length: 75.24
    Episode_Reward/reaching_object: 0.2933
     Episode_Reward/lifting_object: 10.4511
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 56.2500
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 2.14s
                      Time elapsed: 00:20:32
                               ETA: 00:53:07

################################################################################
                     [1m Learning iteration 558/2000 [0m                      

                       Computation: 45409 steps/s (collection: 2.071s, learning 0.094s)
             Mean action noise std: 2.11
          Mean value_function loss: 105.4327
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 47.5513
                       Mean reward: 54.33
               Mean episode length: 72.92
    Episode_Reward/reaching_object: 0.2958
     Episode_Reward/lifting_object: 10.5521
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 55.0417
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 2.16s
                      Time elapsed: 00:20:34
                               ETA: 00:53:05

################################################################################
                     [1m Learning iteration 559/2000 [0m                      

                       Computation: 45155 steps/s (collection: 2.075s, learning 0.102s)
             Mean action noise std: 2.11
          Mean value_function loss: 125.4434
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 47.5515
                       Mean reward: 54.08
               Mean episode length: 70.69
    Episode_Reward/reaching_object: 0.2969
     Episode_Reward/lifting_object: 10.8430
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 54.2500
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 2.18s
                      Time elapsed: 00:20:36
                               ETA: 00:53:03

################################################################################
                     [1m Learning iteration 560/2000 [0m                      

                       Computation: 44317 steps/s (collection: 2.108s, learning 0.111s)
             Mean action noise std: 2.11
          Mean value_function loss: 123.1976
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 47.5525
                       Mean reward: 58.86
               Mean episode length: 74.62
    Episode_Reward/reaching_object: 0.2929
     Episode_Reward/lifting_object: 10.4369
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 56.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 2.22s
                      Time elapsed: 00:20:39
                               ETA: 00:53:00

################################################################################
                     [1m Learning iteration 561/2000 [0m                      

                       Computation: 43709 steps/s (collection: 2.080s, learning 0.169s)
             Mean action noise std: 2.11
          Mean value_function loss: 110.7614
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 47.5537
                       Mean reward: 53.48
               Mean episode length: 71.68
    Episode_Reward/reaching_object: 0.2955
     Episode_Reward/lifting_object: 10.6528
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 55.1250
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 2.25s
                      Time elapsed: 00:20:41
                               ETA: 00:52:58

################################################################################
                     [1m Learning iteration 562/2000 [0m                      

                       Computation: 45671 steps/s (collection: 2.060s, learning 0.092s)
             Mean action noise std: 2.11
          Mean value_function loss: 122.9093
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.5556
                       Mean reward: 64.45
               Mean episode length: 75.14
    Episode_Reward/reaching_object: 0.3014
     Episode_Reward/lifting_object: 11.3990
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 54.2500
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 2.15s
                      Time elapsed: 00:20:43
                               ETA: 00:52:56

################################################################################
                     [1m Learning iteration 563/2000 [0m                      

                       Computation: 44599 steps/s (collection: 2.094s, learning 0.111s)
             Mean action noise std: 2.11
          Mean value_function loss: 127.1285
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 47.5595
                       Mean reward: 52.18
               Mean episode length: 72.67
    Episode_Reward/reaching_object: 0.3003
     Episode_Reward/lifting_object: 11.2078
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 53.8333
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 2.20s
                      Time elapsed: 00:20:45
                               ETA: 00:52:54

################################################################################
                     [1m Learning iteration 564/2000 [0m                      

                       Computation: 44771 steps/s (collection: 2.063s, learning 0.133s)
             Mean action noise std: 2.11
          Mean value_function loss: 129.2035
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 47.5639
                       Mean reward: 53.56
               Mean episode length: 72.22
    Episode_Reward/reaching_object: 0.3049
     Episode_Reward/lifting_object: 11.5908
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 54.9583
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 2.20s
                      Time elapsed: 00:20:48
                               ETA: 00:52:51

################################################################################
                     [1m Learning iteration 565/2000 [0m                      

                       Computation: 44309 steps/s (collection: 2.115s, learning 0.104s)
             Mean action noise std: 2.11
          Mean value_function loss: 114.6549
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 47.5664
                       Mean reward: 62.48
               Mean episode length: 71.75
    Episode_Reward/reaching_object: 0.3020
     Episode_Reward/lifting_object: 11.5584
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 55.0417
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 2.22s
                      Time elapsed: 00:20:50
                               ETA: 00:52:49

################################################################################
                     [1m Learning iteration 566/2000 [0m                      

                       Computation: 43254 steps/s (collection: 2.100s, learning 0.173s)
             Mean action noise std: 2.12
          Mean value_function loss: 114.8968
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 47.5685
                       Mean reward: 63.41
               Mean episode length: 76.98
    Episode_Reward/reaching_object: 0.3001
     Episode_Reward/lifting_object: 11.4089
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 55.2500
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 2.27s
                      Time elapsed: 00:20:52
                               ETA: 00:52:47

################################################################################
                     [1m Learning iteration 567/2000 [0m                      

                       Computation: 44739 steps/s (collection: 2.098s, learning 0.099s)
             Mean action noise std: 2.12
          Mean value_function loss: 119.9940
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 47.5712
                       Mean reward: 54.40
               Mean episode length: 71.20
    Episode_Reward/reaching_object: 0.2992
     Episode_Reward/lifting_object: 11.4002
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 54.2083
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 2.20s
                      Time elapsed: 00:20:54
                               ETA: 00:52:45

################################################################################
                     [1m Learning iteration 568/2000 [0m                      

                       Computation: 44803 steps/s (collection: 2.086s, learning 0.109s)
             Mean action noise std: 2.12
          Mean value_function loss: 108.2975
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 47.5727
                       Mean reward: 56.87
               Mean episode length: 71.33
    Episode_Reward/reaching_object: 0.2980
     Episode_Reward/lifting_object: 11.4842
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 56.7083
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 2.19s
                      Time elapsed: 00:20:56
                               ETA: 00:52:43

################################################################################
                     [1m Learning iteration 569/2000 [0m                      

                       Computation: 44603 steps/s (collection: 2.111s, learning 0.093s)
             Mean action noise std: 2.12
          Mean value_function loss: 125.5431
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 47.5744
                       Mean reward: 56.41
               Mean episode length: 72.99
    Episode_Reward/reaching_object: 0.2973
     Episode_Reward/lifting_object: 11.1555
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 56.2500
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 2.20s
                      Time elapsed: 00:20:59
                               ETA: 00:52:41

################################################################################
                     [1m Learning iteration 570/2000 [0m                      

                       Computation: 44970 steps/s (collection: 2.087s, learning 0.099s)
             Mean action noise std: 2.12
          Mean value_function loss: 117.7840
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.5760
                       Mean reward: 59.21
               Mean episode length: 73.82
    Episode_Reward/reaching_object: 0.2955
     Episode_Reward/lifting_object: 11.3490
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 52.1250
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 2.19s
                      Time elapsed: 00:21:01
                               ETA: 00:52:38

################################################################################
                     [1m Learning iteration 571/2000 [0m                      

                       Computation: 43502 steps/s (collection: 2.110s, learning 0.150s)
             Mean action noise std: 2.12
          Mean value_function loss: 115.3741
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 47.5789
                       Mean reward: 61.12
               Mean episode length: 74.91
    Episode_Reward/reaching_object: 0.3052
     Episode_Reward/lifting_object: 11.9076
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 53.2917
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 2.26s
                      Time elapsed: 00:21:03
                               ETA: 00:52:36

################################################################################
                     [1m Learning iteration 572/2000 [0m                      

                       Computation: 44387 steps/s (collection: 2.102s, learning 0.113s)
             Mean action noise std: 2.12
          Mean value_function loss: 130.9216
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 47.5832
                       Mean reward: 67.39
               Mean episode length: 72.68
    Episode_Reward/reaching_object: 0.3028
     Episode_Reward/lifting_object: 12.1831
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 53.8333
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 2.21s
                      Time elapsed: 00:21:05
                               ETA: 00:52:34

################################################################################
                     [1m Learning iteration 573/2000 [0m                      

                       Computation: 42617 steps/s (collection: 2.145s, learning 0.162s)
             Mean action noise std: 2.12
          Mean value_function loss: 126.6045
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 47.5854
                       Mean reward: 62.11
               Mean episode length: 76.92
    Episode_Reward/reaching_object: 0.3041
     Episode_Reward/lifting_object: 11.7922
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 55.3333
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 2.31s
                      Time elapsed: 00:21:08
                               ETA: 00:52:32

################################################################################
                     [1m Learning iteration 574/2000 [0m                      

                       Computation: 44990 steps/s (collection: 2.092s, learning 0.093s)
             Mean action noise std: 2.12
          Mean value_function loss: 111.5304
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 47.5869
                       Mean reward: 56.79
               Mean episode length: 71.37
    Episode_Reward/reaching_object: 0.2987
     Episode_Reward/lifting_object: 11.9971
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 55.4583
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 2.19s
                      Time elapsed: 00:21:10
                               ETA: 00:52:30

################################################################################
                     [1m Learning iteration 575/2000 [0m                      

                       Computation: 44810 steps/s (collection: 2.069s, learning 0.125s)
             Mean action noise std: 2.12
          Mean value_function loss: 119.5541
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 47.5909
                       Mean reward: 64.32
               Mean episode length: 72.99
    Episode_Reward/reaching_object: 0.2980
     Episode_Reward/lifting_object: 11.9419
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 53.3333
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 2.19s
                      Time elapsed: 00:21:12
                               ETA: 00:52:27

################################################################################
                     [1m Learning iteration 576/2000 [0m                      

                       Computation: 43340 steps/s (collection: 2.113s, learning 0.155s)
             Mean action noise std: 2.12
          Mean value_function loss: 123.8126
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 47.5952
                       Mean reward: 60.01
               Mean episode length: 75.14
    Episode_Reward/reaching_object: 0.3039
     Episode_Reward/lifting_object: 12.2853
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 54.2917
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 2.27s
                      Time elapsed: 00:21:14
                               ETA: 00:52:25

################################################################################
                     [1m Learning iteration 577/2000 [0m                      

                       Computation: 40782 steps/s (collection: 2.289s, learning 0.122s)
             Mean action noise std: 2.12
          Mean value_function loss: 125.9452
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 47.5983
                       Mean reward: 66.46
               Mean episode length: 78.99
    Episode_Reward/reaching_object: 0.3085
     Episode_Reward/lifting_object: 12.4878
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 54.2083
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 2.41s
                      Time elapsed: 00:21:17
                               ETA: 00:52:24

################################################################################
                     [1m Learning iteration 578/2000 [0m                      

                       Computation: 43978 steps/s (collection: 2.141s, learning 0.094s)
             Mean action noise std: 2.12
          Mean value_function loss: 134.2080
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 47.6011
                       Mean reward: 61.41
               Mean episode length: 74.74
    Episode_Reward/reaching_object: 0.3024
     Episode_Reward/lifting_object: 12.1921
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 55.6250
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 2.24s
                      Time elapsed: 00:21:19
                               ETA: 00:52:22

################################################################################
                     [1m Learning iteration 579/2000 [0m                      

                       Computation: 44663 steps/s (collection: 2.098s, learning 0.103s)
             Mean action noise std: 2.12
          Mean value_function loss: 120.4085
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 47.6043
                       Mean reward: 68.29
               Mean episode length: 74.99
    Episode_Reward/reaching_object: 0.3010
     Episode_Reward/lifting_object: 12.5427
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 52.7083
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 2.20s
                      Time elapsed: 00:21:21
                               ETA: 00:52:19

################################################################################
                     [1m Learning iteration 580/2000 [0m                      

                       Computation: 43946 steps/s (collection: 2.118s, learning 0.119s)
             Mean action noise std: 2.12
          Mean value_function loss: 130.4364
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 47.6078
                       Mean reward: 71.26
               Mean episode length: 82.62
    Episode_Reward/reaching_object: 0.3055
     Episode_Reward/lifting_object: 13.1958
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 52.7500
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 2.24s
                      Time elapsed: 00:21:23
                               ETA: 00:52:17

################################################################################
                     [1m Learning iteration 581/2000 [0m                      

                       Computation: 41904 steps/s (collection: 2.228s, learning 0.118s)
             Mean action noise std: 2.12
          Mean value_function loss: 127.7649
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 47.6108
                       Mean reward: 66.84
               Mean episode length: 73.44
    Episode_Reward/reaching_object: 0.2967
     Episode_Reward/lifting_object: 12.6468
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 53.4167
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 2.35s
                      Time elapsed: 00:21:26
                               ETA: 00:52:15

################################################################################
                     [1m Learning iteration 582/2000 [0m                      

                       Computation: 44770 steps/s (collection: 2.105s, learning 0.091s)
             Mean action noise std: 2.12
          Mean value_function loss: 125.7014
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 47.6125
                       Mean reward: 61.88
               Mean episode length: 72.02
    Episode_Reward/reaching_object: 0.3075
     Episode_Reward/lifting_object: 13.3027
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 55.3750
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 2.20s
                      Time elapsed: 00:21:28
                               ETA: 00:52:13

################################################################################
                     [1m Learning iteration 583/2000 [0m                      

                       Computation: 44620 steps/s (collection: 2.075s, learning 0.129s)
             Mean action noise std: 2.12
          Mean value_function loss: 132.4920
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 47.6152
                       Mean reward: 60.76
               Mean episode length: 70.32
    Episode_Reward/reaching_object: 0.2965
     Episode_Reward/lifting_object: 12.7809
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 53.6250
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 2.20s
                      Time elapsed: 00:21:30
                               ETA: 00:52:11

################################################################################
                     [1m Learning iteration 584/2000 [0m                      

                       Computation: 43988 steps/s (collection: 2.109s, learning 0.126s)
             Mean action noise std: 2.12
          Mean value_function loss: 132.6714
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.6172
                       Mean reward: 73.12
               Mean episode length: 75.79
    Episode_Reward/reaching_object: 0.2972
     Episode_Reward/lifting_object: 13.0451
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 51.5417
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 2.23s
                      Time elapsed: 00:21:32
                               ETA: 00:52:09

################################################################################
                     [1m Learning iteration 585/2000 [0m                      

                       Computation: 44429 steps/s (collection: 2.090s, learning 0.123s)
             Mean action noise std: 2.12
          Mean value_function loss: 148.7412
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 47.6214
                       Mean reward: 69.69
               Mean episode length: 79.27
    Episode_Reward/reaching_object: 0.3030
     Episode_Reward/lifting_object: 13.6111
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 55.4167
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 2.21s
                      Time elapsed: 00:21:34
                               ETA: 00:52:06

################################################################################
                     [1m Learning iteration 586/2000 [0m                      

                       Computation: 43980 steps/s (collection: 2.117s, learning 0.118s)
             Mean action noise std: 2.12
          Mean value_function loss: 141.2925
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 47.6262
                       Mean reward: 71.18
               Mean episode length: 75.78
    Episode_Reward/reaching_object: 0.3060
     Episode_Reward/lifting_object: 13.3693
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 53.5417
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 2.24s
                      Time elapsed: 00:21:37
                               ETA: 00:52:04

################################################################################
                     [1m Learning iteration 587/2000 [0m                      

                       Computation: 41784 steps/s (collection: 2.214s, learning 0.139s)
             Mean action noise std: 2.12
          Mean value_function loss: 153.8128
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 47.6277
                       Mean reward: 70.46
               Mean episode length: 73.54
    Episode_Reward/reaching_object: 0.3080
     Episode_Reward/lifting_object: 13.9077
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 53.0833
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 2.35s
                      Time elapsed: 00:21:39
                               ETA: 00:52:02

################################################################################
                     [1m Learning iteration 588/2000 [0m                      

                       Computation: 43970 steps/s (collection: 2.144s, learning 0.092s)
             Mean action noise std: 2.12
          Mean value_function loss: 139.2437
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 47.6304
                       Mean reward: 72.23
               Mean episode length: 77.99
    Episode_Reward/reaching_object: 0.3046
     Episode_Reward/lifting_object: 13.7381
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 55.4583
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 2.24s
                      Time elapsed: 00:21:41
                               ETA: 00:52:00

################################################################################
                     [1m Learning iteration 589/2000 [0m                      

                       Computation: 44987 steps/s (collection: 2.084s, learning 0.101s)
             Mean action noise std: 2.12
          Mean value_function loss: 136.0773
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 47.6329
                       Mean reward: 68.23
               Mean episode length: 74.12
    Episode_Reward/reaching_object: 0.3083
     Episode_Reward/lifting_object: 14.0315
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 51.4583
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 2.19s
                      Time elapsed: 00:21:43
                               ETA: 00:51:58

################################################################################
                     [1m Learning iteration 590/2000 [0m                      

                       Computation: 42597 steps/s (collection: 2.212s, learning 0.096s)
             Mean action noise std: 2.12
          Mean value_function loss: 149.2734
               Mean surrogate loss: 0.0174
                 Mean entropy loss: 47.6344
                       Mean reward: 72.12
               Mean episode length: 77.56
    Episode_Reward/reaching_object: 0.3058
     Episode_Reward/lifting_object: 13.7087
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 55.2500
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 2.31s
                      Time elapsed: 00:21:46
                               ETA: 00:51:56

################################################################################
                     [1m Learning iteration 591/2000 [0m                      

                       Computation: 40953 steps/s (collection: 2.163s, learning 0.237s)
             Mean action noise std: 2.12
          Mean value_function loss: 152.3210
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 47.6351
                       Mean reward: 78.71
               Mean episode length: 78.72
    Episode_Reward/reaching_object: 0.3094
     Episode_Reward/lifting_object: 14.3811
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 52.4167
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 2.40s
                      Time elapsed: 00:21:48
                               ETA: 00:51:54

################################################################################
                     [1m Learning iteration 592/2000 [0m                      

                       Computation: 43385 steps/s (collection: 2.133s, learning 0.133s)
             Mean action noise std: 2.12
          Mean value_function loss: 158.7344
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 47.6354
                       Mean reward: 69.13
               Mean episode length: 74.75
    Episode_Reward/reaching_object: 0.3046
     Episode_Reward/lifting_object: 13.9899
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 54.2917
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 2.27s
                      Time elapsed: 00:21:50
                               ETA: 00:51:52

################################################################################
                     [1m Learning iteration 593/2000 [0m                      

                       Computation: 43280 steps/s (collection: 2.174s, learning 0.098s)
             Mean action noise std: 2.12
          Mean value_function loss: 142.2604
               Mean surrogate loss: 0.0102
                 Mean entropy loss: 47.6355
                       Mean reward: 78.22
               Mean episode length: 74.00
    Episode_Reward/reaching_object: 0.3069
     Episode_Reward/lifting_object: 14.2928
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 53.8750
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 2.27s
                      Time elapsed: 00:21:53
                               ETA: 00:51:50

################################################################################
                     [1m Learning iteration 594/2000 [0m                      

                       Computation: 41887 steps/s (collection: 2.210s, learning 0.137s)
             Mean action noise std: 2.12
          Mean value_function loss: 140.7455
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 47.6361
                       Mean reward: 75.60
               Mean episode length: 71.74
    Episode_Reward/reaching_object: 0.3083
     Episode_Reward/lifting_object: 14.4464
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 52.5833
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 2.35s
                      Time elapsed: 00:21:55
                               ETA: 00:51:48

################################################################################
                     [1m Learning iteration 595/2000 [0m                      

                       Computation: 43865 steps/s (collection: 2.134s, learning 0.107s)
             Mean action noise std: 2.12
          Mean value_function loss: 143.0645
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 47.6378
                       Mean reward: 79.98
               Mean episode length: 76.18
    Episode_Reward/reaching_object: 0.3088
     Episode_Reward/lifting_object: 14.8351
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 52.0833
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 2.24s
                      Time elapsed: 00:21:57
                               ETA: 00:51:46

################################################################################
                     [1m Learning iteration 596/2000 [0m                      

                       Computation: 43587 steps/s (collection: 2.147s, learning 0.109s)
             Mean action noise std: 2.12
          Mean value_function loss: 149.4190
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 47.6412
                       Mean reward: 72.85
               Mean episode length: 73.66
    Episode_Reward/reaching_object: 0.3126
     Episode_Reward/lifting_object: 14.7212
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 54.4167
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 2.26s
                      Time elapsed: 00:22:00
                               ETA: 00:51:44

################################################################################
                     [1m Learning iteration 597/2000 [0m                      

                       Computation: 44602 steps/s (collection: 2.104s, learning 0.100s)
             Mean action noise std: 2.12
          Mean value_function loss: 165.1440
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 47.6445
                       Mean reward: 79.71
               Mean episode length: 78.51
    Episode_Reward/reaching_object: 0.3154
     Episode_Reward/lifting_object: 15.2007
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 52.5000
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 2.20s
                      Time elapsed: 00:22:02
                               ETA: 00:51:42

################################################################################
                     [1m Learning iteration 598/2000 [0m                      

                       Computation: 44596 steps/s (collection: 2.106s, learning 0.098s)
             Mean action noise std: 2.12
          Mean value_function loss: 147.8836
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 47.6452
                       Mean reward: 75.53
               Mean episode length: 72.30
    Episode_Reward/reaching_object: 0.3096
     Episode_Reward/lifting_object: 14.9148
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 52.0833
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 2.20s
                      Time elapsed: 00:22:04
                               ETA: 00:51:40

################################################################################
                     [1m Learning iteration 599/2000 [0m                      

                       Computation: 44277 steps/s (collection: 2.119s, learning 0.102s)
             Mean action noise std: 2.12
          Mean value_function loss: 164.7525
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 47.6460
                       Mean reward: 86.50
               Mean episode length: 78.89
    Episode_Reward/reaching_object: 0.3199
     Episode_Reward/lifting_object: 15.7864
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 53.4583
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 2.22s
                      Time elapsed: 00:22:06
                               ETA: 00:51:37

################################################################################
                     [1m Learning iteration 600/2000 [0m                      

                       Computation: 44914 steps/s (collection: 2.091s, learning 0.098s)
             Mean action noise std: 2.12
          Mean value_function loss: 144.2962
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 47.6484
                       Mean reward: 70.10
               Mean episode length: 75.12
    Episode_Reward/reaching_object: 0.3127
     Episode_Reward/lifting_object: 14.8773
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 53.3333
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 2.19s
                      Time elapsed: 00:22:08
                               ETA: 00:51:35

################################################################################
                     [1m Learning iteration 601/2000 [0m                      

                       Computation: 41756 steps/s (collection: 2.246s, learning 0.108s)
             Mean action noise std: 2.12
          Mean value_function loss: 148.8259
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 47.6503
                       Mean reward: 81.19
               Mean episode length: 79.20
    Episode_Reward/reaching_object: 0.3181
     Episode_Reward/lifting_object: 15.4375
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 53.0833
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 2.35s
                      Time elapsed: 00:22:11
                               ETA: 00:51:33

################################################################################
                     [1m Learning iteration 602/2000 [0m                      

                       Computation: 43168 steps/s (collection: 2.162s, learning 0.116s)
             Mean action noise std: 2.12
          Mean value_function loss: 156.7816
               Mean surrogate loss: 0.0103
                 Mean entropy loss: 47.6510
                       Mean reward: 84.85
               Mean episode length: 79.28
    Episode_Reward/reaching_object: 0.3164
     Episode_Reward/lifting_object: 15.7378
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 51.3333
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 2.28s
                      Time elapsed: 00:22:13
                               ETA: 00:51:31

################################################################################
                     [1m Learning iteration 603/2000 [0m                      

                       Computation: 41822 steps/s (collection: 2.205s, learning 0.146s)
             Mean action noise std: 2.12
          Mean value_function loss: 154.0772
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 47.6510
                       Mean reward: 76.47
               Mean episode length: 73.32
    Episode_Reward/reaching_object: 0.3152
     Episode_Reward/lifting_object: 15.4712
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 53.4583
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 2.35s
                      Time elapsed: 00:22:15
                               ETA: 00:51:29

################################################################################
                     [1m Learning iteration 604/2000 [0m                      

                       Computation: 43231 steps/s (collection: 2.162s, learning 0.112s)
             Mean action noise std: 2.12
          Mean value_function loss: 136.3893
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 47.6512
                       Mean reward: 82.01
               Mean episode length: 79.14
    Episode_Reward/reaching_object: 0.3191
     Episode_Reward/lifting_object: 15.9625
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 53.9167
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 2.27s
                      Time elapsed: 00:22:18
                               ETA: 00:51:27

################################################################################
                     [1m Learning iteration 605/2000 [0m                      

                       Computation: 40994 steps/s (collection: 2.241s, learning 0.157s)
             Mean action noise std: 2.12
          Mean value_function loss: 155.7468
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 47.6516
                       Mean reward: 76.00
               Mean episode length: 74.22
    Episode_Reward/reaching_object: 0.3175
     Episode_Reward/lifting_object: 15.9738
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 51.3333
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 2.40s
                      Time elapsed: 00:22:20
                               ETA: 00:51:25

################################################################################
                     [1m Learning iteration 606/2000 [0m                      

                       Computation: 43702 steps/s (collection: 2.128s, learning 0.121s)
             Mean action noise std: 2.12
          Mean value_function loss: 161.1895
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 47.6523
                       Mean reward: 84.10
               Mean episode length: 79.84
    Episode_Reward/reaching_object: 0.3164
     Episode_Reward/lifting_object: 15.8004
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 53.5417
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 2.25s
                      Time elapsed: 00:22:22
                               ETA: 00:51:23

################################################################################
                     [1m Learning iteration 607/2000 [0m                      

                       Computation: 43285 steps/s (collection: 2.159s, learning 0.112s)
             Mean action noise std: 2.12
          Mean value_function loss: 154.0011
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 47.6540
                       Mean reward: 82.60
               Mean episode length: 78.31
    Episode_Reward/reaching_object: 0.3148
     Episode_Reward/lifting_object: 15.6743
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 51.5000
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 2.27s
                      Time elapsed: 00:22:25
                               ETA: 00:51:21

################################################################################
                     [1m Learning iteration 608/2000 [0m                      

                       Computation: 43621 steps/s (collection: 2.140s, learning 0.113s)
             Mean action noise std: 2.12
          Mean value_function loss: 151.1209
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 47.6549
                       Mean reward: 82.91
               Mean episode length: 79.28
    Episode_Reward/reaching_object: 0.3214
     Episode_Reward/lifting_object: 15.9095
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 49.8750
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 2.25s
                      Time elapsed: 00:22:27
                               ETA: 00:51:19

################################################################################
                     [1m Learning iteration 609/2000 [0m                      

                       Computation: 43782 steps/s (collection: 2.149s, learning 0.097s)
             Mean action noise std: 2.12
          Mean value_function loss: 156.4374
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 47.6556
                       Mean reward: 83.15
               Mean episode length: 78.39
    Episode_Reward/reaching_object: 0.3289
     Episode_Reward/lifting_object: 16.2965
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 51.2917
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 2.25s
                      Time elapsed: 00:22:29
                               ETA: 00:51:17

################################################################################
                     [1m Learning iteration 610/2000 [0m                      

                       Computation: 44018 steps/s (collection: 2.139s, learning 0.094s)
             Mean action noise std: 2.12
          Mean value_function loss: 154.6253
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.6575
                       Mean reward: 75.33
               Mean episode length: 76.03
    Episode_Reward/reaching_object: 0.3285
     Episode_Reward/lifting_object: 16.3284
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 53.1667
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 2.23s
                      Time elapsed: 00:22:31
                               ETA: 00:51:15

################################################################################
                     [1m Learning iteration 611/2000 [0m                      

                       Computation: 43504 steps/s (collection: 2.144s, learning 0.116s)
             Mean action noise std: 2.13
          Mean value_function loss: 171.0573
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 47.6603
                       Mean reward: 74.92
               Mean episode length: 77.39
    Episode_Reward/reaching_object: 0.3210
     Episode_Reward/lifting_object: 16.2366
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 48.0833
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 2.26s
                      Time elapsed: 00:22:34
                               ETA: 00:51:13

################################################################################
                     [1m Learning iteration 612/2000 [0m                      

                       Computation: 43717 steps/s (collection: 2.154s, learning 0.095s)
             Mean action noise std: 2.13
          Mean value_function loss: 161.1352
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 47.6647
                       Mean reward: 75.70
               Mean episode length: 75.42
    Episode_Reward/reaching_object: 0.3210
     Episode_Reward/lifting_object: 15.2975
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0253
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 51.2917
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 2.25s
                      Time elapsed: 00:22:36
                               ETA: 00:51:11

################################################################################
                     [1m Learning iteration 613/2000 [0m                      

                       Computation: 42773 steps/s (collection: 2.196s, learning 0.103s)
             Mean action noise std: 2.13
          Mean value_function loss: 178.3328
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 47.6668
                       Mean reward: 75.90
               Mean episode length: 78.12
    Episode_Reward/reaching_object: 0.3342
     Episode_Reward/lifting_object: 16.3254
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 49.4583
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 2.30s
                      Time elapsed: 00:22:38
                               ETA: 00:51:09

################################################################################
                     [1m Learning iteration 614/2000 [0m                      

                       Computation: 43380 steps/s (collection: 2.160s, learning 0.107s)
             Mean action noise std: 2.13
          Mean value_function loss: 147.4444
               Mean surrogate loss: 0.0121
                 Mean entropy loss: 47.6674
                       Mean reward: 89.45
               Mean episode length: 84.08
    Episode_Reward/reaching_object: 0.3401
     Episode_Reward/lifting_object: 16.3871
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 50.1667
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 2.27s
                      Time elapsed: 00:22:40
                               ETA: 00:51:06

################################################################################
                     [1m Learning iteration 615/2000 [0m                      

                       Computation: 45006 steps/s (collection: 2.085s, learning 0.100s)
             Mean action noise std: 2.13
          Mean value_function loss: 168.8090
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 47.6675
                       Mean reward: 80.81
               Mean episode length: 77.52
    Episode_Reward/reaching_object: 0.3321
     Episode_Reward/lifting_object: 16.8783
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 50.9167
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 2.18s
                      Time elapsed: 00:22:43
                               ETA: 00:51:04

################################################################################
                     [1m Learning iteration 616/2000 [0m                      

                       Computation: 45150 steps/s (collection: 2.078s, learning 0.100s)
             Mean action noise std: 2.13
          Mean value_function loss: 165.3007
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 47.6677
                       Mean reward: 93.20
               Mean episode length: 82.46
    Episode_Reward/reaching_object: 0.3369
     Episode_Reward/lifting_object: 16.8916
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 50.4583
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 2.18s
                      Time elapsed: 00:22:45
                               ETA: 00:51:02

################################################################################
                     [1m Learning iteration 617/2000 [0m                      

                       Computation: 42350 steps/s (collection: 2.215s, learning 0.107s)
             Mean action noise std: 2.13
          Mean value_function loss: 161.8361
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 47.6684
                       Mean reward: 78.59
               Mean episode length: 76.97
    Episode_Reward/reaching_object: 0.3364
     Episode_Reward/lifting_object: 16.8341
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 51.6667
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 2.32s
                      Time elapsed: 00:22:47
                               ETA: 00:51:00

################################################################################
                     [1m Learning iteration 618/2000 [0m                      

                       Computation: 43693 steps/s (collection: 2.137s, learning 0.113s)
             Mean action noise std: 2.13
          Mean value_function loss: 160.4560
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 47.6695
                       Mean reward: 85.86
               Mean episode length: 80.56
    Episode_Reward/reaching_object: 0.3321
     Episode_Reward/lifting_object: 17.0928
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 50.2083
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 2.25s
                      Time elapsed: 00:22:49
                               ETA: 00:50:58

################################################################################
                     [1m Learning iteration 619/2000 [0m                      

                       Computation: 42268 steps/s (collection: 2.215s, learning 0.111s)
             Mean action noise std: 2.13
          Mean value_function loss: 154.4587
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 47.6710
                       Mean reward: 86.41
               Mean episode length: 79.52
    Episode_Reward/reaching_object: 0.3363
     Episode_Reward/lifting_object: 17.4981
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0253
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 53.6667
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 2.33s
                      Time elapsed: 00:22:52
                               ETA: 00:50:56

################################################################################
                     [1m Learning iteration 620/2000 [0m                      

                       Computation: 44755 steps/s (collection: 2.105s, learning 0.092s)
             Mean action noise std: 2.13
          Mean value_function loss: 163.2867
               Mean surrogate loss: 0.0091
                 Mean entropy loss: 47.6722
                       Mean reward: 95.71
               Mean episode length: 80.90
    Episode_Reward/reaching_object: 0.3349
     Episode_Reward/lifting_object: 17.8720
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 50.4583
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 2.20s
                      Time elapsed: 00:22:54
                               ETA: 00:50:54

################################################################################
                     [1m Learning iteration 621/2000 [0m                      

                       Computation: 44863 steps/s (collection: 2.094s, learning 0.098s)
             Mean action noise std: 2.13
          Mean value_function loss: 162.0009
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 47.6724
                       Mean reward: 96.31
               Mean episode length: 80.36
    Episode_Reward/reaching_object: 0.3299
     Episode_Reward/lifting_object: 17.7352
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 52.9167
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 2.19s
                      Time elapsed: 00:22:56
                               ETA: 00:50:51

################################################################################
                     [1m Learning iteration 622/2000 [0m                      

                       Computation: 45134 steps/s (collection: 2.081s, learning 0.097s)
             Mean action noise std: 2.13
          Mean value_function loss: 153.0991
               Mean surrogate loss: 0.0098
                 Mean entropy loss: 47.6727
                       Mean reward: 79.04
               Mean episode length: 70.07
    Episode_Reward/reaching_object: 0.3244
     Episode_Reward/lifting_object: 16.9055
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 50.2917
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 2.18s
                      Time elapsed: 00:22:58
                               ETA: 00:50:49

################################################################################
                     [1m Learning iteration 623/2000 [0m                      

                       Computation: 44288 steps/s (collection: 2.102s, learning 0.118s)
             Mean action noise std: 2.13
          Mean value_function loss: 200.5163
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 47.6728
                       Mean reward: 89.27
               Mean episode length: 82.84
    Episode_Reward/reaching_object: 0.3393
     Episode_Reward/lifting_object: 18.0178
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 49.6667
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 2.22s
                      Time elapsed: 00:23:00
                               ETA: 00:50:47

################################################################################
                     [1m Learning iteration 624/2000 [0m                      

                       Computation: 43338 steps/s (collection: 2.147s, learning 0.121s)
             Mean action noise std: 2.13
          Mean value_function loss: 172.4239
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 47.6736
                       Mean reward: 77.77
               Mean episode length: 72.44
    Episode_Reward/reaching_object: 0.3328
     Episode_Reward/lifting_object: 17.4910
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 50.1250
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 2.27s
                      Time elapsed: 00:23:03
                               ETA: 00:50:45

################################################################################
                     [1m Learning iteration 625/2000 [0m                      

                       Computation: 44510 steps/s (collection: 2.115s, learning 0.094s)
             Mean action noise std: 2.13
          Mean value_function loss: 171.2023
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 47.6759
                       Mean reward: 93.22
               Mean episode length: 79.51
    Episode_Reward/reaching_object: 0.3411
     Episode_Reward/lifting_object: 17.9052
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 50.3750
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 2.21s
                      Time elapsed: 00:23:05
                               ETA: 00:50:43

################################################################################
                     [1m Learning iteration 626/2000 [0m                      

                       Computation: 44193 steps/s (collection: 2.124s, learning 0.100s)
             Mean action noise std: 2.13
          Mean value_function loss: 210.8871
               Mean surrogate loss: 0.0093
                 Mean entropy loss: 47.6773
                       Mean reward: 71.47
               Mean episode length: 74.45
    Episode_Reward/reaching_object: 0.3367
     Episode_Reward/lifting_object: 17.2720
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 50.5000
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 2.22s
                      Time elapsed: 00:23:07
                               ETA: 00:50:40

################################################################################
                     [1m Learning iteration 627/2000 [0m                      

                       Computation: 43495 steps/s (collection: 2.159s, learning 0.102s)
             Mean action noise std: 2.13
          Mean value_function loss: 186.6391
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 47.6777
                       Mean reward: 94.63
               Mean episode length: 81.32
    Episode_Reward/reaching_object: 0.3456
     Episode_Reward/lifting_object: 18.6714
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 50.6667
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 2.26s
                      Time elapsed: 00:23:09
                               ETA: 00:50:38

################################################################################
                     [1m Learning iteration 628/2000 [0m                      

                       Computation: 41310 steps/s (collection: 2.258s, learning 0.122s)
             Mean action noise std: 2.13
          Mean value_function loss: 177.2527
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 47.6783
                       Mean reward: 94.71
               Mean episode length: 79.10
    Episode_Reward/reaching_object: 0.3505
     Episode_Reward/lifting_object: 18.6407
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 47.2083
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 2.38s
                      Time elapsed: 00:23:12
                               ETA: 00:50:36

################################################################################
                     [1m Learning iteration 629/2000 [0m                      

                       Computation: 42754 steps/s (collection: 2.165s, learning 0.135s)
             Mean action noise std: 2.13
          Mean value_function loss: 193.4606
               Mean surrogate loss: 0.0114
                 Mean entropy loss: 47.6788
                       Mean reward: 90.89
               Mean episode length: 84.04
    Episode_Reward/reaching_object: 0.3376
     Episode_Reward/lifting_object: 17.4819
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 49.1667
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 2.30s
                      Time elapsed: 00:23:14
                               ETA: 00:50:34

################################################################################
                     [1m Learning iteration 630/2000 [0m                      

                       Computation: 43048 steps/s (collection: 2.171s, learning 0.113s)
             Mean action noise std: 2.13
          Mean value_function loss: 182.1466
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 47.6791
                       Mean reward: 103.14
               Mean episode length: 82.87
    Episode_Reward/reaching_object: 0.3554
     Episode_Reward/lifting_object: 19.1368
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 50.9167
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 2.28s
                      Time elapsed: 00:23:16
                               ETA: 00:50:32

################################################################################
                     [1m Learning iteration 631/2000 [0m                      

                       Computation: 42785 steps/s (collection: 2.137s, learning 0.161s)
             Mean action noise std: 2.13
          Mean value_function loss: 177.8326
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 47.6799
                       Mean reward: 91.31
               Mean episode length: 79.81
    Episode_Reward/reaching_object: 0.3539
     Episode_Reward/lifting_object: 19.2918
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 50.2917
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 2.30s
                      Time elapsed: 00:23:19
                               ETA: 00:50:30

################################################################################
                     [1m Learning iteration 632/2000 [0m                      

                       Computation: 42375 steps/s (collection: 2.167s, learning 0.153s)
             Mean action noise std: 2.13
          Mean value_function loss: 192.1272
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 47.6810
                       Mean reward: 101.54
               Mean episode length: 84.38
    Episode_Reward/reaching_object: 0.3475
     Episode_Reward/lifting_object: 19.0223
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 48.9583
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 2.32s
                      Time elapsed: 00:23:21
                               ETA: 00:50:28

################################################################################
                     [1m Learning iteration 633/2000 [0m                      

                       Computation: 43624 steps/s (collection: 2.161s, learning 0.093s)
             Mean action noise std: 2.13
          Mean value_function loss: 221.2232
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 47.6831
                       Mean reward: 102.94
               Mean episode length: 87.01
    Episode_Reward/reaching_object: 0.3461
     Episode_Reward/lifting_object: 18.8351
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 52.2917
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 2.25s
                      Time elapsed: 00:23:23
                               ETA: 00:50:26

################################################################################
                     [1m Learning iteration 634/2000 [0m                      

                       Computation: 43055 steps/s (collection: 2.173s, learning 0.110s)
             Mean action noise std: 2.13
          Mean value_function loss: 191.8889
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 47.6849
                       Mean reward: 102.66
               Mean episode length: 80.43
    Episode_Reward/reaching_object: 0.3545
     Episode_Reward/lifting_object: 19.3768
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 47.9583
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 2.28s
                      Time elapsed: 00:23:26
                               ETA: 00:50:24

################################################################################
                     [1m Learning iteration 635/2000 [0m                      

                       Computation: 41806 steps/s (collection: 2.252s, learning 0.099s)
             Mean action noise std: 2.13
          Mean value_function loss: 199.6959
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 47.6866
                       Mean reward: 96.44
               Mean episode length: 84.71
    Episode_Reward/reaching_object: 0.3456
     Episode_Reward/lifting_object: 18.6858
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 48.3333
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 2.35s
                      Time elapsed: 00:23:28
                               ETA: 00:50:22

################################################################################
                     [1m Learning iteration 636/2000 [0m                      

                       Computation: 43889 steps/s (collection: 2.130s, learning 0.110s)
             Mean action noise std: 2.13
          Mean value_function loss: 191.7638
               Mean surrogate loss: 0.0147
                 Mean entropy loss: 47.6876
                       Mean reward: 102.36
               Mean episode length: 81.79
    Episode_Reward/reaching_object: 0.3552
     Episode_Reward/lifting_object: 19.8586
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 50.2917
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 2.24s
                      Time elapsed: 00:23:30
                               ETA: 00:50:20

################################################################################
                     [1m Learning iteration 637/2000 [0m                      

                       Computation: 43903 steps/s (collection: 2.124s, learning 0.115s)
             Mean action noise std: 2.13
          Mean value_function loss: 194.2012
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 47.6879
                       Mean reward: 85.88
               Mean episode length: 78.40
    Episode_Reward/reaching_object: 0.3615
     Episode_Reward/lifting_object: 20.3550
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 48.3750
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 2.24s
                      Time elapsed: 00:23:32
                               ETA: 00:50:18

################################################################################
                     [1m Learning iteration 638/2000 [0m                      

                       Computation: 44371 steps/s (collection: 2.105s, learning 0.111s)
             Mean action noise std: 2.13
          Mean value_function loss: 204.9175
               Mean surrogate loss: 0.0162
                 Mean entropy loss: 47.6882
                       Mean reward: 102.46
               Mean episode length: 80.40
    Episode_Reward/reaching_object: 0.3571
     Episode_Reward/lifting_object: 20.2230
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 47.9167
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 2.22s
                      Time elapsed: 00:23:35
                               ETA: 00:50:16

################################################################################
                     [1m Learning iteration 639/2000 [0m                      

                       Computation: 42744 steps/s (collection: 2.126s, learning 0.174s)
             Mean action noise std: 2.13
          Mean value_function loss: 199.0485
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 47.6883
                       Mean reward: 95.78
               Mean episode length: 80.78
    Episode_Reward/reaching_object: 0.3621
     Episode_Reward/lifting_object: 20.9290
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 49.8333
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 2.30s
                      Time elapsed: 00:23:37
                               ETA: 00:50:14

################################################################################
                     [1m Learning iteration 640/2000 [0m                      

                       Computation: 43897 steps/s (collection: 2.144s, learning 0.096s)
             Mean action noise std: 2.13
          Mean value_function loss: 209.1111
               Mean surrogate loss: 0.0110
                 Mean entropy loss: 47.6887
                       Mean reward: 96.80
               Mean episode length: 79.53
    Episode_Reward/reaching_object: 0.3579
     Episode_Reward/lifting_object: 20.1648
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 49.4583
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 2.24s
                      Time elapsed: 00:23:39
                               ETA: 00:50:11

################################################################################
                     [1m Learning iteration 641/2000 [0m                      

                       Computation: 43207 steps/s (collection: 2.125s, learning 0.150s)
             Mean action noise std: 2.13
          Mean value_function loss: 207.1570
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 47.6891
                       Mean reward: 106.13
               Mean episode length: 82.88
    Episode_Reward/reaching_object: 0.3602
     Episode_Reward/lifting_object: 21.1751
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 52.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 2.28s
                      Time elapsed: 00:23:41
                               ETA: 00:50:09

################################################################################
                     [1m Learning iteration 642/2000 [0m                      

                       Computation: 41656 steps/s (collection: 2.236s, learning 0.124s)
             Mean action noise std: 2.13
          Mean value_function loss: 209.7670
               Mean surrogate loss: 0.0097
                 Mean entropy loss: 47.6892
                       Mean reward: 106.68
               Mean episode length: 78.86
    Episode_Reward/reaching_object: 0.3545
     Episode_Reward/lifting_object: 20.9317
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 50.6250
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 2.36s
                      Time elapsed: 00:23:44
                               ETA: 00:50:07

################################################################################
                     [1m Learning iteration 643/2000 [0m                      

                       Computation: 43183 steps/s (collection: 2.175s, learning 0.102s)
             Mean action noise std: 2.13
          Mean value_function loss: 228.8212
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 47.6894
                       Mean reward: 105.74
               Mean episode length: 79.38
    Episode_Reward/reaching_object: 0.3471
     Episode_Reward/lifting_object: 20.1010
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 51.7917
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 2.28s
                      Time elapsed: 00:23:46
                               ETA: 00:50:05

################################################################################
                     [1m Learning iteration 644/2000 [0m                      

                       Computation: 43835 steps/s (collection: 2.125s, learning 0.118s)
             Mean action noise std: 2.13
          Mean value_function loss: 228.2912
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 47.6901
                       Mean reward: 100.18
               Mean episode length: 77.46
    Episode_Reward/reaching_object: 0.3526
     Episode_Reward/lifting_object: 20.5899
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 49.2500
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 2.24s
                      Time elapsed: 00:23:48
                               ETA: 00:50:03

################################################################################
                     [1m Learning iteration 645/2000 [0m                      

                       Computation: 43164 steps/s (collection: 2.169s, learning 0.108s)
             Mean action noise std: 2.13
          Mean value_function loss: 220.8699
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 47.6910
                       Mean reward: 103.87
               Mean episode length: 77.57
    Episode_Reward/reaching_object: 0.3488
     Episode_Reward/lifting_object: 20.6365
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 52.5833
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 2.28s
                      Time elapsed: 00:23:51
                               ETA: 00:50:01

################################################################################
                     [1m Learning iteration 646/2000 [0m                      

                       Computation: 42949 steps/s (collection: 2.178s, learning 0.111s)
             Mean action noise std: 2.13
          Mean value_function loss: 233.8486
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 47.6906
                       Mean reward: 100.97
               Mean episode length: 77.60
    Episode_Reward/reaching_object: 0.3515
     Episode_Reward/lifting_object: 21.0013
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 48.7917
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 2.29s
                      Time elapsed: 00:23:53
                               ETA: 00:49:59

################################################################################
                     [1m Learning iteration 647/2000 [0m                      

                       Computation: 42898 steps/s (collection: 2.188s, learning 0.104s)
             Mean action noise std: 2.13
          Mean value_function loss: 231.9710
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 47.6910
                       Mean reward: 106.36
               Mean episode length: 78.58
    Episode_Reward/reaching_object: 0.3434
     Episode_Reward/lifting_object: 20.2264
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 48.8333
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 2.29s
                      Time elapsed: 00:23:55
                               ETA: 00:49:57

################################################################################
                     [1m Learning iteration 648/2000 [0m                      

                       Computation: 44369 steps/s (collection: 2.108s, learning 0.107s)
             Mean action noise std: 2.13
          Mean value_function loss: 238.7500
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 47.6917
                       Mean reward: 111.37
               Mean episode length: 86.32
    Episode_Reward/reaching_object: 0.3537
     Episode_Reward/lifting_object: 20.1787
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0253
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 51.2917
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 2.22s
                      Time elapsed: 00:23:57
                               ETA: 00:49:55

################################################################################
                     [1m Learning iteration 649/2000 [0m                      

                       Computation: 43706 steps/s (collection: 2.098s, learning 0.151s)
             Mean action noise std: 2.13
          Mean value_function loss: 229.8893
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 47.6920
                       Mean reward: 112.56
               Mean episode length: 80.23
    Episode_Reward/reaching_object: 0.3647
     Episode_Reward/lifting_object: 21.6559
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 44.9583
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 2.25s
                      Time elapsed: 00:24:00
                               ETA: 00:49:53

################################################################################
                     [1m Learning iteration 650/2000 [0m                      

                       Computation: 43736 steps/s (collection: 2.137s, learning 0.111s)
             Mean action noise std: 2.13
          Mean value_function loss: 230.6928
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 47.6927
                       Mean reward: 125.49
               Mean episode length: 88.84
    Episode_Reward/reaching_object: 0.3722
     Episode_Reward/lifting_object: 22.9837
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 49.3333
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 2.25s
                      Time elapsed: 00:24:02
                               ETA: 00:49:50

################################################################################
                     [1m Learning iteration 651/2000 [0m                      

                       Computation: 43111 steps/s (collection: 2.168s, learning 0.113s)
             Mean action noise std: 2.13
          Mean value_function loss: 240.5390
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 47.6938
                       Mean reward: 122.13
               Mean episode length: 83.02
    Episode_Reward/reaching_object: 0.3711
     Episode_Reward/lifting_object: 23.1101
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 51.7917
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 2.28s
                      Time elapsed: 00:24:04
                               ETA: 00:49:48

################################################################################
                     [1m Learning iteration 652/2000 [0m                      

                       Computation: 43401 steps/s (collection: 2.153s, learning 0.112s)
             Mean action noise std: 2.13
          Mean value_function loss: 253.7854
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 47.6955
                       Mean reward: 97.89
               Mean episode length: 78.49
    Episode_Reward/reaching_object: 0.3633
     Episode_Reward/lifting_object: 22.5492
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 50.1667
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 2.27s
                      Time elapsed: 00:24:06
                               ETA: 00:49:46

################################################################################
                     [1m Learning iteration 653/2000 [0m                      

                       Computation: 43445 steps/s (collection: 2.130s, learning 0.133s)
             Mean action noise std: 2.13
          Mean value_function loss: 235.9632
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 47.6959
                       Mean reward: 122.47
               Mean episode length: 86.88
    Episode_Reward/reaching_object: 0.3614
     Episode_Reward/lifting_object: 22.4990
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 49.6667
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 2.26s
                      Time elapsed: 00:24:09
                               ETA: 00:49:44

################################################################################
                     [1m Learning iteration 654/2000 [0m                      

                       Computation: 44918 steps/s (collection: 2.092s, learning 0.096s)
             Mean action noise std: 2.13
          Mean value_function loss: 253.0516
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 47.6961
                       Mean reward: 103.64
               Mean episode length: 78.42
    Episode_Reward/reaching_object: 0.3616
     Episode_Reward/lifting_object: 22.4063
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 47.7917
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 2.19s
                      Time elapsed: 00:24:11
                               ETA: 00:49:42

################################################################################
                     [1m Learning iteration 655/2000 [0m                      

                       Computation: 42153 steps/s (collection: 2.220s, learning 0.112s)
             Mean action noise std: 2.13
          Mean value_function loss: 250.9704
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 47.6963
                       Mean reward: 108.55
               Mean episode length: 75.92
    Episode_Reward/reaching_object: 0.3561
     Episode_Reward/lifting_object: 22.4404
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 49.7917
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 2.33s
                      Time elapsed: 00:24:13
                               ETA: 00:49:40

################################################################################
                     [1m Learning iteration 656/2000 [0m                      

                       Computation: 43419 steps/s (collection: 2.165s, learning 0.099s)
             Mean action noise std: 2.13
          Mean value_function loss: 267.9164
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 47.6965
                       Mean reward: 131.33
               Mean episode length: 84.94
    Episode_Reward/reaching_object: 0.3633
     Episode_Reward/lifting_object: 22.7217
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 49.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 2.26s
                      Time elapsed: 00:24:15
                               ETA: 00:49:38

################################################################################
                     [1m Learning iteration 657/2000 [0m                      

                       Computation: 41566 steps/s (collection: 2.212s, learning 0.153s)
             Mean action noise std: 2.13
          Mean value_function loss: 272.2232
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 47.6968
                       Mean reward: 124.48
               Mean episode length: 86.14
    Episode_Reward/reaching_object: 0.3622
     Episode_Reward/lifting_object: 23.2927
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 49.7500
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 2.36s
                      Time elapsed: 00:24:18
                               ETA: 00:49:36

################################################################################
                     [1m Learning iteration 658/2000 [0m                      

                       Computation: 44636 steps/s (collection: 2.106s, learning 0.097s)
             Mean action noise std: 2.13
          Mean value_function loss: 283.0474
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 47.6976
                       Mean reward: 109.71
               Mean episode length: 76.64
    Episode_Reward/reaching_object: 0.3577
     Episode_Reward/lifting_object: 23.0810
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 48.0833
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 2.20s
                      Time elapsed: 00:24:20
                               ETA: 00:49:34

################################################################################
                     [1m Learning iteration 659/2000 [0m                      

                       Computation: 45011 steps/s (collection: 2.073s, learning 0.111s)
             Mean action noise std: 2.13
          Mean value_function loss: 279.7018
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 47.6988
                       Mean reward: 121.37
               Mean episode length: 81.58
    Episode_Reward/reaching_object: 0.3588
     Episode_Reward/lifting_object: 23.7749
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 49.9583
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 2.18s
                      Time elapsed: 00:24:22
                               ETA: 00:49:31

################################################################################
                     [1m Learning iteration 660/2000 [0m                      

                       Computation: 44022 steps/s (collection: 2.115s, learning 0.118s)
             Mean action noise std: 2.13
          Mean value_function loss: 286.7851
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 47.6993
                       Mean reward: 121.15
               Mean episode length: 79.71
    Episode_Reward/reaching_object: 0.3528
     Episode_Reward/lifting_object: 23.2025
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 48.9583
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 2.23s
                      Time elapsed: 00:24:24
                               ETA: 00:49:29

################################################################################
                     [1m Learning iteration 661/2000 [0m                      

                       Computation: 42221 steps/s (collection: 2.205s, learning 0.123s)
             Mean action noise std: 2.13
          Mean value_function loss: 295.8009
               Mean surrogate loss: 0.0113
                 Mean entropy loss: 47.6992
                       Mean reward: 135.57
               Mean episode length: 83.23
    Episode_Reward/reaching_object: 0.3658
     Episode_Reward/lifting_object: 25.0029
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 50.7500
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 2.33s
                      Time elapsed: 00:24:27
                               ETA: 00:49:27

################################################################################
                     [1m Learning iteration 662/2000 [0m                      

                       Computation: 40373 steps/s (collection: 2.321s, learning 0.114s)
             Mean action noise std: 2.13
          Mean value_function loss: 276.2964
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 47.6993
                       Mean reward: 114.49
               Mean episode length: 80.52
    Episode_Reward/reaching_object: 0.3584
     Episode_Reward/lifting_object: 24.6742
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 50.5417
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 2.43s
                      Time elapsed: 00:24:29
                               ETA: 00:49:25

################################################################################
                     [1m Learning iteration 663/2000 [0m                      

                       Computation: 42408 steps/s (collection: 2.227s, learning 0.091s)
             Mean action noise std: 2.13
          Mean value_function loss: 308.9576
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 47.6993
                       Mean reward: 118.39
               Mean episode length: 77.85
    Episode_Reward/reaching_object: 0.3610
     Episode_Reward/lifting_object: 25.1429
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 49.1667
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 2.32s
                      Time elapsed: 00:24:31
                               ETA: 00:49:23

################################################################################
                     [1m Learning iteration 664/2000 [0m                      

                       Computation: 44041 steps/s (collection: 2.120s, learning 0.113s)
             Mean action noise std: 2.13
          Mean value_function loss: 301.2618
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 47.6998
                       Mean reward: 120.90
               Mean episode length: 79.57
    Episode_Reward/reaching_object: 0.3548
     Episode_Reward/lifting_object: 24.7985
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 49.3750
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 2.23s
                      Time elapsed: 00:24:34
                               ETA: 00:49:21

################################################################################
                     [1m Learning iteration 665/2000 [0m                      

                       Computation: 43695 steps/s (collection: 2.149s, learning 0.101s)
             Mean action noise std: 2.13
          Mean value_function loss: 317.5372
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 47.7002
                       Mean reward: 132.21
               Mean episode length: 81.99
    Episode_Reward/reaching_object: 0.3529
     Episode_Reward/lifting_object: 24.9175
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 48.7500
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 2.25s
                      Time elapsed: 00:24:36
                               ETA: 00:49:19

################################################################################
                     [1m Learning iteration 666/2000 [0m                      

                       Computation: 26847 steps/s (collection: 3.560s, learning 0.102s)
             Mean action noise std: 2.13
          Mean value_function loss: 307.3495
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 47.7005
                       Mean reward: 148.47
               Mean episode length: 85.73
    Episode_Reward/reaching_object: 0.3678
     Episode_Reward/lifting_object: 26.1295
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 48.3750
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 3.66s
                      Time elapsed: 00:24:40
                               ETA: 00:49:20

################################################################################
                     [1m Learning iteration 667/2000 [0m                      

                       Computation: 13999 steps/s (collection: 6.900s, learning 0.122s)
             Mean action noise std: 2.13
          Mean value_function loss: 330.4605
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 47.7006
                       Mean reward: 143.86
               Mean episode length: 82.47
    Episode_Reward/reaching_object: 0.3675
     Episode_Reward/lifting_object: 26.6790
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 49.6250
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 7.02s
                      Time elapsed: 00:24:47
                               ETA: 00:49:27

################################################################################
                     [1m Learning iteration 668/2000 [0m                      

                       Computation: 14125 steps/s (collection: 6.840s, learning 0.119s)
             Mean action noise std: 2.13
          Mean value_function loss: 342.4770
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 47.7004
                       Mean reward: 145.08
               Mean episode length: 80.56
    Episode_Reward/reaching_object: 0.3671
     Episode_Reward/lifting_object: 26.1649
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 50.8333
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 6.96s
                      Time elapsed: 00:24:54
                               ETA: 00:49:34

################################################################################
                     [1m Learning iteration 669/2000 [0m                      

                       Computation: 13947 steps/s (collection: 6.930s, learning 0.118s)
             Mean action noise std: 2.13
          Mean value_function loss: 352.8143
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 47.7003
                       Mean reward: 135.43
               Mean episode length: 81.86
    Episode_Reward/reaching_object: 0.3802
     Episode_Reward/lifting_object: 27.3716
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 47.1250
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 7.05s
                      Time elapsed: 00:25:01
                               ETA: 00:49:42

################################################################################
                     [1m Learning iteration 670/2000 [0m                      

                       Computation: 14209 steps/s (collection: 6.781s, learning 0.137s)
             Mean action noise std: 2.13
          Mean value_function loss: 344.6050
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 47.7002
                       Mean reward: 145.79
               Mean episode length: 82.94
    Episode_Reward/reaching_object: 0.3723
     Episode_Reward/lifting_object: 27.5086
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 49.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 6.92s
                      Time elapsed: 00:25:08
                               ETA: 00:49:49

################################################################################
                     [1m Learning iteration 671/2000 [0m                      

                       Computation: 13957 steps/s (collection: 6.913s, learning 0.130s)
             Mean action noise std: 2.13
          Mean value_function loss: 344.9965
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 47.7009
                       Mean reward: 133.90
               Mean episode length: 79.09
    Episode_Reward/reaching_object: 0.3898
     Episode_Reward/lifting_object: 29.3628
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 47.5000
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 7.04s
                      Time elapsed: 00:25:15
                               ETA: 00:49:56

################################################################################
                     [1m Learning iteration 672/2000 [0m                      

                       Computation: 14395 steps/s (collection: 6.699s, learning 0.130s)
             Mean action noise std: 2.13
          Mean value_function loss: 347.4231
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 47.7021
                       Mean reward: 147.86
               Mean episode length: 85.10
    Episode_Reward/reaching_object: 0.3956
     Episode_Reward/lifting_object: 31.2620
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 46.0417
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 6.83s
                      Time elapsed: 00:25:21
                               ETA: 00:50:03

################################################################################
                     [1m Learning iteration 673/2000 [0m                      

                       Computation: 14172 steps/s (collection: 6.816s, learning 0.120s)
             Mean action noise std: 2.13
          Mean value_function loss: 357.5362
               Mean surrogate loss: 0.0122
                 Mean entropy loss: 47.7020
                       Mean reward: 146.06
               Mean episode length: 85.19
    Episode_Reward/reaching_object: 0.3927
     Episode_Reward/lifting_object: 30.4852
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 47.9167
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 6.94s
                      Time elapsed: 00:25:28
                               ETA: 00:50:10

################################################################################
                     [1m Learning iteration 674/2000 [0m                      

                       Computation: 14280 steps/s (collection: 6.759s, learning 0.125s)
             Mean action noise std: 2.13
          Mean value_function loss: 353.6837
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 47.7020
                       Mean reward: 151.54
               Mean episode length: 82.75
    Episode_Reward/reaching_object: 0.3967
     Episode_Reward/lifting_object: 32.0652
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 48.5417
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 6.88s
                      Time elapsed: 00:25:35
                               ETA: 00:50:16

################################################################################
                     [1m Learning iteration 675/2000 [0m                      

                       Computation: 22390 steps/s (collection: 4.283s, learning 0.107s)
             Mean action noise std: 2.13
          Mean value_function loss: 365.6838
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 47.7021
                       Mean reward: 147.96
               Mean episode length: 81.20
    Episode_Reward/reaching_object: 0.3900
     Episode_Reward/lifting_object: 31.1764
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 48.0833
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 4.39s
                      Time elapsed: 00:25:40
                               ETA: 00:50:18

################################################################################
                     [1m Learning iteration 676/2000 [0m                      

                       Computation: 45279 steps/s (collection: 2.074s, learning 0.097s)
             Mean action noise std: 2.13
          Mean value_function loss: 360.2038
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 47.7024
                       Mean reward: 150.88
               Mean episode length: 82.93
    Episode_Reward/reaching_object: 0.3876
     Episode_Reward/lifting_object: 30.8125
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 50.1250
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 2.17s
                      Time elapsed: 00:25:42
                               ETA: 00:50:16

################################################################################
                     [1m Learning iteration 677/2000 [0m                      

                       Computation: 47483 steps/s (collection: 1.984s, learning 0.086s)
             Mean action noise std: 2.13
          Mean value_function loss: 388.2432
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 47.7026
                       Mean reward: 153.71
               Mean episode length: 81.75
    Episode_Reward/reaching_object: 0.3906
     Episode_Reward/lifting_object: 30.8334
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 46.8750
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 2.07s
                      Time elapsed: 00:25:44
                               ETA: 00:50:13

################################################################################
                     [1m Learning iteration 678/2000 [0m                      

                       Computation: 47391 steps/s (collection: 1.980s, learning 0.094s)
             Mean action noise std: 2.13
          Mean value_function loss: 376.9484
               Mean surrogate loss: 0.0137
                 Mean entropy loss: 47.7026
                       Mean reward: 151.06
               Mean episode length: 81.97
    Episode_Reward/reaching_object: 0.3858
     Episode_Reward/lifting_object: 30.2720
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 44.8333
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 2.07s
                      Time elapsed: 00:25:46
                               ETA: 00:50:10

################################################################################
                     [1m Learning iteration 679/2000 [0m                      

                       Computation: 47703 steps/s (collection: 1.973s, learning 0.088s)
             Mean action noise std: 2.13
          Mean value_function loss: 391.8316
               Mean surrogate loss: 0.0160
                 Mean entropy loss: 47.7027
                       Mean reward: 152.10
               Mean episode length: 83.61
    Episode_Reward/reaching_object: 0.4087
     Episode_Reward/lifting_object: 33.5736
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 46.0417
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 2.06s
                      Time elapsed: 00:25:48
                               ETA: 00:50:08

################################################################################
                     [1m Learning iteration 680/2000 [0m                      

                       Computation: 47532 steps/s (collection: 1.972s, learning 0.097s)
             Mean action noise std: 2.13
          Mean value_function loss: 381.4361
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 47.7028
                       Mean reward: 178.63
               Mean episode length: 86.12
    Episode_Reward/reaching_object: 0.4110
     Episode_Reward/lifting_object: 34.2262
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 45.0833
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 2.07s
                      Time elapsed: 00:25:50
                               ETA: 00:50:05

################################################################################
                     [1m Learning iteration 681/2000 [0m                      

                       Computation: 47058 steps/s (collection: 1.993s, learning 0.096s)
             Mean action noise std: 2.13
          Mean value_function loss: 377.5229
               Mean surrogate loss: 0.0108
                 Mean entropy loss: 47.7028
                       Mean reward: 173.62
               Mean episode length: 87.01
    Episode_Reward/reaching_object: 0.4075
     Episode_Reward/lifting_object: 33.8708
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 44.3750
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 2.09s
                      Time elapsed: 00:25:52
                               ETA: 00:50:02

################################################################################
                     [1m Learning iteration 682/2000 [0m                      

                       Computation: 47866 steps/s (collection: 1.967s, learning 0.087s)
             Mean action noise std: 2.13
          Mean value_function loss: 367.6013
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 47.7028
                       Mean reward: 163.33
               Mean episode length: 85.64
    Episode_Reward/reaching_object: 0.4210
     Episode_Reward/lifting_object: 34.6952
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 43.6250
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 2.05s
                      Time elapsed: 00:25:54
                               ETA: 00:50:00

################################################################################
                     [1m Learning iteration 683/2000 [0m                      

                       Computation: 47089 steps/s (collection: 1.998s, learning 0.090s)
             Mean action noise std: 2.13
          Mean value_function loss: 395.2245
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 47.7024
                       Mean reward: 173.90
               Mean episode length: 89.83
    Episode_Reward/reaching_object: 0.4160
     Episode_Reward/lifting_object: 34.0470
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 41.4167
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 2.09s
                      Time elapsed: 00:25:56
                               ETA: 00:49:57

################################################################################
                     [1m Learning iteration 684/2000 [0m                      

                       Computation: 47133 steps/s (collection: 1.983s, learning 0.103s)
             Mean action noise std: 2.13
          Mean value_function loss: 404.6737
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 47.7022
                       Mean reward: 187.25
               Mean episode length: 92.42
    Episode_Reward/reaching_object: 0.4211
     Episode_Reward/lifting_object: 34.3729
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 43.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 2.09s
                      Time elapsed: 00:25:58
                               ETA: 00:49:54

################################################################################
                     [1m Learning iteration 685/2000 [0m                      

                       Computation: 47714 steps/s (collection: 1.962s, learning 0.098s)
             Mean action noise std: 2.13
          Mean value_function loss: 385.5170
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 47.7022
                       Mean reward: 192.17
               Mean episode length: 93.55
    Episode_Reward/reaching_object: 0.4373
     Episode_Reward/lifting_object: 35.3475
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 41.6250
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 2.06s
                      Time elapsed: 00:26:00
                               ETA: 00:49:52

################################################################################
                     [1m Learning iteration 686/2000 [0m                      

                       Computation: 47350 steps/s (collection: 1.967s, learning 0.109s)
             Mean action noise std: 2.13
          Mean value_function loss: 374.9220
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 47.7012
                       Mean reward: 191.23
               Mean episode length: 94.51
    Episode_Reward/reaching_object: 0.4448
     Episode_Reward/lifting_object: 35.6493
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 40.4583
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 2.08s
                      Time elapsed: 00:26:03
                               ETA: 00:49:49

################################################################################
                     [1m Learning iteration 687/2000 [0m                      

                       Computation: 47377 steps/s (collection: 1.979s, learning 0.096s)
             Mean action noise std: 2.13
          Mean value_function loss: 365.6294
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 47.7012
                       Mean reward: 180.70
               Mean episode length: 89.64
    Episode_Reward/reaching_object: 0.4691
     Episode_Reward/lifting_object: 38.6378
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 41.2083
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 2.07s
                      Time elapsed: 00:26:05
                               ETA: 00:49:46

################################################################################
                     [1m Learning iteration 688/2000 [0m                      

                       Computation: 46544 steps/s (collection: 2.025s, learning 0.087s)
             Mean action noise std: 2.13
          Mean value_function loss: 373.2372
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 47.7017
                       Mean reward: 221.63
               Mean episode length: 105.15
    Episode_Reward/reaching_object: 0.4654
     Episode_Reward/lifting_object: 37.7097
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 40.2917
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 2.11s
                      Time elapsed: 00:26:07
                               ETA: 00:49:44

################################################################################
                     [1m Learning iteration 689/2000 [0m                      

                       Computation: 47111 steps/s (collection: 2.001s, learning 0.086s)
             Mean action noise std: 2.13
          Mean value_function loss: 373.5335
               Mean surrogate loss: 0.0110
                 Mean entropy loss: 47.7025
                       Mean reward: 184.73
               Mean episode length: 97.77
    Episode_Reward/reaching_object: 0.4678
     Episode_Reward/lifting_object: 38.4730
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 39.2917
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 2.09s
                      Time elapsed: 00:26:09
                               ETA: 00:49:41

################################################################################
                     [1m Learning iteration 690/2000 [0m                      

                       Computation: 46901 steps/s (collection: 2.001s, learning 0.095s)
             Mean action noise std: 2.13
          Mean value_function loss: 368.1498
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 47.7028
                       Mean reward: 199.86
               Mean episode length: 98.25
    Episode_Reward/reaching_object: 0.4605
     Episode_Reward/lifting_object: 37.9719
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 36.4583
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 2.10s
                      Time elapsed: 00:26:11
                               ETA: 00:49:39

################################################################################
                     [1m Learning iteration 691/2000 [0m                      

                       Computation: 47639 steps/s (collection: 1.973s, learning 0.090s)
             Mean action noise std: 2.13
          Mean value_function loss: 379.6136
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 47.7028
                       Mean reward: 200.26
               Mean episode length: 100.13
    Episode_Reward/reaching_object: 0.4835
     Episode_Reward/lifting_object: 39.7636
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 40.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 2.06s
                      Time elapsed: 00:26:13
                               ETA: 00:49:36

################################################################################
                     [1m Learning iteration 692/2000 [0m                      

                       Computation: 46453 steps/s (collection: 2.029s, learning 0.087s)
             Mean action noise std: 2.13
          Mean value_function loss: 406.6694
               Mean surrogate loss: 0.0170
                 Mean entropy loss: 47.7025
                       Mean reward: 201.85
               Mean episode length: 102.77
    Episode_Reward/reaching_object: 0.4924
     Episode_Reward/lifting_object: 41.0321
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0253
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 41.9167
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 2.12s
                      Time elapsed: 00:26:15
                               ETA: 00:49:33

################################################################################
                     [1m Learning iteration 693/2000 [0m                      

                       Computation: 47384 steps/s (collection: 1.989s, learning 0.086s)
             Mean action noise std: 2.13
          Mean value_function loss: 416.4763
               Mean surrogate loss: 0.0102
                 Mean entropy loss: 47.7024
                       Mean reward: 235.45
               Mean episode length: 112.41
    Episode_Reward/reaching_object: 0.4862
     Episode_Reward/lifting_object: 41.1507
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 38.7917
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 2.07s
                      Time elapsed: 00:26:17
                               ETA: 00:49:31

################################################################################
                     [1m Learning iteration 694/2000 [0m                      

                       Computation: 47682 steps/s (collection: 1.975s, learning 0.087s)
             Mean action noise std: 2.13
          Mean value_function loss: 371.4779
               Mean surrogate loss: 0.0135
                 Mean entropy loss: 47.7024
                       Mean reward: 188.96
               Mean episode length: 95.12
    Episode_Reward/reaching_object: 0.4962
     Episode_Reward/lifting_object: 42.9105
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 38.7917
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 2.06s
                      Time elapsed: 00:26:19
                               ETA: 00:49:28

################################################################################
                     [1m Learning iteration 695/2000 [0m                      

                       Computation: 47277 steps/s (collection: 1.978s, learning 0.102s)
             Mean action noise std: 2.13
          Mean value_function loss: 388.2462
               Mean surrogate loss: 0.0119
                 Mean entropy loss: 47.7024
                       Mean reward: 223.41
               Mean episode length: 103.85
    Episode_Reward/reaching_object: 0.5044
     Episode_Reward/lifting_object: 43.2693
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 39.7083
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 2.08s
                      Time elapsed: 00:26:21
                               ETA: 00:49:25

################################################################################
                     [1m Learning iteration 696/2000 [0m                      

                       Computation: 46897 steps/s (collection: 2.001s, learning 0.095s)
             Mean action noise std: 2.13
          Mean value_function loss: 424.2383
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 47.7025
                       Mean reward: 212.64
               Mean episode length: 100.23
    Episode_Reward/reaching_object: 0.4923
     Episode_Reward/lifting_object: 41.7734
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 39.7083
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 2.10s
                      Time elapsed: 00:26:23
                               ETA: 00:49:23

################################################################################
                     [1m Learning iteration 697/2000 [0m                      

                       Computation: 46719 steps/s (collection: 2.010s, learning 0.094s)
             Mean action noise std: 2.13
          Mean value_function loss: 420.5420
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 47.7024
                       Mean reward: 201.45
               Mean episode length: 96.42
    Episode_Reward/reaching_object: 0.4852
     Episode_Reward/lifting_object: 41.3928
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 36.3750
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 2.10s
                      Time elapsed: 00:26:26
                               ETA: 00:49:20

################################################################################
                     [1m Learning iteration 698/2000 [0m                      

                       Computation: 47989 steps/s (collection: 1.944s, learning 0.104s)
             Mean action noise std: 2.13
          Mean value_function loss: 425.4279
               Mean surrogate loss: 0.0103
                 Mean entropy loss: 47.7025
                       Mean reward: 226.63
               Mean episode length: 103.68
    Episode_Reward/reaching_object: 0.5007
     Episode_Reward/lifting_object: 42.8650
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 36.2083
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 2.05s
                      Time elapsed: 00:26:28
                               ETA: 00:49:18

################################################################################
                     [1m Learning iteration 699/2000 [0m                      

                       Computation: 47418 steps/s (collection: 1.980s, learning 0.094s)
             Mean action noise std: 2.13
          Mean value_function loss: 436.8709
               Mean surrogate loss: 0.0124
                 Mean entropy loss: 47.7027
                       Mean reward: 219.21
               Mean episode length: 103.83
    Episode_Reward/reaching_object: 0.5166
     Episode_Reward/lifting_object: 46.0959
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 35.9583
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 2.07s
                      Time elapsed: 00:26:30
                               ETA: 00:49:15

################################################################################
                     [1m Learning iteration 700/2000 [0m                      

                       Computation: 47308 steps/s (collection: 1.976s, learning 0.102s)
             Mean action noise std: 2.13
          Mean value_function loss: 435.5655
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 47.7028
                       Mean reward: 236.72
               Mean episode length: 103.48
    Episode_Reward/reaching_object: 0.5306
     Episode_Reward/lifting_object: 47.2309
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 37.1250
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 2.08s
                      Time elapsed: 00:26:32
                               ETA: 00:49:12

################################################################################
                     [1m Learning iteration 701/2000 [0m                      

                       Computation: 47590 steps/s (collection: 1.977s, learning 0.089s)
             Mean action noise std: 2.13
          Mean value_function loss: 430.8987
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 47.7026
                       Mean reward: 239.13
               Mean episode length: 106.04
    Episode_Reward/reaching_object: 0.5503
     Episode_Reward/lifting_object: 50.6007
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 36.2083
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 2.07s
                      Time elapsed: 00:26:34
                               ETA: 00:49:10

################################################################################
                     [1m Learning iteration 702/2000 [0m                      

                       Computation: 47637 steps/s (collection: 1.974s, learning 0.090s)
             Mean action noise std: 2.13
          Mean value_function loss: 428.4579
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 47.7017
                       Mean reward: 246.30
               Mean episode length: 104.67
    Episode_Reward/reaching_object: 0.5480
     Episode_Reward/lifting_object: 51.1388
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 37.7917
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 2.06s
                      Time elapsed: 00:26:36
                               ETA: 00:49:07

################################################################################
                     [1m Learning iteration 703/2000 [0m                      

                       Computation: 47285 steps/s (collection: 1.990s, learning 0.089s)
             Mean action noise std: 2.13
          Mean value_function loss: 460.3387
               Mean surrogate loss: 0.0143
                 Mean entropy loss: 47.7006
                       Mean reward: 256.83
               Mean episode length: 106.02
    Episode_Reward/reaching_object: 0.5324
     Episode_Reward/lifting_object: 48.8189
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 40.9167
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 2.08s
                      Time elapsed: 00:26:38
                               ETA: 00:49:04

################################################################################
                     [1m Learning iteration 704/2000 [0m                      

                       Computation: 47535 steps/s (collection: 1.971s, learning 0.098s)
             Mean action noise std: 2.13
          Mean value_function loss: 444.8423
               Mean surrogate loss: 0.0238
                 Mean entropy loss: 47.7006
                       Mean reward: 266.89
               Mean episode length: 108.13
    Episode_Reward/reaching_object: 0.5441
     Episode_Reward/lifting_object: 50.0966
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 40.5833
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 2.07s
                      Time elapsed: 00:26:40
                               ETA: 00:49:02

################################################################################
                     [1m Learning iteration 705/2000 [0m                      

                       Computation: 45684 steps/s (collection: 2.063s, learning 0.089s)
             Mean action noise std: 2.13
          Mean value_function loss: 457.6597
               Mean surrogate loss: 0.0103
                 Mean entropy loss: 47.7007
                       Mean reward: 212.95
               Mean episode length: 97.28
    Episode_Reward/reaching_object: 0.5200
     Episode_Reward/lifting_object: 47.1793
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 40.1250
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 2.15s
                      Time elapsed: 00:26:42
                               ETA: 00:48:59

################################################################################
                     [1m Learning iteration 706/2000 [0m                      

                       Computation: 46956 steps/s (collection: 1.999s, learning 0.095s)
             Mean action noise std: 2.13
          Mean value_function loss: 446.6801
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 47.7007
                       Mean reward: 208.54
               Mean episode length: 103.44
    Episode_Reward/reaching_object: 0.4979
     Episode_Reward/lifting_object: 44.4577
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 39.7500
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 2.09s
                      Time elapsed: 00:26:44
                               ETA: 00:48:57

################################################################################
                     [1m Learning iteration 707/2000 [0m                      

                       Computation: 46967 steps/s (collection: 2.003s, learning 0.090s)
             Mean action noise std: 2.13
          Mean value_function loss: 431.8618
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 47.7008
                       Mean reward: 243.12
               Mean episode length: 105.07
    Episode_Reward/reaching_object: 0.5069
     Episode_Reward/lifting_object: 46.5667
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 37.5417
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 2.09s
                      Time elapsed: 00:26:46
                               ETA: 00:48:54

################################################################################
                     [1m Learning iteration 708/2000 [0m                      

                       Computation: 47222 steps/s (collection: 1.987s, learning 0.095s)
             Mean action noise std: 2.13
          Mean value_function loss: 486.6670
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 47.7011
                       Mean reward: 247.58
               Mean episode length: 101.49
    Episode_Reward/reaching_object: 0.5131
     Episode_Reward/lifting_object: 48.0150
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 39.1667
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 2.08s
                      Time elapsed: 00:26:48
                               ETA: 00:48:51

################################################################################
                     [1m Learning iteration 709/2000 [0m                      

                       Computation: 46797 steps/s (collection: 2.013s, learning 0.088s)
             Mean action noise std: 2.13
          Mean value_function loss: 471.8134
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 47.7022
                       Mean reward: 245.69
               Mean episode length: 102.43
    Episode_Reward/reaching_object: 0.5193
     Episode_Reward/lifting_object: 48.9142
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 38.5833
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 2.10s
                      Time elapsed: 00:26:51
                               ETA: 00:48:49

################################################################################
                     [1m Learning iteration 710/2000 [0m                      

                       Computation: 47532 steps/s (collection: 1.977s, learning 0.092s)
             Mean action noise std: 2.13
          Mean value_function loss: 458.9710
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 47.7032
                       Mean reward: 246.56
               Mean episode length: 102.27
    Episode_Reward/reaching_object: 0.5065
     Episode_Reward/lifting_object: 48.3549
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 38.2500
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 2.07s
                      Time elapsed: 00:26:53
                               ETA: 00:48:46

################################################################################
                     [1m Learning iteration 711/2000 [0m                      

                       Computation: 47054 steps/s (collection: 1.997s, learning 0.093s)
             Mean action noise std: 2.13
          Mean value_function loss: 454.4982
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 47.7036
                       Mean reward: 229.15
               Mean episode length: 99.07
    Episode_Reward/reaching_object: 0.4985
     Episode_Reward/lifting_object: 46.5911
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 40.1250
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 2.09s
                      Time elapsed: 00:26:55
                               ETA: 00:48:44

################################################################################
                     [1m Learning iteration 712/2000 [0m                      

                       Computation: 47146 steps/s (collection: 1.987s, learning 0.098s)
             Mean action noise std: 2.13
          Mean value_function loss: 491.8536
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 47.7038
                       Mean reward: 227.68
               Mean episode length: 97.93
    Episode_Reward/reaching_object: 0.5072
     Episode_Reward/lifting_object: 47.1618
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 40.6667
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 2.09s
                      Time elapsed: 00:26:57
                               ETA: 00:48:41

################################################################################
                     [1m Learning iteration 713/2000 [0m                      

                       Computation: 47210 steps/s (collection: 1.969s, learning 0.113s)
             Mean action noise std: 2.13
          Mean value_function loss: 453.6207
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 47.7043
                       Mean reward: 232.49
               Mean episode length: 100.29
    Episode_Reward/reaching_object: 0.4925
     Episode_Reward/lifting_object: 45.3118
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 39.8750
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 2.08s
                      Time elapsed: 00:26:59
                               ETA: 00:48:38

################################################################################
                     [1m Learning iteration 714/2000 [0m                      

                       Computation: 46953 steps/s (collection: 1.993s, learning 0.101s)
             Mean action noise std: 2.13
          Mean value_function loss: 482.0831
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 47.7035
                       Mean reward: 216.54
               Mean episode length: 99.36
    Episode_Reward/reaching_object: 0.4878
     Episode_Reward/lifting_object: 44.6879
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 40.0417
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 2.09s
                      Time elapsed: 00:27:01
                               ETA: 00:48:36

################################################################################
                     [1m Learning iteration 715/2000 [0m                      

                       Computation: 47649 steps/s (collection: 1.975s, learning 0.089s)
             Mean action noise std: 2.13
          Mean value_function loss: 473.0300
               Mean surrogate loss: 0.0247
                 Mean entropy loss: 47.7039
                       Mean reward: 225.59
               Mean episode length: 98.97
    Episode_Reward/reaching_object: 0.4837
     Episode_Reward/lifting_object: 44.2326
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 39.7083
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 2.06s
                      Time elapsed: 00:27:03
                               ETA: 00:48:33

################################################################################
                     [1m Learning iteration 716/2000 [0m                      

                       Computation: 47833 steps/s (collection: 1.966s, learning 0.089s)
             Mean action noise std: 2.13
          Mean value_function loss: 434.3303
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 47.7042
                       Mean reward: 237.70
               Mean episode length: 100.06
    Episode_Reward/reaching_object: 0.4928
     Episode_Reward/lifting_object: 45.6560
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 37.7500
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 2.06s
                      Time elapsed: 00:27:05
                               ETA: 00:48:31

################################################################################
                     [1m Learning iteration 717/2000 [0m                      

                       Computation: 47428 steps/s (collection: 1.981s, learning 0.092s)
             Mean action noise std: 2.13
          Mean value_function loss: 482.2663
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 47.7045
                       Mean reward: 246.75
               Mean episode length: 102.55
    Episode_Reward/reaching_object: 0.5108
     Episode_Reward/lifting_object: 48.3273
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 38.2917
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 2.07s
                      Time elapsed: 00:27:07
                               ETA: 00:48:28

################################################################################
                     [1m Learning iteration 718/2000 [0m                      

                       Computation: 46809 steps/s (collection: 2.011s, learning 0.089s)
             Mean action noise std: 2.13
          Mean value_function loss: 461.3818
               Mean surrogate loss: 0.0111
                 Mean entropy loss: 47.7050
                       Mean reward: 243.19
               Mean episode length: 102.68
    Episode_Reward/reaching_object: 0.4976
     Episode_Reward/lifting_object: 46.4958
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 34.0417
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 2.10s
                      Time elapsed: 00:27:09
                               ETA: 00:48:25

################################################################################
                     [1m Learning iteration 719/2000 [0m                      

                       Computation: 48070 steps/s (collection: 1.959s, learning 0.086s)
             Mean action noise std: 2.13
          Mean value_function loss: 486.6269
               Mean surrogate loss: 0.0142
                 Mean entropy loss: 47.7053
                       Mean reward: 256.95
               Mean episode length: 110.40
    Episode_Reward/reaching_object: 0.5388
     Episode_Reward/lifting_object: 50.8619
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 34.7083
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 2.04s
                      Time elapsed: 00:27:11
                               ETA: 00:48:23

################################################################################
                     [1m Learning iteration 720/2000 [0m                      

                       Computation: 47808 steps/s (collection: 1.971s, learning 0.086s)
             Mean action noise std: 2.13
          Mean value_function loss: 482.0246
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 47.7054
                       Mean reward: 266.24
               Mean episode length: 107.37
    Episode_Reward/reaching_object: 0.5375
     Episode_Reward/lifting_object: 51.3637
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0253
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 34.4583
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 2.06s
                      Time elapsed: 00:27:13
                               ETA: 00:48:20

################################################################################
                     [1m Learning iteration 721/2000 [0m                      

                       Computation: 47775 steps/s (collection: 1.967s, learning 0.091s)
             Mean action noise std: 2.13
          Mean value_function loss: 474.0105
               Mean surrogate loss: 0.0118
                 Mean entropy loss: 47.7054
                       Mean reward: 275.52
               Mean episode length: 111.49
    Episode_Reward/reaching_object: 0.5901
     Episode_Reward/lifting_object: 58.5666
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 29.9583
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 2.06s
                      Time elapsed: 00:27:15
                               ETA: 00:48:17

################################################################################
                     [1m Learning iteration 722/2000 [0m                      

                       Computation: 47536 steps/s (collection: 1.970s, learning 0.098s)
             Mean action noise std: 2.13
          Mean value_function loss: 470.9015
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 47.7054
                       Mean reward: 291.36
               Mean episode length: 118.31
    Episode_Reward/reaching_object: 0.6048
     Episode_Reward/lifting_object: 59.3303
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 30.2917
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 2.07s
                      Time elapsed: 00:27:17
                               ETA: 00:48:15

################################################################################
                     [1m Learning iteration 723/2000 [0m                      

                       Computation: 47320 steps/s (collection: 1.979s, learning 0.099s)
             Mean action noise std: 2.13
          Mean value_function loss: 465.3076
               Mean surrogate loss: 0.0154
                 Mean entropy loss: 47.7055
                       Mean reward: 289.30
               Mean episode length: 111.32
    Episode_Reward/reaching_object: 0.6106
     Episode_Reward/lifting_object: 61.3427
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0271
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 33.8333
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 2.08s
                      Time elapsed: 00:27:20
                               ETA: 00:48:12

################################################################################
                     [1m Learning iteration 724/2000 [0m                      

                       Computation: 46988 steps/s (collection: 2.004s, learning 0.089s)
             Mean action noise std: 2.13
          Mean value_function loss: 457.0206
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 47.7056
                       Mean reward: 346.53
               Mean episode length: 126.04
    Episode_Reward/reaching_object: 0.6468
     Episode_Reward/lifting_object: 64.9853
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 32.3333
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 2.09s
                      Time elapsed: 00:27:22
                               ETA: 00:48:10

################################################################################
                     [1m Learning iteration 725/2000 [0m                      

                       Computation: 47790 steps/s (collection: 1.972s, learning 0.085s)
             Mean action noise std: 2.13
          Mean value_function loss: 430.1278
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 47.7055
                       Mean reward: 345.60
               Mean episode length: 131.31
    Episode_Reward/reaching_object: 0.6305
     Episode_Reward/lifting_object: 62.0400
      Episode_Reward/object_height: 0.0073
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 33.8333
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 2.06s
                      Time elapsed: 00:27:24
                               ETA: 00:48:07

################################################################################
                     [1m Learning iteration 726/2000 [0m                      

                       Computation: 46771 steps/s (collection: 2.013s, learning 0.089s)
             Mean action noise std: 2.13
          Mean value_function loss: 444.0425
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 47.7055
                       Mean reward: 316.25
               Mean episode length: 123.33
    Episode_Reward/reaching_object: 0.6191
     Episode_Reward/lifting_object: 60.9384
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 36.8750
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 2.10s
                      Time elapsed: 00:27:26
                               ETA: 00:48:04

################################################################################
                     [1m Learning iteration 727/2000 [0m                      

                       Computation: 47463 steps/s (collection: 1.979s, learning 0.092s)
             Mean action noise std: 2.13
          Mean value_function loss: 445.3370
               Mean surrogate loss: 0.0120
                 Mean entropy loss: 47.7054
                       Mean reward: 244.87
               Mean episode length: 111.85
    Episode_Reward/reaching_object: 0.6032
     Episode_Reward/lifting_object: 57.6587
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 35.4167
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 2.07s
                      Time elapsed: 00:27:28
                               ETA: 00:48:02

################################################################################
                     [1m Learning iteration 728/2000 [0m                      

                       Computation: 47054 steps/s (collection: 1.980s, learning 0.110s)
             Mean action noise std: 2.13
          Mean value_function loss: 441.5141
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 47.7053
                       Mean reward: 258.31
               Mean episode length: 110.53
    Episode_Reward/reaching_object: 0.5895
     Episode_Reward/lifting_object: 55.6753
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 30.2917
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 2.09s
                      Time elapsed: 00:27:30
                               ETA: 00:47:59

################################################################################
                     [1m Learning iteration 729/2000 [0m                      

                       Computation: 46517 steps/s (collection: 2.013s, learning 0.101s)
             Mean action noise std: 2.13
          Mean value_function loss: 438.9617
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 47.7052
                       Mean reward: 229.93
               Mean episode length: 100.39
    Episode_Reward/reaching_object: 0.5700
     Episode_Reward/lifting_object: 52.5327
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0271
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 29.4583
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 2.11s
                      Time elapsed: 00:27:32
                               ETA: 00:47:57

################################################################################
                     [1m Learning iteration 730/2000 [0m                      

                       Computation: 47121 steps/s (collection: 1.983s, learning 0.104s)
             Mean action noise std: 2.13
          Mean value_function loss: 460.2808
               Mean surrogate loss: 0.0101
                 Mean entropy loss: 47.7052
                       Mean reward: 308.04
               Mean episode length: 117.75
    Episode_Reward/reaching_object: 0.6113
     Episode_Reward/lifting_object: 58.2632
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 27.7917
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 2.09s
                      Time elapsed: 00:27:34
                               ETA: 00:47:54

################################################################################
                     [1m Learning iteration 731/2000 [0m                      

                       Computation: 47540 steps/s (collection: 1.979s, learning 0.089s)
             Mean action noise std: 2.13
          Mean value_function loss: 451.8357
               Mean surrogate loss: 0.0096
                 Mean entropy loss: 47.7052
                       Mean reward: 334.16
               Mean episode length: 128.34
    Episode_Reward/reaching_object: 0.6484
     Episode_Reward/lifting_object: 62.5593
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 28.3750
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 2.07s
                      Time elapsed: 00:27:36
                               ETA: 00:47:52

################################################################################
                     [1m Learning iteration 732/2000 [0m                      

                       Computation: 46601 steps/s (collection: 2.025s, learning 0.085s)
             Mean action noise std: 2.13
          Mean value_function loss: 441.3465
               Mean surrogate loss: 0.0098
                 Mean entropy loss: 47.7052
                       Mean reward: 320.06
               Mean episode length: 119.47
    Episode_Reward/reaching_object: 0.6807
     Episode_Reward/lifting_object: 66.6527
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 24.1667
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 2.11s
                      Time elapsed: 00:27:38
                               ETA: 00:47:49

################################################################################
                     [1m Learning iteration 733/2000 [0m                      

                       Computation: 47632 steps/s (collection: 1.977s, learning 0.087s)
             Mean action noise std: 2.13
          Mean value_function loss: 460.3351
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 47.7052
                       Mean reward: 398.77
               Mean episode length: 140.30
    Episode_Reward/reaching_object: 0.7003
     Episode_Reward/lifting_object: 69.8840
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 27.9583
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 2.06s
                      Time elapsed: 00:27:40
                               ETA: 00:47:46

################################################################################
                     [1m Learning iteration 734/2000 [0m                      

                       Computation: 46565 steps/s (collection: 2.018s, learning 0.094s)
             Mean action noise std: 2.13
          Mean value_function loss: 473.0788
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 47.7051
                       Mean reward: 344.80
               Mean episode length: 129.94
    Episode_Reward/reaching_object: 0.7228
     Episode_Reward/lifting_object: 72.5198
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 27.9167
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 2.11s
                      Time elapsed: 00:27:42
                               ETA: 00:47:44

################################################################################
                     [1m Learning iteration 735/2000 [0m                      

                       Computation: 47413 steps/s (collection: 1.973s, learning 0.100s)
             Mean action noise std: 2.13
          Mean value_function loss: 465.8170
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 47.7052
                       Mean reward: 354.42
               Mean episode length: 131.65
    Episode_Reward/reaching_object: 0.7425
     Episode_Reward/lifting_object: 76.1000
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 28.7500
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 2.07s
                      Time elapsed: 00:27:45
                               ETA: 00:47:41

################################################################################
                     [1m Learning iteration 736/2000 [0m                      

                       Computation: 46866 steps/s (collection: 2.002s, learning 0.096s)
             Mean action noise std: 2.13
          Mean value_function loss: 456.7333
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 47.7055
                       Mean reward: 416.06
               Mean episode length: 141.45
    Episode_Reward/reaching_object: 0.7770
     Episode_Reward/lifting_object: 81.3070
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 3.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 28.8333
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 2.10s
                      Time elapsed: 00:27:47
                               ETA: 00:47:39

################################################################################
                     [1m Learning iteration 737/2000 [0m                      

                       Computation: 46872 steps/s (collection: 2.012s, learning 0.085s)
             Mean action noise std: 2.13
          Mean value_function loss: 449.1756
               Mean surrogate loss: 0.0112
                 Mean entropy loss: 47.7055
                       Mean reward: 311.30
               Mean episode length: 116.21
    Episode_Reward/reaching_object: 0.7382
     Episode_Reward/lifting_object: 76.2434
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 27.3333
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 2.10s
                      Time elapsed: 00:27:49
                               ETA: 00:47:36

################################################################################
                     [1m Learning iteration 738/2000 [0m                      

                       Computation: 48106 steps/s (collection: 1.953s, learning 0.091s)
             Mean action noise std: 2.13
          Mean value_function loss: 444.4106
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 47.7052
                       Mean reward: 365.60
               Mean episode length: 131.24
    Episode_Reward/reaching_object: 0.7526
     Episode_Reward/lifting_object: 77.1110
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 26.7083
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 2.04s
                      Time elapsed: 00:27:51
                               ETA: 00:47:34

################################################################################
                     [1m Learning iteration 739/2000 [0m                      

                       Computation: 47050 steps/s (collection: 1.999s, learning 0.090s)
             Mean action noise std: 2.13
          Mean value_function loss: 480.9012
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 47.7050
                       Mean reward: 341.12
               Mean episode length: 126.68
    Episode_Reward/reaching_object: 0.7157
     Episode_Reward/lifting_object: 72.8571
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 25.8750
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 2.09s
                      Time elapsed: 00:27:53
                               ETA: 00:47:31

################################################################################
                     [1m Learning iteration 740/2000 [0m                      

                       Computation: 46797 steps/s (collection: 2.005s, learning 0.096s)
             Mean action noise std: 2.13
          Mean value_function loss: 462.3611
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 47.7062
                       Mean reward: 350.73
               Mean episode length: 126.05
    Episode_Reward/reaching_object: 0.7308
     Episode_Reward/lifting_object: 73.8344
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 25.9167
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 2.10s
                      Time elapsed: 00:27:55
                               ETA: 00:47:28

################################################################################
                     [1m Learning iteration 741/2000 [0m                      

                       Computation: 47053 steps/s (collection: 2.000s, learning 0.089s)
             Mean action noise std: 2.13
          Mean value_function loss: 473.9989
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 47.7065
                       Mean reward: 396.79
               Mean episode length: 136.69
    Episode_Reward/reaching_object: 0.7514
     Episode_Reward/lifting_object: 75.6595
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 24.9167
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 2.09s
                      Time elapsed: 00:27:57
                               ETA: 00:47:26

################################################################################
                     [1m Learning iteration 742/2000 [0m                      

                       Computation: 46605 steps/s (collection: 2.007s, learning 0.102s)
             Mean action noise std: 2.13
          Mean value_function loss: 465.1510
               Mean surrogate loss: 0.0140
                 Mean entropy loss: 47.7067
                       Mean reward: 392.93
               Mean episode length: 138.72
    Episode_Reward/reaching_object: 0.7624
     Episode_Reward/lifting_object: 77.7699
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 2.7917
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 22.9583
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 2.11s
                      Time elapsed: 00:27:59
                               ETA: 00:47:23

################################################################################
                     [1m Learning iteration 743/2000 [0m                      

                       Computation: 47409 steps/s (collection: 1.976s, learning 0.098s)
             Mean action noise std: 2.13
          Mean value_function loss: 499.0788
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 47.7069
                       Mean reward: 393.58
               Mean episode length: 137.71
    Episode_Reward/reaching_object: 0.7747
     Episode_Reward/lifting_object: 79.9460
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 26.2917
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 2.07s
                      Time elapsed: 00:28:01
                               ETA: 00:47:21

################################################################################
                     [1m Learning iteration 744/2000 [0m                      

                       Computation: 47602 steps/s (collection: 1.965s, learning 0.101s)
             Mean action noise std: 2.13
          Mean value_function loss: 473.8863
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 47.7078
                       Mean reward: 409.99
               Mean episode length: 136.74
    Episode_Reward/reaching_object: 0.7925
     Episode_Reward/lifting_object: 82.8184
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 26.5000
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 2.07s
                      Time elapsed: 00:28:03
                               ETA: 00:47:18

################################################################################
                     [1m Learning iteration 745/2000 [0m                      

                       Computation: 47394 steps/s (collection: 1.980s, learning 0.094s)
             Mean action noise std: 2.13
          Mean value_function loss: 476.9765
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 47.7079
                       Mean reward: 436.23
               Mean episode length: 145.80
    Episode_Reward/reaching_object: 0.7762
     Episode_Reward/lifting_object: 79.7377
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 25.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 2.07s
                      Time elapsed: 00:28:05
                               ETA: 00:47:16

################################################################################
                     [1m Learning iteration 746/2000 [0m                      

                       Computation: 47260 steps/s (collection: 1.987s, learning 0.093s)
             Mean action noise std: 2.13
          Mean value_function loss: 452.1691
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 47.7081
                       Mean reward: 426.68
               Mean episode length: 141.07
    Episode_Reward/reaching_object: 0.8374
     Episode_Reward/lifting_object: 88.5194
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 3.9583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 25.5417
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 2.08s
                      Time elapsed: 00:28:07
                               ETA: 00:47:13

################################################################################
                     [1m Learning iteration 747/2000 [0m                      

                       Computation: 47174 steps/s (collection: 1.995s, learning 0.089s)
             Mean action noise std: 2.13
          Mean value_function loss: 469.4527
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.7083
                       Mean reward: 441.41
               Mean episode length: 145.32
    Episode_Reward/reaching_object: 0.8055
     Episode_Reward/lifting_object: 85.3541
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 22.3750
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 2.08s
                      Time elapsed: 00:28:10
                               ETA: 00:47:11

################################################################################
                     [1m Learning iteration 748/2000 [0m                      

                       Computation: 47125 steps/s (collection: 2.000s, learning 0.086s)
             Mean action noise std: 2.13
          Mean value_function loss: 480.1957
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 47.7085
                       Mean reward: 451.47
               Mean episode length: 150.18
    Episode_Reward/reaching_object: 0.8246
     Episode_Reward/lifting_object: 86.9186
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 22.7083
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 2.09s
                      Time elapsed: 00:28:12
                               ETA: 00:47:08

################################################################################
                     [1m Learning iteration 749/2000 [0m                      

                       Computation: 46370 steps/s (collection: 2.027s, learning 0.093s)
             Mean action noise std: 2.13
          Mean value_function loss: 492.5272
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 47.7091
                       Mean reward: 444.18
               Mean episode length: 146.28
    Episode_Reward/reaching_object: 0.8182
     Episode_Reward/lifting_object: 86.5077
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 21.6250
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 2.12s
                      Time elapsed: 00:28:14
                               ETA: 00:47:06

################################################################################
                     [1m Learning iteration 750/2000 [0m                      

                       Computation: 47802 steps/s (collection: 1.966s, learning 0.090s)
             Mean action noise std: 2.13
          Mean value_function loss: 486.9946
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 47.7101
                       Mean reward: 459.85
               Mean episode length: 149.13
    Episode_Reward/reaching_object: 0.8535
     Episode_Reward/lifting_object: 90.8427
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 21.7917
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 2.06s
                      Time elapsed: 00:28:16
                               ETA: 00:47:03

################################################################################
                     [1m Learning iteration 751/2000 [0m                      

                       Computation: 46717 steps/s (collection: 2.014s, learning 0.090s)
             Mean action noise std: 2.13
          Mean value_function loss: 475.5805
               Mean surrogate loss: 0.0171
                 Mean entropy loss: 47.7099
                       Mean reward: 463.63
               Mean episode length: 149.23
    Episode_Reward/reaching_object: 0.8587
     Episode_Reward/lifting_object: 91.2770
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 21.7917
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 2.10s
                      Time elapsed: 00:28:18
                               ETA: 00:47:00

################################################################################
                     [1m Learning iteration 752/2000 [0m                      

                       Computation: 47393 steps/s (collection: 1.987s, learning 0.088s)
             Mean action noise std: 2.13
          Mean value_function loss: 434.6196
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 47.7100
                       Mean reward: 414.74
               Mean episode length: 140.27
    Episode_Reward/reaching_object: 0.8395
     Episode_Reward/lifting_object: 88.6933
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 22.2083
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 2.07s
                      Time elapsed: 00:28:20
                               ETA: 00:46:58

################################################################################
                     [1m Learning iteration 753/2000 [0m                      

                       Computation: 46629 steps/s (collection: 2.004s, learning 0.104s)
             Mean action noise std: 2.13
          Mean value_function loss: 452.4554
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 47.7101
                       Mean reward: 496.86
               Mean episode length: 161.25
    Episode_Reward/reaching_object: 0.8761
     Episode_Reward/lifting_object: 93.5137
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 5.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 20.1250
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 2.11s
                      Time elapsed: 00:28:22
                               ETA: 00:46:55

################################################################################
                     [1m Learning iteration 754/2000 [0m                      

                       Computation: 46754 steps/s (collection: 2.009s, learning 0.094s)
             Mean action noise std: 2.13
          Mean value_function loss: 458.7374
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 47.7102
                       Mean reward: 517.39
               Mean episode length: 166.61
    Episode_Reward/reaching_object: 0.8999
     Episode_Reward/lifting_object: 96.9907
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 20.3333
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 2.10s
                      Time elapsed: 00:28:24
                               ETA: 00:46:53

################################################################################
                     [1m Learning iteration 755/2000 [0m                      

                       Computation: 47589 steps/s (collection: 1.980s, learning 0.086s)
             Mean action noise std: 2.13
          Mean value_function loss: 456.1455
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 47.7103
                       Mean reward: 431.75
               Mean episode length: 147.22
    Episode_Reward/reaching_object: 0.9057
     Episode_Reward/lifting_object: 96.6286
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 5.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 21.5000
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 2.07s
                      Time elapsed: 00:28:26
                               ETA: 00:46:50

################################################################################
                     [1m Learning iteration 756/2000 [0m                      

                       Computation: 43509 steps/s (collection: 2.108s, learning 0.152s)
             Mean action noise std: 2.13
          Mean value_function loss: 495.0013
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 47.7103
                       Mean reward: 490.49
               Mean episode length: 161.60
    Episode_Reward/reaching_object: 0.9065
     Episode_Reward/lifting_object: 96.3435
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 24.3750
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 2.26s
                      Time elapsed: 00:28:29
                               ETA: 00:46:48

################################################################################
                     [1m Learning iteration 757/2000 [0m                      

                       Computation: 39263 steps/s (collection: 2.313s, learning 0.191s)
             Mean action noise std: 2.13
          Mean value_function loss: 490.8924
               Mean surrogate loss: 0.0177
                 Mean entropy loss: 47.7107
                       Mean reward: 406.73
               Mean episode length: 142.12
    Episode_Reward/reaching_object: 0.8322
     Episode_Reward/lifting_object: 87.5375
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 23.4583
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 2.50s
                      Time elapsed: 00:28:31
                               ETA: 00:46:46

################################################################################
                     [1m Learning iteration 758/2000 [0m                      

                       Computation: 42087 steps/s (collection: 2.240s, learning 0.096s)
             Mean action noise std: 2.13
          Mean value_function loss: 462.0956
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 47.7107
                       Mean reward: 429.66
               Mean episode length: 147.11
    Episode_Reward/reaching_object: 0.8251
     Episode_Reward/lifting_object: 86.6825
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 21.1250
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 2.34s
                      Time elapsed: 00:28:33
                               ETA: 00:46:44

################################################################################
                     [1m Learning iteration 759/2000 [0m                      

                       Computation: 44784 steps/s (collection: 2.087s, learning 0.108s)
             Mean action noise std: 2.13
          Mean value_function loss: 455.1161
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 47.7108
                       Mean reward: 416.54
               Mean episode length: 143.42
    Episode_Reward/reaching_object: 0.8078
     Episode_Reward/lifting_object: 84.1374
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 21.2917
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 2.20s
                      Time elapsed: 00:28:36
                               ETA: 00:46:42

################################################################################
                     [1m Learning iteration 760/2000 [0m                      

                       Computation: 43390 steps/s (collection: 2.124s, learning 0.142s)
             Mean action noise std: 2.13
          Mean value_function loss: 443.4859
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.7112
                       Mean reward: 469.96
               Mean episode length: 152.58
    Episode_Reward/reaching_object: 0.8587
     Episode_Reward/lifting_object: 90.8457
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.8333
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 2.27s
                      Time elapsed: 00:28:38
                               ETA: 00:46:39

################################################################################
                     [1m Learning iteration 761/2000 [0m                      

                       Computation: 45050 steps/s (collection: 2.070s, learning 0.112s)
             Mean action noise std: 2.13
          Mean value_function loss: 420.5033
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 47.7131
                       Mean reward: 498.81
               Mean episode length: 163.83
    Episode_Reward/reaching_object: 0.8479
     Episode_Reward/lifting_object: 89.2112
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.1250
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 2.18s
                      Time elapsed: 00:28:40
                               ETA: 00:46:37

################################################################################
                     [1m Learning iteration 762/2000 [0m                      

                       Computation: 45946 steps/s (collection: 2.034s, learning 0.105s)
             Mean action noise std: 2.13
          Mean value_function loss: 433.8346
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 47.7138
                       Mean reward: 469.18
               Mean episode length: 157.19
    Episode_Reward/reaching_object: 0.9014
     Episode_Reward/lifting_object: 95.5313
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.2083
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 2.14s
                      Time elapsed: 00:28:42
                               ETA: 00:46:35

################################################################################
                     [1m Learning iteration 763/2000 [0m                      

                       Computation: 44284 steps/s (collection: 2.116s, learning 0.104s)
             Mean action noise std: 2.13
          Mean value_function loss: 417.3250
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 47.7138
                       Mean reward: 529.66
               Mean episode length: 168.51
    Episode_Reward/reaching_object: 0.8930
     Episode_Reward/lifting_object: 94.6239
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 16.9167
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 2.22s
                      Time elapsed: 00:28:44
                               ETA: 00:46:32

################################################################################
                     [1m Learning iteration 764/2000 [0m                      

                       Computation: 45902 steps/s (collection: 2.051s, learning 0.091s)
             Mean action noise std: 2.13
          Mean value_function loss: 433.2316
               Mean surrogate loss: 0.0121
                 Mean entropy loss: 47.7135
                       Mean reward: 469.08
               Mean episode length: 157.54
    Episode_Reward/reaching_object: 0.9574
     Episode_Reward/lifting_object: 102.3273
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 17.3750
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 2.14s
                      Time elapsed: 00:28:47
                               ETA: 00:46:30

################################################################################
                     [1m Learning iteration 765/2000 [0m                      

                       Computation: 44612 steps/s (collection: 2.091s, learning 0.113s)
             Mean action noise std: 2.13
          Mean value_function loss: 389.6009
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 47.7137
                       Mean reward: 571.29
               Mean episode length: 178.28
    Episode_Reward/reaching_object: 0.9809
     Episode_Reward/lifting_object: 104.1820
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 2.20s
                      Time elapsed: 00:28:49
                               ETA: 00:46:27

################################################################################
                     [1m Learning iteration 766/2000 [0m                      

                       Computation: 46943 steps/s (collection: 2.000s, learning 0.094s)
             Mean action noise std: 2.13
          Mean value_function loss: 397.5913
               Mean surrogate loss: 0.0103
                 Mean entropy loss: 47.7136
                       Mean reward: 561.51
               Mean episode length: 183.93
    Episode_Reward/reaching_object: 1.0114
     Episode_Reward/lifting_object: 108.5455
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.3750
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 2.09s
                      Time elapsed: 00:28:51
                               ETA: 00:46:25

################################################################################
                     [1m Learning iteration 767/2000 [0m                      

                       Computation: 46418 steps/s (collection: 1.984s, learning 0.134s)
             Mean action noise std: 2.13
          Mean value_function loss: 405.7698
               Mean surrogate loss: 0.0118
                 Mean entropy loss: 47.7136
                       Mean reward: 565.30
               Mean episode length: 172.60
    Episode_Reward/reaching_object: 1.0263
     Episode_Reward/lifting_object: 111.2141
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.2917
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 2.12s
                      Time elapsed: 00:28:53
                               ETA: 00:46:22

################################################################################
                     [1m Learning iteration 768/2000 [0m                      

                       Computation: 45888 steps/s (collection: 2.033s, learning 0.110s)
             Mean action noise std: 2.13
          Mean value_function loss: 397.2830
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 47.7137
                       Mean reward: 597.03
               Mean episode length: 190.20
    Episode_Reward/reaching_object: 1.0594
     Episode_Reward/lifting_object: 114.6692
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 2.14s
                      Time elapsed: 00:28:55
                               ETA: 00:46:20

################################################################################
                     [1m Learning iteration 769/2000 [0m                      

                       Computation: 45974 steps/s (collection: 2.029s, learning 0.110s)
             Mean action noise std: 2.13
          Mean value_function loss: 378.8801
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 47.7137
                       Mean reward: 528.84
               Mean episode length: 165.84
    Episode_Reward/reaching_object: 1.0485
     Episode_Reward/lifting_object: 113.5642
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.4583
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 2.14s
                      Time elapsed: 00:28:57
                               ETA: 00:46:18

################################################################################
                     [1m Learning iteration 770/2000 [0m                      

                       Computation: 44855 steps/s (collection: 2.099s, learning 0.093s)
             Mean action noise std: 2.13
          Mean value_function loss: 382.8707
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 47.7140
                       Mean reward: 556.90
               Mean episode length: 171.04
    Episode_Reward/reaching_object: 0.9807
     Episode_Reward/lifting_object: 104.8323
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 2.19s
                      Time elapsed: 00:28:59
                               ETA: 00:46:15

################################################################################
                     [1m Learning iteration 771/2000 [0m                      

                       Computation: 46512 steps/s (collection: 2.029s, learning 0.085s)
             Mean action noise std: 2.13
          Mean value_function loss: 382.2769
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 47.7143
                       Mean reward: 516.95
               Mean episode length: 168.66
    Episode_Reward/reaching_object: 1.0188
     Episode_Reward/lifting_object: 109.9244
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 12.7500
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 2.11s
                      Time elapsed: 00:29:02
                               ETA: 00:46:13

################################################################################
                     [1m Learning iteration 772/2000 [0m                      

                       Computation: 46553 steps/s (collection: 2.025s, learning 0.087s)
             Mean action noise std: 2.13
          Mean value_function loss: 402.1604
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 47.7144
                       Mean reward: 553.09
               Mean episode length: 173.23
    Episode_Reward/reaching_object: 1.0032
     Episode_Reward/lifting_object: 108.5378
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0365
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.5000
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 2.11s
                      Time elapsed: 00:29:04
                               ETA: 00:46:10

################################################################################
                     [1m Learning iteration 773/2000 [0m                      

                       Computation: 46888 steps/s (collection: 2.005s, learning 0.092s)
             Mean action noise std: 2.13
          Mean value_function loss: 418.9422
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 47.7145
                       Mean reward: 566.80
               Mean episode length: 176.53
    Episode_Reward/reaching_object: 0.9902
     Episode_Reward/lifting_object: 106.7312
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 2.10s
                      Time elapsed: 00:29:06
                               ETA: 00:46:08

################################################################################
                     [1m Learning iteration 774/2000 [0m                      

                       Computation: 45948 steps/s (collection: 2.034s, learning 0.105s)
             Mean action noise std: 2.13
          Mean value_function loss: 463.8223
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 47.7148
                       Mean reward: 523.41
               Mean episode length: 169.57
    Episode_Reward/reaching_object: 0.9868
     Episode_Reward/lifting_object: 106.5986
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 2.14s
                      Time elapsed: 00:29:08
                               ETA: 00:46:05

################################################################################
                     [1m Learning iteration 775/2000 [0m                      

                       Computation: 46640 steps/s (collection: 2.021s, learning 0.087s)
             Mean action noise std: 2.13
          Mean value_function loss: 438.8375
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 47.7146
                       Mean reward: 527.53
               Mean episode length: 168.48
    Episode_Reward/reaching_object: 1.0122
     Episode_Reward/lifting_object: 109.6180
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 18.3750
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 2.11s
                      Time elapsed: 00:29:10
                               ETA: 00:46:03

################################################################################
                     [1m Learning iteration 776/2000 [0m                      

                       Computation: 45218 steps/s (collection: 2.060s, learning 0.114s)
             Mean action noise std: 2.13
          Mean value_function loss: 496.8447
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 47.7136
                       Mean reward: 520.32
               Mean episode length: 166.17
    Episode_Reward/reaching_object: 0.9330
     Episode_Reward/lifting_object: 99.4834
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 21.4167
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 2.17s
                      Time elapsed: 00:29:12
                               ETA: 00:46:00

################################################################################
                     [1m Learning iteration 777/2000 [0m                      

                       Computation: 46117 steps/s (collection: 2.033s, learning 0.099s)
             Mean action noise std: 2.13
          Mean value_function loss: 462.9221
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 47.7138
                       Mean reward: 502.10
               Mean episode length: 160.71
    Episode_Reward/reaching_object: 0.9077
     Episode_Reward/lifting_object: 96.4925
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 19.5833
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 2.13s
                      Time elapsed: 00:29:14
                               ETA: 00:45:58

################################################################################
                     [1m Learning iteration 778/2000 [0m                      

                       Computation: 45463 steps/s (collection: 2.063s, learning 0.100s)
             Mean action noise std: 2.13
          Mean value_function loss: 440.1642
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 47.7149
                       Mean reward: 447.44
               Mean episode length: 148.56
    Episode_Reward/reaching_object: 0.8787
     Episode_Reward/lifting_object: 92.9066
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 19.2083
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 2.16s
                      Time elapsed: 00:29:16
                               ETA: 00:45:56

################################################################################
                     [1m Learning iteration 779/2000 [0m                      

                       Computation: 45985 steps/s (collection: 2.021s, learning 0.117s)
             Mean action noise std: 2.13
          Mean value_function loss: 422.6238
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 47.7147
                       Mean reward: 433.28
               Mean episode length: 148.94
    Episode_Reward/reaching_object: 0.8420
     Episode_Reward/lifting_object: 88.3680
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.3750
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 2.14s
                      Time elapsed: 00:29:19
                               ETA: 00:45:53

################################################################################
                     [1m Learning iteration 780/2000 [0m                      

                       Computation: 46629 steps/s (collection: 2.018s, learning 0.090s)
             Mean action noise std: 2.13
          Mean value_function loss: 394.3481
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 47.7151
                       Mean reward: 533.67
               Mean episode length: 168.64
    Episode_Reward/reaching_object: 0.9019
     Episode_Reward/lifting_object: 96.2885
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 2.11s
                      Time elapsed: 00:29:21
                               ETA: 00:45:51

################################################################################
                     [1m Learning iteration 781/2000 [0m                      

                       Computation: 46259 steps/s (collection: 2.032s, learning 0.093s)
             Mean action noise std: 2.13
          Mean value_function loss: 408.9903
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 47.7154
                       Mean reward: 526.29
               Mean episode length: 167.78
    Episode_Reward/reaching_object: 0.9290
     Episode_Reward/lifting_object: 99.4941
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 2.13s
                      Time elapsed: 00:29:23
                               ETA: 00:45:48

################################################################################
                     [1m Learning iteration 782/2000 [0m                      

                       Computation: 46302 steps/s (collection: 2.031s, learning 0.093s)
             Mean action noise std: 2.13
          Mean value_function loss: 420.5876
               Mean surrogate loss: 0.0133
                 Mean entropy loss: 47.7159
                       Mean reward: 518.16
               Mean episode length: 165.24
    Episode_Reward/reaching_object: 0.9269
     Episode_Reward/lifting_object: 100.5463
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 2.12s
                      Time elapsed: 00:29:25
                               ETA: 00:45:46

################################################################################
                     [1m Learning iteration 783/2000 [0m                      

                       Computation: 46077 steps/s (collection: 2.017s, learning 0.116s)
             Mean action noise std: 2.13
          Mean value_function loss: 388.5953
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 47.7159
                       Mean reward: 496.00
               Mean episode length: 160.00
    Episode_Reward/reaching_object: 0.9425
     Episode_Reward/lifting_object: 101.8202
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 2.13s
                      Time elapsed: 00:29:27
                               ETA: 00:45:43

################################################################################
                     [1m Learning iteration 784/2000 [0m                      

                       Computation: 46135 steps/s (collection: 2.015s, learning 0.116s)
             Mean action noise std: 2.13
          Mean value_function loss: 399.7401
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 47.7160
                       Mean reward: 544.37
               Mean episode length: 173.37
    Episode_Reward/reaching_object: 1.0299
     Episode_Reward/lifting_object: 112.6389
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 2.13s
                      Time elapsed: 00:29:29
                               ETA: 00:45:41

################################################################################
                     [1m Learning iteration 785/2000 [0m                      

                       Computation: 44613 steps/s (collection: 2.105s, learning 0.098s)
             Mean action noise std: 2.13
          Mean value_function loss: 369.1467
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 47.7159
                       Mean reward: 617.11
               Mean episode length: 187.97
    Episode_Reward/reaching_object: 1.0886
     Episode_Reward/lifting_object: 120.9170
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 2.20s
                      Time elapsed: 00:29:31
                               ETA: 00:45:39

################################################################################
                     [1m Learning iteration 786/2000 [0m                      

                       Computation: 46147 steps/s (collection: 2.041s, learning 0.090s)
             Mean action noise std: 2.13
          Mean value_function loss: 3502435242.5057
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.7157
                       Mean reward: 703.97
               Mean episode length: 204.36
    Episode_Reward/reaching_object: 1.1396
     Episode_Reward/lifting_object: 127.4085
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -2.6947
          Episode_Reward/joint_vel: -2955.1577
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 2.13s
                      Time elapsed: 00:29:34
                               ETA: 00:45:36

################################################################################
                     [1m Learning iteration 787/2000 [0m                      

                       Computation: 41694 steps/s (collection: 2.183s, learning 0.175s)
             Mean action noise std: 2.13
          Mean value_function loss: 376.3702
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 47.7165
                       Mean reward: 599.89
               Mean episode length: 182.22
    Episode_Reward/reaching_object: 1.1279
     Episode_Reward/lifting_object: 126.3627
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 2.36s
                      Time elapsed: 00:29:36
                               ETA: 00:45:34

################################################################################
                     [1m Learning iteration 788/2000 [0m                      

                       Computation: 46046 steps/s (collection: 2.044s, learning 0.091s)
             Mean action noise std: 2.14
          Mean value_function loss: 369.1194
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 47.7184
                       Mean reward: 618.70
               Mean episode length: 186.33
    Episode_Reward/reaching_object: 1.1035
     Episode_Reward/lifting_object: 123.0836
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 2.13s
                      Time elapsed: 00:29:38
                               ETA: 00:45:32

################################################################################
                     [1m Learning iteration 789/2000 [0m                      

                       Computation: 46697 steps/s (collection: 2.018s, learning 0.087s)
             Mean action noise std: 2.14
          Mean value_function loss: 358.3772
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 47.7199
                       Mean reward: 548.79
               Mean episode length: 170.38
    Episode_Reward/reaching_object: 1.0820
     Episode_Reward/lifting_object: 119.8120
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 2.11s
                      Time elapsed: 00:29:40
                               ETA: 00:45:29

################################################################################
                     [1m Learning iteration 790/2000 [0m                      

                       Computation: 46996 steps/s (collection: 1.999s, learning 0.093s)
             Mean action noise std: 2.14
          Mean value_function loss: 336.6784
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 47.7213
                       Mean reward: 598.54
               Mean episode length: 181.18
    Episode_Reward/reaching_object: 1.0907
     Episode_Reward/lifting_object: 121.2437
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 2.09s
                      Time elapsed: 00:29:42
                               ETA: 00:45:27

################################################################################
                     [1m Learning iteration 791/2000 [0m                      

                       Computation: 46445 steps/s (collection: 2.016s, learning 0.101s)
             Mean action noise std: 2.14
          Mean value_function loss: 331.6822
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 47.7219
                       Mean reward: 645.09
               Mean episode length: 196.50
    Episode_Reward/reaching_object: 1.0937
     Episode_Reward/lifting_object: 120.6497
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 2.12s
                      Time elapsed: 00:29:44
                               ETA: 00:45:24

################################################################################
                     [1m Learning iteration 792/2000 [0m                      

                       Computation: 45990 steps/s (collection: 2.035s, learning 0.102s)
             Mean action noise std: 2.14
          Mean value_function loss: 340.0787
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 47.7223
                       Mean reward: 634.67
               Mean episode length: 188.58
    Episode_Reward/reaching_object: 1.1361
     Episode_Reward/lifting_object: 126.3620
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 2.14s
                      Time elapsed: 00:29:46
                               ETA: 00:45:22

################################################################################
                     [1m Learning iteration 793/2000 [0m                      

                       Computation: 47227 steps/s (collection: 1.994s, learning 0.087s)
             Mean action noise std: 2.14
          Mean value_function loss: 344.6153
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 47.7230
                       Mean reward: 561.73
               Mean episode length: 172.64
    Episode_Reward/reaching_object: 1.1396
     Episode_Reward/lifting_object: 126.4822
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 2.08s
                      Time elapsed: 00:29:49
                               ETA: 00:45:19

################################################################################
                     [1m Learning iteration 794/2000 [0m                      

                       Computation: 45986 steps/s (collection: 2.049s, learning 0.089s)
             Mean action noise std: 2.14
          Mean value_function loss: 328.2000
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 47.7237
                       Mean reward: 664.16
               Mean episode length: 194.53
    Episode_Reward/reaching_object: 1.1348
     Episode_Reward/lifting_object: 125.8938
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 2.14s
                      Time elapsed: 00:29:51
                               ETA: 00:45:17

################################################################################
                     [1m Learning iteration 795/2000 [0m                      

                       Computation: 47270 steps/s (collection: 1.988s, learning 0.092s)
             Mean action noise std: 2.14
          Mean value_function loss: 342.7600
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 47.7239
                       Mean reward: 603.43
               Mean episode length: 182.22
    Episode_Reward/reaching_object: 1.1157
     Episode_Reward/lifting_object: 123.1743
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 2.08s
                      Time elapsed: 00:29:53
                               ETA: 00:45:14

################################################################################
                     [1m Learning iteration 796/2000 [0m                      

                       Computation: 46718 steps/s (collection: 2.013s, learning 0.092s)
             Mean action noise std: 2.14
          Mean value_function loss: 326.9911
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 47.7239
                       Mean reward: 663.37
               Mean episode length: 196.10
    Episode_Reward/reaching_object: 1.1501
     Episode_Reward/lifting_object: 128.6641
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 2.10s
                      Time elapsed: 00:29:55
                               ETA: 00:45:12

################################################################################
                     [1m Learning iteration 797/2000 [0m                      

                       Computation: 45643 steps/s (collection: 2.043s, learning 0.111s)
             Mean action noise std: 2.14
          Mean value_function loss: 332.1300
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 47.7250
                       Mean reward: 623.41
               Mean episode length: 186.50
    Episode_Reward/reaching_object: 1.2330
     Episode_Reward/lifting_object: 140.0666
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 2.15s
                      Time elapsed: 00:29:57
                               ETA: 00:45:09

################################################################################
                     [1m Learning iteration 798/2000 [0m                      

                       Computation: 44905 steps/s (collection: 2.048s, learning 0.141s)
             Mean action noise std: 2.14
          Mean value_function loss: 354.9819
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 47.7290
                       Mean reward: 651.93
               Mean episode length: 191.64
    Episode_Reward/reaching_object: 1.1276
     Episode_Reward/lifting_object: 126.2164
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 2.19s
                      Time elapsed: 00:29:59
                               ETA: 00:45:07

################################################################################
                     [1m Learning iteration 799/2000 [0m                      

                       Computation: 45753 steps/s (collection: 2.037s, learning 0.112s)
             Mean action noise std: 2.14
          Mean value_function loss: 317.8778
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 47.7306
                       Mean reward: 712.55
               Mean episode length: 206.14
    Episode_Reward/reaching_object: 1.1876
     Episode_Reward/lifting_object: 133.2062
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 2.15s
                      Time elapsed: 00:30:01
                               ETA: 00:45:05

################################################################################
                     [1m Learning iteration 800/2000 [0m                      

                       Computation: 46538 steps/s (collection: 2.017s, learning 0.096s)
             Mean action noise std: 2.14
          Mean value_function loss: 343.7112
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 47.7312
                       Mean reward: 609.33
               Mean episode length: 183.80
    Episode_Reward/reaching_object: 1.1293
     Episode_Reward/lifting_object: 126.1993
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 2.11s
                      Time elapsed: 00:30:03
                               ETA: 00:45:02

################################################################################
                     [1m Learning iteration 801/2000 [0m                      

                       Computation: 44815 steps/s (collection: 2.095s, learning 0.098s)
             Mean action noise std: 2.14
          Mean value_function loss: 348.3059
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 47.7333
                       Mean reward: 651.26
               Mean episode length: 190.19
    Episode_Reward/reaching_object: 1.1347
     Episode_Reward/lifting_object: 128.0294
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 2.19s
                      Time elapsed: 00:30:06
                               ETA: 00:45:00

################################################################################
                     [1m Learning iteration 802/2000 [0m                      

                       Computation: 46223 steps/s (collection: 2.037s, learning 0.090s)
             Mean action noise std: 2.14
          Mean value_function loss: 325.1900
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 47.7343
                       Mean reward: 655.83
               Mean episode length: 195.39
    Episode_Reward/reaching_object: 1.1328
     Episode_Reward/lifting_object: 126.9916
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 2.13s
                      Time elapsed: 00:30:08
                               ETA: 00:44:57

################################################################################
                     [1m Learning iteration 803/2000 [0m                      

                       Computation: 43559 steps/s (collection: 2.144s, learning 0.113s)
             Mean action noise std: 2.14
          Mean value_function loss: 331.8957
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 47.7350
                       Mean reward: 651.80
               Mean episode length: 191.61
    Episode_Reward/reaching_object: 1.1233
     Episode_Reward/lifting_object: 126.1326
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 2.26s
                      Time elapsed: 00:30:10
                               ETA: 00:44:55

################################################################################
                     [1m Learning iteration 804/2000 [0m                      

                       Computation: 44659 steps/s (collection: 2.079s, learning 0.122s)
             Mean action noise std: 2.14
          Mean value_function loss: 327.8323
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 47.7351
                       Mean reward: 690.07
               Mean episode length: 205.39
    Episode_Reward/reaching_object: 1.1640
     Episode_Reward/lifting_object: 130.5574
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 2.20s
                      Time elapsed: 00:30:12
                               ETA: 00:44:53

################################################################################
                     [1m Learning iteration 805/2000 [0m                      

                       Computation: 44199 steps/s (collection: 2.113s, learning 0.112s)
             Mean action noise std: 2.14
          Mean value_function loss: 332.0843
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 47.7356
                       Mean reward: 718.64
               Mean episode length: 211.08
    Episode_Reward/reaching_object: 1.1659
     Episode_Reward/lifting_object: 131.6662
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 2.22s
                      Time elapsed: 00:30:14
                               ETA: 00:44:50

################################################################################
                     [1m Learning iteration 806/2000 [0m                      

                       Computation: 43881 steps/s (collection: 2.099s, learning 0.142s)
             Mean action noise std: 2.14
          Mean value_function loss: 321.8544
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 47.7358
                       Mean reward: 610.72
               Mean episode length: 183.88
    Episode_Reward/reaching_object: 1.1396
     Episode_Reward/lifting_object: 128.6345
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 2.24s
                      Time elapsed: 00:30:17
                               ETA: 00:44:48

################################################################################
                     [1m Learning iteration 807/2000 [0m                      

                       Computation: 41414 steps/s (collection: 2.284s, learning 0.090s)
             Mean action noise std: 2.14
          Mean value_function loss: 369.1825
               Mean surrogate loss: 0.0109
                 Mean entropy loss: 47.7358
                       Mean reward: 682.14
               Mean episode length: 197.38
    Episode_Reward/reaching_object: 1.1901
     Episode_Reward/lifting_object: 135.4734
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 2.37s
                      Time elapsed: 00:30:19
                               ETA: 00:44:46

################################################################################
                     [1m Learning iteration 808/2000 [0m                      

                       Computation: 41844 steps/s (collection: 2.206s, learning 0.144s)
             Mean action noise std: 2.14
          Mean value_function loss: 335.4330
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 47.7360
                       Mean reward: 674.36
               Mean episode length: 199.64
    Episode_Reward/reaching_object: 1.1303
     Episode_Reward/lifting_object: 127.6188
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 2.35s
                      Time elapsed: 00:30:21
                               ETA: 00:44:44

################################################################################
                     [1m Learning iteration 809/2000 [0m                      

                       Computation: 41793 steps/s (collection: 2.240s, learning 0.113s)
             Mean action noise std: 2.14
          Mean value_function loss: 351.9005
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 47.7365
                       Mean reward: 591.01
               Mean episode length: 180.22
    Episode_Reward/reaching_object: 1.1413
     Episode_Reward/lifting_object: 129.7814
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 2.35s
                      Time elapsed: 00:30:24
                               ETA: 00:44:42

################################################################################
                     [1m Learning iteration 810/2000 [0m                      

                       Computation: 44850 steps/s (collection: 2.079s, learning 0.113s)
             Mean action noise std: 2.14
          Mean value_function loss: 377.6644
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 47.7393
                       Mean reward: 548.97
               Mean episode length: 176.08
    Episode_Reward/reaching_object: 1.0774
     Episode_Reward/lifting_object: 120.5493
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 2.19s
                      Time elapsed: 00:30:26
                               ETA: 00:44:40

################################################################################
                     [1m Learning iteration 811/2000 [0m                      

                       Computation: 45164 steps/s (collection: 2.065s, learning 0.112s)
             Mean action noise std: 2.14
          Mean value_function loss: 366.9824
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 47.7435
                       Mean reward: 582.55
               Mean episode length: 181.07
    Episode_Reward/reaching_object: 1.0978
     Episode_Reward/lifting_object: 123.3159
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 2.18s
                      Time elapsed: 00:30:28
                               ETA: 00:44:37

################################################################################
                     [1m Learning iteration 812/2000 [0m                      

                       Computation: 45093 steps/s (collection: 2.071s, learning 0.109s)
             Mean action noise std: 2.14
          Mean value_function loss: 381.0292
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 47.7445
                       Mean reward: 555.64
               Mean episode length: 171.51
    Episode_Reward/reaching_object: 1.0494
     Episode_Reward/lifting_object: 117.7058
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 2.18s
                      Time elapsed: 00:30:30
                               ETA: 00:44:35

################################################################################
                     [1m Learning iteration 813/2000 [0m                      

                       Computation: 44331 steps/s (collection: 2.101s, learning 0.117s)
             Mean action noise std: 2.14
          Mean value_function loss: 345.5507
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 47.7447
                       Mean reward: 607.65
               Mean episode length: 186.32
    Episode_Reward/reaching_object: 1.0893
     Episode_Reward/lifting_object: 121.7335
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 2.22s
                      Time elapsed: 00:30:33
                               ETA: 00:44:33

################################################################################
                     [1m Learning iteration 814/2000 [0m                      

                       Computation: 45033 steps/s (collection: 2.089s, learning 0.094s)
             Mean action noise std: 2.14
          Mean value_function loss: 323.7977
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 47.7449
                       Mean reward: 660.47
               Mean episode length: 198.59
    Episode_Reward/reaching_object: 1.1484
     Episode_Reward/lifting_object: 129.4635
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 2.18s
                      Time elapsed: 00:30:35
                               ETA: 00:44:30

################################################################################
                     [1m Learning iteration 815/2000 [0m                      

                       Computation: 44144 steps/s (collection: 2.118s, learning 0.109s)
             Mean action noise std: 2.14
          Mean value_function loss: 308.5094
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 47.7455
                       Mean reward: 671.84
               Mean episode length: 196.52
    Episode_Reward/reaching_object: 1.1419
     Episode_Reward/lifting_object: 129.7000
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 2.23s
                      Time elapsed: 00:30:37
                               ETA: 00:44:28

################################################################################
                     [1m Learning iteration 816/2000 [0m                      

                       Computation: 45415 steps/s (collection: 2.051s, learning 0.113s)
             Mean action noise std: 2.14
          Mean value_function loss: 283.7855
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 47.7458
                       Mean reward: 647.43
               Mean episode length: 194.75
    Episode_Reward/reaching_object: 1.1485
     Episode_Reward/lifting_object: 129.3293
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 2.16s
                      Time elapsed: 00:30:39
                               ETA: 00:44:26

################################################################################
                     [1m Learning iteration 817/2000 [0m                      

                       Computation: 45838 steps/s (collection: 2.041s, learning 0.104s)
             Mean action noise std: 2.14
          Mean value_function loss: 283.9622
               Mean surrogate loss: 0.0102
                 Mean entropy loss: 47.7461
                       Mean reward: 616.10
               Mean episode length: 188.46
    Episode_Reward/reaching_object: 1.1711
     Episode_Reward/lifting_object: 133.5205
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 2.14s
                      Time elapsed: 00:30:41
                               ETA: 00:44:23

################################################################################
                     [1m Learning iteration 818/2000 [0m                      

                       Computation: 44469 steps/s (collection: 2.074s, learning 0.137s)
             Mean action noise std: 2.14
          Mean value_function loss: 281.9474
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 47.7462
                       Mean reward: 728.33
               Mean episode length: 208.73
    Episode_Reward/reaching_object: 1.2110
     Episode_Reward/lifting_object: 137.9474
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 2.21s
                      Time elapsed: 00:30:43
                               ETA: 00:44:21

################################################################################
                     [1m Learning iteration 819/2000 [0m                      

                       Computation: 44426 steps/s (collection: 2.096s, learning 0.117s)
             Mean action noise std: 2.14
          Mean value_function loss: 307.9216
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 47.7466
                       Mean reward: 753.64
               Mean episode length: 215.89
    Episode_Reward/reaching_object: 1.2274
     Episode_Reward/lifting_object: 139.7432
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 2.21s
                      Time elapsed: 00:30:46
                               ETA: 00:44:18

################################################################################
                     [1m Learning iteration 820/2000 [0m                      

                       Computation: 44401 steps/s (collection: 2.099s, learning 0.115s)
             Mean action noise std: 2.14
          Mean value_function loss: 339.0985
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 47.7470
                       Mean reward: 698.32
               Mean episode length: 203.87
    Episode_Reward/reaching_object: 1.1429
     Episode_Reward/lifting_object: 129.6882
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 2.21s
                      Time elapsed: 00:30:48
                               ETA: 00:44:16

################################################################################
                     [1m Learning iteration 821/2000 [0m                      

                       Computation: 43977 steps/s (collection: 2.136s, learning 0.100s)
             Mean action noise std: 2.14
          Mean value_function loss: 313.7655
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 47.7472
                       Mean reward: 615.90
               Mean episode length: 183.73
    Episode_Reward/reaching_object: 1.1677
     Episode_Reward/lifting_object: 132.9940
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 2.24s
                      Time elapsed: 00:30:50
                               ETA: 00:44:14

################################################################################
                     [1m Learning iteration 822/2000 [0m                      

                       Computation: 45207 steps/s (collection: 2.037s, learning 0.137s)
             Mean action noise std: 2.14
          Mean value_function loss: 302.0800
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 47.7476
                       Mean reward: 689.15
               Mean episode length: 202.06
    Episode_Reward/reaching_object: 1.2157
     Episode_Reward/lifting_object: 138.7824
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 2.17s
                      Time elapsed: 00:30:52
                               ETA: 00:44:12

################################################################################
                     [1m Learning iteration 823/2000 [0m                      

                       Computation: 43303 steps/s (collection: 2.129s, learning 0.142s)
             Mean action noise std: 2.14
          Mean value_function loss: 294.3549
               Mean surrogate loss: 0.0091
                 Mean entropy loss: 47.7478
                       Mean reward: 655.24
               Mean episode length: 193.01
    Episode_Reward/reaching_object: 1.2148
     Episode_Reward/lifting_object: 138.6752
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 2.27s
                      Time elapsed: 00:30:55
                               ETA: 00:44:09

################################################################################
                     [1m Learning iteration 824/2000 [0m                      

                       Computation: 45276 steps/s (collection: 2.084s, learning 0.087s)
             Mean action noise std: 2.14
          Mean value_function loss: 317.4773
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 47.7483
                       Mean reward: 625.22
               Mean episode length: 186.96
    Episode_Reward/reaching_object: 1.1633
     Episode_Reward/lifting_object: 132.4727
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 2.17s
                      Time elapsed: 00:30:57
                               ETA: 00:44:07

################################################################################
                     [1m Learning iteration 825/2000 [0m                      

                       Computation: 46200 steps/s (collection: 2.037s, learning 0.091s)
             Mean action noise std: 2.14
          Mean value_function loss: 288.9712
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 47.7499
                       Mean reward: 697.25
               Mean episode length: 203.23
    Episode_Reward/reaching_object: 1.1394
     Episode_Reward/lifting_object: 128.9491
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 2.13s
                      Time elapsed: 00:30:59
                               ETA: 00:44:05

################################################################################
                     [1m Learning iteration 826/2000 [0m                      

                       Computation: 44788 steps/s (collection: 2.107s, learning 0.088s)
             Mean action noise std: 2.14
          Mean value_function loss: 312.5804
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 47.7526
                       Mean reward: 710.06
               Mean episode length: 205.99
    Episode_Reward/reaching_object: 1.1631
     Episode_Reward/lifting_object: 132.2730
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 2.19s
                      Time elapsed: 00:31:01
                               ETA: 00:44:02

################################################################################
                     [1m Learning iteration 827/2000 [0m                      

                       Computation: 46756 steps/s (collection: 2.015s, learning 0.087s)
             Mean action noise std: 2.14
          Mean value_function loss: 31473.4249
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 47.7578
                       Mean reward: 690.38
               Mean episode length: 200.10
    Episode_Reward/reaching_object: 1.1517
     Episode_Reward/lifting_object: 131.2338
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -11.7726
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 2.10s
                      Time elapsed: 00:31:03
                               ETA: 00:44:00

################################################################################
                     [1m Learning iteration 828/2000 [0m                      

                       Computation: 45882 steps/s (collection: 2.055s, learning 0.088s)
             Mean action noise std: 2.14
          Mean value_function loss: 329.8935
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 47.7602
                       Mean reward: 711.07
               Mean episode length: 209.76
    Episode_Reward/reaching_object: 1.1671
     Episode_Reward/lifting_object: 132.7609
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 2.14s
                      Time elapsed: 00:31:05
                               ETA: 00:43:57

################################################################################
                     [1m Learning iteration 829/2000 [0m                      

                       Computation: 42000 steps/s (collection: 2.166s, learning 0.175s)
             Mean action noise std: 2.14
          Mean value_function loss: 284.7874
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 47.7665
                       Mean reward: 753.61
               Mean episode length: 215.16
    Episode_Reward/reaching_object: 1.2746
     Episode_Reward/lifting_object: 146.0421
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 2.34s
                      Time elapsed: 00:31:08
                               ETA: 00:43:55

################################################################################
                     [1m Learning iteration 830/2000 [0m                      

                       Computation: 44420 steps/s (collection: 2.120s, learning 0.093s)
             Mean action noise std: 2.14
          Mean value_function loss: 278.0558
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 47.7678
                       Mean reward: 740.13
               Mean episode length: 213.65
    Episode_Reward/reaching_object: 1.2488
     Episode_Reward/lifting_object: 143.9799
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 2.21s
                      Time elapsed: 00:31:10
                               ETA: 00:43:53

################################################################################
                     [1m Learning iteration 831/2000 [0m                      

                       Computation: 43563 steps/s (collection: 2.157s, learning 0.100s)
             Mean action noise std: 2.14
          Mean value_function loss: 517688156160.0000
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 47.7680
                       Mean reward: -854158.40
               Mean episode length: 198.14
    Episode_Reward/reaching_object: 1.2329
     Episode_Reward/lifting_object: 142.1171
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -27.4114
          Episode_Reward/joint_vel: -32353.0156
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 2.26s
                      Time elapsed: 00:31:12
                               ETA: 00:43:51

################################################################################
                     [1m Learning iteration 832/2000 [0m                      

                       Computation: 45883 steps/s (collection: 2.055s, learning 0.087s)
             Mean action noise std: 2.14
          Mean value_function loss: 285.3465
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 47.7684
                       Mean reward: 718.56
               Mean episode length: 206.51
    Episode_Reward/reaching_object: 1.2214
     Episode_Reward/lifting_object: 140.2449
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 2.14s
                      Time elapsed: 00:31:14
                               ETA: 00:43:48

################################################################################
                     [1m Learning iteration 833/2000 [0m                      

                       Computation: 45600 steps/s (collection: 2.028s, learning 0.128s)
             Mean action noise std: 2.14
          Mean value_function loss: 271.0969
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 47.7698
                       Mean reward: 741.55
               Mean episode length: 210.22
    Episode_Reward/reaching_object: 1.2657
     Episode_Reward/lifting_object: 146.0878
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 2.16s
                      Time elapsed: 00:31:16
                               ETA: 00:43:46

################################################################################
                     [1m Learning iteration 834/2000 [0m                      

                       Computation: 44051 steps/s (collection: 2.091s, learning 0.140s)
             Mean action noise std: 2.14
          Mean value_function loss: 311.2877
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 47.7710
                       Mean reward: 681.41
               Mean episode length: 198.14
    Episode_Reward/reaching_object: 1.2021
     Episode_Reward/lifting_object: 138.0179
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 2.23s
                      Time elapsed: 00:31:19
                               ETA: 00:43:44

################################################################################
                     [1m Learning iteration 835/2000 [0m                      

                       Computation: 44660 steps/s (collection: 2.102s, learning 0.099s)
             Mean action noise std: 2.14
          Mean value_function loss: 311.0612
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 47.7731
                       Mean reward: 637.77
               Mean episode length: 191.76
    Episode_Reward/reaching_object: 1.1930
     Episode_Reward/lifting_object: 135.9770
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 2.20s
                      Time elapsed: 00:31:21
                               ETA: 00:43:41

################################################################################
                     [1m Learning iteration 836/2000 [0m                      

                       Computation: 44130 steps/s (collection: 2.126s, learning 0.102s)
             Mean action noise std: 2.14
          Mean value_function loss: 301.7842
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 47.7739
                       Mean reward: 673.60
               Mean episode length: 194.70
    Episode_Reward/reaching_object: 1.1857
     Episode_Reward/lifting_object: 135.6679
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 2.23s
                      Time elapsed: 00:31:23
                               ETA: 00:43:39

################################################################################
                     [1m Learning iteration 837/2000 [0m                      

                       Computation: 46487 steps/s (collection: 2.017s, learning 0.098s)
             Mean action noise std: 2.14
          Mean value_function loss: 310.0753
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 47.7743
                       Mean reward: 642.80
               Mean episode length: 188.76
    Episode_Reward/reaching_object: 1.1750
     Episode_Reward/lifting_object: 134.6597
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 2.11s
                      Time elapsed: 00:31:25
                               ETA: 00:43:37

################################################################################
                     [1m Learning iteration 838/2000 [0m                      

                       Computation: 44515 steps/s (collection: 2.117s, learning 0.091s)
             Mean action noise std: 2.14
          Mean value_function loss: 318.9531
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 47.7731
                       Mean reward: 686.35
               Mean episode length: 197.03
    Episode_Reward/reaching_object: 1.1498
     Episode_Reward/lifting_object: 131.5004
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 2.21s
                      Time elapsed: 00:31:27
                               ETA: 00:43:34

################################################################################
                     [1m Learning iteration 839/2000 [0m                      

                       Computation: 45130 steps/s (collection: 2.034s, learning 0.144s)
             Mean action noise std: 2.14
          Mean value_function loss: 361.0924
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 47.7728
                       Mean reward: 680.70
               Mean episode length: 197.27
    Episode_Reward/reaching_object: 1.1619
     Episode_Reward/lifting_object: 132.9677
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 2.18s
                      Time elapsed: 00:31:30
                               ETA: 00:43:32

################################################################################
                     [1m Learning iteration 840/2000 [0m                      

                       Computation: 43676 steps/s (collection: 2.132s, learning 0.118s)
             Mean action noise std: 2.14
          Mean value_function loss: 318.1858
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 47.7756
                       Mean reward: 710.51
               Mean episode length: 203.50
    Episode_Reward/reaching_object: 1.1833
     Episode_Reward/lifting_object: 135.4912
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 2.25s
                      Time elapsed: 00:31:32
                               ETA: 00:43:30

################################################################################
                     [1m Learning iteration 841/2000 [0m                      

                       Computation: 46896 steps/s (collection: 2.005s, learning 0.091s)
             Mean action noise std: 2.14
          Mean value_function loss: 325.2676
               Mean surrogate loss: 0.0163
                 Mean entropy loss: 47.7768
                       Mean reward: 625.69
               Mean episode length: 188.69
    Episode_Reward/reaching_object: 1.1737
     Episode_Reward/lifting_object: 134.5755
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 2.10s
                      Time elapsed: 00:31:34
                               ETA: 00:43:27

################################################################################
                     [1m Learning iteration 842/2000 [0m                      

                       Computation: 45382 steps/s (collection: 2.066s, learning 0.101s)
             Mean action noise std: 2.14
          Mean value_function loss: 296.3048
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 47.7770
                       Mean reward: 642.05
               Mean episode length: 186.39
    Episode_Reward/reaching_object: 1.1935
     Episode_Reward/lifting_object: 137.3887
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 2.17s
                      Time elapsed: 00:31:36
                               ETA: 00:43:25

################################################################################
                     [1m Learning iteration 843/2000 [0m                      

                       Computation: 44733 steps/s (collection: 2.105s, learning 0.092s)
             Mean action noise std: 2.14
          Mean value_function loss: 299.8516
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 47.7774
                       Mean reward: 659.34
               Mean episode length: 192.37
    Episode_Reward/reaching_object: 1.1900
     Episode_Reward/lifting_object: 136.7496
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 2.20s
                      Time elapsed: 00:31:38
                               ETA: 00:43:23

################################################################################
                     [1m Learning iteration 844/2000 [0m                      

                       Computation: 46443 steps/s (collection: 2.027s, learning 0.089s)
             Mean action noise std: 2.14
          Mean value_function loss: 267.6459
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 47.7781
                       Mean reward: 708.12
               Mean episode length: 206.34
    Episode_Reward/reaching_object: 1.2177
     Episode_Reward/lifting_object: 139.5081
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 2.12s
                      Time elapsed: 00:31:40
                               ETA: 00:43:20

################################################################################
                     [1m Learning iteration 845/2000 [0m                      

                       Computation: 46169 steps/s (collection: 2.030s, learning 0.099s)
             Mean action noise std: 2.14
          Mean value_function loss: 279.9869
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 47.7802
                       Mean reward: 751.81
               Mean episode length: 214.28
    Episode_Reward/reaching_object: 1.2157
     Episode_Reward/lifting_object: 139.7139
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 2.13s
                      Time elapsed: 00:31:43
                               ETA: 00:43:18

################################################################################
                     [1m Learning iteration 846/2000 [0m                      

                       Computation: 46222 steps/s (collection: 2.011s, learning 0.116s)
             Mean action noise std: 2.14
          Mean value_function loss: 281.6746
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 47.7820
                       Mean reward: 693.48
               Mean episode length: 203.21
    Episode_Reward/reaching_object: 1.2429
     Episode_Reward/lifting_object: 142.0532
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 2.13s
                      Time elapsed: 00:31:45
                               ETA: 00:43:15

################################################################################
                     [1m Learning iteration 847/2000 [0m                      

                       Computation: 45828 steps/s (collection: 2.021s, learning 0.124s)
             Mean action noise std: 2.14
          Mean value_function loss: 274.4521
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 47.7827
                       Mean reward: 662.72
               Mean episode length: 194.86
    Episode_Reward/reaching_object: 1.1917
     Episode_Reward/lifting_object: 136.0928
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 2.15s
                      Time elapsed: 00:31:47
                               ETA: 00:43:13

################################################################################
                     [1m Learning iteration 848/2000 [0m                      

                       Computation: 45829 steps/s (collection: 2.046s, learning 0.099s)
             Mean action noise std: 2.14
          Mean value_function loss: 265.9283
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 47.7829
                       Mean reward: 700.61
               Mean episode length: 202.77
    Episode_Reward/reaching_object: 1.2603
     Episode_Reward/lifting_object: 144.9837
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 2.15s
                      Time elapsed: 00:31:49
                               ETA: 00:43:10

################################################################################
                     [1m Learning iteration 849/2000 [0m                      

                       Computation: 45445 steps/s (collection: 2.027s, learning 0.136s)
             Mean action noise std: 2.14
          Mean value_function loss: 284.6018
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 47.7831
                       Mean reward: 759.61
               Mean episode length: 215.88
    Episode_Reward/reaching_object: 1.3134
     Episode_Reward/lifting_object: 152.7596
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 2.16s
                      Time elapsed: 00:31:51
                               ETA: 00:43:08

################################################################################
                     [1m Learning iteration 850/2000 [0m                      

                       Computation: 45039 steps/s (collection: 2.091s, learning 0.092s)
             Mean action noise std: 2.14
          Mean value_function loss: 302.5838
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 47.7837
                       Mean reward: 752.68
               Mean episode length: 214.35
    Episode_Reward/reaching_object: 1.2383
     Episode_Reward/lifting_object: 142.8662
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 2.18s
                      Time elapsed: 00:31:53
                               ETA: 00:43:06

################################################################################
                     [1m Learning iteration 851/2000 [0m                      

                       Computation: 44836 steps/s (collection: 2.094s, learning 0.099s)
             Mean action noise std: 2.14
          Mean value_function loss: 280.5683
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 47.7850
                       Mean reward: 754.17
               Mean episode length: 214.98
    Episode_Reward/reaching_object: 1.3304
     Episode_Reward/lifting_object: 155.3951
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 2.19s
                      Time elapsed: 00:31:56
                               ETA: 00:43:03

################################################################################
                     [1m Learning iteration 852/2000 [0m                      

                       Computation: 44174 steps/s (collection: 2.063s, learning 0.162s)
             Mean action noise std: 2.14
          Mean value_function loss: 269.7674
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 47.7875
                       Mean reward: 747.55
               Mean episode length: 213.10
    Episode_Reward/reaching_object: 1.2439
     Episode_Reward/lifting_object: 143.5377
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 2.23s
                      Time elapsed: 00:31:58
                               ETA: 00:43:01

################################################################################
                     [1m Learning iteration 853/2000 [0m                      

                       Computation: 43669 steps/s (collection: 2.120s, learning 0.131s)
             Mean action noise std: 2.14
          Mean value_function loss: 279.1390
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 47.7915
                       Mean reward: 683.68
               Mean episode length: 198.11
    Episode_Reward/reaching_object: 1.2085
     Episode_Reward/lifting_object: 139.0368
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 2.25s
                      Time elapsed: 00:32:00
                               ETA: 00:42:59

################################################################################
                     [1m Learning iteration 854/2000 [0m                      

                       Computation: 44282 steps/s (collection: 2.093s, learning 0.127s)
             Mean action noise std: 2.15
          Mean value_function loss: 266.2599
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 47.7951
                       Mean reward: 736.91
               Mean episode length: 212.02
    Episode_Reward/reaching_object: 1.2485
     Episode_Reward/lifting_object: 144.6491
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 2.22s
                      Time elapsed: 00:32:02
                               ETA: 00:42:57

################################################################################
                     [1m Learning iteration 855/2000 [0m                      

                       Computation: 45787 steps/s (collection: 2.036s, learning 0.111s)
             Mean action noise std: 2.15
          Mean value_function loss: 247.2174
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 47.7994
                       Mean reward: 772.04
               Mean episode length: 217.43
    Episode_Reward/reaching_object: 1.2667
     Episode_Reward/lifting_object: 147.8233
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 2.15s
                      Time elapsed: 00:32:04
                               ETA: 00:42:54

################################################################################
                     [1m Learning iteration 856/2000 [0m                      

                       Computation: 45916 steps/s (collection: 2.049s, learning 0.092s)
             Mean action noise std: 2.15
          Mean value_function loss: 259.1338
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 47.8021
                       Mean reward: 715.26
               Mean episode length: 203.71
    Episode_Reward/reaching_object: 1.2568
     Episode_Reward/lifting_object: 145.8769
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 2.14s
                      Time elapsed: 00:32:07
                               ETA: 00:42:52

################################################################################
                     [1m Learning iteration 857/2000 [0m                      

                       Computation: 44684 steps/s (collection: 2.085s, learning 0.115s)
             Mean action noise std: 2.15
          Mean value_function loss: 263.4979
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 47.8032
                       Mean reward: 701.81
               Mean episode length: 201.40
    Episode_Reward/reaching_object: 1.2848
     Episode_Reward/lifting_object: 150.0720
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0469
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 2.20s
                      Time elapsed: 00:32:09
                               ETA: 00:42:50

################################################################################
                     [1m Learning iteration 858/2000 [0m                      

                       Computation: 44146 steps/s (collection: 2.066s, learning 0.161s)
             Mean action noise std: 2.15
          Mean value_function loss: 297.4339
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 47.8039
                       Mean reward: 756.70
               Mean episode length: 216.01
    Episode_Reward/reaching_object: 1.2840
     Episode_Reward/lifting_object: 149.7621
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 2.23s
                      Time elapsed: 00:32:11
                               ETA: 00:42:47

################################################################################
                     [1m Learning iteration 859/2000 [0m                      

                       Computation: 43829 steps/s (collection: 2.154s, learning 0.089s)
             Mean action noise std: 2.15
          Mean value_function loss: 297.8745
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 47.8053
                       Mean reward: 778.07
               Mean episode length: 218.35
    Episode_Reward/reaching_object: 1.2599
     Episode_Reward/lifting_object: 146.4147
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 2.24s
                      Time elapsed: 00:32:13
                               ETA: 00:42:45

################################################################################
                     [1m Learning iteration 860/2000 [0m                      

                       Computation: 46244 steps/s (collection: 2.033s, learning 0.093s)
             Mean action noise std: 2.15
          Mean value_function loss: 310.0378
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 47.8083
                       Mean reward: 779.41
               Mean episode length: 218.22
    Episode_Reward/reaching_object: 1.2706
     Episode_Reward/lifting_object: 148.8372
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 2.13s
                      Time elapsed: 00:32:15
                               ETA: 00:42:43

################################################################################
                     [1m Learning iteration 861/2000 [0m                      

                       Computation: 44879 steps/s (collection: 2.100s, learning 0.091s)
             Mean action noise std: 2.15
          Mean value_function loss: 318.1764
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 47.8093
                       Mean reward: 746.80
               Mean episode length: 212.54
    Episode_Reward/reaching_object: 1.2600
     Episode_Reward/lifting_object: 147.5443
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 2.19s
                      Time elapsed: 00:32:17
                               ETA: 00:42:40

################################################################################
                     [1m Learning iteration 862/2000 [0m                      

                       Computation: 46335 steps/s (collection: 2.021s, learning 0.100s)
             Mean action noise std: 2.15
          Mean value_function loss: 363.5361
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.8121
                       Mean reward: 712.67
               Mean episode length: 204.23
    Episode_Reward/reaching_object: 1.2126
     Episode_Reward/lifting_object: 141.5576
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 2.12s
                      Time elapsed: 00:32:20
                               ETA: 00:42:38

################################################################################
                     [1m Learning iteration 863/2000 [0m                      

                       Computation: 45853 steps/s (collection: 2.012s, learning 0.132s)
             Mean action noise std: 2.15
          Mean value_function loss: 374.5090
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 47.8189
                       Mean reward: 675.48
               Mean episode length: 194.54
    Episode_Reward/reaching_object: 1.1811
     Episode_Reward/lifting_object: 137.9222
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 2.14s
                      Time elapsed: 00:32:22
                               ETA: 00:42:35

################################################################################
                     [1m Learning iteration 864/2000 [0m                      

                       Computation: 45871 steps/s (collection: 2.020s, learning 0.123s)
             Mean action noise std: 2.15
          Mean value_function loss: 362.6576
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 47.8222
                       Mean reward: 674.09
               Mean episode length: 193.60
    Episode_Reward/reaching_object: 1.1397
     Episode_Reward/lifting_object: 131.2422
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 2.14s
                      Time elapsed: 00:32:24
                               ETA: 00:42:33

################################################################################
                     [1m Learning iteration 865/2000 [0m                      

                       Computation: 45657 steps/s (collection: 2.038s, learning 0.115s)
             Mean action noise std: 2.15
          Mean value_function loss: 343.1891
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 47.8238
                       Mean reward: 657.51
               Mean episode length: 191.83
    Episode_Reward/reaching_object: 1.1430
     Episode_Reward/lifting_object: 132.5196
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 2.15s
                      Time elapsed: 00:32:26
                               ETA: 00:42:31

################################################################################
                     [1m Learning iteration 866/2000 [0m                      

                       Computation: 46610 steps/s (collection: 2.012s, learning 0.097s)
             Mean action noise std: 2.15
          Mean value_function loss: 360.2239
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 47.8280
                       Mean reward: 603.99
               Mean episode length: 179.11
    Episode_Reward/reaching_object: 1.1216
     Episode_Reward/lifting_object: 129.0687
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 2.11s
                      Time elapsed: 00:32:28
                               ETA: 00:42:28

################################################################################
                     [1m Learning iteration 867/2000 [0m                      

                       Computation: 46755 steps/s (collection: 1.997s, learning 0.105s)
             Mean action noise std: 2.15
          Mean value_function loss: 324.8493
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 47.8349
                       Mean reward: 748.70
               Mean episode length: 211.25
    Episode_Reward/reaching_object: 1.2041
     Episode_Reward/lifting_object: 139.7283
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 2.10s
                      Time elapsed: 00:32:30
                               ETA: 00:42:26

################################################################################
                     [1m Learning iteration 868/2000 [0m                      

                       Computation: 44088 steps/s (collection: 2.118s, learning 0.112s)
             Mean action noise std: 2.15
          Mean value_function loss: 304.8640
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 47.8411
                       Mean reward: 740.74
               Mean episode length: 208.43
    Episode_Reward/reaching_object: 1.2341
     Episode_Reward/lifting_object: 144.1997
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 2.23s
                      Time elapsed: 00:32:32
                               ETA: 00:42:24

################################################################################
                     [1m Learning iteration 869/2000 [0m                      

                       Computation: 43756 steps/s (collection: 2.098s, learning 0.149s)
             Mean action noise std: 2.15
          Mean value_function loss: 339.3668
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.8465
                       Mean reward: 758.27
               Mean episode length: 213.14
    Episode_Reward/reaching_object: 1.2202
     Episode_Reward/lifting_object: 141.6472
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 2.25s
                      Time elapsed: 00:32:35
                               ETA: 00:42:21

################################################################################
                     [1m Learning iteration 870/2000 [0m                      

                       Computation: 44395 steps/s (collection: 2.066s, learning 0.149s)
             Mean action noise std: 2.15
          Mean value_function loss: 354.6576
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 47.8524
                       Mean reward: 654.46
               Mean episode length: 190.67
    Episode_Reward/reaching_object: 1.1829
     Episode_Reward/lifting_object: 137.0713
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 2.21s
                      Time elapsed: 00:32:37
                               ETA: 00:42:19

################################################################################
                     [1m Learning iteration 871/2000 [0m                      

                       Computation: 44752 steps/s (collection: 2.083s, learning 0.114s)
             Mean action noise std: 2.15
          Mean value_function loss: 308.3965
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 47.8612
                       Mean reward: 694.31
               Mean episode length: 199.58
    Episode_Reward/reaching_object: 1.2647
     Episode_Reward/lifting_object: 148.0773
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 2.20s
                      Time elapsed: 00:32:39
                               ETA: 00:42:17

################################################################################
                     [1m Learning iteration 872/2000 [0m                      

                       Computation: 44365 steps/s (collection: 2.021s, learning 0.194s)
             Mean action noise std: 2.15
          Mean value_function loss: 317.7095
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 47.8643
                       Mean reward: 737.01
               Mean episode length: 208.45
    Episode_Reward/reaching_object: 1.2329
     Episode_Reward/lifting_object: 143.8054
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 2.22s
                      Time elapsed: 00:32:41
                               ETA: 00:42:14

################################################################################
                     [1m Learning iteration 873/2000 [0m                      

                       Computation: 44934 steps/s (collection: 2.093s, learning 0.095s)
             Mean action noise std: 2.15
          Mean value_function loss: 277.7745
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 47.8665
                       Mean reward: 700.15
               Mean episode length: 201.41
    Episode_Reward/reaching_object: 1.2429
     Episode_Reward/lifting_object: 144.5311
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 2.19s
                      Time elapsed: 00:32:44
                               ETA: 00:42:12

################################################################################
                     [1m Learning iteration 874/2000 [0m                      

                       Computation: 45575 steps/s (collection: 2.039s, learning 0.118s)
             Mean action noise std: 2.15
          Mean value_function loss: 258.4919
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 47.8677
                       Mean reward: 724.53
               Mean episode length: 205.82
    Episode_Reward/reaching_object: 1.2787
     Episode_Reward/lifting_object: 149.7159
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 2.16s
                      Time elapsed: 00:32:46
                               ETA: 00:42:10

################################################################################
                     [1m Learning iteration 875/2000 [0m                      

                       Computation: 44764 steps/s (collection: 2.051s, learning 0.145s)
             Mean action noise std: 2.15
          Mean value_function loss: 264.0788
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 47.8683
                       Mean reward: 763.80
               Mean episode length: 215.79
    Episode_Reward/reaching_object: 1.2518
     Episode_Reward/lifting_object: 146.0561
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0469
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 2.20s
                      Time elapsed: 00:32:48
                               ETA: 00:42:07

################################################################################
                     [1m Learning iteration 876/2000 [0m                      

                       Computation: 45385 steps/s (collection: 2.056s, learning 0.110s)
             Mean action noise std: 2.15
          Mean value_function loss: 284.1995
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 47.8694
                       Mean reward: 775.53
               Mean episode length: 217.37
    Episode_Reward/reaching_object: 1.2772
     Episode_Reward/lifting_object: 149.0606
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 2.17s
                      Time elapsed: 00:32:50
                               ETA: 00:42:05

################################################################################
                     [1m Learning iteration 877/2000 [0m                      

                       Computation: 45774 steps/s (collection: 2.060s, learning 0.088s)
             Mean action noise std: 2.15
          Mean value_function loss: 240.3498
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 47.8725
                       Mean reward: 727.92
               Mean episode length: 207.93
    Episode_Reward/reaching_object: 1.2418
     Episode_Reward/lifting_object: 144.4301
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 2.15s
                      Time elapsed: 00:32:52
                               ETA: 00:42:03

################################################################################
                     [1m Learning iteration 878/2000 [0m                      

                       Computation: 45810 steps/s (collection: 2.044s, learning 0.102s)
             Mean action noise std: 2.15
          Mean value_function loss: 240.7741
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 47.8770
                       Mean reward: 756.35
               Mean episode length: 213.28
    Episode_Reward/reaching_object: 1.2922
     Episode_Reward/lifting_object: 151.1156
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 2.15s
                      Time elapsed: 00:32:54
                               ETA: 00:42:00

################################################################################
                     [1m Learning iteration 879/2000 [0m                      

                       Computation: 45936 steps/s (collection: 2.041s, learning 0.099s)
             Mean action noise std: 2.16
          Mean value_function loss: 213.4233
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 47.8791
                       Mean reward: 815.46
               Mean episode length: 228.20
    Episode_Reward/reaching_object: 1.3337
     Episode_Reward/lifting_object: 155.3731
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 2.14s
                      Time elapsed: 00:32:57
                               ETA: 00:41:58

################################################################################
                     [1m Learning iteration 880/2000 [0m                      

                       Computation: 45461 steps/s (collection: 2.065s, learning 0.098s)
             Mean action noise std: 2.16
          Mean value_function loss: 237.3779
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 47.8830
                       Mean reward: 753.25
               Mean episode length: 212.43
    Episode_Reward/reaching_object: 1.3093
     Episode_Reward/lifting_object: 152.8096
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 2.16s
                      Time elapsed: 00:32:59
                               ETA: 00:41:56

################################################################################
                     [1m Learning iteration 881/2000 [0m                      

                       Computation: 44919 steps/s (collection: 2.067s, learning 0.122s)
             Mean action noise std: 2.16
          Mean value_function loss: 227.9538
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 47.8849
                       Mean reward: 797.18
               Mean episode length: 223.20
    Episode_Reward/reaching_object: 1.3091
     Episode_Reward/lifting_object: 153.4440
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 2.19s
                      Time elapsed: 00:33:01
                               ETA: 00:41:53

################################################################################
                     [1m Learning iteration 882/2000 [0m                      

                       Computation: 45788 steps/s (collection: 2.021s, learning 0.126s)
             Mean action noise std: 2.16
          Mean value_function loss: 211.2337
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 47.8871
                       Mean reward: 834.64
               Mean episode length: 231.39
    Episode_Reward/reaching_object: 1.3356
     Episode_Reward/lifting_object: 156.4885
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 2.15s
                      Time elapsed: 00:33:03
                               ETA: 00:41:51

################################################################################
                     [1m Learning iteration 883/2000 [0m                      

                       Computation: 44528 steps/s (collection: 2.108s, learning 0.100s)
             Mean action noise std: 2.16
          Mean value_function loss: 239.0886
               Mean surrogate loss: 0.0121
                 Mean entropy loss: 47.8882
                       Mean reward: 771.10
               Mean episode length: 215.68
    Episode_Reward/reaching_object: 1.3329
     Episode_Reward/lifting_object: 156.2229
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 2.21s
                      Time elapsed: 00:33:05
                               ETA: 00:41:49

################################################################################
                     [1m Learning iteration 884/2000 [0m                      

                       Computation: 43471 steps/s (collection: 2.162s, learning 0.100s)
             Mean action noise std: 2.16
          Mean value_function loss: 234.3219
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 47.8886
                       Mean reward: 814.29
               Mean episode length: 224.55
    Episode_Reward/reaching_object: 1.3128
     Episode_Reward/lifting_object: 153.6405
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 2.26s
                      Time elapsed: 00:33:07
                               ETA: 00:41:46

################################################################################
                     [1m Learning iteration 885/2000 [0m                      

                       Computation: 45777 steps/s (collection: 2.052s, learning 0.095s)
             Mean action noise std: 2.16
          Mean value_function loss: 248.7000
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 47.8889
                       Mean reward: 735.96
               Mean episode length: 208.74
    Episode_Reward/reaching_object: 1.3037
     Episode_Reward/lifting_object: 152.5578
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 2.15s
                      Time elapsed: 00:33:10
                               ETA: 00:41:44

################################################################################
                     [1m Learning iteration 886/2000 [0m                      

                       Computation: 42790 steps/s (collection: 2.119s, learning 0.178s)
             Mean action noise std: 2.16
          Mean value_function loss: 274.6739
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 47.8901
                       Mean reward: 734.18
               Mean episode length: 210.90
    Episode_Reward/reaching_object: 1.2762
     Episode_Reward/lifting_object: 148.6680
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 2.30s
                      Time elapsed: 00:33:12
                               ETA: 00:41:42

################################################################################
                     [1m Learning iteration 887/2000 [0m                      

                       Computation: 44571 steps/s (collection: 2.108s, learning 0.097s)
             Mean action noise std: 2.16
          Mean value_function loss: 302.3796
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 47.8927
                       Mean reward: 733.23
               Mean episode length: 207.96
    Episode_Reward/reaching_object: 1.2863
     Episode_Reward/lifting_object: 150.7545
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 2.21s
                      Time elapsed: 00:33:14
                               ETA: 00:41:40

################################################################################
                     [1m Learning iteration 888/2000 [0m                      

                       Computation: 43670 steps/s (collection: 2.146s, learning 0.105s)
             Mean action noise std: 2.16
          Mean value_function loss: 343.7402
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 47.8946
                       Mean reward: 721.08
               Mean episode length: 202.01
    Episode_Reward/reaching_object: 1.1675
     Episode_Reward/lifting_object: 136.3679
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 2.25s
                      Time elapsed: 00:33:16
                               ETA: 00:41:37

################################################################################
                     [1m Learning iteration 889/2000 [0m                      

                       Computation: 44801 steps/s (collection: 2.102s, learning 0.092s)
             Mean action noise std: 2.16
          Mean value_function loss: 368.5059
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 47.8958
                       Mean reward: 719.86
               Mean episode length: 204.38
    Episode_Reward/reaching_object: 1.1722
     Episode_Reward/lifting_object: 136.0686
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 2.19s
                      Time elapsed: 00:33:19
                               ETA: 00:41:35

################################################################################
                     [1m Learning iteration 890/2000 [0m                      

                       Computation: 44992 steps/s (collection: 2.093s, learning 0.092s)
             Mean action noise std: 2.16
          Mean value_function loss: 322.8654
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 47.8963
                       Mean reward: 700.73
               Mean episode length: 199.92
    Episode_Reward/reaching_object: 1.2324
     Episode_Reward/lifting_object: 144.7795
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 2.18s
                      Time elapsed: 00:33:21
                               ETA: 00:41:33

################################################################################
                     [1m Learning iteration 891/2000 [0m                      

                       Computation: 44477 steps/s (collection: 2.064s, learning 0.146s)
             Mean action noise std: 2.16
          Mean value_function loss: 294.7760
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 47.8966
                       Mean reward: 699.81
               Mean episode length: 203.64
    Episode_Reward/reaching_object: 1.2304
     Episode_Reward/lifting_object: 143.2550
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 2.21s
                      Time elapsed: 00:33:23
                               ETA: 00:41:30

################################################################################
                     [1m Learning iteration 892/2000 [0m                      

                       Computation: 44661 steps/s (collection: 2.064s, learning 0.137s)
             Mean action noise std: 2.16
          Mean value_function loss: 246.3531
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 47.8971
                       Mean reward: 774.69
               Mean episode length: 215.93
    Episode_Reward/reaching_object: 1.2518
     Episode_Reward/lifting_object: 146.5862
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 2.20s
                      Time elapsed: 00:33:25
                               ETA: 00:41:28

################################################################################
                     [1m Learning iteration 893/2000 [0m                      

                       Computation: 42542 steps/s (collection: 2.215s, learning 0.096s)
             Mean action noise std: 2.16
          Mean value_function loss: 218.4512
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 47.8982
                       Mean reward: 781.01
               Mean episode length: 220.27
    Episode_Reward/reaching_object: 1.3078
     Episode_Reward/lifting_object: 153.6718
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 2.31s
                      Time elapsed: 00:33:27
                               ETA: 00:41:26

################################################################################
                     [1m Learning iteration 894/2000 [0m                      

                       Computation: 43453 steps/s (collection: 2.151s, learning 0.112s)
             Mean action noise std: 2.16
          Mean value_function loss: 232.0888
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 47.8990
                       Mean reward: 789.83
               Mean episode length: 220.63
    Episode_Reward/reaching_object: 1.2921
     Episode_Reward/lifting_object: 151.9280
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 2.26s
                      Time elapsed: 00:33:30
                               ETA: 00:41:24

################################################################################
                     [1m Learning iteration 895/2000 [0m                      

                       Computation: 42340 steps/s (collection: 2.169s, learning 0.153s)
             Mean action noise std: 2.16
          Mean value_function loss: 235.0152
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 47.9003
                       Mean reward: 742.45
               Mean episode length: 211.53
    Episode_Reward/reaching_object: 1.3161
     Episode_Reward/lifting_object: 154.1701
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 2.32s
                      Time elapsed: 00:33:32
                               ETA: 00:41:22

################################################################################
                     [1m Learning iteration 896/2000 [0m                      

                       Computation: 41032 steps/s (collection: 2.211s, learning 0.185s)
             Mean action noise std: 2.16
          Mean value_function loss: 257.1613
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 47.9091
                       Mean reward: 777.60
               Mean episode length: 220.27
    Episode_Reward/reaching_object: 1.2811
     Episode_Reward/lifting_object: 149.5781
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 2.40s
                      Time elapsed: 00:33:34
                               ETA: 00:41:19

################################################################################
                     [1m Learning iteration 897/2000 [0m                      

                       Computation: 44275 steps/s (collection: 2.128s, learning 0.092s)
             Mean action noise std: 2.16
          Mean value_function loss: 251.4820
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 47.9241
                       Mean reward: 762.26
               Mean episode length: 217.71
    Episode_Reward/reaching_object: 1.2633
     Episode_Reward/lifting_object: 146.8568
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 2.22s
                      Time elapsed: 00:33:37
                               ETA: 00:41:17

################################################################################
                     [1m Learning iteration 898/2000 [0m                      

                       Computation: 43347 steps/s (collection: 2.129s, learning 0.138s)
             Mean action noise std: 2.16
          Mean value_function loss: 262.7950
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 47.9284
                       Mean reward: 801.35
               Mean episode length: 221.57
    Episode_Reward/reaching_object: 1.3036
     Episode_Reward/lifting_object: 153.3200
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 2.27s
                      Time elapsed: 00:33:39
                               ETA: 00:41:15

################################################################################
                     [1m Learning iteration 899/2000 [0m                      

                       Computation: 44281 steps/s (collection: 2.094s, learning 0.126s)
             Mean action noise std: 2.16
          Mean value_function loss: 259.9015
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.9357
                       Mean reward: 814.96
               Mean episode length: 224.95
    Episode_Reward/reaching_object: 1.3213
     Episode_Reward/lifting_object: 155.3511
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 2.22s
                      Time elapsed: 00:33:41
                               ETA: 00:41:13

################################################################################
                     [1m Learning iteration 900/2000 [0m                      

                       Computation: 44826 steps/s (collection: 2.086s, learning 0.107s)
             Mean action noise std: 2.16
          Mean value_function loss: 252.6790
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 47.9434
                       Mean reward: 807.71
               Mean episode length: 225.33
    Episode_Reward/reaching_object: 1.3291
     Episode_Reward/lifting_object: 156.0378
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 2.19s
                      Time elapsed: 00:33:43
                               ETA: 00:41:10

################################################################################
                     [1m Learning iteration 901/2000 [0m                      

                       Computation: 43993 steps/s (collection: 2.129s, learning 0.106s)
             Mean action noise std: 2.16
          Mean value_function loss: 236.0309
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 47.9462
                       Mean reward: 796.58
               Mean episode length: 223.05
    Episode_Reward/reaching_object: 1.3669
     Episode_Reward/lifting_object: 161.6957
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 2.23s
                      Time elapsed: 00:33:46
                               ETA: 00:41:08

################################################################################
                     [1m Learning iteration 902/2000 [0m                      

                       Computation: 44751 steps/s (collection: 2.093s, learning 0.104s)
             Mean action noise std: 2.16
          Mean value_function loss: 228.1956
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 47.9525
                       Mean reward: 812.84
               Mean episode length: 227.96
    Episode_Reward/reaching_object: 1.3420
     Episode_Reward/lifting_object: 158.6560
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 2.20s
                      Time elapsed: 00:33:48
                               ETA: 00:41:06

################################################################################
                     [1m Learning iteration 903/2000 [0m                      

                       Computation: 44517 steps/s (collection: 2.080s, learning 0.128s)
             Mean action noise std: 2.16
          Mean value_function loss: 233.4277
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 47.9617
                       Mean reward: 760.34
               Mean episode length: 214.10
    Episode_Reward/reaching_object: 1.2956
     Episode_Reward/lifting_object: 152.8226
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 2.21s
                      Time elapsed: 00:33:50
                               ETA: 00:41:04

################################################################################
                     [1m Learning iteration 904/2000 [0m                      

                       Computation: 42060 steps/s (collection: 2.234s, learning 0.104s)
             Mean action noise std: 2.17
          Mean value_function loss: 238.5866
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 47.9660
                       Mean reward: 786.49
               Mean episode length: 220.05
    Episode_Reward/reaching_object: 1.3177
     Episode_Reward/lifting_object: 155.8343
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 2.34s
                      Time elapsed: 00:33:52
                               ETA: 00:41:01

################################################################################
                     [1m Learning iteration 905/2000 [0m                      

                       Computation: 44222 steps/s (collection: 2.108s, learning 0.115s)
             Mean action noise std: 2.17
          Mean value_function loss: 193.6828
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 47.9686
                       Mean reward: 787.24
               Mean episode length: 220.32
    Episode_Reward/reaching_object: 1.3055
     Episode_Reward/lifting_object: 154.7257
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 2.22s
                      Time elapsed: 00:33:55
                               ETA: 00:40:59

################################################################################
                     [1m Learning iteration 906/2000 [0m                      

                       Computation: 45330 steps/s (collection: 2.076s, learning 0.093s)
             Mean action noise std: 2.17
          Mean value_function loss: 208.5495
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 47.9697
                       Mean reward: 833.59
               Mean episode length: 231.20
    Episode_Reward/reaching_object: 1.3598
     Episode_Reward/lifting_object: 161.5275
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 2.17s
                      Time elapsed: 00:33:57
                               ETA: 00:40:57

################################################################################
                     [1m Learning iteration 907/2000 [0m                      

                       Computation: 45729 steps/s (collection: 2.052s, learning 0.098s)
             Mean action noise std: 2.17
          Mean value_function loss: 224.6705
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 47.9716
                       Mean reward: 809.99
               Mean episode length: 225.46
    Episode_Reward/reaching_object: 1.3134
     Episode_Reward/lifting_object: 155.0753
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 2.15s
                      Time elapsed: 00:33:59
                               ETA: 00:40:54

################################################################################
                     [1m Learning iteration 908/2000 [0m                      

                       Computation: 44680 steps/s (collection: 2.090s, learning 0.111s)
             Mean action noise std: 2.17
          Mean value_function loss: 215.1087
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 47.9754
                       Mean reward: 771.42
               Mean episode length: 215.08
    Episode_Reward/reaching_object: 1.3634
     Episode_Reward/lifting_object: 162.5723
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 2.20s
                      Time elapsed: 00:34:01
                               ETA: 00:40:52

################################################################################
                     [1m Learning iteration 909/2000 [0m                      

                       Computation: 43186 steps/s (collection: 2.180s, learning 0.096s)
             Mean action noise std: 2.17
          Mean value_function loss: 237.9722
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 47.9803
                       Mean reward: 830.57
               Mean episode length: 228.77
    Episode_Reward/reaching_object: 1.3502
     Episode_Reward/lifting_object: 160.4485
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 2.28s
                      Time elapsed: 00:34:03
                               ETA: 00:40:50

################################################################################
                     [1m Learning iteration 910/2000 [0m                      

                       Computation: 45069 steps/s (collection: 2.089s, learning 0.092s)
             Mean action noise std: 2.17
          Mean value_function loss: 233.6987
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 47.9837
                       Mean reward: 815.29
               Mean episode length: 226.52
    Episode_Reward/reaching_object: 1.3661
     Episode_Reward/lifting_object: 162.3687
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 2.18s
                      Time elapsed: 00:34:06
                               ETA: 00:40:48

################################################################################
                     [1m Learning iteration 911/2000 [0m                      

                       Computation: 44100 steps/s (collection: 2.119s, learning 0.111s)
             Mean action noise std: 2.17
          Mean value_function loss: 261.8702
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 47.9880
                       Mean reward: 771.17
               Mean episode length: 214.59
    Episode_Reward/reaching_object: 1.2943
     Episode_Reward/lifting_object: 153.3836
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 2.23s
                      Time elapsed: 00:34:08
                               ETA: 00:40:45

################################################################################
                     [1m Learning iteration 912/2000 [0m                      

                       Computation: 45660 steps/s (collection: 2.046s, learning 0.107s)
             Mean action noise std: 2.17
          Mean value_function loss: 238.0578
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 47.9901
                       Mean reward: 775.11
               Mean episode length: 216.96
    Episode_Reward/reaching_object: 1.2889
     Episode_Reward/lifting_object: 153.1772
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 2.15s
                      Time elapsed: 00:34:10
                               ETA: 00:40:43

################################################################################
                     [1m Learning iteration 913/2000 [0m                      

                       Computation: 45754 steps/s (collection: 2.050s, learning 0.099s)
             Mean action noise std: 2.17
          Mean value_function loss: 224.3128
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 47.9981
                       Mean reward: 782.26
               Mean episode length: 217.21
    Episode_Reward/reaching_object: 1.3327
     Episode_Reward/lifting_object: 157.8344
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 2.15s
                      Time elapsed: 00:34:12
                               ETA: 00:40:41

################################################################################
                     [1m Learning iteration 914/2000 [0m                      

                       Computation: 44123 steps/s (collection: 2.123s, learning 0.105s)
             Mean action noise std: 2.17
          Mean value_function loss: 240.5121
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 48.0039
                       Mean reward: 765.30
               Mean episode length: 214.29
    Episode_Reward/reaching_object: 1.3055
     Episode_Reward/lifting_object: 154.5669
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 2.23s
                      Time elapsed: 00:34:14
                               ETA: 00:40:38

################################################################################
                     [1m Learning iteration 915/2000 [0m                      

                       Computation: 44586 steps/s (collection: 2.081s, learning 0.124s)
             Mean action noise std: 2.17
          Mean value_function loss: 218.2366
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 48.0109
                       Mean reward: 823.14
               Mean episode length: 228.04
    Episode_Reward/reaching_object: 1.3301
     Episode_Reward/lifting_object: 158.1231
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 2.20s
                      Time elapsed: 00:34:16
                               ETA: 00:40:36

################################################################################
                     [1m Learning iteration 916/2000 [0m                      

                       Computation: 44824 steps/s (collection: 2.092s, learning 0.101s)
             Mean action noise std: 2.17
          Mean value_function loss: 250.4340
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 48.0178
                       Mean reward: 773.69
               Mean episode length: 215.29
    Episode_Reward/reaching_object: 1.2828
     Episode_Reward/lifting_object: 152.4055
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 2.19s
                      Time elapsed: 00:34:19
                               ETA: 00:40:34

################################################################################
                     [1m Learning iteration 917/2000 [0m                      

                       Computation: 45897 steps/s (collection: 2.043s, learning 0.099s)
             Mean action noise std: 2.17
          Mean value_function loss: 201.3731
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.0205
                       Mean reward: 833.16
               Mean episode length: 227.30
    Episode_Reward/reaching_object: 1.3715
     Episode_Reward/lifting_object: 163.9160
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 2.14s
                      Time elapsed: 00:34:21
                               ETA: 00:40:31

################################################################################
                     [1m Learning iteration 918/2000 [0m                      

                       Computation: 44714 steps/s (collection: 2.087s, learning 0.111s)
             Mean action noise std: 2.17
          Mean value_function loss: 244.2927
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 48.0238
                       Mean reward: 719.48
               Mean episode length: 207.53
    Episode_Reward/reaching_object: 1.3284
     Episode_Reward/lifting_object: 157.7071
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 2.20s
                      Time elapsed: 00:34:23
                               ETA: 00:40:29

################################################################################
                     [1m Learning iteration 919/2000 [0m                      

                       Computation: 44994 steps/s (collection: 2.086s, learning 0.099s)
             Mean action noise std: 2.17
          Mean value_function loss: 209.4463
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 48.0271
                       Mean reward: 848.19
               Mean episode length: 232.24
    Episode_Reward/reaching_object: 1.3694
     Episode_Reward/lifting_object: 163.6810
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 2.18s
                      Time elapsed: 00:34:25
                               ETA: 00:40:27

################################################################################
                     [1m Learning iteration 920/2000 [0m                      

                       Computation: 44563 steps/s (collection: 2.106s, learning 0.100s)
             Mean action noise std: 2.17
          Mean value_function loss: 224.9785
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 48.0304
                       Mean reward: 803.21
               Mean episode length: 223.76
    Episode_Reward/reaching_object: 1.3571
     Episode_Reward/lifting_object: 162.0134
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 2.21s
                      Time elapsed: 00:34:27
                               ETA: 00:40:24

################################################################################
                     [1m Learning iteration 921/2000 [0m                      

                       Computation: 44686 steps/s (collection: 2.076s, learning 0.124s)
             Mean action noise std: 2.18
          Mean value_function loss: 194.5545
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 48.0404
                       Mean reward: 817.34
               Mean episode length: 225.28
    Episode_Reward/reaching_object: 1.3488
     Episode_Reward/lifting_object: 161.4051
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 2.20s
                      Time elapsed: 00:34:30
                               ETA: 00:40:22

################################################################################
                     [1m Learning iteration 922/2000 [0m                      

                       Computation: 45207 steps/s (collection: 2.079s, learning 0.096s)
             Mean action noise std: 2.18
          Mean value_function loss: 196.9502
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 48.0505
                       Mean reward: 786.17
               Mean episode length: 218.29
    Episode_Reward/reaching_object: 1.3758
     Episode_Reward/lifting_object: 164.8343
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 2.17s
                      Time elapsed: 00:34:32
                               ETA: 00:40:20

################################################################################
                     [1m Learning iteration 923/2000 [0m                      

                       Computation: 45182 steps/s (collection: 2.050s, learning 0.126s)
             Mean action noise std: 2.18
          Mean value_function loss: 192.5655
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 48.0573
                       Mean reward: 833.60
               Mean episode length: 228.91
    Episode_Reward/reaching_object: 1.3712
     Episode_Reward/lifting_object: 164.9911
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 2.18s
                      Time elapsed: 00:34:34
                               ETA: 00:40:17

################################################################################
                     [1m Learning iteration 924/2000 [0m                      

                       Computation: 44531 steps/s (collection: 2.109s, learning 0.098s)
             Mean action noise std: 2.18
          Mean value_function loss: 187.3635
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 48.0653
                       Mean reward: 837.90
               Mean episode length: 231.69
    Episode_Reward/reaching_object: 1.3741
     Episode_Reward/lifting_object: 164.6827
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 2.21s
                      Time elapsed: 00:34:36
                               ETA: 00:40:15

################################################################################
                     [1m Learning iteration 925/2000 [0m                      

                       Computation: 44576 steps/s (collection: 2.093s, learning 0.112s)
             Mean action noise std: 2.18
          Mean value_function loss: 227.3167
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 48.0763
                       Mean reward: 779.10
               Mean episode length: 220.52
    Episode_Reward/reaching_object: 1.2953
     Episode_Reward/lifting_object: 153.8095
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 2.21s
                      Time elapsed: 00:34:38
                               ETA: 00:40:13

################################################################################
                     [1m Learning iteration 926/2000 [0m                      

                       Computation: 44035 steps/s (collection: 2.105s, learning 0.128s)
             Mean action noise std: 2.18
          Mean value_function loss: 230.9153
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 48.0859
                       Mean reward: 779.75
               Mean episode length: 217.88
    Episode_Reward/reaching_object: 1.3051
     Episode_Reward/lifting_object: 155.5496
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 2.23s
                      Time elapsed: 00:34:41
                               ETA: 00:40:11

################################################################################
                     [1m Learning iteration 927/2000 [0m                      

                       Computation: 44947 steps/s (collection: 2.080s, learning 0.108s)
             Mean action noise std: 2.18
          Mean value_function loss: 183.3285
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 48.0903
                       Mean reward: 816.11
               Mean episode length: 226.72
    Episode_Reward/reaching_object: 1.3617
     Episode_Reward/lifting_object: 162.9904
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 2.19s
                      Time elapsed: 00:34:43
                               ETA: 00:40:08

################################################################################
                     [1m Learning iteration 928/2000 [0m                      

                       Computation: 44412 steps/s (collection: 2.115s, learning 0.099s)
             Mean action noise std: 2.18
          Mean value_function loss: 202.4638
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 48.0951
                       Mean reward: 798.01
               Mean episode length: 220.34
    Episode_Reward/reaching_object: 1.3436
     Episode_Reward/lifting_object: 161.5694
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 2.21s
                      Time elapsed: 00:34:45
                               ETA: 00:40:06

################################################################################
                     [1m Learning iteration 929/2000 [0m                      

                       Computation: 45541 steps/s (collection: 2.068s, learning 0.091s)
             Mean action noise std: 2.18
          Mean value_function loss: 208.4872
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 48.0981
                       Mean reward: 788.51
               Mean episode length: 218.84
    Episode_Reward/reaching_object: 1.3674
     Episode_Reward/lifting_object: 164.4801
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 2.16s
                      Time elapsed: 00:34:47
                               ETA: 00:40:04

################################################################################
                     [1m Learning iteration 930/2000 [0m                      

                       Computation: 44713 steps/s (collection: 2.107s, learning 0.091s)
             Mean action noise std: 2.18
          Mean value_function loss: 219.8063
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 48.0993
                       Mean reward: 800.41
               Mean episode length: 222.54
    Episode_Reward/reaching_object: 1.3378
     Episode_Reward/lifting_object: 160.2139
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 2.20s
                      Time elapsed: 00:34:49
                               ETA: 00:40:01

################################################################################
                     [1m Learning iteration 931/2000 [0m                      

                       Computation: 44521 steps/s (collection: 2.077s, learning 0.131s)
             Mean action noise std: 2.18
          Mean value_function loss: 191.4514
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 48.0999
                       Mean reward: 842.70
               Mean episode length: 231.60
    Episode_Reward/reaching_object: 1.3628
     Episode_Reward/lifting_object: 164.1101
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 2.21s
                      Time elapsed: 00:34:52
                               ETA: 00:39:59

################################################################################
                     [1m Learning iteration 932/2000 [0m                      

                       Computation: 45310 steps/s (collection: 2.064s, learning 0.105s)
             Mean action noise std: 2.18
          Mean value_function loss: 220.1850
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 48.1005
                       Mean reward: 789.85
               Mean episode length: 219.14
    Episode_Reward/reaching_object: 1.3477
     Episode_Reward/lifting_object: 162.3000
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 2.17s
                      Time elapsed: 00:34:54
                               ETA: 00:39:57

################################################################################
                     [1m Learning iteration 933/2000 [0m                      

                       Computation: 45576 steps/s (collection: 2.060s, learning 0.097s)
             Mean action noise std: 2.18
          Mean value_function loss: 248.8411
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 48.1015
                       Mean reward: 776.82
               Mean episode length: 215.88
    Episode_Reward/reaching_object: 1.3194
     Episode_Reward/lifting_object: 157.9574
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 2.16s
                      Time elapsed: 00:34:56
                               ETA: 00:39:54

################################################################################
                     [1m Learning iteration 934/2000 [0m                      

                       Computation: 45163 steps/s (collection: 2.083s, learning 0.094s)
             Mean action noise std: 2.18
          Mean value_function loss: 245.8923
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 48.1067
                       Mean reward: 794.34
               Mean episode length: 221.86
    Episode_Reward/reaching_object: 1.3478
     Episode_Reward/lifting_object: 161.9538
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 2.18s
                      Time elapsed: 00:34:58
                               ETA: 00:39:52

################################################################################
                     [1m Learning iteration 935/2000 [0m                      

                       Computation: 44957 steps/s (collection: 2.084s, learning 0.103s)
             Mean action noise std: 2.18
          Mean value_function loss: 277.6462
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 48.1168
                       Mean reward: 798.85
               Mean episode length: 222.59
    Episode_Reward/reaching_object: 1.2557
     Episode_Reward/lifting_object: 149.5280
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 2.19s
                      Time elapsed: 00:35:00
                               ETA: 00:39:50

################################################################################
                     [1m Learning iteration 936/2000 [0m                      

                       Computation: 45085 steps/s (collection: 2.069s, learning 0.111s)
             Mean action noise std: 2.18
          Mean value_function loss: 254.1792
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 48.1254
                       Mean reward: 783.81
               Mean episode length: 218.67
    Episode_Reward/reaching_object: 1.3350
     Episode_Reward/lifting_object: 160.2875
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 2.18s
                      Time elapsed: 00:35:02
                               ETA: 00:39:47

################################################################################
                     [1m Learning iteration 937/2000 [0m                      

                       Computation: 44440 steps/s (collection: 2.112s, learning 0.100s)
             Mean action noise std: 2.18
          Mean value_function loss: 268.9637
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 48.1300
                       Mean reward: 756.00
               Mean episode length: 211.38
    Episode_Reward/reaching_object: 1.2708
     Episode_Reward/lifting_object: 151.4684
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 2.21s
                      Time elapsed: 00:35:05
                               ETA: 00:39:45

################################################################################
                     [1m Learning iteration 938/2000 [0m                      

                       Computation: 44431 steps/s (collection: 2.103s, learning 0.109s)
             Mean action noise std: 2.19
          Mean value_function loss: 255.3972
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 48.1401
                       Mean reward: 823.38
               Mean episode length: 229.27
    Episode_Reward/reaching_object: 1.3039
     Episode_Reward/lifting_object: 155.8716
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 2.21s
                      Time elapsed: 00:35:07
                               ETA: 00:39:43

################################################################################
                     [1m Learning iteration 939/2000 [0m                      

                       Computation: 45289 steps/s (collection: 2.072s, learning 0.099s)
             Mean action noise std: 2.19
          Mean value_function loss: 254.8297
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 48.1591
                       Mean reward: 790.16
               Mean episode length: 220.97
    Episode_Reward/reaching_object: 1.3230
     Episode_Reward/lifting_object: 158.3306
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 2.17s
                      Time elapsed: 00:35:09
                               ETA: 00:39:41

################################################################################
                     [1m Learning iteration 940/2000 [0m                      

                       Computation: 45323 steps/s (collection: 2.068s, learning 0.101s)
             Mean action noise std: 2.19
          Mean value_function loss: 251.4755
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 48.1715
                       Mean reward: 784.41
               Mean episode length: 217.03
    Episode_Reward/reaching_object: 1.3111
     Episode_Reward/lifting_object: 157.1129
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 2.17s
                      Time elapsed: 00:35:11
                               ETA: 00:39:38

################################################################################
                     [1m Learning iteration 941/2000 [0m                      

                       Computation: 44757 steps/s (collection: 2.091s, learning 0.105s)
             Mean action noise std: 2.19
          Mean value_function loss: 216.0017
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 48.1817
                       Mean reward: 809.57
               Mean episode length: 225.55
    Episode_Reward/reaching_object: 1.3174
     Episode_Reward/lifting_object: 157.5217
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 2.20s
                      Time elapsed: 00:35:13
                               ETA: 00:39:36

################################################################################
                     [1m Learning iteration 942/2000 [0m                      

                       Computation: 44051 steps/s (collection: 2.073s, learning 0.158s)
             Mean action noise std: 2.19
          Mean value_function loss: 242.0469
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 48.1876
                       Mean reward: 844.97
               Mean episode length: 230.72
    Episode_Reward/reaching_object: 1.3379
     Episode_Reward/lifting_object: 160.9701
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 2.23s
                      Time elapsed: 00:35:16
                               ETA: 00:39:34

################################################################################
                     [1m Learning iteration 943/2000 [0m                      

                       Computation: 43484 steps/s (collection: 2.148s, learning 0.113s)
             Mean action noise std: 2.19
          Mean value_function loss: 247.3951
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 48.1969
                       Mean reward: 817.20
               Mean episode length: 223.87
    Episode_Reward/reaching_object: 1.3183
     Episode_Reward/lifting_object: 157.8722
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 2.26s
                      Time elapsed: 00:35:18
                               ETA: 00:39:31

################################################################################
                     [1m Learning iteration 944/2000 [0m                      

                       Computation: 44588 steps/s (collection: 2.095s, learning 0.110s)
             Mean action noise std: 2.19
          Mean value_function loss: 256.0764
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 48.2085
                       Mean reward: 786.18
               Mean episode length: 218.23
    Episode_Reward/reaching_object: 1.2966
     Episode_Reward/lifting_object: 155.5796
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 2.20s
                      Time elapsed: 00:35:20
                               ETA: 00:39:29

################################################################################
                     [1m Learning iteration 945/2000 [0m                      

                       Computation: 45048 steps/s (collection: 2.078s, learning 0.104s)
             Mean action noise std: 2.19
          Mean value_function loss: 225.9825
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 48.2220
                       Mean reward: 803.74
               Mean episode length: 223.70
    Episode_Reward/reaching_object: 1.3506
     Episode_Reward/lifting_object: 162.4754
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 2.18s
                      Time elapsed: 00:35:22
                               ETA: 00:39:27

################################################################################
                     [1m Learning iteration 946/2000 [0m                      

                       Computation: 43697 steps/s (collection: 2.137s, learning 0.113s)
             Mean action noise std: 2.20
          Mean value_function loss: 247.0863
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 48.2268
                       Mean reward: 772.96
               Mean episode length: 214.19
    Episode_Reward/reaching_object: 1.3000
     Episode_Reward/lifting_object: 156.5298
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 2.25s
                      Time elapsed: 00:35:25
                               ETA: 00:39:25

################################################################################
                     [1m Learning iteration 947/2000 [0m                      

                       Computation: 43898 steps/s (collection: 2.102s, learning 0.137s)
             Mean action noise std: 2.20
          Mean value_function loss: 213.9801
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 48.2320
                       Mean reward: 798.53
               Mean episode length: 220.66
    Episode_Reward/reaching_object: 1.3327
     Episode_Reward/lifting_object: 161.4436
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 2.24s
                      Time elapsed: 00:35:27
                               ETA: 00:39:22

################################################################################
                     [1m Learning iteration 948/2000 [0m                      

                       Computation: 44150 steps/s (collection: 2.131s, learning 0.096s)
             Mean action noise std: 2.20
          Mean value_function loss: 250.6600
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 48.2374
                       Mean reward: 830.27
               Mean episode length: 225.79
    Episode_Reward/reaching_object: 1.3267
     Episode_Reward/lifting_object: 160.9145
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 2.23s
                      Time elapsed: 00:35:29
                               ETA: 00:39:20

################################################################################
                     [1m Learning iteration 949/2000 [0m                      

                       Computation: 44659 steps/s (collection: 2.109s, learning 0.093s)
             Mean action noise std: 2.20
          Mean value_function loss: 241.6924
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 48.2411
                       Mean reward: 841.30
               Mean episode length: 233.21
    Episode_Reward/reaching_object: 1.3700
     Episode_Reward/lifting_object: 165.6233
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 2.20s
                      Time elapsed: 00:35:31
                               ETA: 00:39:18

################################################################################
                     [1m Learning iteration 950/2000 [0m                      

                       Computation: 44519 steps/s (collection: 2.111s, learning 0.097s)
             Mean action noise std: 2.20
          Mean value_function loss: 238.7871
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.2492
                       Mean reward: 805.79
               Mean episode length: 222.03
    Episode_Reward/reaching_object: 1.2985
     Episode_Reward/lifting_object: 156.6894
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 2.21s
                      Time elapsed: 00:35:33
                               ETA: 00:39:16

################################################################################
                     [1m Learning iteration 951/2000 [0m                      

                       Computation: 43690 steps/s (collection: 2.138s, learning 0.112s)
             Mean action noise std: 2.20
          Mean value_function loss: 231.6901
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 48.2652
                       Mean reward: 810.19
               Mean episode length: 222.81
    Episode_Reward/reaching_object: 1.3251
     Episode_Reward/lifting_object: 161.1566
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 2.25s
                      Time elapsed: 00:35:36
                               ETA: 00:39:13

################################################################################
                     [1m Learning iteration 952/2000 [0m                      

                       Computation: 44773 steps/s (collection: 2.081s, learning 0.115s)
             Mean action noise std: 2.20
          Mean value_function loss: 183.9328
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 48.2860
                       Mean reward: 866.44
               Mean episode length: 237.54
    Episode_Reward/reaching_object: 1.3445
     Episode_Reward/lifting_object: 162.9482
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 2.20s
                      Time elapsed: 00:35:38
                               ETA: 00:39:11

################################################################################
                     [1m Learning iteration 953/2000 [0m                      

                       Computation: 44927 steps/s (collection: 2.094s, learning 0.094s)
             Mean action noise std: 2.20
          Mean value_function loss: 227.2441
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 48.2965
                       Mean reward: 796.31
               Mean episode length: 220.08
    Episode_Reward/reaching_object: 1.3450
     Episode_Reward/lifting_object: 163.9214
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 2.19s
                      Time elapsed: 00:35:40
                               ETA: 00:39:09

################################################################################
                     [1m Learning iteration 954/2000 [0m                      

                       Computation: 44655 steps/s (collection: 2.103s, learning 0.099s)
             Mean action noise std: 2.20
          Mean value_function loss: 187.9374
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 48.3021
                       Mean reward: 818.39
               Mean episode length: 224.42
    Episode_Reward/reaching_object: 1.3440
     Episode_Reward/lifting_object: 163.1234
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 2.20s
                      Time elapsed: 00:35:42
                               ETA: 00:39:06

################################################################################
                     [1m Learning iteration 955/2000 [0m                      

                       Computation: 45225 steps/s (collection: 2.071s, learning 0.103s)
             Mean action noise std: 2.20
          Mean value_function loss: 178.5742
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 48.3120
                       Mean reward: 835.63
               Mean episode length: 229.57
    Episode_Reward/reaching_object: 1.3452
     Episode_Reward/lifting_object: 163.6022
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 2.17s
                      Time elapsed: 00:35:44
                               ETA: 00:39:04

################################################################################
                     [1m Learning iteration 956/2000 [0m                      

                       Computation: 45773 steps/s (collection: 2.057s, learning 0.091s)
             Mean action noise std: 2.21
          Mean value_function loss: 188.8945
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 48.3217
                       Mean reward: 871.74
               Mean episode length: 234.50
    Episode_Reward/reaching_object: 1.3567
     Episode_Reward/lifting_object: 165.5327
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 2.15s
                      Time elapsed: 00:35:47
                               ETA: 00:39:02

################################################################################
                     [1m Learning iteration 957/2000 [0m                      

                       Computation: 45350 steps/s (collection: 2.067s, learning 0.101s)
             Mean action noise std: 2.21
          Mean value_function loss: 220.1204
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 48.3330
                       Mean reward: 844.43
               Mean episode length: 233.49
    Episode_Reward/reaching_object: 1.3569
     Episode_Reward/lifting_object: 164.6429
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 2.17s
                      Time elapsed: 00:35:49
                               ETA: 00:38:59

################################################################################
                     [1m Learning iteration 958/2000 [0m                      

                       Computation: 44277 steps/s (collection: 2.102s, learning 0.119s)
             Mean action noise std: 2.21
          Mean value_function loss: 200.5639
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 48.3421
                       Mean reward: 875.75
               Mean episode length: 237.69
    Episode_Reward/reaching_object: 1.3645
     Episode_Reward/lifting_object: 166.0465
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 2.22s
                      Time elapsed: 00:35:51
                               ETA: 00:38:57

################################################################################
                     [1m Learning iteration 959/2000 [0m                      

                       Computation: 44898 steps/s (collection: 2.091s, learning 0.098s)
             Mean action noise std: 2.21
          Mean value_function loss: 219.3884
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 48.3499
                       Mean reward: 807.11
               Mean episode length: 221.75
    Episode_Reward/reaching_object: 1.2945
     Episode_Reward/lifting_object: 156.7075
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 2.19s
                      Time elapsed: 00:35:53
                               ETA: 00:38:55

################################################################################
                     [1m Learning iteration 960/2000 [0m                      

                       Computation: 45139 steps/s (collection: 2.080s, learning 0.098s)
             Mean action noise std: 2.21
          Mean value_function loss: 234.9288
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 48.3634
                       Mean reward: 798.66
               Mean episode length: 221.28
    Episode_Reward/reaching_object: 1.3106
     Episode_Reward/lifting_object: 159.0992
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 2.18s
                      Time elapsed: 00:35:55
                               ETA: 00:38:53

################################################################################
                     [1m Learning iteration 961/2000 [0m                      

                       Computation: 44046 steps/s (collection: 2.113s, learning 0.119s)
             Mean action noise std: 2.21
          Mean value_function loss: 210.0992
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 48.3755
                       Mean reward: 824.17
               Mean episode length: 228.26
    Episode_Reward/reaching_object: 1.3406
     Episode_Reward/lifting_object: 162.7696
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 2.23s
                      Time elapsed: 00:35:58
                               ETA: 00:38:50

################################################################################
                     [1m Learning iteration 962/2000 [0m                      

                       Computation: 44949 steps/s (collection: 2.088s, learning 0.099s)
             Mean action noise std: 2.21
          Mean value_function loss: 305.4424
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 48.3889
                       Mean reward: 819.79
               Mean episode length: 224.80
    Episode_Reward/reaching_object: 1.2867
     Episode_Reward/lifting_object: 155.9796
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 2.19s
                      Time elapsed: 00:36:00
                               ETA: 00:38:48

################################################################################
                     [1m Learning iteration 963/2000 [0m                      

                       Computation: 45009 steps/s (collection: 2.078s, learning 0.106s)
             Mean action noise std: 2.21
          Mean value_function loss: 335.6740
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 48.3988
                       Mean reward: 758.77
               Mean episode length: 212.77
    Episode_Reward/reaching_object: 1.2885
     Episode_Reward/lifting_object: 156.5110
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 2.18s
                      Time elapsed: 00:36:02
                               ETA: 00:38:46

################################################################################
                     [1m Learning iteration 964/2000 [0m                      

                       Computation: 44865 steps/s (collection: 2.068s, learning 0.123s)
             Mean action noise std: 2.22
          Mean value_function loss: 246.0723
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.4111
                       Mean reward: 790.68
               Mean episode length: 217.10
    Episode_Reward/reaching_object: 1.2952
     Episode_Reward/lifting_object: 158.5648
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 2.19s
                      Time elapsed: 00:36:04
                               ETA: 00:38:43

################################################################################
                     [1m Learning iteration 965/2000 [0m                      

                       Computation: 45902 steps/s (collection: 2.049s, learning 0.093s)
             Mean action noise std: 2.22
          Mean value_function loss: 312.5545
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 48.4281
                       Mean reward: 767.02
               Mean episode length: 212.85
    Episode_Reward/reaching_object: 1.2540
     Episode_Reward/lifting_object: 152.8788
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 2.14s
                      Time elapsed: 00:36:06
                               ETA: 00:38:41

################################################################################
                     [1m Learning iteration 966/2000 [0m                      

                       Computation: 44532 steps/s (collection: 2.107s, learning 0.101s)
             Mean action noise std: 2.22
          Mean value_function loss: 265.3810
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 48.4365
                       Mean reward: 791.69
               Mean episode length: 219.57
    Episode_Reward/reaching_object: 1.2746
     Episode_Reward/lifting_object: 155.0656
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 2.21s
                      Time elapsed: 00:36:08
                               ETA: 00:38:39

################################################################################
                     [1m Learning iteration 967/2000 [0m                      

                       Computation: 44672 steps/s (collection: 2.110s, learning 0.091s)
             Mean action noise std: 2.22
          Mean value_function loss: 266.6046
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 48.4426
                       Mean reward: 806.76
               Mean episode length: 221.80
    Episode_Reward/reaching_object: 1.3080
     Episode_Reward/lifting_object: 160.2240
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 2.20s
                      Time elapsed: 00:36:11
                               ETA: 00:38:36

################################################################################
                     [1m Learning iteration 968/2000 [0m                      

                       Computation: 45311 steps/s (collection: 2.068s, learning 0.102s)
             Mean action noise std: 2.22
          Mean value_function loss: 241.3915
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 48.4514
                       Mean reward: 821.86
               Mean episode length: 224.66
    Episode_Reward/reaching_object: 1.3114
     Episode_Reward/lifting_object: 161.2750
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 2.17s
                      Time elapsed: 00:36:13
                               ETA: 00:38:34

################################################################################
                     [1m Learning iteration 969/2000 [0m                      

                       Computation: 44672 steps/s (collection: 2.095s, learning 0.106s)
             Mean action noise std: 2.22
          Mean value_function loss: 263.1629
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 48.4604
                       Mean reward: 787.48
               Mean episode length: 218.75
    Episode_Reward/reaching_object: 1.2435
     Episode_Reward/lifting_object: 152.0181
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 2.20s
                      Time elapsed: 00:36:15
                               ETA: 00:38:32

################################################################################
                     [1m Learning iteration 970/2000 [0m                      

                       Computation: 43760 steps/s (collection: 2.105s, learning 0.142s)
             Mean action noise std: 2.22
          Mean value_function loss: 269.0436
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 48.4746
                       Mean reward: 765.30
               Mean episode length: 211.42
    Episode_Reward/reaching_object: 1.2675
     Episode_Reward/lifting_object: 155.7506
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 2.25s
                      Time elapsed: 00:36:17
                               ETA: 00:38:30

################################################################################
                     [1m Learning iteration 971/2000 [0m                      

                       Computation: 44805 steps/s (collection: 2.085s, learning 0.109s)
             Mean action noise std: 2.22
          Mean value_function loss: 227.8173
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.4849
                       Mean reward: 852.01
               Mean episode length: 231.74
    Episode_Reward/reaching_object: 1.3313
     Episode_Reward/lifting_object: 163.2272
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 2.19s
                      Time elapsed: 00:36:19
                               ETA: 00:38:27

################################################################################
                     [1m Learning iteration 972/2000 [0m                      

                       Computation: 44890 steps/s (collection: 2.095s, learning 0.095s)
             Mean action noise std: 2.23
          Mean value_function loss: 239.2474
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.4945
                       Mean reward: 838.14
               Mean episode length: 226.95
    Episode_Reward/reaching_object: 1.2937
     Episode_Reward/lifting_object: 159.0799
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 2.19s
                      Time elapsed: 00:36:22
                               ETA: 00:38:25

################################################################################
                     [1m Learning iteration 973/2000 [0m                      

                       Computation: 43652 steps/s (collection: 2.129s, learning 0.123s)
             Mean action noise std: 2.23
          Mean value_function loss: 224.3531
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 48.5007
                       Mean reward: 808.54
               Mean episode length: 221.39
    Episode_Reward/reaching_object: 1.2813
     Episode_Reward/lifting_object: 156.7545
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 2.25s
                      Time elapsed: 00:36:24
                               ETA: 00:38:23

################################################################################
                     [1m Learning iteration 974/2000 [0m                      

                       Computation: 43106 steps/s (collection: 2.137s, learning 0.143s)
             Mean action noise std: 2.23
          Mean value_function loss: 223.8226
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 48.5144
                       Mean reward: 767.87
               Mean episode length: 214.18
    Episode_Reward/reaching_object: 1.2927
     Episode_Reward/lifting_object: 158.9552
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 2.28s
                      Time elapsed: 00:36:26
                               ETA: 00:38:21

################################################################################
                     [1m Learning iteration 975/2000 [0m                      

                       Computation: 44329 steps/s (collection: 2.096s, learning 0.122s)
             Mean action noise std: 2.23
          Mean value_function loss: 236.6311
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.5333
                       Mean reward: 797.13
               Mean episode length: 217.69
    Episode_Reward/reaching_object: 1.2847
     Episode_Reward/lifting_object: 157.8680
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 2.22s
                      Time elapsed: 00:36:28
                               ETA: 00:38:18

################################################################################
                     [1m Learning iteration 976/2000 [0m                      

                       Computation: 42003 steps/s (collection: 2.225s, learning 0.115s)
             Mean action noise std: 2.23
          Mean value_function loss: 183.1100
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 48.5534
                       Mean reward: 820.38
               Mean episode length: 224.45
    Episode_Reward/reaching_object: 1.3258
     Episode_Reward/lifting_object: 164.0022
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 2.34s
                      Time elapsed: 00:36:31
                               ETA: 00:38:16

################################################################################
                     [1m Learning iteration 977/2000 [0m                      

                       Computation: 41967 steps/s (collection: 2.237s, learning 0.106s)
             Mean action noise std: 2.23
          Mean value_function loss: 208.3487
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 48.5681
                       Mean reward: 790.37
               Mean episode length: 217.12
    Episode_Reward/reaching_object: 1.3060
     Episode_Reward/lifting_object: 161.3097
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 2.34s
                      Time elapsed: 00:36:33
                               ETA: 00:38:14

################################################################################
                     [1m Learning iteration 978/2000 [0m                      

                       Computation: 40267 steps/s (collection: 2.337s, learning 0.104s)
             Mean action noise std: 2.23
          Mean value_function loss: 209.1247
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 48.5770
                       Mean reward: 813.67
               Mean episode length: 223.46
    Episode_Reward/reaching_object: 1.3362
     Episode_Reward/lifting_object: 165.2759
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 2.44s
                      Time elapsed: 00:36:36
                               ETA: 00:38:12

################################################################################
                     [1m Learning iteration 979/2000 [0m                      

                       Computation: 41774 steps/s (collection: 2.256s, learning 0.097s)
             Mean action noise std: 2.24
          Mean value_function loss: 225.7608
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 48.5911
                       Mean reward: 744.10
               Mean episode length: 207.19
    Episode_Reward/reaching_object: 1.2639
     Episode_Reward/lifting_object: 155.5520
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 2.35s
                      Time elapsed: 00:36:38
                               ETA: 00:38:10

################################################################################
                     [1m Learning iteration 980/2000 [0m                      

                       Computation: 32693 steps/s (collection: 2.889s, learning 0.118s)
             Mean action noise std: 2.24
          Mean value_function loss: 215.8760
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 48.6041
                       Mean reward: 837.66
               Mean episode length: 227.80
    Episode_Reward/reaching_object: 1.3268
     Episode_Reward/lifting_object: 164.1304
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 3.01s
                      Time elapsed: 00:36:41
                               ETA: 00:38:08

################################################################################
                     [1m Learning iteration 981/2000 [0m                      

                       Computation: 33714 steps/s (collection: 2.811s, learning 0.105s)
             Mean action noise std: 2.24
          Mean value_function loss: 225.1661
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 48.6214
                       Mean reward: 813.63
               Mean episode length: 222.90
    Episode_Reward/reaching_object: 1.3027
     Episode_Reward/lifting_object: 161.5783
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 2.92s
                      Time elapsed: 00:36:44
                               ETA: 00:38:07

################################################################################
                     [1m Learning iteration 982/2000 [0m                      

                       Computation: 38478 steps/s (collection: 2.421s, learning 0.134s)
             Mean action noise std: 2.24
          Mean value_function loss: 165.4863
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 48.6432
                       Mean reward: 799.65
               Mean episode length: 220.89
    Episode_Reward/reaching_object: 1.3225
     Episode_Reward/lifting_object: 164.0812
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 2.55s
                      Time elapsed: 00:36:46
                               ETA: 00:38:05

################################################################################
                     [1m Learning iteration 983/2000 [0m                      

                       Computation: 41431 steps/s (collection: 2.250s, learning 0.123s)
             Mean action noise std: 2.24
          Mean value_function loss: 183.1933
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 48.6574
                       Mean reward: 832.42
               Mean episode length: 225.85
    Episode_Reward/reaching_object: 1.3141
     Episode_Reward/lifting_object: 163.7783
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 2.37s
                      Time elapsed: 00:36:49
                               ETA: 00:38:03

################################################################################
                     [1m Learning iteration 984/2000 [0m                      

                       Computation: 34864 steps/s (collection: 2.655s, learning 0.164s)
             Mean action noise std: 2.24
          Mean value_function loss: 203.1413
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 48.6664
                       Mean reward: 821.89
               Mean episode length: 224.80
    Episode_Reward/reaching_object: 1.3249
     Episode_Reward/lifting_object: 165.8188
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 2.82s
                      Time elapsed: 00:36:52
                               ETA: 00:38:01

################################################################################
                     [1m Learning iteration 985/2000 [0m                      

                       Computation: 35850 steps/s (collection: 2.558s, learning 0.185s)
             Mean action noise std: 2.24
          Mean value_function loss: 176.0666
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.6718
                       Mean reward: 857.93
               Mean episode length: 232.83
    Episode_Reward/reaching_object: 1.3205
     Episode_Reward/lifting_object: 165.3333
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 2.74s
                      Time elapsed: 00:36:54
                               ETA: 00:37:59

################################################################################
                     [1m Learning iteration 986/2000 [0m                      

                       Computation: 40777 steps/s (collection: 2.308s, learning 0.103s)
             Mean action noise std: 2.25
          Mean value_function loss: 158.5084
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 48.6858
                       Mean reward: 869.85
               Mean episode length: 235.04
    Episode_Reward/reaching_object: 1.3300
     Episode_Reward/lifting_object: 166.9365
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 2.41s
                      Time elapsed: 00:36:57
                               ETA: 00:37:57

################################################################################
                     [1m Learning iteration 987/2000 [0m                      

                       Computation: 44341 steps/s (collection: 2.124s, learning 0.093s)
             Mean action noise std: 2.25
          Mean value_function loss: 170.0472
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 48.7021
                       Mean reward: 816.65
               Mean episode length: 223.38
    Episode_Reward/reaching_object: 1.3303
     Episode_Reward/lifting_object: 166.6472
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 2.22s
                      Time elapsed: 00:36:59
                               ETA: 00:37:55

################################################################################
                     [1m Learning iteration 988/2000 [0m                      

                       Computation: 43643 steps/s (collection: 2.148s, learning 0.104s)
             Mean action noise std: 2.25
          Mean value_function loss: 190.1082
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 48.7136
                       Mean reward: 842.88
               Mean episode length: 230.46
    Episode_Reward/reaching_object: 1.3203
     Episode_Reward/lifting_object: 165.5669
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 2.25s
                      Time elapsed: 00:37:01
                               ETA: 00:37:53

################################################################################
                     [1m Learning iteration 989/2000 [0m                      

                       Computation: 44065 steps/s (collection: 2.139s, learning 0.092s)
             Mean action noise std: 2.25
          Mean value_function loss: 173.3949
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.7245
                       Mean reward: 850.78
               Mean episode length: 230.56
    Episode_Reward/reaching_object: 1.3538
     Episode_Reward/lifting_object: 170.4384
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 2.23s
                      Time elapsed: 00:37:03
                               ETA: 00:37:51

################################################################################
                     [1m Learning iteration 990/2000 [0m                      

                       Computation: 45241 steps/s (collection: 2.076s, learning 0.097s)
             Mean action noise std: 2.25
          Mean value_function loss: 188.5764
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 48.7307
                       Mean reward: 827.27
               Mean episode length: 228.42
    Episode_Reward/reaching_object: 1.3105
     Episode_Reward/lifting_object: 163.8354
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 2.17s
                      Time elapsed: 00:37:06
                               ETA: 00:37:48

################################################################################
                     [1m Learning iteration 991/2000 [0m                      

                       Computation: 44642 steps/s (collection: 2.092s, learning 0.111s)
             Mean action noise std: 2.25
          Mean value_function loss: 195.7785
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 48.7444
                       Mean reward: 837.47
               Mean episode length: 229.62
    Episode_Reward/reaching_object: 1.3147
     Episode_Reward/lifting_object: 165.7207
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 2.20s
                      Time elapsed: 00:37:08
                               ETA: 00:37:46

################################################################################
                     [1m Learning iteration 992/2000 [0m                      

                       Computation: 41241 steps/s (collection: 2.254s, learning 0.130s)
             Mean action noise std: 2.25
          Mean value_function loss: 204.0108
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 48.7562
                       Mean reward: 779.38
               Mean episode length: 216.20
    Episode_Reward/reaching_object: 1.2853
     Episode_Reward/lifting_object: 160.7818
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 2.38s
                      Time elapsed: 00:37:10
                               ETA: 00:37:44

################################################################################
                     [1m Learning iteration 993/2000 [0m                      

                       Computation: 43922 steps/s (collection: 2.146s, learning 0.093s)
             Mean action noise std: 2.25
          Mean value_function loss: 163.5068
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 48.7605
                       Mean reward: 826.86
               Mean episode length: 226.17
    Episode_Reward/reaching_object: 1.3166
     Episode_Reward/lifting_object: 166.1686
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 2.24s
                      Time elapsed: 00:37:12
                               ETA: 00:37:42

################################################################################
                     [1m Learning iteration 994/2000 [0m                      

                       Computation: 41434 steps/s (collection: 2.257s, learning 0.116s)
             Mean action noise std: 2.25
          Mean value_function loss: 145.5730
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 48.7636
                       Mean reward: 837.41
               Mean episode length: 228.88
    Episode_Reward/reaching_object: 1.3613
     Episode_Reward/lifting_object: 172.6190
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 2.37s
                      Time elapsed: 00:37:15
                               ETA: 00:37:40

################################################################################
                     [1m Learning iteration 995/2000 [0m                      

                       Computation: 42780 steps/s (collection: 2.186s, learning 0.112s)
             Mean action noise std: 2.25
          Mean value_function loss: 209.3056
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 48.7668
                       Mean reward: 838.29
               Mean episode length: 231.07
    Episode_Reward/reaching_object: 1.3134
     Episode_Reward/lifting_object: 165.9970
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 2.30s
                      Time elapsed: 00:37:17
                               ETA: 00:37:37

################################################################################
                     [1m Learning iteration 996/2000 [0m                      

                       Computation: 41044 steps/s (collection: 2.240s, learning 0.155s)
             Mean action noise std: 2.26
          Mean value_function loss: 192.9304
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 48.7720
                       Mean reward: 780.52
               Mean episode length: 216.55
    Episode_Reward/reaching_object: 1.3168
     Episode_Reward/lifting_object: 166.2939
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 2.40s
                      Time elapsed: 00:37:19
                               ETA: 00:37:35

################################################################################
                     [1m Learning iteration 997/2000 [0m                      

                       Computation: 42316 steps/s (collection: 2.226s, learning 0.097s)
             Mean action noise std: 2.26
          Mean value_function loss: 199.9747
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.7846
                       Mean reward: 791.90
               Mean episode length: 216.88
    Episode_Reward/reaching_object: 1.2959
     Episode_Reward/lifting_object: 162.9020
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 2.32s
                      Time elapsed: 00:37:22
                               ETA: 00:37:33

################################################################################
                     [1m Learning iteration 998/2000 [0m                      

                       Computation: 44846 steps/s (collection: 2.098s, learning 0.094s)
             Mean action noise std: 2.26
          Mean value_function loss: 187.9982
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 48.7978
                       Mean reward: 807.75
               Mean episode length: 222.10
    Episode_Reward/reaching_object: 1.3363
     Episode_Reward/lifting_object: 168.8465
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 2.19s
                      Time elapsed: 00:37:24
                               ETA: 00:37:31

################################################################################
                     [1m Learning iteration 999/2000 [0m                      

                       Computation: 41362 steps/s (collection: 2.275s, learning 0.102s)
             Mean action noise std: 2.26
          Mean value_function loss: 186.0035
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 48.8102
                       Mean reward: 827.82
               Mean episode length: 225.69
    Episode_Reward/reaching_object: 1.3093
     Episode_Reward/lifting_object: 165.4364
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 2.38s
                      Time elapsed: 00:37:26
                               ETA: 00:37:29

################################################################################
                     [1m Learning iteration 1000/2000 [0m                     

                       Computation: 13481 steps/s (collection: 7.168s, learning 0.124s)
             Mean action noise std: 2.26
          Mean value_function loss: 175.1550
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 48.8252
                       Mean reward: 829.00
               Mean episode length: 225.36
    Episode_Reward/reaching_object: 1.3210
     Episode_Reward/lifting_object: 167.4399
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 7.29s
                      Time elapsed: 00:37:34
                               ETA: 00:37:31

################################################################################
                     [1m Learning iteration 1001/2000 [0m                     

                       Computation: 13673 steps/s (collection: 7.022s, learning 0.168s)
             Mean action noise std: 2.26
          Mean value_function loss: 174.9011
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 48.8345
                       Mean reward: 860.97
               Mean episode length: 232.03
    Episode_Reward/reaching_object: 1.3178
     Episode_Reward/lifting_object: 166.6007
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 7.19s
                      Time elapsed: 00:37:41
                               ETA: 00:37:34

################################################################################
                     [1m Learning iteration 1002/2000 [0m                     

                       Computation: 13526 steps/s (collection: 7.151s, learning 0.116s)
             Mean action noise std: 2.26
          Mean value_function loss: 196.0255
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 48.8429
                       Mean reward: 813.26
               Mean episode length: 222.88
    Episode_Reward/reaching_object: 1.3317
     Episode_Reward/lifting_object: 167.2725
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 7.27s
                      Time elapsed: 00:37:48
                               ETA: 00:37:37

################################################################################
                     [1m Learning iteration 1003/2000 [0m                     

                       Computation: 14319 steps/s (collection: 6.734s, learning 0.131s)
             Mean action noise std: 2.26
          Mean value_function loss: 165.8179
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.8536
                       Mean reward: 871.50
               Mean episode length: 235.06
    Episode_Reward/reaching_object: 1.3258
     Episode_Reward/lifting_object: 166.4871
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 6.87s
                      Time elapsed: 00:37:55
                               ETA: 00:37:39

################################################################################
                     [1m Learning iteration 1004/2000 [0m                     

                       Computation: 14180 steps/s (collection: 6.807s, learning 0.125s)
             Mean action noise std: 2.27
          Mean value_function loss: 179.2835
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 48.8658
                       Mean reward: 792.07
               Mean episode length: 219.54
    Episode_Reward/reaching_object: 1.3240
     Episode_Reward/lifting_object: 166.3687
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 6.93s
                      Time elapsed: 00:38:02
                               ETA: 00:37:41

################################################################################
                     [1m Learning iteration 1005/2000 [0m                     

                       Computation: 14330 steps/s (collection: 6.736s, learning 0.124s)
             Mean action noise std: 2.27
          Mean value_function loss: 192.5471
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.8798
                       Mean reward: 812.12
               Mean episode length: 220.78
    Episode_Reward/reaching_object: 1.3289
     Episode_Reward/lifting_object: 167.9251
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 6.86s
                      Time elapsed: 00:38:09
                               ETA: 00:37:44

################################################################################
                     [1m Learning iteration 1006/2000 [0m                     

                       Computation: 14379 steps/s (collection: 6.721s, learning 0.115s)
             Mean action noise std: 2.27
          Mean value_function loss: 177.7372
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 48.8914
                       Mean reward: 805.46
               Mean episode length: 219.47
    Episode_Reward/reaching_object: 1.3154
     Episode_Reward/lifting_object: 165.0037
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 6.84s
                      Time elapsed: 00:38:16
                               ETA: 00:37:46

################################################################################
                     [1m Learning iteration 1007/2000 [0m                     

                       Computation: 14303 steps/s (collection: 6.747s, learning 0.126s)
             Mean action noise std: 2.27
          Mean value_function loss: 192.4333
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 48.9036
                       Mean reward: 829.51
               Mean episode length: 225.79
    Episode_Reward/reaching_object: 1.3145
     Episode_Reward/lifting_object: 164.6271
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 6.87s
                      Time elapsed: 00:38:22
                               ETA: 00:37:48

################################################################################
                     [1m Learning iteration 1008/2000 [0m                     

                       Computation: 17681 steps/s (collection: 5.452s, learning 0.108s)
             Mean action noise std: 2.27
          Mean value_function loss: 185.4115
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 48.9147
                       Mean reward: 817.90
               Mean episode length: 224.76
    Episode_Reward/reaching_object: 1.3011
     Episode_Reward/lifting_object: 162.9678
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 5.56s
                      Time elapsed: 00:38:28
                               ETA: 00:37:49

################################################################################
                     [1m Learning iteration 1009/2000 [0m                     

                       Computation: 47362 steps/s (collection: 1.970s, learning 0.106s)
             Mean action noise std: 2.27
          Mean value_function loss: 150.6893
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 48.9296
                       Mean reward: 792.84
               Mean episode length: 217.93
    Episode_Reward/reaching_object: 1.3253
     Episode_Reward/lifting_object: 166.9615
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 2.08s
                      Time elapsed: 00:38:30
                               ETA: 00:37:47

################################################################################
                     [1m Learning iteration 1010/2000 [0m                     

                       Computation: 46390 steps/s (collection: 2.022s, learning 0.098s)
             Mean action noise std: 2.27
          Mean value_function loss: 154.4165
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 48.9414
                       Mean reward: 786.04
               Mean episode length: 217.44
    Episode_Reward/reaching_object: 1.3076
     Episode_Reward/lifting_object: 163.5107
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 2.12s
                      Time elapsed: 00:38:32
                               ETA: 00:37:44

################################################################################
                     [1m Learning iteration 1011/2000 [0m                     

                       Computation: 46979 steps/s (collection: 2.000s, learning 0.093s)
             Mean action noise std: 2.28
          Mean value_function loss: 156.6155
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.9561
                       Mean reward: 880.33
               Mean episode length: 236.28
    Episode_Reward/reaching_object: 1.3436
     Episode_Reward/lifting_object: 168.5762
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 2.09s
                      Time elapsed: 00:38:34
                               ETA: 00:37:42

################################################################################
                     [1m Learning iteration 1012/2000 [0m                     

                       Computation: 47974 steps/s (collection: 1.964s, learning 0.086s)
             Mean action noise std: 2.28
          Mean value_function loss: 158.0131
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 48.9760
                       Mean reward: 814.25
               Mean episode length: 224.36
    Episode_Reward/reaching_object: 1.3116
     Episode_Reward/lifting_object: 164.5606
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 2.05s
                      Time elapsed: 00:38:36
                               ETA: 00:37:39

################################################################################
                     [1m Learning iteration 1013/2000 [0m                     

                       Computation: 47693 steps/s (collection: 1.973s, learning 0.089s)
             Mean action noise std: 2.28
          Mean value_function loss: 151.1421
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 48.9927
                       Mean reward: 828.44
               Mean episode length: 228.09
    Episode_Reward/reaching_object: 1.3284
     Episode_Reward/lifting_object: 166.6790
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 2.06s
                      Time elapsed: 00:38:38
                               ETA: 00:37:37

################################################################################
                     [1m Learning iteration 1014/2000 [0m                     

                       Computation: 47928 steps/s (collection: 1.961s, learning 0.090s)
             Mean action noise std: 2.28
          Mean value_function loss: 142.3635
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 49.0098
                       Mean reward: 849.15
               Mean episode length: 231.84
    Episode_Reward/reaching_object: 1.3392
     Episode_Reward/lifting_object: 167.9997
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 2.05s
                      Time elapsed: 00:38:40
                               ETA: 00:37:34

################################################################################
                     [1m Learning iteration 1015/2000 [0m                     

                       Computation: 48118 steps/s (collection: 1.954s, learning 0.089s)
             Mean action noise std: 2.28
          Mean value_function loss: 135.5153
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 49.0334
                       Mean reward: 889.55
               Mean episode length: 238.79
    Episode_Reward/reaching_object: 1.3896
     Episode_Reward/lifting_object: 174.5103
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 2.04s
                      Time elapsed: 00:38:43
                               ETA: 00:37:32

################################################################################
                     [1m Learning iteration 1016/2000 [0m                     

                       Computation: 47130 steps/s (collection: 1.997s, learning 0.089s)
             Mean action noise std: 2.29
          Mean value_function loss: 159.2546
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 49.0487
                       Mean reward: 841.56
               Mean episode length: 227.64
    Episode_Reward/reaching_object: 1.3391
     Episode_Reward/lifting_object: 167.7335
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 2.09s
                      Time elapsed: 00:38:45
                               ETA: 00:37:29

################################################################################
                     [1m Learning iteration 1017/2000 [0m                     

                       Computation: 47580 steps/s (collection: 1.964s, learning 0.102s)
             Mean action noise std: 2.29
          Mean value_function loss: 145.7825
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 49.0671
                       Mean reward: 878.59
               Mean episode length: 237.35
    Episode_Reward/reaching_object: 1.3510
     Episode_Reward/lifting_object: 169.4965
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 2.07s
                      Time elapsed: 00:38:47
                               ETA: 00:37:27

################################################################################
                     [1m Learning iteration 1018/2000 [0m                     

                       Computation: 47752 steps/s (collection: 1.966s, learning 0.093s)
             Mean action noise std: 2.29
          Mean value_function loss: 183.4375
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 49.0892
                       Mean reward: 851.70
               Mean episode length: 231.66
    Episode_Reward/reaching_object: 1.3629
     Episode_Reward/lifting_object: 170.2167
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 2.06s
                      Time elapsed: 00:38:49
                               ETA: 00:37:24

################################################################################
                     [1m Learning iteration 1019/2000 [0m                     

                       Computation: 46853 steps/s (collection: 2.011s, learning 0.087s)
             Mean action noise std: 2.29
          Mean value_function loss: 155.0710
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 49.1044
                       Mean reward: 849.03
               Mean episode length: 230.76
    Episode_Reward/reaching_object: 1.3555
     Episode_Reward/lifting_object: 169.0149
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 2.10s
                      Time elapsed: 00:38:51
                               ETA: 00:37:22

################################################################################
                     [1m Learning iteration 1020/2000 [0m                     

                       Computation: 48419 steps/s (collection: 1.945s, learning 0.086s)
             Mean action noise std: 2.29
          Mean value_function loss: 138.4421
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 49.1225
                       Mean reward: 845.09
               Mean episode length: 229.15
    Episode_Reward/reaching_object: 1.3496
     Episode_Reward/lifting_object: 168.2563
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 2.03s
                      Time elapsed: 00:38:53
                               ETA: 00:37:19

################################################################################
                     [1m Learning iteration 1021/2000 [0m                     

                       Computation: 47999 steps/s (collection: 1.962s, learning 0.086s)
             Mean action noise std: 2.29
          Mean value_function loss: 141.0211
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 49.1283
                       Mean reward: 827.97
               Mean episode length: 225.54
    Episode_Reward/reaching_object: 1.3106
     Episode_Reward/lifting_object: 163.6109
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 2.05s
                      Time elapsed: 00:38:55
                               ETA: 00:37:17

################################################################################
                     [1m Learning iteration 1022/2000 [0m                     

                       Computation: 47370 steps/s (collection: 1.959s, learning 0.116s)
             Mean action noise std: 2.30
          Mean value_function loss: 165.6130
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 49.1382
                       Mean reward: 897.25
               Mean episode length: 240.77
    Episode_Reward/reaching_object: 1.3668
     Episode_Reward/lifting_object: 170.0047
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 2.08s
                      Time elapsed: 00:38:57
                               ETA: 00:37:14

################################################################################
                     [1m Learning iteration 1023/2000 [0m                     

                       Computation: 47352 steps/s (collection: 1.960s, learning 0.116s)
             Mean action noise std: 2.30
          Mean value_function loss: 133.8416
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 49.1618
                       Mean reward: 824.68
               Mean episode length: 225.97
    Episode_Reward/reaching_object: 1.3461
     Episode_Reward/lifting_object: 167.8878
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 2.08s
                      Time elapsed: 00:38:59
                               ETA: 00:37:12

################################################################################
                     [1m Learning iteration 1024/2000 [0m                     

                       Computation: 45932 steps/s (collection: 2.034s, learning 0.107s)
             Mean action noise std: 2.30
          Mean value_function loss: 132.5447
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 49.1764
                       Mean reward: 840.96
               Mean episode length: 227.94
    Episode_Reward/reaching_object: 1.3737
     Episode_Reward/lifting_object: 172.2692
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 2.14s
                      Time elapsed: 00:39:01
                               ETA: 00:37:09

################################################################################
                     [1m Learning iteration 1025/2000 [0m                     

                       Computation: 46148 steps/s (collection: 2.043s, learning 0.088s)
             Mean action noise std: 2.30
          Mean value_function loss: 144.1733
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 49.1912
                       Mean reward: 844.54
               Mean episode length: 228.22
    Episode_Reward/reaching_object: 1.3398
     Episode_Reward/lifting_object: 168.1545
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 2.13s
                      Time elapsed: 00:39:03
                               ETA: 00:37:07

################################################################################
                     [1m Learning iteration 1026/2000 [0m                     

                       Computation: 46880 steps/s (collection: 2.005s, learning 0.092s)
             Mean action noise std: 2.30
          Mean value_function loss: 161.8218
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 49.2051
                       Mean reward: 862.00
               Mean episode length: 231.28
    Episode_Reward/reaching_object: 1.3481
     Episode_Reward/lifting_object: 169.1716
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 2.10s
                      Time elapsed: 00:39:05
                               ETA: 00:37:04

################################################################################
                     [1m Learning iteration 1027/2000 [0m                     

                       Computation: 47020 steps/s (collection: 1.999s, learning 0.092s)
             Mean action noise std: 2.30
          Mean value_function loss: 152.3247
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 49.2179
                       Mean reward: 886.35
               Mean episode length: 239.01
    Episode_Reward/reaching_object: 1.3746
     Episode_Reward/lifting_object: 172.5155
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 2.09s
                      Time elapsed: 00:39:08
                               ETA: 00:37:02

################################################################################
                     [1m Learning iteration 1028/2000 [0m                     

                       Computation: 48383 steps/s (collection: 1.933s, learning 0.099s)
             Mean action noise std: 2.31
          Mean value_function loss: 127.9306
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.2298
                       Mean reward: 890.04
               Mean episode length: 239.52
    Episode_Reward/reaching_object: 1.3419
     Episode_Reward/lifting_object: 167.8555
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 2.03s
                      Time elapsed: 00:39:10
                               ETA: 00:36:59

################################################################################
                     [1m Learning iteration 1029/2000 [0m                     

                       Computation: 48043 steps/s (collection: 1.958s, learning 0.089s)
             Mean action noise std: 2.31
          Mean value_function loss: 150.1293
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 49.2414
                       Mean reward: 848.16
               Mean episode length: 231.31
    Episode_Reward/reaching_object: 1.3653
     Episode_Reward/lifting_object: 171.4394
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 2.05s
                      Time elapsed: 00:39:12
                               ETA: 00:36:57

################################################################################
                     [1m Learning iteration 1030/2000 [0m                     

                       Computation: 48989 steps/s (collection: 1.920s, learning 0.087s)
             Mean action noise std: 2.31
          Mean value_function loss: 168.2514
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 49.2519
                       Mean reward: 845.15
               Mean episode length: 229.82
    Episode_Reward/reaching_object: 1.3486
     Episode_Reward/lifting_object: 169.4525
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 2.01s
                      Time elapsed: 00:39:14
                               ETA: 00:36:54

################################################################################
                     [1m Learning iteration 1031/2000 [0m                     

                       Computation: 48362 steps/s (collection: 1.947s, learning 0.086s)
             Mean action noise std: 2.31
          Mean value_function loss: 149.8033
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 49.2630
                       Mean reward: 844.91
               Mean episode length: 228.70
    Episode_Reward/reaching_object: 1.3027
     Episode_Reward/lifting_object: 163.7328
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 2.03s
                      Time elapsed: 00:39:16
                               ETA: 00:36:52

################################################################################
                     [1m Learning iteration 1032/2000 [0m                     

                       Computation: 47580 steps/s (collection: 1.979s, learning 0.087s)
             Mean action noise std: 2.31
          Mean value_function loss: 186.0276
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 49.2740
                       Mean reward: 828.71
               Mean episode length: 227.68
    Episode_Reward/reaching_object: 1.3439
     Episode_Reward/lifting_object: 169.3183
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 2.07s
                      Time elapsed: 00:39:18
                               ETA: 00:36:49

################################################################################
                     [1m Learning iteration 1033/2000 [0m                     

                       Computation: 48069 steps/s (collection: 1.960s, learning 0.085s)
             Mean action noise std: 2.31
          Mean value_function loss: 123.4678
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 49.2905
                       Mean reward: 828.71
               Mean episode length: 226.73
    Episode_Reward/reaching_object: 1.3400
     Episode_Reward/lifting_object: 169.0759
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 2.05s
                      Time elapsed: 00:39:20
                               ETA: 00:36:47

################################################################################
                     [1m Learning iteration 1034/2000 [0m                     

                       Computation: 48340 steps/s (collection: 1.935s, learning 0.099s)
             Mean action noise std: 2.31
          Mean value_function loss: 128.0017
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 49.3058
                       Mean reward: 868.21
               Mean episode length: 235.02
    Episode_Reward/reaching_object: 1.3488
     Episode_Reward/lifting_object: 170.3557
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 2.03s
                      Time elapsed: 00:39:22
                               ETA: 00:36:44

################################################################################
                     [1m Learning iteration 1035/2000 [0m                     

                       Computation: 47962 steps/s (collection: 1.957s, learning 0.093s)
             Mean action noise std: 2.31
          Mean value_function loss: 139.1916
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 49.3152
                       Mean reward: 839.23
               Mean episode length: 228.22
    Episode_Reward/reaching_object: 1.3310
     Episode_Reward/lifting_object: 167.9988
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 2.05s
                      Time elapsed: 00:39:24
                               ETA: 00:36:42

################################################################################
                     [1m Learning iteration 1036/2000 [0m                     

                       Computation: 47539 steps/s (collection: 1.961s, learning 0.107s)
             Mean action noise std: 2.31
          Mean value_function loss: 114.6818
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 49.3176
                       Mean reward: 873.37
               Mean episode length: 235.28
    Episode_Reward/reaching_object: 1.3562
     Episode_Reward/lifting_object: 170.5556
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 2.07s
                      Time elapsed: 00:39:26
                               ETA: 00:36:39

################################################################################
                     [1m Learning iteration 1037/2000 [0m                     

                       Computation: 47240 steps/s (collection: 1.980s, learning 0.101s)
             Mean action noise std: 2.32
          Mean value_function loss: 152.1273
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 49.3225
                       Mean reward: 841.75
               Mean episode length: 227.88
    Episode_Reward/reaching_object: 1.3444
     Episode_Reward/lifting_object: 170.5137
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 2.08s
                      Time elapsed: 00:39:28
                               ETA: 00:36:37

################################################################################
                     [1m Learning iteration 1038/2000 [0m                     

                       Computation: 47561 steps/s (collection: 1.950s, learning 0.117s)
             Mean action noise std: 2.32
          Mean value_function loss: 173.6745
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 49.3323
                       Mean reward: 824.97
               Mean episode length: 223.55
    Episode_Reward/reaching_object: 1.3364
     Episode_Reward/lifting_object: 169.6718
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 2.07s
                      Time elapsed: 00:39:30
                               ETA: 00:36:34

################################################################################
                     [1m Learning iteration 1039/2000 [0m                     

                       Computation: 47504 steps/s (collection: 1.962s, learning 0.107s)
             Mean action noise std: 2.32
          Mean value_function loss: 154.3303
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 49.3420
                       Mean reward: 844.19
               Mean episode length: 228.23
    Episode_Reward/reaching_object: 1.3280
     Episode_Reward/lifting_object: 167.8228
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 2.07s
                      Time elapsed: 00:39:32
                               ETA: 00:36:32

################################################################################
                     [1m Learning iteration 1040/2000 [0m                     

                       Computation: 47217 steps/s (collection: 1.995s, learning 0.087s)
             Mean action noise std: 2.32
          Mean value_function loss: 189.4397
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 49.3564
                       Mean reward: 857.34
               Mean episode length: 232.47
    Episode_Reward/reaching_object: 1.3517
     Episode_Reward/lifting_object: 171.2939
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 2.08s
                      Time elapsed: 00:39:34
                               ETA: 00:36:29

################################################################################
                     [1m Learning iteration 1041/2000 [0m                     

                       Computation: 46962 steps/s (collection: 1.997s, learning 0.096s)
             Mean action noise std: 2.32
          Mean value_function loss: 129.1759
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 49.3730
                       Mean reward: 854.39
               Mean episode length: 231.82
    Episode_Reward/reaching_object: 1.3593
     Episode_Reward/lifting_object: 172.1614
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 2.09s
                      Time elapsed: 00:39:36
                               ETA: 00:36:27

################################################################################
                     [1m Learning iteration 1042/2000 [0m                     

                       Computation: 48390 steps/s (collection: 1.930s, learning 0.101s)
             Mean action noise std: 2.32
          Mean value_function loss: 178.4617
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 49.3853
                       Mean reward: 818.39
               Mean episode length: 223.51
    Episode_Reward/reaching_object: 1.3238
     Episode_Reward/lifting_object: 166.9661
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 2.03s
                      Time elapsed: 00:39:38
                               ETA: 00:36:24

################################################################################
                     [1m Learning iteration 1043/2000 [0m                     

                       Computation: 48125 steps/s (collection: 1.949s, learning 0.094s)
             Mean action noise std: 2.32
          Mean value_function loss: 142.7843
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 49.3989
                       Mean reward: 872.24
               Mean episode length: 234.58
    Episode_Reward/reaching_object: 1.3467
     Episode_Reward/lifting_object: 170.5672
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 2.04s
                      Time elapsed: 00:39:40
                               ETA: 00:36:22

################################################################################
                     [1m Learning iteration 1044/2000 [0m                     

                       Computation: 47926 steps/s (collection: 1.957s, learning 0.094s)
             Mean action noise std: 2.33
          Mean value_function loss: 132.4965
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 49.4162
                       Mean reward: 875.94
               Mean episode length: 236.58
    Episode_Reward/reaching_object: 1.3457
     Episode_Reward/lifting_object: 169.9252
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 2.05s
                      Time elapsed: 00:39:42
                               ETA: 00:36:19

################################################################################
                     [1m Learning iteration 1045/2000 [0m                     

                       Computation: 47982 steps/s (collection: 1.955s, learning 0.094s)
             Mean action noise std: 2.33
          Mean value_function loss: 148.2132
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 49.4329
                       Mean reward: 870.20
               Mean episode length: 236.53
    Episode_Reward/reaching_object: 1.3340
     Episode_Reward/lifting_object: 168.3627
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 2.05s
                      Time elapsed: 00:39:44
                               ETA: 00:36:17

################################################################################
                     [1m Learning iteration 1046/2000 [0m                     

                       Computation: 47256 steps/s (collection: 1.977s, learning 0.104s)
             Mean action noise std: 2.33
          Mean value_function loss: 151.9243
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.4516
                       Mean reward: 860.43
               Mean episode length: 234.47
    Episode_Reward/reaching_object: 1.3265
     Episode_Reward/lifting_object: 167.3340
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 2.08s
                      Time elapsed: 00:39:47
                               ETA: 00:36:15

################################################################################
                     [1m Learning iteration 1047/2000 [0m                     

                       Computation: 47715 steps/s (collection: 1.971s, learning 0.089s)
             Mean action noise std: 2.33
          Mean value_function loss: 158.0333
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 49.4640
                       Mean reward: 859.04
               Mean episode length: 230.60
    Episode_Reward/reaching_object: 1.3640
     Episode_Reward/lifting_object: 173.2476
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 2.06s
                      Time elapsed: 00:39:49
                               ETA: 00:36:12

################################################################################
                     [1m Learning iteration 1048/2000 [0m                     

                       Computation: 47361 steps/s (collection: 1.989s, learning 0.087s)
             Mean action noise std: 2.33
          Mean value_function loss: 134.7669
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 49.4754
                       Mean reward: 877.33
               Mean episode length: 236.14
    Episode_Reward/reaching_object: 1.3181
     Episode_Reward/lifting_object: 167.2980
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 2.08s
                      Time elapsed: 00:39:51
                               ETA: 00:36:10

################################################################################
                     [1m Learning iteration 1049/2000 [0m                     

                       Computation: 47034 steps/s (collection: 2.002s, learning 0.088s)
             Mean action noise std: 2.33
          Mean value_function loss: 178.8797
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 49.4976
                       Mean reward: 809.98
               Mean episode length: 228.55
    Episode_Reward/reaching_object: 1.3504
     Episode_Reward/lifting_object: 169.6642
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 2.09s
                      Time elapsed: 00:39:53
                               ETA: 00:36:07

################################################################################
                     [1m Learning iteration 1050/2000 [0m                     

                       Computation: 47255 steps/s (collection: 1.985s, learning 0.096s)
             Mean action noise std: 2.34
          Mean value_function loss: 123.8840
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 49.5157
                       Mean reward: 917.05
               Mean episode length: 244.40
    Episode_Reward/reaching_object: 1.3752
     Episode_Reward/lifting_object: 174.4504
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 2.08s
                      Time elapsed: 00:39:55
                               ETA: 00:36:05

################################################################################
                     [1m Learning iteration 1051/2000 [0m                     

                       Computation: 45591 steps/s (collection: 2.049s, learning 0.107s)
             Mean action noise std: 2.34
          Mean value_function loss: 149.1845
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 49.5346
                       Mean reward: 872.83
               Mean episode length: 234.63
    Episode_Reward/reaching_object: 1.3311
     Episode_Reward/lifting_object: 168.2641
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 2.16s
                      Time elapsed: 00:39:57
                               ETA: 00:36:02

################################################################################
                     [1m Learning iteration 1052/2000 [0m                     

                       Computation: 42546 steps/s (collection: 2.189s, learning 0.121s)
             Mean action noise std: 2.34
          Mean value_function loss: 169.6408
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 49.5473
                       Mean reward: 868.54
               Mean episode length: 233.34
    Episode_Reward/reaching_object: 1.2961
     Episode_Reward/lifting_object: 164.3546
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 2.31s
                      Time elapsed: 00:39:59
                               ETA: 00:36:00

################################################################################
                     [1m Learning iteration 1053/2000 [0m                     

                       Computation: 45787 steps/s (collection: 2.041s, learning 0.106s)
             Mean action noise std: 2.34
          Mean value_function loss: 133.4695
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 49.5601
                       Mean reward: 863.30
               Mean episode length: 233.03
    Episode_Reward/reaching_object: 1.3433
     Episode_Reward/lifting_object: 170.3147
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 2.15s
                      Time elapsed: 00:40:01
                               ETA: 00:35:58

################################################################################
                     [1m Learning iteration 1054/2000 [0m                     

                       Computation: 43676 steps/s (collection: 2.140s, learning 0.111s)
             Mean action noise std: 2.34
          Mean value_function loss: 149.5511
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 49.5709
                       Mean reward: 848.08
               Mean episode length: 231.13
    Episode_Reward/reaching_object: 1.3317
     Episode_Reward/lifting_object: 168.5812
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 2.25s
                      Time elapsed: 00:40:04
                               ETA: 00:35:55

################################################################################
                     [1m Learning iteration 1055/2000 [0m                     

                       Computation: 38461 steps/s (collection: 2.294s, learning 0.262s)
             Mean action noise std: 2.34
          Mean value_function loss: 157.9853
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 49.5762
                       Mean reward: 823.85
               Mean episode length: 225.40
    Episode_Reward/reaching_object: 1.3194
     Episode_Reward/lifting_object: 166.5775
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 2.56s
                      Time elapsed: 00:40:06
                               ETA: 00:35:53

################################################################################
                     [1m Learning iteration 1056/2000 [0m                     

                       Computation: 35939 steps/s (collection: 2.582s, learning 0.154s)
             Mean action noise std: 2.34
          Mean value_function loss: 120.5712
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 49.5857
                       Mean reward: 884.40
               Mean episode length: 237.68
    Episode_Reward/reaching_object: 1.3636
     Episode_Reward/lifting_object: 172.6490
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 2.74s
                      Time elapsed: 00:40:09
                               ETA: 00:35:51

################################################################################
                     [1m Learning iteration 1057/2000 [0m                     

                       Computation: 41156 steps/s (collection: 2.261s, learning 0.128s)
             Mean action noise std: 2.35
          Mean value_function loss: 167.6789
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 49.5995
                       Mean reward: 810.27
               Mean episode length: 222.22
    Episode_Reward/reaching_object: 1.3128
     Episode_Reward/lifting_object: 166.3608
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 2.39s
                      Time elapsed: 00:40:11
                               ETA: 00:35:49

################################################################################
                     [1m Learning iteration 1058/2000 [0m                     

                       Computation: 43977 steps/s (collection: 2.139s, learning 0.096s)
             Mean action noise std: 2.35
          Mean value_function loss: 138.6054
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 49.6092
                       Mean reward: 838.84
               Mean episode length: 226.51
    Episode_Reward/reaching_object: 1.3771
     Episode_Reward/lifting_object: 174.9451
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 2.24s
                      Time elapsed: 00:40:14
                               ETA: 00:35:47

################################################################################
                     [1m Learning iteration 1059/2000 [0m                     

                       Computation: 45631 steps/s (collection: 2.069s, learning 0.086s)
             Mean action noise std: 2.35
          Mean value_function loss: 135.4435
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 49.6180
                       Mean reward: 866.40
               Mean episode length: 235.37
    Episode_Reward/reaching_object: 1.3608
     Episode_Reward/lifting_object: 172.4944
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 2.15s
                      Time elapsed: 00:40:16
                               ETA: 00:35:45

################################################################################
                     [1m Learning iteration 1060/2000 [0m                     

                       Computation: 44399 steps/s (collection: 2.117s, learning 0.098s)
             Mean action noise std: 2.35
          Mean value_function loss: 139.0200
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 49.6296
                       Mean reward: 869.45
               Mean episode length: 234.73
    Episode_Reward/reaching_object: 1.3485
     Episode_Reward/lifting_object: 170.4752
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 2.21s
                      Time elapsed: 00:40:18
                               ETA: 00:35:42

################################################################################
                     [1m Learning iteration 1061/2000 [0m                     

                       Computation: 47181 steps/s (collection: 1.995s, learning 0.088s)
             Mean action noise std: 2.35
          Mean value_function loss: 181.2493
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 49.6413
                       Mean reward: 827.72
               Mean episode length: 225.59
    Episode_Reward/reaching_object: 1.3134
     Episode_Reward/lifting_object: 165.8640
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 2.08s
                      Time elapsed: 00:40:20
                               ETA: 00:35:40

################################################################################
                     [1m Learning iteration 1062/2000 [0m                     

                       Computation: 45771 steps/s (collection: 2.062s, learning 0.086s)
             Mean action noise std: 2.35
          Mean value_function loss: 174.2553
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 49.6534
                       Mean reward: 806.97
               Mean episode length: 221.02
    Episode_Reward/reaching_object: 1.3035
     Episode_Reward/lifting_object: 164.3659
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 2.15s
                      Time elapsed: 00:40:22
                               ETA: 00:35:37

################################################################################
                     [1m Learning iteration 1063/2000 [0m                     

                       Computation: 47588 steps/s (collection: 1.977s, learning 0.089s)
             Mean action noise std: 2.35
          Mean value_function loss: 149.5891
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 49.6633
                       Mean reward: 888.05
               Mean episode length: 238.33
    Episode_Reward/reaching_object: 1.3423
     Episode_Reward/lifting_object: 169.7240
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 2.07s
                      Time elapsed: 00:40:24
                               ETA: 00:35:35

################################################################################
                     [1m Learning iteration 1064/2000 [0m                     

                       Computation: 47674 steps/s (collection: 1.964s, learning 0.098s)
             Mean action noise std: 2.35
          Mean value_function loss: 148.5171
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 49.6742
                       Mean reward: 860.26
               Mean episode length: 233.28
    Episode_Reward/reaching_object: 1.3320
     Episode_Reward/lifting_object: 167.7946
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 2.06s
                      Time elapsed: 00:40:26
                               ETA: 00:35:32

################################################################################
                     [1m Learning iteration 1065/2000 [0m                     

                       Computation: 47088 steps/s (collection: 1.971s, learning 0.117s)
             Mean action noise std: 2.35
          Mean value_function loss: 140.0797
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 49.6791
                       Mean reward: 845.19
               Mean episode length: 228.51
    Episode_Reward/reaching_object: 1.3695
     Episode_Reward/lifting_object: 172.0022
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 2.09s
                      Time elapsed: 00:40:28
                               ETA: 00:35:30

################################################################################
                     [1m Learning iteration 1066/2000 [0m                     

                       Computation: 46839 steps/s (collection: 1.973s, learning 0.126s)
             Mean action noise std: 2.36
          Mean value_function loss: 123.5519
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 49.6895
                       Mean reward: 861.96
               Mean episode length: 232.48
    Episode_Reward/reaching_object: 1.3430
     Episode_Reward/lifting_object: 169.2315
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 2.10s
                      Time elapsed: 00:40:31
                               ETA: 00:35:28

################################################################################
                     [1m Learning iteration 1067/2000 [0m                     

                       Computation: 47781 steps/s (collection: 1.941s, learning 0.117s)
             Mean action noise std: 2.36
          Mean value_function loss: 98.2957
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 49.7104
                       Mean reward: 889.25
               Mean episode length: 241.01
    Episode_Reward/reaching_object: 1.3656
     Episode_Reward/lifting_object: 172.0749
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 2.06s
                      Time elapsed: 00:40:33
                               ETA: 00:35:25

################################################################################
                     [1m Learning iteration 1068/2000 [0m                     

                       Computation: 47233 steps/s (collection: 1.990s, learning 0.092s)
             Mean action noise std: 2.36
          Mean value_function loss: 122.2232
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 49.7282
                       Mean reward: 876.37
               Mean episode length: 235.72
    Episode_Reward/reaching_object: 1.3636
     Episode_Reward/lifting_object: 171.7623
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 2.08s
                      Time elapsed: 00:40:35
                               ETA: 00:35:23

################################################################################
                     [1m Learning iteration 1069/2000 [0m                     

                       Computation: 47395 steps/s (collection: 1.986s, learning 0.088s)
             Mean action noise std: 2.36
          Mean value_function loss: 145.2264
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 49.7398
                       Mean reward: 822.31
               Mean episode length: 223.09
    Episode_Reward/reaching_object: 1.3336
     Episode_Reward/lifting_object: 167.0111
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 2.07s
                      Time elapsed: 00:40:37
                               ETA: 00:35:20

################################################################################
                     [1m Learning iteration 1070/2000 [0m                     

                       Computation: 45926 steps/s (collection: 2.054s, learning 0.086s)
             Mean action noise std: 2.36
          Mean value_function loss: 115.4855
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 49.7478
                       Mean reward: 889.93
               Mean episode length: 239.88
    Episode_Reward/reaching_object: 1.3946
     Episode_Reward/lifting_object: 174.7968
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 2.14s
                      Time elapsed: 00:40:39
                               ETA: 00:35:18

################################################################################
                     [1m Learning iteration 1071/2000 [0m                     

                       Computation: 45963 steps/s (collection: 2.046s, learning 0.093s)
             Mean action noise std: 2.36
          Mean value_function loss: 115.2321
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 49.7569
                       Mean reward: 864.99
               Mean episode length: 232.39
    Episode_Reward/reaching_object: 1.3666
     Episode_Reward/lifting_object: 171.8368
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 2.14s
                      Time elapsed: 00:40:41
                               ETA: 00:35:15

################################################################################
                     [1m Learning iteration 1072/2000 [0m                     

                       Computation: 45151 steps/s (collection: 2.085s, learning 0.093s)
             Mean action noise std: 2.36
          Mean value_function loss: 92.4808
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 49.7655
                       Mean reward: 886.67
               Mean episode length: 239.29
    Episode_Reward/reaching_object: 1.3873
     Episode_Reward/lifting_object: 174.3881
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 2.18s
                      Time elapsed: 00:40:43
                               ETA: 00:35:13

################################################################################
                     [1m Learning iteration 1073/2000 [0m                     

                       Computation: 43699 steps/s (collection: 2.156s, learning 0.094s)
             Mean action noise std: 2.37
          Mean value_function loss: 115.1341
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 49.7834
                       Mean reward: 845.42
               Mean episode length: 230.03
    Episode_Reward/reaching_object: 1.3705
     Episode_Reward/lifting_object: 171.4681
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 2.25s
                      Time elapsed: 00:40:45
                               ETA: 00:35:11

################################################################################
                     [1m Learning iteration 1074/2000 [0m                     

                       Computation: 44975 steps/s (collection: 2.089s, learning 0.097s)
             Mean action noise std: 2.37
          Mean value_function loss: 115.7973
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 49.8024
                       Mean reward: 842.50
               Mean episode length: 227.63
    Episode_Reward/reaching_object: 1.3721
     Episode_Reward/lifting_object: 172.1559
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 2.19s
                      Time elapsed: 00:40:48
                               ETA: 00:35:08

################################################################################
                     [1m Learning iteration 1075/2000 [0m                     

                       Computation: 34088 steps/s (collection: 2.595s, learning 0.289s)
             Mean action noise std: 2.37
          Mean value_function loss: 115.6980
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 49.8166
                       Mean reward: 865.66
               Mean episode length: 234.12
    Episode_Reward/reaching_object: 1.3856
     Episode_Reward/lifting_object: 174.0714
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 2.88s
                      Time elapsed: 00:40:51
                               ETA: 00:35:07

################################################################################
                     [1m Learning iteration 1076/2000 [0m                     

                       Computation: 32801 steps/s (collection: 2.786s, learning 0.211s)
             Mean action noise std: 2.37
          Mean value_function loss: 131.3376
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 49.8317
                       Mean reward: 809.66
               Mean episode length: 220.32
    Episode_Reward/reaching_object: 1.3395
     Episode_Reward/lifting_object: 167.2548
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 3.00s
                      Time elapsed: 00:40:54
                               ETA: 00:35:05

################################################################################
                     [1m Learning iteration 1077/2000 [0m                     

                       Computation: 43017 steps/s (collection: 2.195s, learning 0.090s)
             Mean action noise std: 2.37
          Mean value_function loss: 117.1058
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 49.8476
                       Mean reward: 859.74
               Mean episode length: 231.13
    Episode_Reward/reaching_object: 1.3903
     Episode_Reward/lifting_object: 174.3427
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 2.29s
                      Time elapsed: 00:40:56
                               ETA: 00:35:03

################################################################################
                     [1m Learning iteration 1078/2000 [0m                     

                       Computation: 46314 steps/s (collection: 2.008s, learning 0.114s)
             Mean action noise std: 2.37
          Mean value_function loss: 150.6639
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.8539
                       Mean reward: 856.03
               Mean episode length: 232.17
    Episode_Reward/reaching_object: 1.3324
     Episode_Reward/lifting_object: 166.3832
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 2.12s
                      Time elapsed: 00:40:58
                               ETA: 00:35:00

################################################################################
                     [1m Learning iteration 1079/2000 [0m                     

                       Computation: 46670 steps/s (collection: 1.997s, learning 0.109s)
             Mean action noise std: 2.38
          Mean value_function loss: 125.4421
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 49.8638
                       Mean reward: 883.36
               Mean episode length: 237.90
    Episode_Reward/reaching_object: 1.3717
     Episode_Reward/lifting_object: 171.9152
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 2.11s
                      Time elapsed: 00:41:00
                               ETA: 00:34:58

################################################################################
                     [1m Learning iteration 1080/2000 [0m                     

                       Computation: 46447 steps/s (collection: 2.004s, learning 0.113s)
             Mean action noise std: 2.38
          Mean value_function loss: 138.9867
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 49.8776
                       Mean reward: 857.68
               Mean episode length: 230.15
    Episode_Reward/reaching_object: 1.3854
     Episode_Reward/lifting_object: 173.8776
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 2.12s
                      Time elapsed: 00:41:02
                               ETA: 00:34:55

################################################################################
                     [1m Learning iteration 1081/2000 [0m                     

                       Computation: 48024 steps/s (collection: 1.950s, learning 0.097s)
             Mean action noise std: 2.38
          Mean value_function loss: 105.8461
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 49.8922
                       Mean reward: 887.85
               Mean episode length: 238.84
    Episode_Reward/reaching_object: 1.3906
     Episode_Reward/lifting_object: 174.2756
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 2.05s
                      Time elapsed: 00:41:04
                               ETA: 00:34:53

################################################################################
                     [1m Learning iteration 1082/2000 [0m                     

                       Computation: 47766 steps/s (collection: 1.954s, learning 0.104s)
             Mean action noise std: 2.38
          Mean value_function loss: 111.4859
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 49.9087
                       Mean reward: 885.78
               Mean episode length: 238.79
    Episode_Reward/reaching_object: 1.3895
     Episode_Reward/lifting_object: 174.2210
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 2.06s
                      Time elapsed: 00:41:06
                               ETA: 00:34:50

################################################################################
                     [1m Learning iteration 1083/2000 [0m                     

                       Computation: 47705 steps/s (collection: 1.958s, learning 0.103s)
             Mean action noise std: 2.38
          Mean value_function loss: 133.1876
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 49.9231
                       Mean reward: 861.67
               Mean episode length: 233.98
    Episode_Reward/reaching_object: 1.3575
     Episode_Reward/lifting_object: 170.5033
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 2.06s
                      Time elapsed: 00:41:08
                               ETA: 00:34:48

################################################################################
                     [1m Learning iteration 1084/2000 [0m                     

                       Computation: 46630 steps/s (collection: 2.004s, learning 0.105s)
             Mean action noise std: 2.38
          Mean value_function loss: 136.8083
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 49.9327
                       Mean reward: 843.77
               Mean episode length: 227.96
    Episode_Reward/reaching_object: 1.3393
     Episode_Reward/lifting_object: 168.5269
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 2.11s
                      Time elapsed: 00:41:10
                               ETA: 00:34:46

################################################################################
                     [1m Learning iteration 1085/2000 [0m                     

                       Computation: 47214 steps/s (collection: 1.982s, learning 0.100s)
             Mean action noise std: 2.38
          Mean value_function loss: 122.0400
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 49.9422
                       Mean reward: 897.25
               Mean episode length: 239.81
    Episode_Reward/reaching_object: 1.3975
     Episode_Reward/lifting_object: 176.6766
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 2.08s
                      Time elapsed: 00:41:13
                               ETA: 00:34:43

################################################################################
                     [1m Learning iteration 1086/2000 [0m                     

                       Computation: 46809 steps/s (collection: 1.992s, learning 0.108s)
             Mean action noise std: 2.39
          Mean value_function loss: 97.2490
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 49.9533
                       Mean reward: 905.28
               Mean episode length: 240.88
    Episode_Reward/reaching_object: 1.3888
     Episode_Reward/lifting_object: 175.9279
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 2.10s
                      Time elapsed: 00:41:15
                               ETA: 00:34:41

################################################################################
                     [1m Learning iteration 1087/2000 [0m                     

                       Computation: 46594 steps/s (collection: 2.018s, learning 0.092s)
             Mean action noise std: 2.39
          Mean value_function loss: 100.4549
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 49.9716
                       Mean reward: 877.31
               Mean episode length: 236.38
    Episode_Reward/reaching_object: 1.3699
     Episode_Reward/lifting_object: 173.5362
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 2.11s
                      Time elapsed: 00:41:17
                               ETA: 00:34:38

################################################################################
                     [1m Learning iteration 1088/2000 [0m                     

                       Computation: 47281 steps/s (collection: 1.989s, learning 0.090s)
             Mean action noise std: 2.39
          Mean value_function loss: 103.8445
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 49.9842
                       Mean reward: 865.08
               Mean episode length: 232.43
    Episode_Reward/reaching_object: 1.3605
     Episode_Reward/lifting_object: 171.9644
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 2.08s
                      Time elapsed: 00:41:19
                               ETA: 00:34:36

################################################################################
                     [1m Learning iteration 1089/2000 [0m                     

                       Computation: 46696 steps/s (collection: 2.015s, learning 0.090s)
             Mean action noise std: 2.39
          Mean value_function loss: 110.3740
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 49.9990
                       Mean reward: 862.71
               Mean episode length: 232.89
    Episode_Reward/reaching_object: 1.3725
     Episode_Reward/lifting_object: 172.9078
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 2.11s
                      Time elapsed: 00:41:21
                               ETA: 00:34:33

################################################################################
                     [1m Learning iteration 1090/2000 [0m                     

                       Computation: 47633 steps/s (collection: 1.976s, learning 0.088s)
             Mean action noise std: 2.39
          Mean value_function loss: 110.9426
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 50.0198
                       Mean reward: 881.77
               Mean episode length: 236.34
    Episode_Reward/reaching_object: 1.3609
     Episode_Reward/lifting_object: 170.4288
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 2.06s
                      Time elapsed: 00:41:23
                               ETA: 00:34:31

################################################################################
                     [1m Learning iteration 1091/2000 [0m                     

                       Computation: 46707 steps/s (collection: 2.010s, learning 0.095s)
             Mean action noise std: 2.39
          Mean value_function loss: 95.0401
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 50.0345
                       Mean reward: 897.15
               Mean episode length: 240.50
    Episode_Reward/reaching_object: 1.3863
     Episode_Reward/lifting_object: 174.9909
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 2.10s
                      Time elapsed: 00:41:25
                               ETA: 00:34:29

################################################################################
                     [1m Learning iteration 1092/2000 [0m                     

                       Computation: 47142 steps/s (collection: 1.994s, learning 0.092s)
             Mean action noise std: 2.39
          Mean value_function loss: 104.7433
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 50.0403
                       Mean reward: 887.47
               Mean episode length: 240.21
    Episode_Reward/reaching_object: 1.3899
     Episode_Reward/lifting_object: 175.5673
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 2.09s
                      Time elapsed: 00:41:27
                               ETA: 00:34:26

################################################################################
                     [1m Learning iteration 1093/2000 [0m                     

                       Computation: 46605 steps/s (collection: 1.999s, learning 0.111s)
             Mean action noise std: 2.40
          Mean value_function loss: 119.4039
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 50.0503
                       Mean reward: 852.92
               Mean episode length: 231.66
    Episode_Reward/reaching_object: 1.3793
     Episode_Reward/lifting_object: 174.5019
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 2.11s
                      Time elapsed: 00:41:29
                               ETA: 00:34:24

################################################################################
                     [1m Learning iteration 1094/2000 [0m                     

                       Computation: 47167 steps/s (collection: 1.977s, learning 0.107s)
             Mean action noise std: 2.40
          Mean value_function loss: 111.1400
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 50.0675
                       Mean reward: 867.86
               Mean episode length: 233.00
    Episode_Reward/reaching_object: 1.3635
     Episode_Reward/lifting_object: 172.2622
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 2.08s
                      Time elapsed: 00:41:31
                               ETA: 00:34:21

################################################################################
                     [1m Learning iteration 1095/2000 [0m                     

                       Computation: 47125 steps/s (collection: 1.991s, learning 0.095s)
             Mean action noise std: 2.40
          Mean value_function loss: 96.0955
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 50.0838
                       Mean reward: 909.98
               Mean episode length: 243.44
    Episode_Reward/reaching_object: 1.3383
     Episode_Reward/lifting_object: 168.9306
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 2.09s
                      Time elapsed: 00:41:33
                               ETA: 00:34:19

################################################################################
                     [1m Learning iteration 1096/2000 [0m                     

                       Computation: 47602 steps/s (collection: 1.978s, learning 0.088s)
             Mean action noise std: 2.40
          Mean value_function loss: 88.8218
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 50.0920
                       Mean reward: 887.56
               Mean episode length: 237.79
    Episode_Reward/reaching_object: 1.4000
     Episode_Reward/lifting_object: 177.4530
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 2.07s
                      Time elapsed: 00:41:36
                               ETA: 00:34:16

################################################################################
                     [1m Learning iteration 1097/2000 [0m                     

                       Computation: 47295 steps/s (collection: 1.973s, learning 0.106s)
             Mean action noise std: 2.40
          Mean value_function loss: 111.6850
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 50.1110
                       Mean reward: 858.46
               Mean episode length: 232.71
    Episode_Reward/reaching_object: 1.3719
     Episode_Reward/lifting_object: 172.9884
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 2.08s
                      Time elapsed: 00:41:38
                               ETA: 00:34:14

################################################################################
                     [1m Learning iteration 1098/2000 [0m                     

                       Computation: 47251 steps/s (collection: 1.990s, learning 0.091s)
             Mean action noise std: 2.41
          Mean value_function loss: 115.2213
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 50.1312
                       Mean reward: 844.77
               Mean episode length: 229.88
    Episode_Reward/reaching_object: 1.3752
     Episode_Reward/lifting_object: 173.8156
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 2.08s
                      Time elapsed: 00:41:40
                               ETA: 00:34:12

################################################################################
                     [1m Learning iteration 1099/2000 [0m                     

                       Computation: 47496 steps/s (collection: 1.982s, learning 0.088s)
             Mean action noise std: 2.41
          Mean value_function loss: 103.6323
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 50.1441
                       Mean reward: 862.65
               Mean episode length: 231.95
    Episode_Reward/reaching_object: 1.3595
     Episode_Reward/lifting_object: 172.4034
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 2.07s
                      Time elapsed: 00:41:42
                               ETA: 00:34:09

################################################################################
                     [1m Learning iteration 1100/2000 [0m                     

                       Computation: 48341 steps/s (collection: 1.941s, learning 0.092s)
             Mean action noise std: 2.41
          Mean value_function loss: 85.6458
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 50.1543
                       Mean reward: 885.10
               Mean episode length: 237.49
    Episode_Reward/reaching_object: 1.3780
     Episode_Reward/lifting_object: 174.5816
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 2.03s
                      Time elapsed: 00:41:44
                               ETA: 00:34:07

################################################################################
                     [1m Learning iteration 1101/2000 [0m                     

                       Computation: 46325 steps/s (collection: 2.035s, learning 0.087s)
             Mean action noise std: 2.41
          Mean value_function loss: 107.3640
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 50.1646
                       Mean reward: 858.95
               Mean episode length: 231.93
    Episode_Reward/reaching_object: 1.3749
     Episode_Reward/lifting_object: 173.6632
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 2.12s
                      Time elapsed: 00:41:46
                               ETA: 00:34:04

################################################################################
                     [1m Learning iteration 1102/2000 [0m                     

                       Computation: 45556 steps/s (collection: 2.064s, learning 0.094s)
             Mean action noise std: 2.41
          Mean value_function loss: 120.7804
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 50.1755
                       Mean reward: 851.98
               Mean episode length: 230.11
    Episode_Reward/reaching_object: 1.3744
     Episode_Reward/lifting_object: 173.4537
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 2.16s
                      Time elapsed: 00:41:48
                               ETA: 00:34:02

################################################################################
                     [1m Learning iteration 1103/2000 [0m                     

                       Computation: 46611 steps/s (collection: 2.020s, learning 0.089s)
             Mean action noise std: 2.41
          Mean value_function loss: 107.4630
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 50.1922
                       Mean reward: 892.68
               Mean episode length: 239.55
    Episode_Reward/reaching_object: 1.3592
     Episode_Reward/lifting_object: 170.5952
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 2.11s
                      Time elapsed: 00:41:50
                               ETA: 00:33:59

################################################################################
                     [1m Learning iteration 1104/2000 [0m                     

                       Computation: 47428 steps/s (collection: 1.983s, learning 0.090s)
             Mean action noise std: 2.41
          Mean value_function loss: 110.1070
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 50.2079
                       Mean reward: 887.28
               Mean episode length: 238.10
    Episode_Reward/reaching_object: 1.3787
     Episode_Reward/lifting_object: 173.8688
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 2.07s
                      Time elapsed: 00:41:52
                               ETA: 00:33:57

################################################################################
                     [1m Learning iteration 1105/2000 [0m                     

                       Computation: 46443 steps/s (collection: 2.017s, learning 0.100s)
             Mean action noise std: 2.41
          Mean value_function loss: 89.2907
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 50.2195
                       Mean reward: 874.04
               Mean episode length: 234.42
    Episode_Reward/reaching_object: 1.3941
     Episode_Reward/lifting_object: 176.2513
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 2.12s
                      Time elapsed: 00:41:54
                               ETA: 00:33:55

################################################################################
                     [1m Learning iteration 1106/2000 [0m                     

                       Computation: 46642 steps/s (collection: 2.006s, learning 0.101s)
             Mean action noise std: 2.42
          Mean value_function loss: 129.6513
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 50.2269
                       Mean reward: 851.90
               Mean episode length: 229.09
    Episode_Reward/reaching_object: 1.3668
     Episode_Reward/lifting_object: 173.2440
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 2.11s
                      Time elapsed: 00:41:56
                               ETA: 00:33:52

################################################################################
                     [1m Learning iteration 1107/2000 [0m                     

                       Computation: 47121 steps/s (collection: 1.985s, learning 0.101s)
             Mean action noise std: 2.42
          Mean value_function loss: 118.7916
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 50.2328
                       Mean reward: 857.77
               Mean episode length: 231.63
    Episode_Reward/reaching_object: 1.3444
     Episode_Reward/lifting_object: 169.8064
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 2.09s
                      Time elapsed: 00:41:59
                               ETA: 00:33:50

################################################################################
                     [1m Learning iteration 1108/2000 [0m                     

                       Computation: 47349 steps/s (collection: 1.978s, learning 0.099s)
             Mean action noise std: 2.42
          Mean value_function loss: 132.6230
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 50.2441
                       Mean reward: 847.02
               Mean episode length: 228.39
    Episode_Reward/reaching_object: 1.3802
     Episode_Reward/lifting_object: 174.5765
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 2.08s
                      Time elapsed: 00:42:01
                               ETA: 00:33:47

################################################################################
                     [1m Learning iteration 1109/2000 [0m                     

                       Computation: 46762 steps/s (collection: 1.998s, learning 0.105s)
             Mean action noise std: 2.42
          Mean value_function loss: 126.6618
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 50.2572
                       Mean reward: 881.03
               Mean episode length: 235.66
    Episode_Reward/reaching_object: 1.3722
     Episode_Reward/lifting_object: 174.0359
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 2.10s
                      Time elapsed: 00:42:03
                               ETA: 00:33:45

################################################################################
                     [1m Learning iteration 1110/2000 [0m                     

                       Computation: 47487 steps/s (collection: 1.983s, learning 0.087s)
             Mean action noise std: 2.42
          Mean value_function loss: 124.5275
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 50.2665
                       Mean reward: 883.77
               Mean episode length: 236.58
    Episode_Reward/reaching_object: 1.3743
     Episode_Reward/lifting_object: 174.1861
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 2.07s
                      Time elapsed: 00:42:05
                               ETA: 00:33:42

################################################################################
                     [1m Learning iteration 1111/2000 [0m                     

                       Computation: 47849 steps/s (collection: 1.964s, learning 0.090s)
             Mean action noise std: 2.42
          Mean value_function loss: 123.0250
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.2730
                       Mean reward: 840.58
               Mean episode length: 227.39
    Episode_Reward/reaching_object: 1.3383
     Episode_Reward/lifting_object: 169.2624
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 2.05s
                      Time elapsed: 00:42:07
                               ETA: 00:33:40

################################################################################
                     [1m Learning iteration 1112/2000 [0m                     

                       Computation: 46474 steps/s (collection: 2.014s, learning 0.101s)
             Mean action noise std: 2.42
          Mean value_function loss: 115.4531
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 50.2805
                       Mean reward: 848.96
               Mean episode length: 229.80
    Episode_Reward/reaching_object: 1.3711
     Episode_Reward/lifting_object: 174.0735
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 2.12s
                      Time elapsed: 00:42:09
                               ETA: 00:33:38

################################################################################
                     [1m Learning iteration 1113/2000 [0m                     

                       Computation: 47056 steps/s (collection: 1.990s, learning 0.099s)
             Mean action noise std: 2.42
          Mean value_function loss: 128.3961
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 50.2914
                       Mean reward: 901.52
               Mean episode length: 240.92
    Episode_Reward/reaching_object: 1.3357
     Episode_Reward/lifting_object: 169.7992
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 2.09s
                      Time elapsed: 00:42:11
                               ETA: 00:33:35

################################################################################
                     [1m Learning iteration 1114/2000 [0m                     

                       Computation: 46242 steps/s (collection: 2.031s, learning 0.095s)
             Mean action noise std: 2.42
          Mean value_function loss: 105.5791
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 50.3021
                       Mean reward: 868.43
               Mean episode length: 233.84
    Episode_Reward/reaching_object: 1.3418
     Episode_Reward/lifting_object: 170.3190
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 2.13s
                      Time elapsed: 00:42:13
                               ETA: 00:33:33

################################################################################
                     [1m Learning iteration 1115/2000 [0m                     

                       Computation: 47363 steps/s (collection: 1.987s, learning 0.089s)
             Mean action noise std: 2.42
          Mean value_function loss: 89.8453
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 50.3044
                       Mean reward: 883.72
               Mean episode length: 239.06
    Episode_Reward/reaching_object: 1.3602
     Episode_Reward/lifting_object: 172.8564
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 2.08s
                      Time elapsed: 00:42:15
                               ETA: 00:33:30

################################################################################
                     [1m Learning iteration 1116/2000 [0m                     

                       Computation: 46764 steps/s (collection: 2.013s, learning 0.090s)
             Mean action noise std: 2.43
          Mean value_function loss: 126.5467
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.3105
                       Mean reward: 873.39
               Mean episode length: 233.75
    Episode_Reward/reaching_object: 1.3446
     Episode_Reward/lifting_object: 170.6807
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 2.10s
                      Time elapsed: 00:42:17
                               ETA: 00:33:28

################################################################################
                     [1m Learning iteration 1117/2000 [0m                     

                       Computation: 46697 steps/s (collection: 2.007s, learning 0.098s)
             Mean action noise std: 2.43
          Mean value_function loss: 124.9350
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.3231
                       Mean reward: 840.35
               Mean episode length: 227.13
    Episode_Reward/reaching_object: 1.3420
     Episode_Reward/lifting_object: 170.0824
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 2.11s
                      Time elapsed: 00:42:19
                               ETA: 00:33:26

################################################################################
                     [1m Learning iteration 1118/2000 [0m                     

                       Computation: 45050 steps/s (collection: 2.088s, learning 0.094s)
             Mean action noise std: 2.43
          Mean value_function loss: 122.8709
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 50.3381
                       Mean reward: 819.99
               Mean episode length: 221.99
    Episode_Reward/reaching_object: 1.3489
     Episode_Reward/lifting_object: 171.3036
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 2.18s
                      Time elapsed: 00:42:22
                               ETA: 00:33:23

################################################################################
                     [1m Learning iteration 1119/2000 [0m                     

                       Computation: 44797 steps/s (collection: 2.059s, learning 0.136s)
             Mean action noise std: 2.43
          Mean value_function loss: 107.6189
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 50.3474
                       Mean reward: 853.76
               Mean episode length: 230.94
    Episode_Reward/reaching_object: 1.3747
     Episode_Reward/lifting_object: 174.0648
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 2.19s
                      Time elapsed: 00:42:24
                               ETA: 00:33:21

################################################################################
                     [1m Learning iteration 1120/2000 [0m                     

                       Computation: 45256 steps/s (collection: 2.078s, learning 0.095s)
             Mean action noise std: 2.43
          Mean value_function loss: 158.3858
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 50.3597
                       Mean reward: 922.68
               Mean episode length: 245.04
    Episode_Reward/reaching_object: 1.3721
     Episode_Reward/lifting_object: 174.1255
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 2.17s
                      Time elapsed: 00:42:26
                               ETA: 00:33:19

################################################################################
                     [1m Learning iteration 1121/2000 [0m                     

                       Computation: 45153 steps/s (collection: 2.080s, learning 0.097s)
             Mean action noise std: 2.43
          Mean value_function loss: 121.9214
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 50.3694
                       Mean reward: 888.30
               Mean episode length: 238.59
    Episode_Reward/reaching_object: 1.3683
     Episode_Reward/lifting_object: 171.9986
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 2.18s
                      Time elapsed: 00:42:28
                               ETA: 00:33:16

################################################################################
                     [1m Learning iteration 1122/2000 [0m                     

                       Computation: 47039 steps/s (collection: 2.005s, learning 0.085s)
             Mean action noise std: 2.43
          Mean value_function loss: 96.6867
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 50.3839
                       Mean reward: 886.28
               Mean episode length: 237.70
    Episode_Reward/reaching_object: 1.3616
     Episode_Reward/lifting_object: 171.7916
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 2.09s
                      Time elapsed: 00:42:30
                               ETA: 00:33:14

################################################################################
                     [1m Learning iteration 1123/2000 [0m                     

                       Computation: 47157 steps/s (collection: 1.993s, learning 0.092s)
             Mean action noise std: 2.43
          Mean value_function loss: 110.1189
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 50.3959
                       Mean reward: 885.85
               Mean episode length: 237.81
    Episode_Reward/reaching_object: 1.3922
     Episode_Reward/lifting_object: 175.4633
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 2.08s
                      Time elapsed: 00:42:32
                               ETA: 00:33:11

################################################################################
                     [1m Learning iteration 1124/2000 [0m                     

                       Computation: 46835 steps/s (collection: 2.011s, learning 0.088s)
             Mean action noise std: 2.44
          Mean value_function loss: 148.6995
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.4028
                       Mean reward: 884.14
               Mean episode length: 235.53
    Episode_Reward/reaching_object: 1.3624
     Episode_Reward/lifting_object: 171.8027
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 2.10s
                      Time elapsed: 00:42:34
                               ETA: 00:33:09

################################################################################
                     [1m Learning iteration 1125/2000 [0m                     

                       Computation: 45773 steps/s (collection: 2.046s, learning 0.102s)
             Mean action noise std: 2.44
          Mean value_function loss: 117.8661
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 50.4160
                       Mean reward: 890.51
               Mean episode length: 239.48
    Episode_Reward/reaching_object: 1.3539
     Episode_Reward/lifting_object: 170.2080
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 2.15s
                      Time elapsed: 00:42:37
                               ETA: 00:33:07

################################################################################
                     [1m Learning iteration 1126/2000 [0m                     

                       Computation: 46131 steps/s (collection: 2.036s, learning 0.095s)
             Mean action noise std: 2.44
          Mean value_function loss: 96.8527
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 50.4309
                       Mean reward: 881.62
               Mean episode length: 236.72
    Episode_Reward/reaching_object: 1.3820
     Episode_Reward/lifting_object: 174.9927
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 2.13s
                      Time elapsed: 00:42:39
                               ETA: 00:33:04

################################################################################
                     [1m Learning iteration 1127/2000 [0m                     

                       Computation: 47467 steps/s (collection: 1.982s, learning 0.089s)
             Mean action noise std: 2.44
          Mean value_function loss: 103.2897
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 50.4422
                       Mean reward: 848.96
               Mean episode length: 229.13
    Episode_Reward/reaching_object: 1.3889
     Episode_Reward/lifting_object: 174.7076
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 2.07s
                      Time elapsed: 00:42:41
                               ETA: 00:33:02

################################################################################
                     [1m Learning iteration 1128/2000 [0m                     

                       Computation: 44001 steps/s (collection: 2.118s, learning 0.116s)
             Mean action noise std: 2.44
          Mean value_function loss: 108.4282
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 50.4531
                       Mean reward: 884.60
               Mean episode length: 236.32
    Episode_Reward/reaching_object: 1.4081
     Episode_Reward/lifting_object: 177.0298
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 2.23s
                      Time elapsed: 00:42:43
                               ETA: 00:33:00

################################################################################
                     [1m Learning iteration 1129/2000 [0m                     

                       Computation: 44375 steps/s (collection: 2.108s, learning 0.107s)
             Mean action noise std: 2.44
          Mean value_function loss: 125.3728
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 50.4642
                       Mean reward: 830.10
               Mean episode length: 223.05
    Episode_Reward/reaching_object: 1.3486
     Episode_Reward/lifting_object: 169.3515
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 2.22s
                      Time elapsed: 00:42:45
                               ETA: 00:32:57

################################################################################
                     [1m Learning iteration 1130/2000 [0m                     

                       Computation: 47245 steps/s (collection: 1.991s, learning 0.090s)
             Mean action noise std: 2.44
          Mean value_function loss: 111.4989
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.4768
                       Mean reward: 860.08
               Mean episode length: 231.18
    Episode_Reward/reaching_object: 1.3690
     Episode_Reward/lifting_object: 172.7614
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 2.08s
                      Time elapsed: 00:42:47
                               ETA: 00:32:55

################################################################################
                     [1m Learning iteration 1131/2000 [0m                     

                       Computation: 45406 steps/s (collection: 2.071s, learning 0.094s)
             Mean action noise std: 2.44
          Mean value_function loss: 108.5833
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.4814
                       Mean reward: 826.09
               Mean episode length: 223.58
    Episode_Reward/reaching_object: 1.3594
     Episode_Reward/lifting_object: 171.1684
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 2.16s
                      Time elapsed: 00:42:50
                               ETA: 00:32:52

################################################################################
                     [1m Learning iteration 1132/2000 [0m                     

                       Computation: 46114 steps/s (collection: 2.025s, learning 0.107s)
             Mean action noise std: 2.45
          Mean value_function loss: 108.6053
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 50.4884
                       Mean reward: 882.85
               Mean episode length: 236.61
    Episode_Reward/reaching_object: 1.3710
     Episode_Reward/lifting_object: 172.6724
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 2.13s
                      Time elapsed: 00:42:52
                               ETA: 00:32:50

################################################################################
                     [1m Learning iteration 1133/2000 [0m                     

                       Computation: 47100 steps/s (collection: 1.997s, learning 0.091s)
             Mean action noise std: 2.45
          Mean value_function loss: 105.6268
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.5019
                       Mean reward: 874.02
               Mean episode length: 234.20
    Episode_Reward/reaching_object: 1.3738
     Episode_Reward/lifting_object: 173.1445
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 2.09s
                      Time elapsed: 00:42:54
                               ETA: 00:32:48

################################################################################
                     [1m Learning iteration 1134/2000 [0m                     

                       Computation: 47442 steps/s (collection: 1.980s, learning 0.093s)
             Mean action noise std: 2.45
          Mean value_function loss: 124.2796
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 50.5125
                       Mean reward: 860.97
               Mean episode length: 232.23
    Episode_Reward/reaching_object: 1.3659
     Episode_Reward/lifting_object: 172.8541
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 2.07s
                      Time elapsed: 00:42:56
                               ETA: 00:32:45

################################################################################
                     [1m Learning iteration 1135/2000 [0m                     

                       Computation: 47234 steps/s (collection: 1.987s, learning 0.094s)
             Mean action noise std: 2.45
          Mean value_function loss: 108.3339
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 50.5215
                       Mean reward: 869.94
               Mean episode length: 233.20
    Episode_Reward/reaching_object: 1.3782
     Episode_Reward/lifting_object: 173.9839
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 2.08s
                      Time elapsed: 00:42:58
                               ETA: 00:32:43

################################################################################
                     [1m Learning iteration 1136/2000 [0m                     

                       Computation: 47139 steps/s (collection: 1.987s, learning 0.098s)
             Mean action noise std: 2.45
          Mean value_function loss: 92.5811
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.5277
                       Mean reward: 881.58
               Mean episode length: 237.10
    Episode_Reward/reaching_object: 1.3743
     Episode_Reward/lifting_object: 173.4119
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 2.09s
                      Time elapsed: 00:43:00
                               ETA: 00:32:40

################################################################################
                     [1m Learning iteration 1137/2000 [0m                     

                       Computation: 46934 steps/s (collection: 1.994s, learning 0.101s)
             Mean action noise std: 2.45
          Mean value_function loss: 105.4676
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.5397
                       Mean reward: 886.72
               Mean episode length: 237.64
    Episode_Reward/reaching_object: 1.3977
     Episode_Reward/lifting_object: 177.4100
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 2.09s
                      Time elapsed: 00:43:02
                               ETA: 00:32:38

################################################################################
                     [1m Learning iteration 1138/2000 [0m                     

                       Computation: 45759 steps/s (collection: 2.057s, learning 0.092s)
             Mean action noise std: 2.45
          Mean value_function loss: 117.7236
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 50.5527
                       Mean reward: 912.08
               Mean episode length: 243.84
    Episode_Reward/reaching_object: 1.3885
     Episode_Reward/lifting_object: 176.5946
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 2.15s
                      Time elapsed: 00:43:04
                               ETA: 00:32:36

################################################################################
                     [1m Learning iteration 1139/2000 [0m                     

                       Computation: 46598 steps/s (collection: 2.018s, learning 0.092s)
             Mean action noise std: 2.45
          Mean value_function loss: 115.7174
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 50.5611
                       Mean reward: 918.26
               Mean episode length: 244.96
    Episode_Reward/reaching_object: 1.3920
     Episode_Reward/lifting_object: 176.9099
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 2.11s
                      Time elapsed: 00:43:06
                               ETA: 00:32:33

################################################################################
                     [1m Learning iteration 1140/2000 [0m                     

                       Computation: 46604 steps/s (collection: 2.015s, learning 0.094s)
             Mean action noise std: 2.46
          Mean value_function loss: 90.5000
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 50.5751
                       Mean reward: 881.10
               Mean episode length: 236.27
    Episode_Reward/reaching_object: 1.3677
     Episode_Reward/lifting_object: 174.3429
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 2.11s
                      Time elapsed: 00:43:08
                               ETA: 00:32:31

################################################################################
                     [1m Learning iteration 1141/2000 [0m                     

                       Computation: 46888 steps/s (collection: 2.008s, learning 0.089s)
             Mean action noise std: 2.46
          Mean value_function loss: 119.9808
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 50.5829
                       Mean reward: 879.98
               Mean episode length: 236.30
    Episode_Reward/reaching_object: 1.3664
     Episode_Reward/lifting_object: 174.2840
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 2.10s
                      Time elapsed: 00:43:11
                               ETA: 00:32:28

################################################################################
                     [1m Learning iteration 1142/2000 [0m                     

                       Computation: 47189 steps/s (collection: 1.998s, learning 0.086s)
             Mean action noise std: 2.46
          Mean value_function loss: 131.2997
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 50.5876
                       Mean reward: 874.05
               Mean episode length: 234.97
    Episode_Reward/reaching_object: 1.3307
     Episode_Reward/lifting_object: 169.2342
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 2.08s
                      Time elapsed: 00:43:13
                               ETA: 00:32:26

################################################################################
                     [1m Learning iteration 1143/2000 [0m                     

                       Computation: 45835 steps/s (collection: 2.045s, learning 0.100s)
             Mean action noise std: 2.46
          Mean value_function loss: 104.5277
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 50.5988
                       Mean reward: 887.36
               Mean episode length: 236.79
    Episode_Reward/reaching_object: 1.3563
     Episode_Reward/lifting_object: 172.7636
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 2.14s
                      Time elapsed: 00:43:15
                               ETA: 00:32:24

################################################################################
                     [1m Learning iteration 1144/2000 [0m                     

                       Computation: 47032 steps/s (collection: 1.994s, learning 0.096s)
             Mean action noise std: 2.46
          Mean value_function loss: 74.8274
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 50.6178
                       Mean reward: 881.86
               Mean episode length: 236.20
    Episode_Reward/reaching_object: 1.3961
     Episode_Reward/lifting_object: 177.4382
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 2.09s
                      Time elapsed: 00:43:17
                               ETA: 00:32:21

################################################################################
                     [1m Learning iteration 1145/2000 [0m                     

                       Computation: 47648 steps/s (collection: 1.972s, learning 0.092s)
             Mean action noise std: 2.46
          Mean value_function loss: 102.8663
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 50.6310
                       Mean reward: 879.00
               Mean episode length: 236.07
    Episode_Reward/reaching_object: 1.3914
     Episode_Reward/lifting_object: 177.8070
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 2.06s
                      Time elapsed: 00:43:19
                               ETA: 00:32:19

################################################################################
                     [1m Learning iteration 1146/2000 [0m                     

                       Computation: 47124 steps/s (collection: 1.999s, learning 0.087s)
             Mean action noise std: 2.46
          Mean value_function loss: 112.9455
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 50.6456
                       Mean reward: 853.17
               Mean episode length: 232.12
    Episode_Reward/reaching_object: 1.3542
     Episode_Reward/lifting_object: 171.7729
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 2.09s
                      Time elapsed: 00:43:21
                               ETA: 00:32:16

################################################################################
                     [1m Learning iteration 1147/2000 [0m                     

                       Computation: 46874 steps/s (collection: 2.009s, learning 0.088s)
             Mean action noise std: 2.46
          Mean value_function loss: 92.6396
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.6525
                       Mean reward: 893.56
               Mean episode length: 240.34
    Episode_Reward/reaching_object: 1.3799
     Episode_Reward/lifting_object: 174.2006
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 2.10s
                      Time elapsed: 00:43:23
                               ETA: 00:32:14

################################################################################
                     [1m Learning iteration 1148/2000 [0m                     

                       Computation: 47296 steps/s (collection: 1.980s, learning 0.098s)
             Mean action noise std: 2.46
          Mean value_function loss: 103.9993
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 50.6596
                       Mean reward: 891.76
               Mean episode length: 238.36
    Episode_Reward/reaching_object: 1.3812
     Episode_Reward/lifting_object: 175.1657
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 2.08s
                      Time elapsed: 00:43:25
                               ETA: 00:32:12

################################################################################
                     [1m Learning iteration 1149/2000 [0m                     

                       Computation: 46723 steps/s (collection: 2.007s, learning 0.097s)
             Mean action noise std: 2.47
          Mean value_function loss: 93.5841
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.6656
                       Mean reward: 887.12
               Mean episode length: 238.03
    Episode_Reward/reaching_object: 1.3748
     Episode_Reward/lifting_object: 174.5508
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 2.10s
                      Time elapsed: 00:43:27
                               ETA: 00:32:09

################################################################################
                     [1m Learning iteration 1150/2000 [0m                     

                       Computation: 46902 steps/s (collection: 2.000s, learning 0.096s)
             Mean action noise std: 2.47
          Mean value_function loss: 129.9295
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 50.6769
                       Mean reward: 833.92
               Mean episode length: 226.21
    Episode_Reward/reaching_object: 1.3358
     Episode_Reward/lifting_object: 168.8775
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 2.10s
                      Time elapsed: 00:43:29
                               ETA: 00:32:07

################################################################################
                     [1m Learning iteration 1151/2000 [0m                     

                       Computation: 45658 steps/s (collection: 2.040s, learning 0.113s)
             Mean action noise std: 2.47
          Mean value_function loss: 113.5824
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 50.6979
                       Mean reward: 841.63
               Mean episode length: 228.28
    Episode_Reward/reaching_object: 1.3501
     Episode_Reward/lifting_object: 170.9961
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 2.15s
                      Time elapsed: 00:43:32
                               ETA: 00:32:05

################################################################################
                     [1m Learning iteration 1152/2000 [0m                     

                       Computation: 46442 steps/s (collection: 2.016s, learning 0.100s)
             Mean action noise std: 2.47
          Mean value_function loss: 117.6550
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 50.7178
                       Mean reward: 867.95
               Mean episode length: 233.18
    Episode_Reward/reaching_object: 1.3732
     Episode_Reward/lifting_object: 173.8391
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 2.12s
                      Time elapsed: 00:43:34
                               ETA: 00:32:02

################################################################################
                     [1m Learning iteration 1153/2000 [0m                     

                       Computation: 47007 steps/s (collection: 1.992s, learning 0.099s)
             Mean action noise std: 2.47
          Mean value_function loss: 121.1748
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.7319
                       Mean reward: 880.35
               Mean episode length: 237.43
    Episode_Reward/reaching_object: 1.3682
     Episode_Reward/lifting_object: 173.2861
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 2.09s
                      Time elapsed: 00:43:36
                               ETA: 00:32:00

################################################################################
                     [1m Learning iteration 1154/2000 [0m                     

                       Computation: 46827 steps/s (collection: 1.998s, learning 0.101s)
             Mean action noise std: 2.48
          Mean value_function loss: 116.3476
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 50.7482
                       Mean reward: 875.25
               Mean episode length: 234.40
    Episode_Reward/reaching_object: 1.3690
     Episode_Reward/lifting_object: 172.7597
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 2.10s
                      Time elapsed: 00:43:38
                               ETA: 00:31:57

################################################################################
                     [1m Learning iteration 1155/2000 [0m                     

                       Computation: 47498 steps/s (collection: 1.976s, learning 0.094s)
             Mean action noise std: 2.48
          Mean value_function loss: 127.2927
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.7637
                       Mean reward: 873.42
               Mean episode length: 235.20
    Episode_Reward/reaching_object: 1.3790
     Episode_Reward/lifting_object: 174.3525
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 2.07s
                      Time elapsed: 00:43:40
                               ETA: 00:31:55

################################################################################
                     [1m Learning iteration 1156/2000 [0m                     

                       Computation: 47201 steps/s (collection: 1.980s, learning 0.103s)
             Mean action noise std: 2.48
          Mean value_function loss: 125.6321
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 50.7813
                       Mean reward: 908.52
               Mean episode length: 241.75
    Episode_Reward/reaching_object: 1.3794
     Episode_Reward/lifting_object: 174.6516
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 2.08s
                      Time elapsed: 00:43:42
                               ETA: 00:31:53

################################################################################
                     [1m Learning iteration 1157/2000 [0m                     

                       Computation: 46886 steps/s (collection: 2.007s, learning 0.089s)
             Mean action noise std: 2.48
          Mean value_function loss: 171.9760
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.7932
                       Mean reward: 831.41
               Mean episode length: 225.01
    Episode_Reward/reaching_object: 1.3532
     Episode_Reward/lifting_object: 170.7111
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 2.10s
                      Time elapsed: 00:43:44
                               ETA: 00:31:50

################################################################################
                     [1m Learning iteration 1158/2000 [0m                     

                       Computation: 47397 steps/s (collection: 1.981s, learning 0.093s)
             Mean action noise std: 2.48
          Mean value_function loss: 117.1012
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 50.8060
                       Mean reward: 897.87
               Mean episode length: 240.91
    Episode_Reward/reaching_object: 1.3708
     Episode_Reward/lifting_object: 173.9155
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 2.07s
                      Time elapsed: 00:43:46
                               ETA: 00:31:48

################################################################################
                     [1m Learning iteration 1159/2000 [0m                     

                       Computation: 47008 steps/s (collection: 1.998s, learning 0.093s)
             Mean action noise std: 2.48
          Mean value_function loss: 133.8248
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 50.8223
                       Mean reward: 884.25
               Mean episode length: 237.96
    Episode_Reward/reaching_object: 1.3779
     Episode_Reward/lifting_object: 175.0870
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 2.09s
                      Time elapsed: 00:43:48
                               ETA: 00:31:45

################################################################################
                     [1m Learning iteration 1160/2000 [0m                     

                       Computation: 47335 steps/s (collection: 1.985s, learning 0.092s)
             Mean action noise std: 2.49
          Mean value_function loss: 118.4703
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.8402
                       Mean reward: 906.22
               Mean episode length: 241.83
    Episode_Reward/reaching_object: 1.3733
     Episode_Reward/lifting_object: 175.0087
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 2.08s
                      Time elapsed: 00:43:50
                               ETA: 00:31:43

################################################################################
                     [1m Learning iteration 1161/2000 [0m                     

                       Computation: 47506 steps/s (collection: 1.977s, learning 0.093s)
             Mean action noise std: 2.49
          Mean value_function loss: 123.4763
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.8549
                       Mean reward: 851.44
               Mean episode length: 228.57
    Episode_Reward/reaching_object: 1.3658
     Episode_Reward/lifting_object: 173.5349
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 2.07s
                      Time elapsed: 00:43:52
                               ETA: 00:31:41

################################################################################
                     [1m Learning iteration 1162/2000 [0m                     

                       Computation: 47326 steps/s (collection: 1.990s, learning 0.088s)
             Mean action noise std: 2.49
          Mean value_function loss: 112.7651
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 50.8717
                       Mean reward: 830.22
               Mean episode length: 226.62
    Episode_Reward/reaching_object: 1.3571
     Episode_Reward/lifting_object: 172.4548
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 2.08s
                      Time elapsed: 00:43:54
                               ETA: 00:31:38

################################################################################
                     [1m Learning iteration 1163/2000 [0m                     

                       Computation: 46696 steps/s (collection: 2.016s, learning 0.090s)
             Mean action noise std: 2.49
          Mean value_function loss: 102.6197
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 50.8871
                       Mean reward: 895.68
               Mean episode length: 239.74
    Episode_Reward/reaching_object: 1.3763
     Episode_Reward/lifting_object: 175.6951
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 2.11s
                      Time elapsed: 00:43:57
                               ETA: 00:31:36

################################################################################
                     [1m Learning iteration 1164/2000 [0m                     

                       Computation: 47497 steps/s (collection: 1.983s, learning 0.087s)
             Mean action noise std: 2.49
          Mean value_function loss: 89.8587
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 50.9022
                       Mean reward: 868.40
               Mean episode length: 232.60
    Episode_Reward/reaching_object: 1.3896
     Episode_Reward/lifting_object: 177.9952
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 2.07s
                      Time elapsed: 00:43:59
                               ETA: 00:31:33

################################################################################
                     [1m Learning iteration 1165/2000 [0m                     

                       Computation: 46540 steps/s (collection: 2.005s, learning 0.107s)
             Mean action noise std: 2.49
          Mean value_function loss: 145.9567
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 50.9181
                       Mean reward: 859.26
               Mean episode length: 233.04
    Episode_Reward/reaching_object: 1.3727
     Episode_Reward/lifting_object: 175.0585
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 2.11s
                      Time elapsed: 00:44:01
                               ETA: 00:31:31

################################################################################
                     [1m Learning iteration 1166/2000 [0m                     

                       Computation: 46718 steps/s (collection: 1.989s, learning 0.115s)
             Mean action noise std: 2.50
          Mean value_function loss: 115.3001
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.9407
                       Mean reward: 868.20
               Mean episode length: 234.68
    Episode_Reward/reaching_object: 1.3368
     Episode_Reward/lifting_object: 169.6369
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 2.10s
                      Time elapsed: 00:44:03
                               ETA: 00:31:29

################################################################################
                     [1m Learning iteration 1167/2000 [0m                     

                       Computation: 46999 steps/s (collection: 1.988s, learning 0.103s)
             Mean action noise std: 2.50
          Mean value_function loss: 160.5168
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 50.9542
                       Mean reward: 883.57
               Mean episode length: 237.00
    Episode_Reward/reaching_object: 1.3327
     Episode_Reward/lifting_object: 169.3454
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 2.09s
                      Time elapsed: 00:44:05
                               ETA: 00:31:26

################################################################################
                     [1m Learning iteration 1168/2000 [0m                     

                       Computation: 46830 steps/s (collection: 2.006s, learning 0.093s)
             Mean action noise std: 2.50
          Mean value_function loss: 110.8246
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 50.9640
                       Mean reward: 836.47
               Mean episode length: 226.21
    Episode_Reward/reaching_object: 1.3426
     Episode_Reward/lifting_object: 170.8604
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 2.10s
                      Time elapsed: 00:44:07
                               ETA: 00:31:24

################################################################################
                     [1m Learning iteration 1169/2000 [0m                     

                       Computation: 47431 steps/s (collection: 1.982s, learning 0.091s)
             Mean action noise std: 2.50
          Mean value_function loss: 103.8362
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 50.9815
                       Mean reward: 921.51
               Mean episode length: 244.91
    Episode_Reward/reaching_object: 1.3429
     Episode_Reward/lifting_object: 171.2437
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 2.07s
                      Time elapsed: 00:44:09
                               ETA: 00:31:21

################################################################################
                     [1m Learning iteration 1170/2000 [0m                     

                       Computation: 47814 steps/s (collection: 1.971s, learning 0.085s)
             Mean action noise std: 2.50
          Mean value_function loss: 131.1057
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 50.9939
                       Mean reward: 853.20
               Mean episode length: 230.20
    Episode_Reward/reaching_object: 1.3570
     Episode_Reward/lifting_object: 173.4037
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 2.06s
                      Time elapsed: 00:44:11
                               ETA: 00:31:19

################################################################################
                     [1m Learning iteration 1171/2000 [0m                     

                       Computation: 47488 steps/s (collection: 1.978s, learning 0.092s)
             Mean action noise std: 2.50
          Mean value_function loss: 138.2671
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.0003
                       Mean reward: 890.06
               Mean episode length: 237.56
    Episode_Reward/reaching_object: 1.3744
     Episode_Reward/lifting_object: 174.7609
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 2.07s
                      Time elapsed: 00:44:13
                               ETA: 00:31:17

################################################################################
                     [1m Learning iteration 1172/2000 [0m                     

                       Computation: 47232 steps/s (collection: 1.984s, learning 0.097s)
             Mean action noise std: 2.51
          Mean value_function loss: 127.1882
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 51.0140
                       Mean reward: 896.22
               Mean episode length: 239.82
    Episode_Reward/reaching_object: 1.3448
     Episode_Reward/lifting_object: 171.2328
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 2.08s
                      Time elapsed: 00:44:15
                               ETA: 00:31:14

################################################################################
                     [1m Learning iteration 1173/2000 [0m                     

                       Computation: 46828 steps/s (collection: 2.006s, learning 0.093s)
             Mean action noise std: 2.51
          Mean value_function loss: 148.5328
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 51.0255
                       Mean reward: 864.18
               Mean episode length: 233.44
    Episode_Reward/reaching_object: 1.3739
     Episode_Reward/lifting_object: 175.4886
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 2.10s
                      Time elapsed: 00:44:17
                               ETA: 00:31:12

################################################################################
                     [1m Learning iteration 1174/2000 [0m                     

                       Computation: 47104 steps/s (collection: 1.995s, learning 0.092s)
             Mean action noise std: 2.51
          Mean value_function loss: 138.7344
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 51.0313
                       Mean reward: 870.52
               Mean episode length: 233.16
    Episode_Reward/reaching_object: 1.3299
     Episode_Reward/lifting_object: 170.2017
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 2.09s
                      Time elapsed: 00:44:20
                               ETA: 00:31:09

################################################################################
                     [1m Learning iteration 1175/2000 [0m                     

                       Computation: 47644 steps/s (collection: 1.969s, learning 0.095s)
             Mean action noise std: 2.51
          Mean value_function loss: 158.1546
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 51.0354
                       Mean reward: 844.82
               Mean episode length: 228.64
    Episode_Reward/reaching_object: 1.3386
     Episode_Reward/lifting_object: 170.0307
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 2.06s
                      Time elapsed: 00:44:22
                               ETA: 00:31:07

################################################################################
                     [1m Learning iteration 1176/2000 [0m                     

                       Computation: 47181 steps/s (collection: 1.994s, learning 0.089s)
             Mean action noise std: 2.51
          Mean value_function loss: 134.5580
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.0444
                       Mean reward: 841.69
               Mean episode length: 227.79
    Episode_Reward/reaching_object: 1.3357
     Episode_Reward/lifting_object: 169.8773
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 2.08s
                      Time elapsed: 00:44:24
                               ETA: 00:31:05

################################################################################
                     [1m Learning iteration 1177/2000 [0m                     

                       Computation: 46873 steps/s (collection: 2.012s, learning 0.086s)
             Mean action noise std: 2.51
          Mean value_function loss: 155.9382
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 51.0548
                       Mean reward: 868.94
               Mean episode length: 234.48
    Episode_Reward/reaching_object: 1.3408
     Episode_Reward/lifting_object: 171.4265
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 2.10s
                      Time elapsed: 00:44:26
                               ETA: 00:31:02

################################################################################
                     [1m Learning iteration 1178/2000 [0m                     

                       Computation: 46785 steps/s (collection: 2.013s, learning 0.088s)
             Mean action noise std: 2.51
          Mean value_function loss: 131.9284
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.0626
                       Mean reward: 879.26
               Mean episode length: 234.92
    Episode_Reward/reaching_object: 1.3688
     Episode_Reward/lifting_object: 174.5434
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 2.10s
                      Time elapsed: 00:44:28
                               ETA: 00:31:00

################################################################################
                     [1m Learning iteration 1179/2000 [0m                     

                       Computation: 46408 steps/s (collection: 2.031s, learning 0.088s)
             Mean action noise std: 2.51
          Mean value_function loss: 158.0006
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.0754
                       Mean reward: 866.92
               Mean episode length: 234.02
    Episode_Reward/reaching_object: 1.3632
     Episode_Reward/lifting_object: 173.3258
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 2.12s
                      Time elapsed: 00:44:30
                               ETA: 00:30:58

################################################################################
                     [1m Learning iteration 1180/2000 [0m                     

                       Computation: 46817 steps/s (collection: 1.994s, learning 0.106s)
             Mean action noise std: 2.51
          Mean value_function loss: 175.6824
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.0911
                       Mean reward: 837.87
               Mean episode length: 227.06
    Episode_Reward/reaching_object: 1.3478
     Episode_Reward/lifting_object: 170.9417
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 2.10s
                      Time elapsed: 00:44:32
                               ETA: 00:30:55

################################################################################
                     [1m Learning iteration 1181/2000 [0m                     

                       Computation: 46627 steps/s (collection: 2.001s, learning 0.107s)
             Mean action noise std: 2.52
          Mean value_function loss: 142.6451
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.1023
                       Mean reward: 877.38
               Mean episode length: 235.20
    Episode_Reward/reaching_object: 1.3525
     Episode_Reward/lifting_object: 171.7135
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 2.11s
                      Time elapsed: 00:44:34
                               ETA: 00:30:53

################################################################################
                     [1m Learning iteration 1182/2000 [0m                     

                       Computation: 46819 steps/s (collection: 2.012s, learning 0.088s)
             Mean action noise std: 2.52
          Mean value_function loss: 149.9515
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 51.1145
                       Mean reward: 848.54
               Mean episode length: 228.42
    Episode_Reward/reaching_object: 1.3508
     Episode_Reward/lifting_object: 170.9800
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 2.10s
                      Time elapsed: 00:44:36
                               ETA: 00:30:50

################################################################################
                     [1m Learning iteration 1183/2000 [0m                     

                       Computation: 46478 steps/s (collection: 2.022s, learning 0.093s)
             Mean action noise std: 2.52
          Mean value_function loss: 140.4841
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.1256
                       Mean reward: 871.80
               Mean episode length: 235.89
    Episode_Reward/reaching_object: 1.3883
     Episode_Reward/lifting_object: 175.2951
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 2.12s
                      Time elapsed: 00:44:38
                               ETA: 00:30:48

################################################################################
                     [1m Learning iteration 1184/2000 [0m                     

                       Computation: 46042 steps/s (collection: 2.047s, learning 0.089s)
             Mean action noise std: 2.52
          Mean value_function loss: 128.0097
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.1397
                       Mean reward: 858.89
               Mean episode length: 230.99
    Episode_Reward/reaching_object: 1.3650
     Episode_Reward/lifting_object: 171.6028
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 2.14s
                      Time elapsed: 00:44:41
                               ETA: 00:30:46

################################################################################
                     [1m Learning iteration 1185/2000 [0m                     

                       Computation: 47564 steps/s (collection: 1.979s, learning 0.088s)
             Mean action noise std: 2.52
          Mean value_function loss: 114.4925
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 51.1505
                       Mean reward: 835.82
               Mean episode length: 225.13
    Episode_Reward/reaching_object: 1.3791
     Episode_Reward/lifting_object: 175.3832
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 2.07s
                      Time elapsed: 00:44:43
                               ETA: 00:30:43

################################################################################
                     [1m Learning iteration 1186/2000 [0m                     

                       Computation: 47033 steps/s (collection: 2.000s, learning 0.090s)
             Mean action noise std: 2.52
          Mean value_function loss: 110.1394
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.1644
                       Mean reward: 872.37
               Mean episode length: 234.64
    Episode_Reward/reaching_object: 1.3750
     Episode_Reward/lifting_object: 173.7856
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 2.09s
                      Time elapsed: 00:44:45
                               ETA: 00:30:41

################################################################################
                     [1m Learning iteration 1187/2000 [0m                     

                       Computation: 46597 steps/s (collection: 2.020s, learning 0.090s)
             Mean action noise std: 2.52
          Mean value_function loss: 120.3343
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.1779
                       Mean reward: 897.09
               Mean episode length: 238.82
    Episode_Reward/reaching_object: 1.3760
     Episode_Reward/lifting_object: 174.6564
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 2.11s
                      Time elapsed: 00:44:47
                               ETA: 00:30:39

################################################################################
                     [1m Learning iteration 1188/2000 [0m                     

                       Computation: 46366 steps/s (collection: 2.031s, learning 0.090s)
             Mean action noise std: 2.53
          Mean value_function loss: 143.9770
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 51.1917
                       Mean reward: 886.74
               Mean episode length: 237.44
    Episode_Reward/reaching_object: 1.3655
     Episode_Reward/lifting_object: 173.1536
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 2.12s
                      Time elapsed: 00:44:49
                               ETA: 00:30:36

################################################################################
                     [1m Learning iteration 1189/2000 [0m                     

                       Computation: 46453 steps/s (collection: 2.030s, learning 0.086s)
             Mean action noise std: 2.53
          Mean value_function loss: 118.3077
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 51.2128
                       Mean reward: 860.35
               Mean episode length: 232.95
    Episode_Reward/reaching_object: 1.3669
     Episode_Reward/lifting_object: 173.0887
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 2.12s
                      Time elapsed: 00:44:51
                               ETA: 00:30:34

################################################################################
                     [1m Learning iteration 1190/2000 [0m                     

                       Computation: 47473 steps/s (collection: 1.980s, learning 0.091s)
             Mean action noise std: 2.53
          Mean value_function loss: 139.5352
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 51.2247
                       Mean reward: 869.70
               Mean episode length: 233.61
    Episode_Reward/reaching_object: 1.3646
     Episode_Reward/lifting_object: 172.4750
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 2.07s
                      Time elapsed: 00:44:53
                               ETA: 00:30:31

################################################################################
                     [1m Learning iteration 1191/2000 [0m                     

                       Computation: 46743 steps/s (collection: 2.015s, learning 0.089s)
             Mean action noise std: 2.53
          Mean value_function loss: 124.0611
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 51.2394
                       Mean reward: 838.19
               Mean episode length: 225.97
    Episode_Reward/reaching_object: 1.3303
     Episode_Reward/lifting_object: 167.3848
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 2.10s
                      Time elapsed: 00:44:55
                               ETA: 00:30:29

################################################################################
                     [1m Learning iteration 1192/2000 [0m                     

                       Computation: 47046 steps/s (collection: 1.999s, learning 0.091s)
             Mean action noise std: 2.53
          Mean value_function loss: 119.7534
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 51.2470
                       Mean reward: 864.92
               Mean episode length: 232.87
    Episode_Reward/reaching_object: 1.3708
     Episode_Reward/lifting_object: 174.1271
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 2.09s
                      Time elapsed: 00:44:57
                               ETA: 00:30:27

################################################################################
                     [1m Learning iteration 1193/2000 [0m                     

                       Computation: 47042 steps/s (collection: 2.002s, learning 0.088s)
             Mean action noise std: 2.53
          Mean value_function loss: 133.5278
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 51.2601
                       Mean reward: 865.41
               Mean episode length: 231.01
    Episode_Reward/reaching_object: 1.3754
     Episode_Reward/lifting_object: 173.7066
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 2.09s
                      Time elapsed: 00:44:59
                               ETA: 00:30:24

################################################################################
                     [1m Learning iteration 1194/2000 [0m                     

                       Computation: 46484 steps/s (collection: 2.007s, learning 0.108s)
             Mean action noise std: 2.53
          Mean value_function loss: 89.5764
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 51.2633
                       Mean reward: 856.42
               Mean episode length: 230.34
    Episode_Reward/reaching_object: 1.3653
     Episode_Reward/lifting_object: 172.5591
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 2.11s
                      Time elapsed: 00:45:02
                               ETA: 00:30:22

################################################################################
                     [1m Learning iteration 1195/2000 [0m                     

                       Computation: 46874 steps/s (collection: 1.980s, learning 0.117s)
             Mean action noise std: 2.53
          Mean value_function loss: 121.8691
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 51.2665
                       Mean reward: 861.47
               Mean episode length: 232.41
    Episode_Reward/reaching_object: 1.3904
     Episode_Reward/lifting_object: 175.8127
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 2.10s
                      Time elapsed: 00:45:04
                               ETA: 00:30:20

################################################################################
                     [1m Learning iteration 1196/2000 [0m                     

                       Computation: 46295 steps/s (collection: 2.015s, learning 0.109s)
             Mean action noise std: 2.54
          Mean value_function loss: 123.8287
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 51.2726
                       Mean reward: 860.64
               Mean episode length: 231.77
    Episode_Reward/reaching_object: 1.3699
     Episode_Reward/lifting_object: 173.7293
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 2.12s
                      Time elapsed: 00:45:06
                               ETA: 00:30:17

################################################################################
                     [1m Learning iteration 1197/2000 [0m                     

                       Computation: 46349 steps/s (collection: 2.031s, learning 0.090s)
             Mean action noise std: 2.54
          Mean value_function loss: 161.0277
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 51.2844
                       Mean reward: 851.50
               Mean episode length: 230.15
    Episode_Reward/reaching_object: 1.3442
     Episode_Reward/lifting_object: 170.7553
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 2.12s
                      Time elapsed: 00:45:08
                               ETA: 00:30:15

################################################################################
                     [1m Learning iteration 1198/2000 [0m                     

                       Computation: 46203 steps/s (collection: 2.037s, learning 0.091s)
             Mean action noise std: 2.54
          Mean value_function loss: 113.2014
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 51.3001
                       Mean reward: 880.29
               Mean episode length: 236.57
    Episode_Reward/reaching_object: 1.3625
     Episode_Reward/lifting_object: 173.2769
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 2.13s
                      Time elapsed: 00:45:10
                               ETA: 00:30:13

################################################################################
                     [1m Learning iteration 1199/2000 [0m                     

                       Computation: 46498 steps/s (collection: 2.022s, learning 0.092s)
             Mean action noise std: 2.54
          Mean value_function loss: 128.6587
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 51.3151
                       Mean reward: 892.93
               Mean episode length: 239.33
    Episode_Reward/reaching_object: 1.3689
     Episode_Reward/lifting_object: 174.4148
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 2.11s
                      Time elapsed: 00:45:12
                               ETA: 00:30:10

################################################################################
                     [1m Learning iteration 1200/2000 [0m                     

                       Computation: 46133 steps/s (collection: 2.039s, learning 0.092s)
             Mean action noise std: 2.54
          Mean value_function loss: 144.0234
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 51.3297
                       Mean reward: 841.30
               Mean episode length: 226.84
    Episode_Reward/reaching_object: 1.3410
     Episode_Reward/lifting_object: 170.8224
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 2.13s
                      Time elapsed: 00:45:14
                               ETA: 00:30:08

################################################################################
                     [1m Learning iteration 1201/2000 [0m                     

                       Computation: 45412 steps/s (collection: 2.056s, learning 0.109s)
             Mean action noise std: 2.54
          Mean value_function loss: 120.7264
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 51.3471
                       Mean reward: 862.54
               Mean episode length: 232.06
    Episode_Reward/reaching_object: 1.3556
     Episode_Reward/lifting_object: 173.3716
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 2.16s
                      Time elapsed: 00:45:16
                               ETA: 00:30:05

################################################################################
                     [1m Learning iteration 1202/2000 [0m                     

                       Computation: 46378 steps/s (collection: 2.036s, learning 0.084s)
             Mean action noise std: 2.55
          Mean value_function loss: 121.6248
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 51.3639
                       Mean reward: 916.45
               Mean episode length: 244.33
    Episode_Reward/reaching_object: 1.3483
     Episode_Reward/lifting_object: 172.1525
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 2.12s
                      Time elapsed: 00:45:19
                               ETA: 00:30:03

################################################################################
                     [1m Learning iteration 1203/2000 [0m                     

                       Computation: 46990 steps/s (collection: 2.004s, learning 0.088s)
             Mean action noise std: 2.55
          Mean value_function loss: 117.5355
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 51.3772
                       Mean reward: 862.89
               Mean episode length: 231.02
    Episode_Reward/reaching_object: 1.3705
     Episode_Reward/lifting_object: 175.2283
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 2.09s
                      Time elapsed: 00:45:21
                               ETA: 00:30:01

################################################################################
                     [1m Learning iteration 1204/2000 [0m                     

                       Computation: 46773 steps/s (collection: 1.997s, learning 0.105s)
             Mean action noise std: 2.55
          Mean value_function loss: 142.2347
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 51.3832
                       Mean reward: 886.83
               Mean episode length: 237.66
    Episode_Reward/reaching_object: 1.3544
     Episode_Reward/lifting_object: 172.9181
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 2.10s
                      Time elapsed: 00:45:23
                               ETA: 00:29:58

################################################################################
                     [1m Learning iteration 1205/2000 [0m                     

                       Computation: 45640 steps/s (collection: 2.057s, learning 0.097s)
             Mean action noise std: 2.55
          Mean value_function loss: 136.2673
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 51.3863
                       Mean reward: 863.08
               Mean episode length: 231.86
    Episode_Reward/reaching_object: 1.3411
     Episode_Reward/lifting_object: 170.2011
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 2.15s
                      Time elapsed: 00:45:25
                               ETA: 00:29:56

################################################################################
                     [1m Learning iteration 1206/2000 [0m                     

                       Computation: 46285 steps/s (collection: 2.031s, learning 0.093s)
             Mean action noise std: 2.55
          Mean value_function loss: 130.7901
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 51.3902
                       Mean reward: 865.77
               Mean episode length: 232.78
    Episode_Reward/reaching_object: 1.3388
     Episode_Reward/lifting_object: 171.3648
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 2.12s
                      Time elapsed: 00:45:27
                               ETA: 00:29:54

################################################################################
                     [1m Learning iteration 1207/2000 [0m                     

                       Computation: 46532 steps/s (collection: 2.025s, learning 0.088s)
             Mean action noise std: 2.55
          Mean value_function loss: 131.5976
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 51.3942
                       Mean reward: 819.31
               Mean episode length: 222.00
    Episode_Reward/reaching_object: 1.3203
     Episode_Reward/lifting_object: 167.8441
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 2.11s
                      Time elapsed: 00:45:29
                               ETA: 00:29:51

################################################################################
                     [1m Learning iteration 1208/2000 [0m                     

                       Computation: 45675 steps/s (collection: 2.045s, learning 0.107s)
             Mean action noise std: 2.55
          Mean value_function loss: 134.4507
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 51.4045
                       Mean reward: 846.91
               Mean episode length: 228.15
    Episode_Reward/reaching_object: 1.3532
     Episode_Reward/lifting_object: 173.2311
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 2.15s
                      Time elapsed: 00:45:31
                               ETA: 00:29:49

################################################################################
                     [1m Learning iteration 1209/2000 [0m                     

                       Computation: 45699 steps/s (collection: 2.051s, learning 0.100s)
             Mean action noise std: 2.55
          Mean value_function loss: 145.3751
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.4231
                       Mean reward: 831.03
               Mean episode length: 224.56
    Episode_Reward/reaching_object: 1.3670
     Episode_Reward/lifting_object: 174.1962
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 2.15s
                      Time elapsed: 00:45:33
                               ETA: 00:29:47

################################################################################
                     [1m Learning iteration 1210/2000 [0m                     

                       Computation: 44805 steps/s (collection: 2.087s, learning 0.107s)
             Mean action noise std: 2.55
          Mean value_function loss: 121.4183
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 51.4351
                       Mean reward: 881.16
               Mean episode length: 235.53
    Episode_Reward/reaching_object: 1.3286
     Episode_Reward/lifting_object: 169.3645
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 2.19s
                      Time elapsed: 00:45:36
                               ETA: 00:29:44

################################################################################
                     [1m Learning iteration 1211/2000 [0m                     

                       Computation: 46461 steps/s (collection: 2.017s, learning 0.099s)
             Mean action noise std: 2.56
          Mean value_function loss: 109.9720
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.4445
                       Mean reward: 886.19
               Mean episode length: 236.62
    Episode_Reward/reaching_object: 1.3502
     Episode_Reward/lifting_object: 172.5218
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 2.12s
                      Time elapsed: 00:45:38
                               ETA: 00:29:42

################################################################################
                     [1m Learning iteration 1212/2000 [0m                     

                       Computation: 45603 steps/s (collection: 2.059s, learning 0.097s)
             Mean action noise std: 2.56
          Mean value_function loss: 126.2261
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.4564
                       Mean reward: 850.74
               Mean episode length: 228.36
    Episode_Reward/reaching_object: 1.3482
     Episode_Reward/lifting_object: 171.4441
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 2.16s
                      Time elapsed: 00:45:40
                               ETA: 00:29:40

################################################################################
                     [1m Learning iteration 1213/2000 [0m                     

                       Computation: 46234 steps/s (collection: 2.033s, learning 0.093s)
             Mean action noise std: 2.56
          Mean value_function loss: 134.5731
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 51.4659
                       Mean reward: 849.74
               Mean episode length: 227.07
    Episode_Reward/reaching_object: 1.3428
     Episode_Reward/lifting_object: 171.4688
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 2.13s
                      Time elapsed: 00:45:42
                               ETA: 00:29:37

################################################################################
                     [1m Learning iteration 1214/2000 [0m                     

                       Computation: 46307 steps/s (collection: 2.026s, learning 0.097s)
             Mean action noise std: 2.56
          Mean value_function loss: 106.1707
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 51.4776
                       Mean reward: 860.21
               Mean episode length: 232.06
    Episode_Reward/reaching_object: 1.3496
     Episode_Reward/lifting_object: 172.1808
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 2.12s
                      Time elapsed: 00:45:44
                               ETA: 00:29:35

################################################################################
                     [1m Learning iteration 1215/2000 [0m                     

                       Computation: 46819 steps/s (collection: 2.008s, learning 0.092s)
             Mean action noise std: 2.56
          Mean value_function loss: 141.2069
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 51.4825
                       Mean reward: 836.59
               Mean episode length: 225.91
    Episode_Reward/reaching_object: 1.3658
     Episode_Reward/lifting_object: 173.5067
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 2.10s
                      Time elapsed: 00:45:46
                               ETA: 00:29:33

################################################################################
                     [1m Learning iteration 1216/2000 [0m                     

                       Computation: 46587 steps/s (collection: 2.016s, learning 0.094s)
             Mean action noise std: 2.56
          Mean value_function loss: 110.3769
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 51.4888
                       Mean reward: 901.35
               Mean episode length: 240.95
    Episode_Reward/reaching_object: 1.3473
     Episode_Reward/lifting_object: 170.7504
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 2.11s
                      Time elapsed: 00:45:48
                               ETA: 00:29:30

################################################################################
                     [1m Learning iteration 1217/2000 [0m                     

                       Computation: 46405 steps/s (collection: 2.026s, learning 0.093s)
             Mean action noise std: 2.56
          Mean value_function loss: 119.1319
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 51.4983
                       Mean reward: 888.92
               Mean episode length: 238.92
    Episode_Reward/reaching_object: 1.3571
     Episode_Reward/lifting_object: 172.2983
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 2.12s
                      Time elapsed: 00:45:50
                               ETA: 00:29:28

################################################################################
                     [1m Learning iteration 1218/2000 [0m                     

                       Computation: 46229 steps/s (collection: 2.018s, learning 0.109s)
             Mean action noise std: 2.56
          Mean value_function loss: 112.2760
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 51.5083
                       Mean reward: 889.51
               Mean episode length: 238.48
    Episode_Reward/reaching_object: 1.3554
     Episode_Reward/lifting_object: 172.1988
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 2.13s
                      Time elapsed: 00:45:53
                               ETA: 00:29:26

################################################################################
                     [1m Learning iteration 1219/2000 [0m                     

                       Computation: 46471 steps/s (collection: 2.021s, learning 0.094s)
             Mean action noise std: 2.56
          Mean value_function loss: 110.3934
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 51.5220
                       Mean reward: 864.47
               Mean episode length: 233.04
    Episode_Reward/reaching_object: 1.3627
     Episode_Reward/lifting_object: 172.8114
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 2.12s
                      Time elapsed: 00:45:55
                               ETA: 00:29:23

################################################################################
                     [1m Learning iteration 1220/2000 [0m                     

                       Computation: 46891 steps/s (collection: 2.003s, learning 0.094s)
             Mean action noise std: 2.57
          Mean value_function loss: 123.1520
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 51.5328
                       Mean reward: 883.39
               Mean episode length: 236.28
    Episode_Reward/reaching_object: 1.3733
     Episode_Reward/lifting_object: 174.5772
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 2.10s
                      Time elapsed: 00:45:57
                               ETA: 00:29:21

################################################################################
                     [1m Learning iteration 1221/2000 [0m                     

                       Computation: 46645 steps/s (collection: 2.018s, learning 0.090s)
             Mean action noise std: 2.57
          Mean value_function loss: 126.2099
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 51.5439
                       Mean reward: 821.93
               Mean episode length: 221.69
    Episode_Reward/reaching_object: 1.3396
     Episode_Reward/lifting_object: 170.4058
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 2.11s
                      Time elapsed: 00:45:59
                               ETA: 00:29:19

################################################################################
                     [1m Learning iteration 1222/2000 [0m                     

                       Computation: 45974 steps/s (collection: 2.037s, learning 0.101s)
             Mean action noise std: 2.57
          Mean value_function loss: 149.0751
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 51.5532
                       Mean reward: 811.90
               Mean episode length: 221.95
    Episode_Reward/reaching_object: 1.3422
     Episode_Reward/lifting_object: 170.3970
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 2.14s
                      Time elapsed: 00:46:01
                               ETA: 00:29:16

################################################################################
                     [1m Learning iteration 1223/2000 [0m                     

                       Computation: 46065 steps/s (collection: 2.032s, learning 0.102s)
             Mean action noise std: 2.57
          Mean value_function loss: 163.6709
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 51.5705
                       Mean reward: 829.78
               Mean episode length: 224.53
    Episode_Reward/reaching_object: 1.3190
     Episode_Reward/lifting_object: 167.4735
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 2.13s
                      Time elapsed: 00:46:03
                               ETA: 00:29:14

################################################################################
                     [1m Learning iteration 1224/2000 [0m                     

                       Computation: 45732 steps/s (collection: 2.047s, learning 0.103s)
             Mean action noise std: 2.57
          Mean value_function loss: 102.0660
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 51.5884
                       Mean reward: 871.44
               Mean episode length: 233.30
    Episode_Reward/reaching_object: 1.3668
     Episode_Reward/lifting_object: 174.5593
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 2.15s
                      Time elapsed: 00:46:05
                               ETA: 00:29:12

################################################################################
                     [1m Learning iteration 1225/2000 [0m                     

                       Computation: 46795 steps/s (collection: 2.008s, learning 0.093s)
             Mean action noise std: 2.57
          Mean value_function loss: 102.1117
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 51.5924
                       Mean reward: 824.52
               Mean episode length: 223.25
    Episode_Reward/reaching_object: 1.3392
     Episode_Reward/lifting_object: 170.5536
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 2.10s
                      Time elapsed: 00:46:07
                               ETA: 00:29:09

################################################################################
                     [1m Learning iteration 1226/2000 [0m                     

                       Computation: 46352 steps/s (collection: 2.028s, learning 0.093s)
             Mean action noise std: 2.57
          Mean value_function loss: 135.4554
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.5986
                       Mean reward: 898.61
               Mean episode length: 239.69
    Episode_Reward/reaching_object: 1.3900
     Episode_Reward/lifting_object: 177.2309
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 2.12s
                      Time elapsed: 00:46:10
                               ETA: 00:29:07

################################################################################
                     [1m Learning iteration 1227/2000 [0m                     

                       Computation: 46194 steps/s (collection: 2.032s, learning 0.096s)
             Mean action noise std: 2.58
          Mean value_function loss: 108.2948
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 51.6103
                       Mean reward: 869.91
               Mean episode length: 233.11
    Episode_Reward/reaching_object: 1.3450
     Episode_Reward/lifting_object: 171.6126
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 2.13s
                      Time elapsed: 00:46:12
                               ETA: 00:29:05

################################################################################
                     [1m Learning iteration 1228/2000 [0m                     

                       Computation: 46526 steps/s (collection: 1.998s, learning 0.115s)
             Mean action noise std: 2.58
          Mean value_function loss: 105.7814
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 51.6219
                       Mean reward: 876.86
               Mean episode length: 236.32
    Episode_Reward/reaching_object: 1.3878
     Episode_Reward/lifting_object: 176.7594
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 2.11s
                      Time elapsed: 00:46:14
                               ETA: 00:29:02

################################################################################
                     [1m Learning iteration 1229/2000 [0m                     

                       Computation: 45838 steps/s (collection: 2.052s, learning 0.092s)
             Mean action noise std: 2.58
          Mean value_function loss: 100.4704
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.6307
                       Mean reward: 897.20
               Mean episode length: 238.86
    Episode_Reward/reaching_object: 1.3715
     Episode_Reward/lifting_object: 175.0456
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 2.14s
                      Time elapsed: 00:46:16
                               ETA: 00:29:00

################################################################################
                     [1m Learning iteration 1230/2000 [0m                     

                       Computation: 45646 steps/s (collection: 2.051s, learning 0.103s)
             Mean action noise std: 2.58
          Mean value_function loss: 125.2170
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.6410
                       Mean reward: 864.18
               Mean episode length: 232.72
    Episode_Reward/reaching_object: 1.3513
     Episode_Reward/lifting_object: 172.8168
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 2.15s
                      Time elapsed: 00:46:18
                               ETA: 00:28:58

################################################################################
                     [1m Learning iteration 1231/2000 [0m                     

                       Computation: 46370 steps/s (collection: 2.029s, learning 0.091s)
             Mean action noise std: 2.58
          Mean value_function loss: 91.9859
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.6528
                       Mean reward: 888.51
               Mean episode length: 237.33
    Episode_Reward/reaching_object: 1.3890
     Episode_Reward/lifting_object: 178.3002
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 2.12s
                      Time elapsed: 00:46:20
                               ETA: 00:28:55

################################################################################
                     [1m Learning iteration 1232/2000 [0m                     

                       Computation: 46487 steps/s (collection: 2.025s, learning 0.090s)
             Mean action noise std: 2.58
          Mean value_function loss: 106.4234
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 51.6644
                       Mean reward: 908.92
               Mean episode length: 241.60
    Episode_Reward/reaching_object: 1.3485
     Episode_Reward/lifting_object: 172.9064
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 2.11s
                      Time elapsed: 00:46:22
                               ETA: 00:28:53

################################################################################
                     [1m Learning iteration 1233/2000 [0m                     

                       Computation: 46296 steps/s (collection: 2.028s, learning 0.096s)
             Mean action noise std: 2.58
          Mean value_function loss: 121.5382
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 51.6734
                       Mean reward: 892.78
               Mean episode length: 238.25
    Episode_Reward/reaching_object: 1.3630
     Episode_Reward/lifting_object: 174.7975
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 2.12s
                      Time elapsed: 00:46:24
                               ETA: 00:28:50

################################################################################
                     [1m Learning iteration 1234/2000 [0m                     

                       Computation: 45709 steps/s (collection: 2.056s, learning 0.095s)
             Mean action noise std: 2.59
          Mean value_function loss: 136.8559
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 51.6862
                       Mean reward: 891.07
               Mean episode length: 237.76
    Episode_Reward/reaching_object: 1.3628
     Episode_Reward/lifting_object: 175.0106
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 2.15s
                      Time elapsed: 00:46:27
                               ETA: 00:28:48

################################################################################
                     [1m Learning iteration 1235/2000 [0m                     

                       Computation: 46301 steps/s (collection: 2.031s, learning 0.092s)
             Mean action noise std: 2.59
          Mean value_function loss: 115.2698
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.6994
                       Mean reward: 878.85
               Mean episode length: 235.03
    Episode_Reward/reaching_object: 1.3433
     Episode_Reward/lifting_object: 172.3508
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 2.12s
                      Time elapsed: 00:46:29
                               ETA: 00:28:46

################################################################################
                     [1m Learning iteration 1236/2000 [0m                     

                       Computation: 45858 steps/s (collection: 2.051s, learning 0.092s)
             Mean action noise std: 2.59
          Mean value_function loss: 151.9665
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 51.7118
                       Mean reward: 823.04
               Mean episode length: 222.65
    Episode_Reward/reaching_object: 1.3498
     Episode_Reward/lifting_object: 173.4155
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 2.14s
                      Time elapsed: 00:46:31
                               ETA: 00:28:44

################################################################################
                     [1m Learning iteration 1237/2000 [0m                     

                       Computation: 44730 steps/s (collection: 2.082s, learning 0.116s)
             Mean action noise std: 2.59
          Mean value_function loss: 129.4208
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 51.7321
                       Mean reward: 884.64
               Mean episode length: 237.04
    Episode_Reward/reaching_object: 1.3558
     Episode_Reward/lifting_object: 173.8451
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 2.20s
                      Time elapsed: 00:46:33
                               ETA: 00:28:41

################################################################################
                     [1m Learning iteration 1238/2000 [0m                     

                       Computation: 45899 steps/s (collection: 2.040s, learning 0.102s)
             Mean action noise std: 2.59
          Mean value_function loss: 122.2453
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 51.7527
                       Mean reward: 845.98
               Mean episode length: 226.88
    Episode_Reward/reaching_object: 1.3336
     Episode_Reward/lifting_object: 171.3980
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 2.14s
                      Time elapsed: 00:46:35
                               ETA: 00:28:39

################################################################################
                     [1m Learning iteration 1239/2000 [0m                     

                       Computation: 45754 steps/s (collection: 2.053s, learning 0.096s)
             Mean action noise std: 2.60
          Mean value_function loss: 87.6571
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 51.7642
                       Mean reward: 889.69
               Mean episode length: 237.74
    Episode_Reward/reaching_object: 1.3670
     Episode_Reward/lifting_object: 175.8799
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 2.15s
                      Time elapsed: 00:46:37
                               ETA: 00:28:37

################################################################################
                     [1m Learning iteration 1240/2000 [0m                     

                       Computation: 46257 steps/s (collection: 2.018s, learning 0.107s)
             Mean action noise std: 2.60
          Mean value_function loss: 136.5991
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 51.7848
                       Mean reward: 866.20
               Mean episode length: 232.59
    Episode_Reward/reaching_object: 1.3497
     Episode_Reward/lifting_object: 173.5231
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 2.13s
                      Time elapsed: 00:46:39
                               ETA: 00:28:34

################################################################################
                     [1m Learning iteration 1241/2000 [0m                     

                       Computation: 45550 steps/s (collection: 2.044s, learning 0.114s)
             Mean action noise std: 2.60
          Mean value_function loss: 140.0652
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 51.7984
                       Mean reward: 858.87
               Mean episode length: 232.62
    Episode_Reward/reaching_object: 1.3514
     Episode_Reward/lifting_object: 172.5970
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 2.16s
                      Time elapsed: 00:46:42
                               ETA: 00:28:32

################################################################################
                     [1m Learning iteration 1242/2000 [0m                     

                       Computation: 45740 steps/s (collection: 2.057s, learning 0.092s)
             Mean action noise std: 2.60
          Mean value_function loss: 115.0079
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 51.8081
                       Mean reward: 851.81
               Mean episode length: 230.09
    Episode_Reward/reaching_object: 1.3195
     Episode_Reward/lifting_object: 168.5080
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 2.15s
                      Time elapsed: 00:46:44
                               ETA: 00:28:30

################################################################################
                     [1m Learning iteration 1243/2000 [0m                     

                       Computation: 43096 steps/s (collection: 2.096s, learning 0.185s)
             Mean action noise std: 2.60
          Mean value_function loss: 154.1735
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 51.8178
                       Mean reward: 858.02
               Mean episode length: 229.83
    Episode_Reward/reaching_object: 1.3493
     Episode_Reward/lifting_object: 173.0979
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 2.28s
                      Time elapsed: 00:46:46
                               ETA: 00:28:27

################################################################################
                     [1m Learning iteration 1244/2000 [0m                     

                       Computation: 41898 steps/s (collection: 2.249s, learning 0.098s)
             Mean action noise std: 2.60
          Mean value_function loss: 114.1387
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 51.8377
                       Mean reward: 902.44
               Mean episode length: 240.25
    Episode_Reward/reaching_object: 1.3512
     Episode_Reward/lifting_object: 173.6667
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 2.35s
                      Time elapsed: 00:46:48
                               ETA: 00:28:25

################################################################################
                     [1m Learning iteration 1245/2000 [0m                     

                       Computation: 45070 steps/s (collection: 2.079s, learning 0.102s)
             Mean action noise std: 2.60
          Mean value_function loss: 137.9802
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 51.8517
                       Mean reward: 861.44
               Mean episode length: 231.14
    Episode_Reward/reaching_object: 1.3304
     Episode_Reward/lifting_object: 171.7583
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 2.18s
                      Time elapsed: 00:46:51
                               ETA: 00:28:23

################################################################################
                     [1m Learning iteration 1246/2000 [0m                     

                       Computation: 43142 steps/s (collection: 2.169s, learning 0.110s)
             Mean action noise std: 2.60
          Mean value_function loss: 113.5114
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 51.8573
                       Mean reward: 856.41
               Mean episode length: 230.41
    Episode_Reward/reaching_object: 1.3348
     Episode_Reward/lifting_object: 172.0746
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 2.28s
                      Time elapsed: 00:46:53
                               ETA: 00:28:21

################################################################################
                     [1m Learning iteration 1247/2000 [0m                     

                       Computation: 44182 steps/s (collection: 2.117s, learning 0.108s)
             Mean action noise std: 2.61
          Mean value_function loss: 109.7539
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 51.8639
                       Mean reward: 864.26
               Mean episode length: 232.24
    Episode_Reward/reaching_object: 1.3536
     Episode_Reward/lifting_object: 174.5525
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 2.22s
                      Time elapsed: 00:46:55
                               ETA: 00:28:18

################################################################################
                     [1m Learning iteration 1248/2000 [0m                     

                       Computation: 45050 steps/s (collection: 2.085s, learning 0.097s)
             Mean action noise std: 2.61
          Mean value_function loss: 116.1404
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 51.8752
                       Mean reward: 840.45
               Mean episode length: 226.58
    Episode_Reward/reaching_object: 1.3544
     Episode_Reward/lifting_object: 174.2661
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 2.18s
                      Time elapsed: 00:46:57
                               ETA: 00:28:16

################################################################################
                     [1m Learning iteration 1249/2000 [0m                     

                       Computation: 44830 steps/s (collection: 2.104s, learning 0.089s)
             Mean action noise std: 2.61
          Mean value_function loss: 115.1373
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 51.8885
                       Mean reward: 887.87
               Mean episode length: 237.90
    Episode_Reward/reaching_object: 1.3747
     Episode_Reward/lifting_object: 176.9362
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 2.19s
                      Time elapsed: 00:46:59
                               ETA: 00:28:14

################################################################################
                     [1m Learning iteration 1250/2000 [0m                     

                       Computation: 44560 steps/s (collection: 2.099s, learning 0.107s)
             Mean action noise std: 2.61
          Mean value_function loss: 116.9146
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 51.9071
                       Mean reward: 881.90
               Mean episode length: 235.96
    Episode_Reward/reaching_object: 1.3621
     Episode_Reward/lifting_object: 175.5596
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 2.21s
                      Time elapsed: 00:47:02
                               ETA: 00:28:11

################################################################################
                     [1m Learning iteration 1251/2000 [0m                     

                       Computation: 42748 steps/s (collection: 2.192s, learning 0.108s)
             Mean action noise std: 2.61
          Mean value_function loss: 102.8395
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 51.9280
                       Mean reward: 898.25
               Mean episode length: 240.17
    Episode_Reward/reaching_object: 1.3758
     Episode_Reward/lifting_object: 177.1215
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 2.30s
                      Time elapsed: 00:47:04
                               ETA: 00:28:09

################################################################################
                     [1m Learning iteration 1252/2000 [0m                     

                       Computation: 42370 steps/s (collection: 2.177s, learning 0.143s)
             Mean action noise std: 2.61
          Mean value_function loss: 101.9384
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 51.9333
                       Mean reward: 883.95
               Mean episode length: 237.82
    Episode_Reward/reaching_object: 1.3543
     Episode_Reward/lifting_object: 174.5000
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 2.32s
                      Time elapsed: 00:47:06
                               ETA: 00:28:07

################################################################################
                     [1m Learning iteration 1253/2000 [0m                     

                       Computation: 41910 steps/s (collection: 2.250s, learning 0.095s)
             Mean action noise std: 2.62
          Mean value_function loss: 109.5664
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 51.9454
                       Mean reward: 863.72
               Mean episode length: 232.86
    Episode_Reward/reaching_object: 1.3305
     Episode_Reward/lifting_object: 170.7631
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 2.35s
                      Time elapsed: 00:47:09
                               ETA: 00:28:05

################################################################################
                     [1m Learning iteration 1254/2000 [0m                     

                       Computation: 44000 steps/s (collection: 2.102s, learning 0.133s)
             Mean action noise std: 2.62
          Mean value_function loss: 119.5749
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.9661
                       Mean reward: 892.68
               Mean episode length: 239.53
    Episode_Reward/reaching_object: 1.3541
     Episode_Reward/lifting_object: 174.1436
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 2.23s
                      Time elapsed: 00:47:11
                               ETA: 00:28:03

################################################################################
                     [1m Learning iteration 1255/2000 [0m                     

                       Computation: 45037 steps/s (collection: 2.090s, learning 0.092s)
             Mean action noise std: 2.62
          Mean value_function loss: 157.4471
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.9831
                       Mean reward: 841.16
               Mean episode length: 230.67
    Episode_Reward/reaching_object: 1.3294
     Episode_Reward/lifting_object: 170.2844
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 2.18s
                      Time elapsed: 00:47:13
                               ETA: 00:28:00

################################################################################
                     [1m Learning iteration 1256/2000 [0m                     

                       Computation: 44793 steps/s (collection: 2.095s, learning 0.100s)
             Mean action noise std: 2.62
          Mean value_function loss: 124.7589
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.9930
                       Mean reward: 872.40
               Mean episode length: 233.69
    Episode_Reward/reaching_object: 1.3243
     Episode_Reward/lifting_object: 170.6234
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 2.19s
                      Time elapsed: 00:47:15
                               ETA: 00:27:58

################################################################################
                     [1m Learning iteration 1257/2000 [0m                     

                       Computation: 43423 steps/s (collection: 2.157s, learning 0.107s)
             Mean action noise std: 2.62
          Mean value_function loss: 118.5129
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 52.0048
                       Mean reward: 882.49
               Mean episode length: 235.18
    Episode_Reward/reaching_object: 1.3385
     Episode_Reward/lifting_object: 172.8087
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 2.26s
                      Time elapsed: 00:47:18
                               ETA: 00:27:56

################################################################################
                     [1m Learning iteration 1258/2000 [0m                     

                       Computation: 44247 steps/s (collection: 2.113s, learning 0.109s)
             Mean action noise std: 2.62
          Mean value_function loss: 124.5395
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 52.0156
                       Mean reward: 832.67
               Mean episode length: 223.19
    Episode_Reward/reaching_object: 1.3384
     Episode_Reward/lifting_object: 173.3270
      Episode_Reward/object_height: 0.0271
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 2.22s
                      Time elapsed: 00:47:20
                               ETA: 00:27:53

################################################################################
                     [1m Learning iteration 1259/2000 [0m                     

                       Computation: 43415 steps/s (collection: 2.161s, learning 0.103s)
             Mean action noise std: 2.63
          Mean value_function loss: 108.1671
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.0260
                       Mean reward: 884.22
               Mean episode length: 237.08
    Episode_Reward/reaching_object: 1.3524
     Episode_Reward/lifting_object: 174.6082
      Episode_Reward/object_height: 0.0277
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 2.26s
                      Time elapsed: 00:47:22
                               ETA: 00:27:51

################################################################################
                     [1m Learning iteration 1260/2000 [0m                     

                       Computation: 44607 steps/s (collection: 2.110s, learning 0.094s)
             Mean action noise std: 2.63
          Mean value_function loss: 105.0979
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 52.0360
                       Mean reward: 895.55
               Mean episode length: 238.72
    Episode_Reward/reaching_object: 1.3417
     Episode_Reward/lifting_object: 173.6033
      Episode_Reward/object_height: 0.0278
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 2.20s
                      Time elapsed: 00:47:24
                               ETA: 00:27:49

################################################################################
                     [1m Learning iteration 1261/2000 [0m                     

                       Computation: 45361 steps/s (collection: 2.070s, learning 0.098s)
             Mean action noise std: 2.63
          Mean value_function loss: 125.3361
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.0441
                       Mean reward: 862.49
               Mean episode length: 233.97
    Episode_Reward/reaching_object: 1.3390
     Episode_Reward/lifting_object: 172.0045
      Episode_Reward/object_height: 0.0273
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 2.17s
                      Time elapsed: 00:47:26
                               ETA: 00:27:47

################################################################################
                     [1m Learning iteration 1262/2000 [0m                     

                       Computation: 44183 steps/s (collection: 2.121s, learning 0.104s)
             Mean action noise std: 2.63
          Mean value_function loss: 126.7475
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 52.0571
                       Mean reward: 876.78
               Mean episode length: 235.84
    Episode_Reward/reaching_object: 1.3434
     Episode_Reward/lifting_object: 172.7954
      Episode_Reward/object_height: 0.0272
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 2.22s
                      Time elapsed: 00:47:29
                               ETA: 00:27:44

################################################################################
                     [1m Learning iteration 1263/2000 [0m                     

                       Computation: 42588 steps/s (collection: 2.140s, learning 0.169s)
             Mean action noise std: 2.63
          Mean value_function loss: 132.9511
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.0695
                       Mean reward: 899.81
               Mean episode length: 239.61
    Episode_Reward/reaching_object: 1.2993
     Episode_Reward/lifting_object: 166.9301
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 2.31s
                      Time elapsed: 00:47:31
                               ETA: 00:27:42

################################################################################
                     [1m Learning iteration 1264/2000 [0m                     

                       Computation: 44264 steps/s (collection: 2.119s, learning 0.102s)
             Mean action noise std: 2.63
          Mean value_function loss: 87.3547
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 52.0802
                       Mean reward: 905.99
               Mean episode length: 242.12
    Episode_Reward/reaching_object: 1.3387
     Episode_Reward/lifting_object: 172.0197
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 2.22s
                      Time elapsed: 00:47:33
                               ETA: 00:27:40

################################################################################
                     [1m Learning iteration 1265/2000 [0m                     

                       Computation: 43397 steps/s (collection: 2.121s, learning 0.145s)
             Mean action noise std: 2.63
          Mean value_function loss: 116.3274
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 52.0899
                       Mean reward: 859.72
               Mean episode length: 230.51
    Episode_Reward/reaching_object: 1.3264
     Episode_Reward/lifting_object: 171.0115
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 2.27s
                      Time elapsed: 00:47:35
                               ETA: 00:27:38

################################################################################
                     [1m Learning iteration 1266/2000 [0m                     

                       Computation: 45456 steps/s (collection: 2.069s, learning 0.094s)
             Mean action noise std: 2.63
          Mean value_function loss: 132.1890
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 52.0977
                       Mean reward: 859.68
               Mean episode length: 229.22
    Episode_Reward/reaching_object: 1.3247
     Episode_Reward/lifting_object: 171.6781
      Episode_Reward/object_height: 0.0268
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 2.16s
                      Time elapsed: 00:47:38
                               ETA: 00:27:35

################################################################################
                     [1m Learning iteration 1267/2000 [0m                     

                       Computation: 45319 steps/s (collection: 2.076s, learning 0.094s)
             Mean action noise std: 2.64
          Mean value_function loss: 100.5862
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.1090
                       Mean reward: 851.52
               Mean episode length: 229.01
    Episode_Reward/reaching_object: 1.3167
     Episode_Reward/lifting_object: 169.5081
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 2.17s
                      Time elapsed: 00:47:40
                               ETA: 00:27:33

################################################################################
                     [1m Learning iteration 1268/2000 [0m                     

                       Computation: 45022 steps/s (collection: 2.087s, learning 0.096s)
             Mean action noise std: 2.64
          Mean value_function loss: 98.9613
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 52.1191
                       Mean reward: 900.32
               Mean episode length: 240.59
    Episode_Reward/reaching_object: 1.3592
     Episode_Reward/lifting_object: 175.1227
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 2.18s
                      Time elapsed: 00:47:42
                               ETA: 00:27:31

################################################################################
                     [1m Learning iteration 1269/2000 [0m                     

                       Computation: 44844 steps/s (collection: 2.085s, learning 0.108s)
             Mean action noise std: 2.64
          Mean value_function loss: 143.2268
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 52.1285
                       Mean reward: 850.60
               Mean episode length: 229.26
    Episode_Reward/reaching_object: 1.3177
     Episode_Reward/lifting_object: 168.9410
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 2.19s
                      Time elapsed: 00:47:44
                               ETA: 00:27:28

################################################################################
                     [1m Learning iteration 1270/2000 [0m                     

                       Computation: 45107 steps/s (collection: 2.072s, learning 0.107s)
             Mean action noise std: 2.64
          Mean value_function loss: 107.5280
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 52.1298
                       Mean reward: 863.00
               Mean episode length: 230.74
    Episode_Reward/reaching_object: 1.3494
     Episode_Reward/lifting_object: 174.5479
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 2.18s
                      Time elapsed: 00:47:46
                               ETA: 00:27:26

################################################################################
                     [1m Learning iteration 1271/2000 [0m                     

                       Computation: 44637 steps/s (collection: 2.094s, learning 0.109s)
             Mean action noise std: 2.64
          Mean value_function loss: 89.9608
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 52.1336
                       Mean reward: 865.84
               Mean episode length: 232.29
    Episode_Reward/reaching_object: 1.3471
     Episode_Reward/lifting_object: 174.0401
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 2.20s
                      Time elapsed: 00:47:48
                               ETA: 00:27:24

################################################################################
                     [1m Learning iteration 1272/2000 [0m                     

                       Computation: 44556 steps/s (collection: 2.097s, learning 0.109s)
             Mean action noise std: 2.64
          Mean value_function loss: 103.4910
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.1436
                       Mean reward: 881.18
               Mean episode length: 235.86
    Episode_Reward/reaching_object: 1.3452
     Episode_Reward/lifting_object: 173.1436
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 2.21s
                      Time elapsed: 00:47:51
                               ETA: 00:27:21

################################################################################
                     [1m Learning iteration 1273/2000 [0m                     

                       Computation: 44139 steps/s (collection: 2.134s, learning 0.093s)
             Mean action noise std: 2.64
          Mean value_function loss: 111.5948
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.1574
                       Mean reward: 860.72
               Mean episode length: 231.56
    Episode_Reward/reaching_object: 1.3580
     Episode_Reward/lifting_object: 175.5337
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 2.23s
                      Time elapsed: 00:47:53
                               ETA: 00:27:19

################################################################################
                     [1m Learning iteration 1274/2000 [0m                     

                       Computation: 43648 steps/s (collection: 2.133s, learning 0.120s)
             Mean action noise std: 2.64
          Mean value_function loss: 135.2548
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.1704
                       Mean reward: 856.80
               Mean episode length: 230.63
    Episode_Reward/reaching_object: 1.3090
     Episode_Reward/lifting_object: 169.1718
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 2.25s
                      Time elapsed: 00:47:55
                               ETA: 00:27:17

################################################################################
                     [1m Learning iteration 1275/2000 [0m                     

                       Computation: 44440 steps/s (collection: 2.087s, learning 0.125s)
             Mean action noise std: 2.64
          Mean value_function loss: 90.2258
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 52.1783
                       Mean reward: 913.17
               Mean episode length: 243.72
    Episode_Reward/reaching_object: 1.3563
     Episode_Reward/lifting_object: 175.4505
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 2.21s
                      Time elapsed: 00:47:57
                               ETA: 00:27:15

################################################################################
                     [1m Learning iteration 1276/2000 [0m                     

                       Computation: 44650 steps/s (collection: 2.086s, learning 0.116s)
             Mean action noise std: 2.65
          Mean value_function loss: 111.8330
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 52.1907
                       Mean reward: 880.86
               Mean episode length: 236.20
    Episode_Reward/reaching_object: 1.3587
     Episode_Reward/lifting_object: 176.1275
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 2.20s
                      Time elapsed: 00:48:00
                               ETA: 00:27:12

################################################################################
                     [1m Learning iteration 1277/2000 [0m                     

                       Computation: 44376 steps/s (collection: 2.121s, learning 0.095s)
             Mean action noise std: 2.65
          Mean value_function loss: 128.2116
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.2035
                       Mean reward: 853.49
               Mean episode length: 228.96
    Episode_Reward/reaching_object: 1.3234
     Episode_Reward/lifting_object: 171.6582
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 2.22s
                      Time elapsed: 00:48:02
                               ETA: 00:27:10

################################################################################
                     [1m Learning iteration 1278/2000 [0m                     

                       Computation: 43109 steps/s (collection: 2.178s, learning 0.103s)
             Mean action noise std: 2.65
          Mean value_function loss: 109.7092
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 52.2139
                       Mean reward: 881.36
               Mean episode length: 234.81
    Episode_Reward/reaching_object: 1.3362
     Episode_Reward/lifting_object: 173.0317
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 2.28s
                      Time elapsed: 00:48:04
                               ETA: 00:27:08

################################################################################
                     [1m Learning iteration 1279/2000 [0m                     

                       Computation: 44851 steps/s (collection: 2.076s, learning 0.116s)
             Mean action noise std: 2.65
          Mean value_function loss: 131.1521
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.2259
                       Mean reward: 881.87
               Mean episode length: 234.89
    Episode_Reward/reaching_object: 1.3386
     Episode_Reward/lifting_object: 174.0341
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 2.19s
                      Time elapsed: 00:48:06
                               ETA: 00:27:06

################################################################################
                     [1m Learning iteration 1280/2000 [0m                     

                       Computation: 43395 steps/s (collection: 2.102s, learning 0.164s)
             Mean action noise std: 2.65
          Mean value_function loss: 152.0403
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.2350
                       Mean reward: 877.76
               Mean episode length: 234.15
    Episode_Reward/reaching_object: 1.3000
     Episode_Reward/lifting_object: 168.7175
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 2.27s
                      Time elapsed: 00:48:09
                               ETA: 00:27:03

################################################################################
                     [1m Learning iteration 1281/2000 [0m                     

                       Computation: 44828 steps/s (collection: 2.088s, learning 0.105s)
             Mean action noise std: 2.65
          Mean value_function loss: 124.4215
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 52.2453
                       Mean reward: 865.83
               Mean episode length: 232.03
    Episode_Reward/reaching_object: 1.3314
     Episode_Reward/lifting_object: 172.7403
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 2.19s
                      Time elapsed: 00:48:11
                               ETA: 00:27:01

################################################################################
                     [1m Learning iteration 1282/2000 [0m                     

                       Computation: 45528 steps/s (collection: 2.065s, learning 0.094s)
             Mean action noise std: 2.65
          Mean value_function loss: 122.4518
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 52.2531
                       Mean reward: 839.86
               Mean episode length: 226.35
    Episode_Reward/reaching_object: 1.3205
     Episode_Reward/lifting_object: 171.4705
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 2.16s
                      Time elapsed: 00:48:13
                               ETA: 00:26:59

################################################################################
                     [1m Learning iteration 1283/2000 [0m                     

                       Computation: 44963 steps/s (collection: 2.093s, learning 0.093s)
             Mean action noise std: 2.66
          Mean value_function loss: 119.0718
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 52.2673
                       Mean reward: 878.14
               Mean episode length: 235.60
    Episode_Reward/reaching_object: 1.3332
     Episode_Reward/lifting_object: 173.2634
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 2.19s
                      Time elapsed: 00:48:15
                               ETA: 00:26:56

################################################################################
                     [1m Learning iteration 1284/2000 [0m                     

                       Computation: 44157 steps/s (collection: 2.116s, learning 0.110s)
             Mean action noise std: 2.66
          Mean value_function loss: 151.8964
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 52.2834
                       Mean reward: 852.31
               Mean episode length: 227.73
    Episode_Reward/reaching_object: 1.3021
     Episode_Reward/lifting_object: 169.2219
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 2.23s
                      Time elapsed: 00:48:17
                               ETA: 00:26:54

################################################################################
                     [1m Learning iteration 1285/2000 [0m                     

                       Computation: 43276 steps/s (collection: 2.124s, learning 0.147s)
             Mean action noise std: 2.66
          Mean value_function loss: 139.7050
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 52.2885
                       Mean reward: 867.79
               Mean episode length: 232.75
    Episode_Reward/reaching_object: 1.3332
     Episode_Reward/lifting_object: 172.9225
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 2.27s
                      Time elapsed: 00:48:20
                               ETA: 00:26:52

################################################################################
                     [1m Learning iteration 1286/2000 [0m                     

                       Computation: 44284 steps/s (collection: 2.126s, learning 0.094s)
             Mean action noise std: 2.66
          Mean value_function loss: 117.5393
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 52.2990
                       Mean reward: 908.39
               Mean episode length: 241.38
    Episode_Reward/reaching_object: 1.3562
     Episode_Reward/lifting_object: 176.9214
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 2.22s
                      Time elapsed: 00:48:22
                               ETA: 00:26:50

################################################################################
                     [1m Learning iteration 1287/2000 [0m                     

                       Computation: 44729 steps/s (collection: 2.101s, learning 0.097s)
             Mean action noise std: 2.66
          Mean value_function loss: 95.3485
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.3084
                       Mean reward: 901.63
               Mean episode length: 239.82
    Episode_Reward/reaching_object: 1.3498
     Episode_Reward/lifting_object: 175.6484
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 2.20s
                      Time elapsed: 00:48:24
                               ETA: 00:26:47

################################################################################
                     [1m Learning iteration 1288/2000 [0m                     

                       Computation: 44314 steps/s (collection: 2.089s, learning 0.129s)
             Mean action noise std: 2.66
          Mean value_function loss: 98.8912
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.3204
                       Mean reward: 900.57
               Mean episode length: 240.64
    Episode_Reward/reaching_object: 1.3263
     Episode_Reward/lifting_object: 172.2667
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 2.22s
                      Time elapsed: 00:48:26
                               ETA: 00:26:45

################################################################################
                     [1m Learning iteration 1289/2000 [0m                     

                       Computation: 42737 steps/s (collection: 2.159s, learning 0.142s)
             Mean action noise std: 2.66
          Mean value_function loss: 149.5878
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.3290
                       Mean reward: 838.82
               Mean episode length: 226.30
    Episode_Reward/reaching_object: 1.3082
     Episode_Reward/lifting_object: 170.1566
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 2.30s
                      Time elapsed: 00:48:28
                               ETA: 00:26:43

################################################################################
                     [1m Learning iteration 1290/2000 [0m                     

                       Computation: 43296 steps/s (collection: 2.143s, learning 0.128s)
             Mean action noise std: 2.67
          Mean value_function loss: 165.5668
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 52.3430
                       Mean reward: 844.18
               Mean episode length: 227.41
    Episode_Reward/reaching_object: 1.2902
     Episode_Reward/lifting_object: 167.6663
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 2.27s
                      Time elapsed: 00:48:31
                               ETA: 00:26:41

################################################################################
                     [1m Learning iteration 1291/2000 [0m                     

                       Computation: 43045 steps/s (collection: 2.177s, learning 0.107s)
             Mean action noise std: 2.67
          Mean value_function loss: 127.7971
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 52.3534
                       Mean reward: 875.06
               Mean episode length: 234.85
    Episode_Reward/reaching_object: 1.3124
     Episode_Reward/lifting_object: 170.8889
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 2.28s
                      Time elapsed: 00:48:33
                               ETA: 00:26:38

################################################################################
                     [1m Learning iteration 1292/2000 [0m                     

                       Computation: 44702 steps/s (collection: 2.105s, learning 0.094s)
             Mean action noise std: 2.67
          Mean value_function loss: 98.6346
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 52.3568
                       Mean reward: 920.99
               Mean episode length: 245.29
    Episode_Reward/reaching_object: 1.3510
     Episode_Reward/lifting_object: 175.9114
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 2.20s
                      Time elapsed: 00:48:35
                               ETA: 00:26:36

################################################################################
                     [1m Learning iteration 1293/2000 [0m                     

                       Computation: 43967 steps/s (collection: 2.140s, learning 0.096s)
             Mean action noise std: 2.67
          Mean value_function loss: 156.0203
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.3675
                       Mean reward: 854.02
               Mean episode length: 229.66
    Episode_Reward/reaching_object: 1.3002
     Episode_Reward/lifting_object: 169.6697
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 2.24s
                      Time elapsed: 00:48:37
                               ETA: 00:26:34

################################################################################
                     [1m Learning iteration 1294/2000 [0m                     

                       Computation: 43755 steps/s (collection: 2.112s, learning 0.135s)
             Mean action noise std: 2.67
          Mean value_function loss: 139.2572
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 52.3801
                       Mean reward: 849.00
               Mean episode length: 226.68
    Episode_Reward/reaching_object: 1.3192
     Episode_Reward/lifting_object: 172.0626
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 2.25s
                      Time elapsed: 00:48:40
                               ETA: 00:26:32

################################################################################
                     [1m Learning iteration 1295/2000 [0m                     

                       Computation: 41763 steps/s (collection: 2.195s, learning 0.159s)
             Mean action noise std: 2.67
          Mean value_function loss: 108.4854
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 52.3882
                       Mean reward: 903.18
               Mean episode length: 241.85
    Episode_Reward/reaching_object: 1.3181
     Episode_Reward/lifting_object: 171.4355
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 2.35s
                      Time elapsed: 00:48:42
                               ETA: 00:26:29

################################################################################
                     [1m Learning iteration 1296/2000 [0m                     

                       Computation: 43811 steps/s (collection: 2.124s, learning 0.120s)
             Mean action noise std: 2.67
          Mean value_function loss: 104.1441
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.3968
                       Mean reward: 868.53
               Mean episode length: 232.33
    Episode_Reward/reaching_object: 1.3442
     Episode_Reward/lifting_object: 175.6754
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 2.24s
                      Time elapsed: 00:48:44
                               ETA: 00:26:27

################################################################################
                     [1m Learning iteration 1297/2000 [0m                     

                       Computation: 44155 steps/s (collection: 2.116s, learning 0.110s)
             Mean action noise std: 2.67
          Mean value_function loss: 141.9240
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 52.4056
                       Mean reward: 799.14
               Mean episode length: 215.24
    Episode_Reward/reaching_object: 1.3076
     Episode_Reward/lifting_object: 170.3031
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 2.23s
                      Time elapsed: 00:48:47
                               ETA: 00:26:25

################################################################################
                     [1m Learning iteration 1298/2000 [0m                     

                       Computation: 44277 steps/s (collection: 2.089s, learning 0.131s)
             Mean action noise std: 2.67
          Mean value_function loss: 139.8381
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 52.4164
                       Mean reward: 823.86
               Mean episode length: 222.38
    Episode_Reward/reaching_object: 1.3104
     Episode_Reward/lifting_object: 170.4556
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 2.22s
                      Time elapsed: 00:48:49
                               ETA: 00:26:23

################################################################################
                     [1m Learning iteration 1299/2000 [0m                     

                       Computation: 44566 steps/s (collection: 2.113s, learning 0.093s)
             Mean action noise std: 2.68
          Mean value_function loss: 120.2266
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.4301
                       Mean reward: 856.39
               Mean episode length: 230.51
    Episode_Reward/reaching_object: 1.2980
     Episode_Reward/lifting_object: 168.7643
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 2.21s
                      Time elapsed: 00:48:51
                               ETA: 00:26:20

################################################################################
                     [1m Learning iteration 1300/2000 [0m                     

                       Computation: 43951 steps/s (collection: 2.138s, learning 0.099s)
             Mean action noise std: 2.68
          Mean value_function loss: 120.4323
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 52.4430
                       Mean reward: 832.91
               Mean episode length: 224.69
    Episode_Reward/reaching_object: 1.3107
     Episode_Reward/lifting_object: 170.5402
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 2.24s
                      Time elapsed: 00:48:53
                               ETA: 00:26:18

################################################################################
                     [1m Learning iteration 1301/2000 [0m                     

                       Computation: 42407 steps/s (collection: 2.215s, learning 0.103s)
             Mean action noise std: 2.68
          Mean value_function loss: 146.7405
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.4561
                       Mean reward: 896.72
               Mean episode length: 238.59
    Episode_Reward/reaching_object: 1.3109
     Episode_Reward/lifting_object: 171.1410
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 2.32s
                      Time elapsed: 00:48:56
                               ETA: 00:26:16

################################################################################
                     [1m Learning iteration 1302/2000 [0m                     

                       Computation: 45172 steps/s (collection: 2.080s, learning 0.096s)
             Mean action noise std: 2.68
          Mean value_function loss: 111.2298
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.4665
                       Mean reward: 884.25
               Mean episode length: 236.52
    Episode_Reward/reaching_object: 1.3531
     Episode_Reward/lifting_object: 176.1676
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 2.18s
                      Time elapsed: 00:48:58
                               ETA: 00:26:13

################################################################################
                     [1m Learning iteration 1303/2000 [0m                     

                       Computation: 44638 steps/s (collection: 2.088s, learning 0.115s)
             Mean action noise std: 2.68
          Mean value_function loss: 155.8410
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 52.4766
                       Mean reward: 848.95
               Mean episode length: 228.17
    Episode_Reward/reaching_object: 1.2917
     Episode_Reward/lifting_object: 168.1429
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 2.20s
                      Time elapsed: 00:49:00
                               ETA: 00:26:11

################################################################################
                     [1m Learning iteration 1304/2000 [0m                     

                       Computation: 44389 steps/s (collection: 2.113s, learning 0.102s)
             Mean action noise std: 2.68
          Mean value_function loss: 114.4762
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 52.4846
                       Mean reward: 858.61
               Mean episode length: 230.95
    Episode_Reward/reaching_object: 1.3427
     Episode_Reward/lifting_object: 175.0472
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 2.21s
                      Time elapsed: 00:49:02
                               ETA: 00:26:09

################################################################################
                     [1m Learning iteration 1305/2000 [0m                     

                       Computation: 41977 steps/s (collection: 2.220s, learning 0.122s)
             Mean action noise std: 2.68
          Mean value_function loss: 124.4109
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 52.4909
                       Mean reward: 895.34
               Mean episode length: 240.14
    Episode_Reward/reaching_object: 1.3235
     Episode_Reward/lifting_object: 171.7008
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 2.34s
                      Time elapsed: 00:49:04
                               ETA: 00:26:07

################################################################################
                     [1m Learning iteration 1306/2000 [0m                     

                       Computation: 42485 steps/s (collection: 2.201s, learning 0.113s)
             Mean action noise std: 2.68
          Mean value_function loss: 138.3180
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 52.4931
                       Mean reward: 868.95
               Mean episode length: 232.73
    Episode_Reward/reaching_object: 1.3146
     Episode_Reward/lifting_object: 171.3966
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 2.31s
                      Time elapsed: 00:49:07
                               ETA: 00:26:04

################################################################################
                     [1m Learning iteration 1307/2000 [0m                     

                       Computation: 37544 steps/s (collection: 2.487s, learning 0.132s)
             Mean action noise std: 2.68
          Mean value_function loss: 130.9270
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 52.4961
                       Mean reward: 858.50
               Mean episode length: 232.68
    Episode_Reward/reaching_object: 1.3208
     Episode_Reward/lifting_object: 172.9047
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 2.62s
                      Time elapsed: 00:49:09
                               ETA: 00:26:02

################################################################################
                     [1m Learning iteration 1308/2000 [0m                     

                       Computation: 27156 steps/s (collection: 3.311s, learning 0.309s)
             Mean action noise std: 2.68
          Mean value_function loss: 179.9146
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 52.4985
                       Mean reward: 851.61
               Mean episode length: 229.95
    Episode_Reward/reaching_object: 1.2877
     Episode_Reward/lifting_object: 168.9709
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 3.62s
                      Time elapsed: 00:49:13
                               ETA: 00:26:01

################################################################################
                     [1m Learning iteration 1309/2000 [0m                     

                       Computation: 34594 steps/s (collection: 2.629s, learning 0.212s)
             Mean action noise std: 2.68
          Mean value_function loss: 159.7884
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 52.4995
                       Mean reward: 909.37
               Mean episode length: 240.96
    Episode_Reward/reaching_object: 1.3016
     Episode_Reward/lifting_object: 171.5665
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 2.84s
                      Time elapsed: 00:49:16
                               ETA: 00:25:59

################################################################################
                     [1m Learning iteration 1310/2000 [0m                     

                       Computation: 35191 steps/s (collection: 2.581s, learning 0.213s)
             Mean action noise std: 2.68
          Mean value_function loss: 144.4382
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 52.5008
                       Mean reward: 874.10
               Mean episode length: 233.43
    Episode_Reward/reaching_object: 1.3038
     Episode_Reward/lifting_object: 171.0544
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 2.79s
                      Time elapsed: 00:49:19
                               ETA: 00:25:57

################################################################################
                     [1m Learning iteration 1311/2000 [0m                     

                       Computation: 39208 steps/s (collection: 2.411s, learning 0.096s)
             Mean action noise std: 2.68
          Mean value_function loss: 207.8690
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 52.5017
                       Mean reward: 827.37
               Mean episode length: 222.99
    Episode_Reward/reaching_object: 1.3022
     Episode_Reward/lifting_object: 170.7906
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 2.51s
                      Time elapsed: 00:49:21
                               ETA: 00:25:55

################################################################################
                     [1m Learning iteration 1312/2000 [0m                     

                       Computation: 42698 steps/s (collection: 2.202s, learning 0.100s)
             Mean action noise std: 2.68
          Mean value_function loss: 241.1913
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 52.5027
                       Mean reward: 829.78
               Mean episode length: 224.27
    Episode_Reward/reaching_object: 1.2687
     Episode_Reward/lifting_object: 166.2363
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 2.30s
                      Time elapsed: 00:49:23
                               ETA: 00:25:53

################################################################################
                     [1m Learning iteration 1313/2000 [0m                     

                       Computation: 42094 steps/s (collection: 2.228s, learning 0.108s)
             Mean action noise std: 2.68
          Mean value_function loss: 242.1736
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 52.5034
                       Mean reward: 814.91
               Mean episode length: 221.41
    Episode_Reward/reaching_object: 1.2579
     Episode_Reward/lifting_object: 164.6106
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 2.34s
                      Time elapsed: 00:49:26
                               ETA: 00:25:50

################################################################################
                     [1m Learning iteration 1314/2000 [0m                     

                       Computation: 33415 steps/s (collection: 2.728s, learning 0.214s)
             Mean action noise std: 2.68
          Mean value_function loss: 258.0422
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 52.5042
                       Mean reward: 808.45
               Mean episode length: 220.22
    Episode_Reward/reaching_object: 1.2563
     Episode_Reward/lifting_object: 165.0849
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 2.94s
                      Time elapsed: 00:49:29
                               ETA: 00:25:48

################################################################################
                     [1m Learning iteration 1315/2000 [0m                     

                       Computation: 30301 steps/s (collection: 2.962s, learning 0.283s)
             Mean action noise std: 2.68
          Mean value_function loss: 276.7622
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 52.5053
                       Mean reward: 809.93
               Mean episode length: 217.90
    Episode_Reward/reaching_object: 1.2120
     Episode_Reward/lifting_object: 159.7068
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 3.24s
                      Time elapsed: 00:49:32
                               ETA: 00:25:47

################################################################################
                     [1m Learning iteration 1316/2000 [0m                     

                       Computation: 32705 steps/s (collection: 2.722s, learning 0.284s)
             Mean action noise std: 2.68
          Mean value_function loss: 276.8750
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 52.5076
                       Mean reward: 821.42
               Mean episode length: 221.59
    Episode_Reward/reaching_object: 1.2443
     Episode_Reward/lifting_object: 163.8013
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 3.01s
                      Time elapsed: 00:49:35
                               ETA: 00:25:45

################################################################################
                     [1m Learning iteration 1317/2000 [0m                     

                       Computation: 32055 steps/s (collection: 2.750s, learning 0.317s)
             Mean action noise std: 2.69
          Mean value_function loss: 297.4521
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 52.5167
                       Mean reward: 811.04
               Mean episode length: 219.47
    Episode_Reward/reaching_object: 1.2163
     Episode_Reward/lifting_object: 160.1213
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 3.07s
                      Time elapsed: 00:49:38
                               ETA: 00:25:43

################################################################################
                     [1m Learning iteration 1318/2000 [0m                     

                       Computation: 35053 steps/s (collection: 2.636s, learning 0.169s)
             Mean action noise std: 2.69
          Mean value_function loss: 242.7766
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 52.5248
                       Mean reward: 854.26
               Mean episode length: 229.46
    Episode_Reward/reaching_object: 1.2473
     Episode_Reward/lifting_object: 165.2158
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 2.80s
                      Time elapsed: 00:49:41
                               ETA: 00:25:41

################################################################################
                     [1m Learning iteration 1319/2000 [0m                     

                       Computation: 37768 steps/s (collection: 2.501s, learning 0.102s)
             Mean action noise std: 2.69
          Mean value_function loss: 261.5477
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 52.5349
                       Mean reward: 862.16
               Mean episode length: 231.36
    Episode_Reward/reaching_object: 1.2546
     Episode_Reward/lifting_object: 165.1718
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 2.60s
                      Time elapsed: 00:49:43
                               ETA: 00:25:39

################################################################################
                     [1m Learning iteration 1320/2000 [0m                     

                       Computation: 41900 steps/s (collection: 2.246s, learning 0.101s)
             Mean action noise std: 2.69
          Mean value_function loss: 214.1398
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 52.5459
                       Mean reward: 846.20
               Mean episode length: 227.06
    Episode_Reward/reaching_object: 1.2571
     Episode_Reward/lifting_object: 166.1587
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 2.35s
                      Time elapsed: 00:49:46
                               ETA: 00:25:37

################################################################################
                     [1m Learning iteration 1321/2000 [0m                     

                       Computation: 42029 steps/s (collection: 2.213s, learning 0.126s)
             Mean action noise std: 2.69
          Mean value_function loss: 238.0640
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 52.5499
                       Mean reward: 847.46
               Mean episode length: 227.63
    Episode_Reward/reaching_object: 1.2785
     Episode_Reward/lifting_object: 168.5253
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 2.34s
                      Time elapsed: 00:49:48
                               ETA: 00:25:35

################################################################################
                     [1m Learning iteration 1322/2000 [0m                     

                       Computation: 41688 steps/s (collection: 2.246s, learning 0.112s)
             Mean action noise std: 2.69
          Mean value_function loss: 262.8511
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 52.5544
                       Mean reward: 810.68
               Mean episode length: 221.52
    Episode_Reward/reaching_object: 1.2421
     Episode_Reward/lifting_object: 162.5331
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 2.36s
                      Time elapsed: 00:49:51
                               ETA: 00:25:32

################################################################################
                     [1m Learning iteration 1323/2000 [0m                     

                       Computation: 42151 steps/s (collection: 2.225s, learning 0.108s)
             Mean action noise std: 2.69
          Mean value_function loss: 211.2044
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 52.5590
                       Mean reward: 845.33
               Mean episode length: 227.92
    Episode_Reward/reaching_object: 1.2966
     Episode_Reward/lifting_object: 171.8255
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 2.33s
                      Time elapsed: 00:49:53
                               ETA: 00:25:30

################################################################################
                     [1m Learning iteration 1324/2000 [0m                     

                       Computation: 42174 steps/s (collection: 2.213s, learning 0.118s)
             Mean action noise std: 2.69
          Mean value_function loss: 225.4525
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 52.5663
                       Mean reward: 862.61
               Mean episode length: 232.63
    Episode_Reward/reaching_object: 1.2860
     Episode_Reward/lifting_object: 169.7392
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 2.33s
                      Time elapsed: 00:49:55
                               ETA: 00:25:28

################################################################################
                     [1m Learning iteration 1325/2000 [0m                     

                       Computation: 41356 steps/s (collection: 2.253s, learning 0.124s)
             Mean action noise std: 2.69
          Mean value_function loss: 245.7847
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 52.5788
                       Mean reward: 815.20
               Mean episode length: 219.86
    Episode_Reward/reaching_object: 1.2570
     Episode_Reward/lifting_object: 164.2953
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 2.38s
                      Time elapsed: 00:49:58
                               ETA: 00:25:26

################################################################################
                     [1m Learning iteration 1326/2000 [0m                     

                       Computation: 41367 steps/s (collection: 2.264s, learning 0.112s)
             Mean action noise std: 2.70
          Mean value_function loss: 171.5771
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 52.5929
                       Mean reward: 850.92
               Mean episode length: 228.40
    Episode_Reward/reaching_object: 1.2977
     Episode_Reward/lifting_object: 171.2850
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 2.38s
                      Time elapsed: 00:50:00
                               ETA: 00:25:23

################################################################################
                     [1m Learning iteration 1327/2000 [0m                     

                       Computation: 41844 steps/s (collection: 2.222s, learning 0.127s)
             Mean action noise std: 2.70
          Mean value_function loss: 147.9858
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 52.6031
                       Mean reward: 864.25
               Mean episode length: 231.85
    Episode_Reward/reaching_object: 1.2868
     Episode_Reward/lifting_object: 169.0961
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 2.35s
                      Time elapsed: 00:50:02
                               ETA: 00:25:21

################################################################################
                     [1m Learning iteration 1328/2000 [0m                     

                       Computation: 42687 steps/s (collection: 2.205s, learning 0.098s)
             Mean action noise std: 2.70
          Mean value_function loss: 180.4466
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.6191
                       Mean reward: 853.16
               Mean episode length: 229.01
    Episode_Reward/reaching_object: 1.2810
     Episode_Reward/lifting_object: 168.2907
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 2.30s
                      Time elapsed: 00:50:05
                               ETA: 00:25:19

################################################################################
                     [1m Learning iteration 1329/2000 [0m                     

                       Computation: 39267 steps/s (collection: 2.277s, learning 0.226s)
             Mean action noise std: 2.70
          Mean value_function loss: 156.2656
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 52.6329
                       Mean reward: 831.58
               Mean episode length: 224.78
    Episode_Reward/reaching_object: 1.2947
     Episode_Reward/lifting_object: 170.3380
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 2.50s
                      Time elapsed: 00:50:07
                               ETA: 00:25:17

################################################################################
                     [1m Learning iteration 1330/2000 [0m                     

                       Computation: 40812 steps/s (collection: 2.270s, learning 0.139s)
             Mean action noise std: 2.70
          Mean value_function loss: 120.0427
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.6481
                       Mean reward: 818.64
               Mean episode length: 221.67
    Episode_Reward/reaching_object: 1.3073
     Episode_Reward/lifting_object: 171.2385
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 2.41s
                      Time elapsed: 00:50:09
                               ETA: 00:25:15

################################################################################
                     [1m Learning iteration 1331/2000 [0m                     

                       Computation: 36792 steps/s (collection: 2.476s, learning 0.196s)
             Mean action noise std: 2.70
          Mean value_function loss: 124.7353
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 52.6554
                       Mean reward: 857.08
               Mean episode length: 230.08
    Episode_Reward/reaching_object: 1.3500
     Episode_Reward/lifting_object: 176.6655
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 2.67s
                      Time elapsed: 00:50:12
                               ETA: 00:25:13

################################################################################
                     [1m Learning iteration 1332/2000 [0m                     

                       Computation: 38150 steps/s (collection: 2.394s, learning 0.183s)
             Mean action noise std: 2.71
          Mean value_function loss: 132.2463
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.6650
                       Mean reward: 870.82
               Mean episode length: 232.78
    Episode_Reward/reaching_object: 1.3173
     Episode_Reward/lifting_object: 172.0940
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 2.58s
                      Time elapsed: 00:50:15
                               ETA: 00:25:11

################################################################################
                     [1m Learning iteration 1333/2000 [0m                     

                       Computation: 15041 steps/s (collection: 6.386s, learning 0.149s)
             Mean action noise std: 2.71
          Mean value_function loss: 142.6009
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.6764
                       Mean reward: 867.55
               Mean episode length: 232.69
    Episode_Reward/reaching_object: 1.3437
     Episode_Reward/lifting_object: 175.9768
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 6.54s
                      Time elapsed: 00:50:21
                               ETA: 00:25:10

################################################################################
                     [1m Learning iteration 1334/2000 [0m                     

                       Computation: 11527 steps/s (collection: 8.367s, learning 0.161s)
             Mean action noise std: 2.71
          Mean value_function loss: 155.9993
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.6849
                       Mean reward: 861.92
               Mean episode length: 231.74
    Episode_Reward/reaching_object: 1.3171
     Episode_Reward/lifting_object: 170.8819
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 8.53s
                      Time elapsed: 00:50:30
                               ETA: 00:25:11

################################################################################
                     [1m Learning iteration 1335/2000 [0m                     

                       Computation: 13371 steps/s (collection: 7.232s, learning 0.120s)
             Mean action noise std: 2.71
          Mean value_function loss: 142.0883
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.6969
                       Mean reward: 875.82
               Mean episode length: 234.33
    Episode_Reward/reaching_object: 1.3327
     Episode_Reward/lifting_object: 173.8538
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 7.35s
                      Time elapsed: 00:50:37
                               ETA: 00:25:12

################################################################################
                     [1m Learning iteration 1336/2000 [0m                     

                       Computation: 14414 steps/s (collection: 6.690s, learning 0.130s)
             Mean action noise std: 2.71
          Mean value_function loss: 171.5215
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.7041
                       Mean reward: 876.81
               Mean episode length: 234.96
    Episode_Reward/reaching_object: 1.3195
     Episode_Reward/lifting_object: 170.1505
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 6.82s
                      Time elapsed: 00:50:44
                               ETA: 00:25:11

################################################################################
                     [1m Learning iteration 1337/2000 [0m                     

                       Computation: 14319 steps/s (collection: 6.748s, learning 0.117s)
             Mean action noise std: 2.71
          Mean value_function loss: 122.6413
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.7114
                       Mean reward: 873.33
               Mean episode length: 232.98
    Episode_Reward/reaching_object: 1.3343
     Episode_Reward/lifting_object: 173.0696
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 6.87s
                      Time elapsed: 00:50:51
                               ETA: 00:25:11

################################################################################
                     [1m Learning iteration 1338/2000 [0m                     

                       Computation: 14140 steps/s (collection: 6.825s, learning 0.127s)
             Mean action noise std: 2.71
          Mean value_function loss: 111.1881
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 52.7233
                       Mean reward: 908.80
               Mean episode length: 241.71
    Episode_Reward/reaching_object: 1.3492
     Episode_Reward/lifting_object: 175.2296
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 6.95s
                      Time elapsed: 00:50:58
                               ETA: 00:25:12

################################################################################
                     [1m Learning iteration 1339/2000 [0m                     

                       Computation: 14052 steps/s (collection: 6.850s, learning 0.145s)
             Mean action noise std: 2.71
          Mean value_function loss: 160.1990
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.7325
                       Mean reward: 858.66
               Mean episode length: 230.64
    Episode_Reward/reaching_object: 1.3001
     Episode_Reward/lifting_object: 167.2380
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 7.00s
                      Time elapsed: 00:51:05
                               ETA: 00:25:12

################################################################################
                     [1m Learning iteration 1340/2000 [0m                     

                       Computation: 14151 steps/s (collection: 6.821s, learning 0.125s)
             Mean action noise std: 2.72
          Mean value_function loss: 149.3696
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 52.7470
                       Mean reward: 859.80
               Mean episode length: 231.06
    Episode_Reward/reaching_object: 1.2838
     Episode_Reward/lifting_object: 165.8282
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 6.95s
                      Time elapsed: 00:51:12
                               ETA: 00:25:12

################################################################################
                     [1m Learning iteration 1341/2000 [0m                     

                       Computation: 13293 steps/s (collection: 7.288s, learning 0.108s)
             Mean action noise std: 2.72
          Mean value_function loss: 109.0456
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 52.7664
                       Mean reward: 866.20
               Mean episode length: 231.21
    Episode_Reward/reaching_object: 1.3329
     Episode_Reward/lifting_object: 172.7594
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 7.40s
                      Time elapsed: 00:51:19
                               ETA: 00:25:12

################################################################################
                     [1m Learning iteration 1342/2000 [0m                     

                       Computation: 47477 steps/s (collection: 1.974s, learning 0.097s)
             Mean action noise std: 2.72
          Mean value_function loss: 121.5860
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 52.7755
                       Mean reward: 889.21
               Mean episode length: 237.85
    Episode_Reward/reaching_object: 1.3296
     Episode_Reward/lifting_object: 173.1924
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 2.07s
                      Time elapsed: 00:51:21
                               ETA: 00:25:09

################################################################################
                     [1m Learning iteration 1343/2000 [0m                     

                       Computation: 47743 steps/s (collection: 1.950s, learning 0.109s)
             Mean action noise std: 2.72
          Mean value_function loss: 126.6679
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 52.7872
                       Mean reward: 880.40
               Mean episode length: 234.09
    Episode_Reward/reaching_object: 1.3302
     Episode_Reward/lifting_object: 173.0002
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 2.06s
                      Time elapsed: 00:51:23
                               ETA: 00:25:07

################################################################################
                     [1m Learning iteration 1344/2000 [0m                     

                       Computation: 47249 steps/s (collection: 1.965s, learning 0.116s)
             Mean action noise std: 2.72
          Mean value_function loss: 113.4450
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 52.7979
                       Mean reward: 912.37
               Mean episode length: 244.54
    Episode_Reward/reaching_object: 1.3371
     Episode_Reward/lifting_object: 173.6122
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 2.08s
                      Time elapsed: 00:51:25
                               ETA: 00:25:05

################################################################################
                     [1m Learning iteration 1345/2000 [0m                     

                       Computation: 44913 steps/s (collection: 2.089s, learning 0.100s)
             Mean action noise std: 2.72
          Mean value_function loss: 128.4377
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 52.8116
                       Mean reward: 883.86
               Mean episode length: 237.10
    Episode_Reward/reaching_object: 1.3273
     Episode_Reward/lifting_object: 173.2572
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 2.19s
                      Time elapsed: 00:51:28
                               ETA: 00:25:02

################################################################################
                     [1m Learning iteration 1346/2000 [0m                     

                       Computation: 47238 steps/s (collection: 1.989s, learning 0.092s)
             Mean action noise std: 2.73
          Mean value_function loss: 133.6975
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.8291
                       Mean reward: 877.68
               Mean episode length: 234.84
    Episode_Reward/reaching_object: 1.3408
     Episode_Reward/lifting_object: 174.3776
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 2.08s
                      Time elapsed: 00:51:30
                               ETA: 00:25:00

################################################################################
                     [1m Learning iteration 1347/2000 [0m                     

                       Computation: 47873 steps/s (collection: 1.967s, learning 0.086s)
             Mean action noise std: 2.73
          Mean value_function loss: 179.9408
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 52.8409
                       Mean reward: 817.65
               Mean episode length: 222.38
    Episode_Reward/reaching_object: 1.2906
     Episode_Reward/lifting_object: 167.0281
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 2.05s
                      Time elapsed: 00:51:32
                               ETA: 00:24:57

################################################################################
                     [1m Learning iteration 1348/2000 [0m                     

                       Computation: 48378 steps/s (collection: 1.943s, learning 0.089s)
             Mean action noise std: 2.73
          Mean value_function loss: 162.3753
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.8476
                       Mean reward: 851.17
               Mean episode length: 230.61
    Episode_Reward/reaching_object: 1.3053
     Episode_Reward/lifting_object: 168.3134
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 2.03s
                      Time elapsed: 00:51:34
                               ETA: 00:24:55

################################################################################
                     [1m Learning iteration 1349/2000 [0m                     

                       Computation: 47764 steps/s (collection: 1.956s, learning 0.102s)
             Mean action noise std: 2.73
          Mean value_function loss: 166.1379
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 52.8583
                       Mean reward: 857.33
               Mean episode length: 230.79
    Episode_Reward/reaching_object: 1.3271
     Episode_Reward/lifting_object: 171.5772
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 2.06s
                      Time elapsed: 00:51:36
                               ETA: 00:24:53

################################################################################
                     [1m Learning iteration 1350/2000 [0m                     

                       Computation: 47277 steps/s (collection: 1.982s, learning 0.098s)
             Mean action noise std: 2.73
          Mean value_function loss: 197.5466
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.8703
                       Mean reward: 863.24
               Mean episode length: 231.14
    Episode_Reward/reaching_object: 1.3109
     Episode_Reward/lifting_object: 170.3187
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 2.08s
                      Time elapsed: 00:51:38
                               ETA: 00:24:50

################################################################################
                     [1m Learning iteration 1351/2000 [0m                     

                       Computation: 46814 steps/s (collection: 1.999s, learning 0.101s)
             Mean action noise std: 2.73
          Mean value_function loss: 177.7143
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 52.8765
                       Mean reward: 836.56
               Mean episode length: 225.48
    Episode_Reward/reaching_object: 1.2627
     Episode_Reward/lifting_object: 162.7904
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 2.10s
                      Time elapsed: 00:51:40
                               ETA: 00:24:48

################################################################################
                     [1m Learning iteration 1352/2000 [0m                     

                       Computation: 48512 steps/s (collection: 1.931s, learning 0.096s)
             Mean action noise std: 2.73
          Mean value_function loss: 204.9969
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 52.8869
                       Mean reward: 860.23
               Mean episode length: 230.67
    Episode_Reward/reaching_object: 1.2903
     Episode_Reward/lifting_object: 167.2217
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 2.03s
                      Time elapsed: 00:51:42
                               ETA: 00:24:45

################################################################################
                     [1m Learning iteration 1353/2000 [0m                     

                       Computation: 48238 steps/s (collection: 1.947s, learning 0.091s)
             Mean action noise std: 2.73
          Mean value_function loss: 197.3365
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 52.8937
                       Mean reward: 843.70
               Mean episode length: 227.04
    Episode_Reward/reaching_object: 1.2972
     Episode_Reward/lifting_object: 167.9717
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 2.04s
                      Time elapsed: 00:51:44
                               ETA: 00:24:43

################################################################################
                     [1m Learning iteration 1354/2000 [0m                     

                       Computation: 47573 steps/s (collection: 1.977s, learning 0.090s)
             Mean action noise std: 2.74
          Mean value_function loss: 183.0329
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 52.9002
                       Mean reward: 863.11
               Mean episode length: 231.81
    Episode_Reward/reaching_object: 1.2865
     Episode_Reward/lifting_object: 167.6710
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 2.07s
                      Time elapsed: 00:51:46
                               ETA: 00:24:41

################################################################################
                     [1m Learning iteration 1355/2000 [0m                     

                       Computation: 47695 steps/s (collection: 1.973s, learning 0.088s)
             Mean action noise std: 2.74
          Mean value_function loss: 188.4245
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.9098
                       Mean reward: 877.01
               Mean episode length: 234.79
    Episode_Reward/reaching_object: 1.2686
     Episode_Reward/lifting_object: 164.0196
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 2.06s
                      Time elapsed: 00:51:48
                               ETA: 00:24:38

################################################################################
                     [1m Learning iteration 1356/2000 [0m                     

                       Computation: 48428 steps/s (collection: 1.941s, learning 0.089s)
             Mean action noise std: 2.74
          Mean value_function loss: 202.0114
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 52.9172
                       Mean reward: 813.20
               Mean episode length: 222.36
    Episode_Reward/reaching_object: 1.3034
     Episode_Reward/lifting_object: 168.8747
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 2.03s
                      Time elapsed: 00:51:50
                               ETA: 00:24:36

################################################################################
                     [1m Learning iteration 1357/2000 [0m                     

                       Computation: 47927 steps/s (collection: 1.956s, learning 0.096s)
             Mean action noise std: 2.74
          Mean value_function loss: 137.4963
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.9216
                       Mean reward: 893.12
               Mean episode length: 238.53
    Episode_Reward/reaching_object: 1.3430
     Episode_Reward/lifting_object: 175.0399
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 2.05s
                      Time elapsed: 00:51:52
                               ETA: 00:24:33

################################################################################
                     [1m Learning iteration 1358/2000 [0m                     

                       Computation: 47730 steps/s (collection: 1.969s, learning 0.091s)
             Mean action noise std: 2.74
          Mean value_function loss: 153.7868
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 52.9340
                       Mean reward: 861.48
               Mean episode length: 232.10
    Episode_Reward/reaching_object: 1.3041
     Episode_Reward/lifting_object: 169.6665
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 2.06s
                      Time elapsed: 00:51:54
                               ETA: 00:24:31

################################################################################
                     [1m Learning iteration 1359/2000 [0m                     

                       Computation: 47804 steps/s (collection: 1.964s, learning 0.092s)
             Mean action noise std: 2.74
          Mean value_function loss: 138.4901
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 52.9459
                       Mean reward: 893.14
               Mean episode length: 239.61
    Episode_Reward/reaching_object: 1.3089
     Episode_Reward/lifting_object: 170.2536
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 2.06s
                      Time elapsed: 00:51:56
                               ETA: 00:24:29

################################################################################
                     [1m Learning iteration 1360/2000 [0m                     

                       Computation: 47928 steps/s (collection: 1.965s, learning 0.086s)
             Mean action noise std: 2.74
          Mean value_function loss: 163.8287
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 52.9597
                       Mean reward: 846.61
               Mean episode length: 228.04
    Episode_Reward/reaching_object: 1.3410
     Episode_Reward/lifting_object: 173.5149
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 2.05s
                      Time elapsed: 00:51:58
                               ETA: 00:24:26

################################################################################
                     [1m Learning iteration 1361/2000 [0m                     

                       Computation: 47115 steps/s (collection: 1.998s, learning 0.088s)
             Mean action noise std: 2.74
          Mean value_function loss: 169.7538
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 52.9687
                       Mean reward: 863.07
               Mean episode length: 230.90
    Episode_Reward/reaching_object: 1.2967
     Episode_Reward/lifting_object: 167.5411
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 2.09s
                      Time elapsed: 00:52:00
                               ETA: 00:24:24

################################################################################
                     [1m Learning iteration 1362/2000 [0m                     

                       Computation: 47686 steps/s (collection: 1.968s, learning 0.093s)
             Mean action noise std: 2.75
          Mean value_function loss: 164.1985
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.9753
                       Mean reward: 871.85
               Mean episode length: 233.89
    Episode_Reward/reaching_object: 1.3062
     Episode_Reward/lifting_object: 168.7606
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 2.06s
                      Time elapsed: 00:52:03
                               ETA: 00:24:21

################################################################################
                     [1m Learning iteration 1363/2000 [0m                     

                       Computation: 47966 steps/s (collection: 1.951s, learning 0.099s)
             Mean action noise std: 2.75
          Mean value_function loss: 166.1955
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 52.9872
                       Mean reward: 884.00
               Mean episode length: 237.78
    Episode_Reward/reaching_object: 1.3536
     Episode_Reward/lifting_object: 175.5472
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 2.05s
                      Time elapsed: 00:52:05
                               ETA: 00:24:19

################################################################################
                     [1m Learning iteration 1364/2000 [0m                     

                       Computation: 47602 steps/s (collection: 1.972s, learning 0.093s)
             Mean action noise std: 2.75
          Mean value_function loss: 182.5224
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 52.9987
                       Mean reward: 774.51
               Mean episode length: 211.63
    Episode_Reward/reaching_object: 1.2956
     Episode_Reward/lifting_object: 167.1169
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 2.07s
                      Time elapsed: 00:52:07
                               ETA: 00:24:17

################################################################################
                     [1m Learning iteration 1365/2000 [0m                     

                       Computation: 47729 steps/s (collection: 1.971s, learning 0.089s)
             Mean action noise std: 2.75
          Mean value_function loss: 148.0569
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 53.0064
                       Mean reward: 816.67
               Mean episode length: 220.98
    Episode_Reward/reaching_object: 1.2971
     Episode_Reward/lifting_object: 167.9447
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 2.06s
                      Time elapsed: 00:52:09
                               ETA: 00:24:14

################################################################################
                     [1m Learning iteration 1366/2000 [0m                     

                       Computation: 47452 steps/s (collection: 1.980s, learning 0.092s)
             Mean action noise std: 2.75
          Mean value_function loss: 142.5078
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.0158
                       Mean reward: 878.35
               Mean episode length: 237.32
    Episode_Reward/reaching_object: 1.3230
     Episode_Reward/lifting_object: 171.3944
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 2.07s
                      Time elapsed: 00:52:11
                               ETA: 00:24:12

################################################################################
                     [1m Learning iteration 1367/2000 [0m                     

                       Computation: 44220 steps/s (collection: 2.100s, learning 0.123s)
             Mean action noise std: 2.75
          Mean value_function loss: 137.1099
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 53.0283
                       Mean reward: 873.72
               Mean episode length: 234.80
    Episode_Reward/reaching_object: 1.3277
     Episode_Reward/lifting_object: 171.8997
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 2.22s
                      Time elapsed: 00:52:13
                               ETA: 00:24:09

################################################################################
                     [1m Learning iteration 1368/2000 [0m                     

                       Computation: 43649 steps/s (collection: 2.147s, learning 0.106s)
             Mean action noise std: 2.75
          Mean value_function loss: 141.0997
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 53.0419
                       Mean reward: 845.46
               Mean episode length: 229.82
    Episode_Reward/reaching_object: 1.3226
     Episode_Reward/lifting_object: 171.2892
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 2.25s
                      Time elapsed: 00:52:15
                               ETA: 00:24:07

################################################################################
                     [1m Learning iteration 1369/2000 [0m                     

                       Computation: 41885 steps/s (collection: 2.195s, learning 0.152s)
             Mean action noise std: 2.76
          Mean value_function loss: 157.2099
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 53.0558
                       Mean reward: 842.66
               Mean episode length: 227.00
    Episode_Reward/reaching_object: 1.3216
     Episode_Reward/lifting_object: 171.2637
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 2.35s
                      Time elapsed: 00:52:18
                               ETA: 00:24:05

################################################################################
                     [1m Learning iteration 1370/2000 [0m                     

                       Computation: 38047 steps/s (collection: 2.464s, learning 0.120s)
             Mean action noise std: 2.76
          Mean value_function loss: 168.1399
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 53.0712
                       Mean reward: 876.21
               Mean episode length: 234.06
    Episode_Reward/reaching_object: 1.3288
     Episode_Reward/lifting_object: 172.3737
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 2.58s
                      Time elapsed: 00:52:20
                               ETA: 00:24:03

################################################################################
                     [1m Learning iteration 1371/2000 [0m                     

                       Computation: 35629 steps/s (collection: 2.591s, learning 0.168s)
             Mean action noise std: 2.76
          Mean value_function loss: 155.0830
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.0786
                       Mean reward: 835.77
               Mean episode length: 225.52
    Episode_Reward/reaching_object: 1.2809
     Episode_Reward/lifting_object: 165.2336
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 2.76s
                      Time elapsed: 00:52:23
                               ETA: 00:24:01

################################################################################
                     [1m Learning iteration 1372/2000 [0m                     

                       Computation: 35917 steps/s (collection: 2.497s, learning 0.240s)
             Mean action noise std: 2.76
          Mean value_function loss: 113.8806
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 53.0897
                       Mean reward: 876.82
               Mean episode length: 234.52
    Episode_Reward/reaching_object: 1.3334
     Episode_Reward/lifting_object: 172.4744
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 2.74s
                      Time elapsed: 00:52:26
                               ETA: 00:23:59

################################################################################
                     [1m Learning iteration 1373/2000 [0m                     

                       Computation: 40378 steps/s (collection: 2.344s, learning 0.091s)
             Mean action noise std: 2.76
          Mean value_function loss: 138.3323
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.0984
                       Mean reward: 868.24
               Mean episode length: 232.42
    Episode_Reward/reaching_object: 1.3382
     Episode_Reward/lifting_object: 172.7713
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 2.43s
                      Time elapsed: 00:52:28
                               ETA: 00:23:56

################################################################################
                     [1m Learning iteration 1374/2000 [0m                     

                       Computation: 42653 steps/s (collection: 2.163s, learning 0.142s)
             Mean action noise std: 2.76
          Mean value_function loss: 98.6236
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 53.1067
                       Mean reward: 909.15
               Mean episode length: 241.99
    Episode_Reward/reaching_object: 1.3532
     Episode_Reward/lifting_object: 174.7920
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 2.30s
                      Time elapsed: 00:52:30
                               ETA: 00:23:54

################################################################################
                     [1m Learning iteration 1375/2000 [0m                     

                       Computation: 42754 steps/s (collection: 2.155s, learning 0.144s)
             Mean action noise std: 2.77
          Mean value_function loss: 86.2209
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 53.1193
                       Mean reward: 890.18
               Mean episode length: 237.40
    Episode_Reward/reaching_object: 1.3776
     Episode_Reward/lifting_object: 177.6480
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 2.30s
                      Time elapsed: 00:52:33
                               ETA: 00:23:52

################################################################################
                     [1m Learning iteration 1376/2000 [0m                     

                       Computation: 44199 steps/s (collection: 2.107s, learning 0.117s)
             Mean action noise std: 2.77
          Mean value_function loss: 98.1914
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 53.1307
                       Mean reward: 894.40
               Mean episode length: 238.94
    Episode_Reward/reaching_object: 1.3563
     Episode_Reward/lifting_object: 175.0622
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 2.22s
                      Time elapsed: 00:52:35
                               ETA: 00:23:49

################################################################################
                     [1m Learning iteration 1377/2000 [0m                     

                       Computation: 36557 steps/s (collection: 2.490s, learning 0.199s)
             Mean action noise std: 2.77
          Mean value_function loss: 93.7607
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 53.1420
                       Mean reward: 877.11
               Mean episode length: 235.35
    Episode_Reward/reaching_object: 1.3558
     Episode_Reward/lifting_object: 174.6115
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 2.69s
                      Time elapsed: 00:52:38
                               ETA: 00:23:47

################################################################################
                     [1m Learning iteration 1378/2000 [0m                     

                       Computation: 40644 steps/s (collection: 2.276s, learning 0.143s)
             Mean action noise std: 2.77
          Mean value_function loss: 94.3174
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 53.1562
                       Mean reward: 871.23
               Mean episode length: 234.07
    Episode_Reward/reaching_object: 1.3567
     Episode_Reward/lifting_object: 174.7963
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 2.42s
                      Time elapsed: 00:52:40
                               ETA: 00:23:45

################################################################################
                     [1m Learning iteration 1379/2000 [0m                     

                       Computation: 43603 steps/s (collection: 2.171s, learning 0.083s)
             Mean action noise std: 2.77
          Mean value_function loss: 80.3378
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 53.1648
                       Mean reward: 898.91
               Mean episode length: 240.21
    Episode_Reward/reaching_object: 1.3818
     Episode_Reward/lifting_object: 178.4797
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 2.25s
                      Time elapsed: 00:52:42
                               ETA: 00:23:43

################################################################################
                     [1m Learning iteration 1380/2000 [0m                     

                       Computation: 47827 steps/s (collection: 1.969s, learning 0.087s)
             Mean action noise std: 2.77
          Mean value_function loss: 108.7902
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 53.1772
                       Mean reward: 831.17
               Mean episode length: 227.84
    Episode_Reward/reaching_object: 1.3292
     Episode_Reward/lifting_object: 169.9204
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 2.06s
                      Time elapsed: 00:52:44
                               ETA: 00:23:40

################################################################################
                     [1m Learning iteration 1381/2000 [0m                     

                       Computation: 47010 steps/s (collection: 1.998s, learning 0.093s)
             Mean action noise std: 2.77
          Mean value_function loss: 97.0197
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.1952
                       Mean reward: 880.82
               Mean episode length: 235.19
    Episode_Reward/reaching_object: 1.3813
     Episode_Reward/lifting_object: 178.6210
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 2.09s
                      Time elapsed: 00:52:46
                               ETA: 00:23:38

################################################################################
                     [1m Learning iteration 1382/2000 [0m                     

                       Computation: 47226 steps/s (collection: 1.989s, learning 0.093s)
             Mean action noise std: 2.78
          Mean value_function loss: 89.5183
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.2039
                       Mean reward: 892.08
               Mean episode length: 238.90
    Episode_Reward/reaching_object: 1.3643
     Episode_Reward/lifting_object: 176.0882
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 2.08s
                      Time elapsed: 00:52:49
                               ETA: 00:23:36

################################################################################
                     [1m Learning iteration 1383/2000 [0m                     

                       Computation: 47664 steps/s (collection: 1.972s, learning 0.090s)
             Mean action noise std: 2.78
          Mean value_function loss: 89.2185
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.2163
                       Mean reward: 903.58
               Mean episode length: 240.25
    Episode_Reward/reaching_object: 1.3580
     Episode_Reward/lifting_object: 174.5163
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 2.06s
                      Time elapsed: 00:52:51
                               ETA: 00:23:33

################################################################################
                     [1m Learning iteration 1384/2000 [0m                     

                       Computation: 46898 steps/s (collection: 2.004s, learning 0.093s)
             Mean action noise std: 2.78
          Mean value_function loss: 110.4233
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.2288
                       Mean reward: 881.14
               Mean episode length: 234.38
    Episode_Reward/reaching_object: 1.3494
     Episode_Reward/lifting_object: 174.2067
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 2.10s
                      Time elapsed: 00:52:53
                               ETA: 00:23:31

################################################################################
                     [1m Learning iteration 1385/2000 [0m                     

                       Computation: 47473 steps/s (collection: 1.966s, learning 0.105s)
             Mean action noise std: 2.78
          Mean value_function loss: 153.8213
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 53.2365
                       Mean reward: 849.68
               Mean episode length: 228.14
    Episode_Reward/reaching_object: 1.3373
     Episode_Reward/lifting_object: 171.7663
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 2.07s
                      Time elapsed: 00:52:55
                               ETA: 00:23:28

################################################################################
                     [1m Learning iteration 1386/2000 [0m                     

                       Computation: 46941 steps/s (collection: 1.985s, learning 0.109s)
             Mean action noise std: 2.78
          Mean value_function loss: 91.4728
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 53.2456
                       Mean reward: 904.03
               Mean episode length: 239.99
    Episode_Reward/reaching_object: 1.3567
     Episode_Reward/lifting_object: 174.8061
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 2.09s
                      Time elapsed: 00:52:57
                               ETA: 00:23:26

################################################################################
                     [1m Learning iteration 1387/2000 [0m                     

                       Computation: 46912 steps/s (collection: 1.976s, learning 0.120s)
             Mean action noise std: 2.78
          Mean value_function loss: 122.6002
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 53.2607
                       Mean reward: 836.96
               Mean episode length: 225.04
    Episode_Reward/reaching_object: 1.3507
     Episode_Reward/lifting_object: 173.9189
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 2.10s
                      Time elapsed: 00:52:59
                               ETA: 00:23:24

################################################################################
                     [1m Learning iteration 1388/2000 [0m                     

                       Computation: 47011 steps/s (collection: 1.999s, learning 0.092s)
             Mean action noise std: 2.79
          Mean value_function loss: 121.2573
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 53.2819
                       Mean reward: 910.81
               Mean episode length: 241.93
    Episode_Reward/reaching_object: 1.3415
     Episode_Reward/lifting_object: 172.9758
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 2.09s
                      Time elapsed: 00:53:01
                               ETA: 00:23:21

################################################################################
                     [1m Learning iteration 1389/2000 [0m                     

                       Computation: 44631 steps/s (collection: 2.115s, learning 0.088s)
             Mean action noise std: 2.79
          Mean value_function loss: 120.0199
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.3008
                       Mean reward: 900.70
               Mean episode length: 240.05
    Episode_Reward/reaching_object: 1.3722
     Episode_Reward/lifting_object: 177.3736
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 2.20s
                      Time elapsed: 00:53:03
                               ETA: 00:23:19

################################################################################
                     [1m Learning iteration 1390/2000 [0m                     

                       Computation: 46279 steps/s (collection: 2.028s, learning 0.096s)
             Mean action noise std: 2.79
          Mean value_function loss: 115.6022
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 53.3138
                       Mean reward: 905.92
               Mean episode length: 241.07
    Episode_Reward/reaching_object: 1.3687
     Episode_Reward/lifting_object: 177.2065
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 2.12s
                      Time elapsed: 00:53:05
                               ETA: 00:23:17

################################################################################
                     [1m Learning iteration 1391/2000 [0m                     

                       Computation: 46993 steps/s (collection: 1.999s, learning 0.093s)
             Mean action noise std: 2.79
          Mean value_function loss: 108.7363
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 53.3157
                       Mean reward: 889.45
               Mean episode length: 237.87
    Episode_Reward/reaching_object: 1.3563
     Episode_Reward/lifting_object: 175.3013
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 2.09s
                      Time elapsed: 00:53:07
                               ETA: 00:23:14

################################################################################
                     [1m Learning iteration 1392/2000 [0m                     

                       Computation: 47089 steps/s (collection: 1.998s, learning 0.090s)
             Mean action noise std: 2.79
          Mean value_function loss: 99.6194
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 53.3239
                       Mean reward: 905.81
               Mean episode length: 240.49
    Episode_Reward/reaching_object: 1.3552
     Episode_Reward/lifting_object: 174.3211
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 2.09s
                      Time elapsed: 00:53:10
                               ETA: 00:23:12

################################################################################
                     [1m Learning iteration 1393/2000 [0m                     

                       Computation: 47098 steps/s (collection: 1.994s, learning 0.093s)
             Mean action noise std: 2.79
          Mean value_function loss: 129.4074
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.3382
                       Mean reward: 897.72
               Mean episode length: 238.57
    Episode_Reward/reaching_object: 1.3344
     Episode_Reward/lifting_object: 171.9861
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 2.09s
                      Time elapsed: 00:53:12
                               ETA: 00:23:09

################################################################################
                     [1m Learning iteration 1394/2000 [0m                     

                       Computation: 47035 steps/s (collection: 2.001s, learning 0.089s)
             Mean action noise std: 2.79
          Mean value_function loss: 129.6197
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.3485
                       Mean reward: 886.01
               Mean episode length: 235.42
    Episode_Reward/reaching_object: 1.3236
     Episode_Reward/lifting_object: 170.0000
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 2.09s
                      Time elapsed: 00:53:14
                               ETA: 00:23:07

################################################################################
                     [1m Learning iteration 1395/2000 [0m                     

                       Computation: 47730 steps/s (collection: 1.966s, learning 0.094s)
             Mean action noise std: 2.80
          Mean value_function loss: 122.8135
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 53.3623
                       Mean reward: 888.44
               Mean episode length: 237.52
    Episode_Reward/reaching_object: 1.3544
     Episode_Reward/lifting_object: 174.6403
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 2.06s
                      Time elapsed: 00:53:16
                               ETA: 00:23:05

################################################################################
                     [1m Learning iteration 1396/2000 [0m                     

                       Computation: 46844 steps/s (collection: 1.996s, learning 0.102s)
             Mean action noise std: 2.80
          Mean value_function loss: 126.3363
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.3677
                       Mean reward: 853.36
               Mean episode length: 229.76
    Episode_Reward/reaching_object: 1.3332
     Episode_Reward/lifting_object: 171.5665
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 2.10s
                      Time elapsed: 00:53:18
                               ETA: 00:23:02

################################################################################
                     [1m Learning iteration 1397/2000 [0m                     

                       Computation: 46676 steps/s (collection: 2.005s, learning 0.101s)
             Mean action noise std: 2.80
          Mean value_function loss: 147.6309
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 53.3756
                       Mean reward: 823.34
               Mean episode length: 221.66
    Episode_Reward/reaching_object: 1.3221
     Episode_Reward/lifting_object: 170.4726
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 2.11s
                      Time elapsed: 00:53:20
                               ETA: 00:23:00

################################################################################
                     [1m Learning iteration 1398/2000 [0m                     

                       Computation: 47835 steps/s (collection: 1.961s, learning 0.094s)
             Mean action noise std: 2.80
          Mean value_function loss: 123.6996
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.3848
                       Mean reward: 839.89
               Mean episode length: 226.83
    Episode_Reward/reaching_object: 1.3185
     Episode_Reward/lifting_object: 170.0137
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 2.06s
                      Time elapsed: 00:53:22
                               ETA: 00:22:58

################################################################################
                     [1m Learning iteration 1399/2000 [0m                     

                       Computation: 46439 steps/s (collection: 2.001s, learning 0.116s)
             Mean action noise std: 2.80
          Mean value_function loss: 112.2036
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 53.3951
                       Mean reward: 819.90
               Mean episode length: 221.25
    Episode_Reward/reaching_object: 1.3253
     Episode_Reward/lifting_object: 170.8099
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 2.12s
                      Time elapsed: 00:53:24
                               ETA: 00:22:55

################################################################################
                     [1m Learning iteration 1400/2000 [0m                     

                       Computation: 46969 steps/s (collection: 1.992s, learning 0.101s)
             Mean action noise std: 2.80
          Mean value_function loss: 124.1954
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 53.4047
                       Mean reward: 905.80
               Mean episode length: 241.50
    Episode_Reward/reaching_object: 1.3287
     Episode_Reward/lifting_object: 170.9439
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 2.09s
                      Time elapsed: 00:53:26
                               ETA: 00:22:53

################################################################################
                     [1m Learning iteration 1401/2000 [0m                     

                       Computation: 46307 steps/s (collection: 2.032s, learning 0.091s)
             Mean action noise std: 2.80
          Mean value_function loss: 128.1315
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.4178
                       Mean reward: 871.29
               Mean episode length: 231.91
    Episode_Reward/reaching_object: 1.3355
     Episode_Reward/lifting_object: 172.4454
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 2.12s
                      Time elapsed: 00:53:28
                               ETA: 00:22:50

################################################################################
                     [1m Learning iteration 1402/2000 [0m                     

                       Computation: 46404 steps/s (collection: 2.030s, learning 0.089s)
             Mean action noise std: 2.80
          Mean value_function loss: 103.3076
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 53.4258
                       Mean reward: 898.85
               Mean episode length: 239.73
    Episode_Reward/reaching_object: 1.3512
     Episode_Reward/lifting_object: 174.4585
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 2.12s
                      Time elapsed: 00:53:30
                               ETA: 00:22:48

################################################################################
                     [1m Learning iteration 1403/2000 [0m                     

                       Computation: 46253 steps/s (collection: 2.036s, learning 0.090s)
             Mean action noise std: 2.81
          Mean value_function loss: 94.7156
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 53.4351
                       Mean reward: 900.95
               Mean episode length: 240.52
    Episode_Reward/reaching_object: 1.3492
     Episode_Reward/lifting_object: 174.0776
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 2.13s
                      Time elapsed: 00:53:33
                               ETA: 00:22:46

################################################################################
                     [1m Learning iteration 1404/2000 [0m                     

                       Computation: 46259 steps/s (collection: 2.039s, learning 0.087s)
             Mean action noise std: 2.81
          Mean value_function loss: 100.4063
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 53.4399
                       Mean reward: 875.25
               Mean episode length: 234.10
    Episode_Reward/reaching_object: 1.3434
     Episode_Reward/lifting_object: 173.9664
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 2.13s
                      Time elapsed: 00:53:35
                               ETA: 00:22:43

################################################################################
                     [1m Learning iteration 1405/2000 [0m                     

                       Computation: 47261 steps/s (collection: 1.987s, learning 0.093s)
             Mean action noise std: 2.81
          Mean value_function loss: 92.9310
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 53.4447
                       Mean reward: 921.49
               Mean episode length: 245.61
    Episode_Reward/reaching_object: 1.3667
     Episode_Reward/lifting_object: 177.3217
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 2.08s
                      Time elapsed: 00:53:37
                               ETA: 00:22:41

################################################################################
                     [1m Learning iteration 1406/2000 [0m                     

                       Computation: 47583 steps/s (collection: 1.979s, learning 0.087s)
             Mean action noise std: 2.81
          Mean value_function loss: 104.2334
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 53.4556
                       Mean reward: 881.46
               Mean episode length: 235.13
    Episode_Reward/reaching_object: 1.3617
     Episode_Reward/lifting_object: 176.0630
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 2.07s
                      Time elapsed: 00:53:39
                               ETA: 00:22:39

################################################################################
                     [1m Learning iteration 1407/2000 [0m                     

                       Computation: 46457 steps/s (collection: 2.021s, learning 0.095s)
             Mean action noise std: 2.81
          Mean value_function loss: 95.2844
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.4718
                       Mean reward: 904.81
               Mean episode length: 240.75
    Episode_Reward/reaching_object: 1.3630
     Episode_Reward/lifting_object: 176.8494
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 2.12s
                      Time elapsed: 00:53:41
                               ETA: 00:22:36

################################################################################
                     [1m Learning iteration 1408/2000 [0m                     

                       Computation: 47102 steps/s (collection: 1.993s, learning 0.094s)
             Mean action noise std: 2.81
          Mean value_function loss: 91.7667
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 53.4833
                       Mean reward: 878.92
               Mean episode length: 235.94
    Episode_Reward/reaching_object: 1.3388
     Episode_Reward/lifting_object: 174.1759
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 2.09s
                      Time elapsed: 00:53:43
                               ETA: 00:22:34

################################################################################
                     [1m Learning iteration 1409/2000 [0m                     

                       Computation: 47111 steps/s (collection: 1.999s, learning 0.088s)
             Mean action noise std: 2.81
          Mean value_function loss: 91.3360
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 53.4916
                       Mean reward: 871.68
               Mean episode length: 234.40
    Episode_Reward/reaching_object: 1.3653
     Episode_Reward/lifting_object: 177.1028
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 2.09s
                      Time elapsed: 00:53:45
                               ETA: 00:22:32

################################################################################
                     [1m Learning iteration 1410/2000 [0m                     

                       Computation: 47228 steps/s (collection: 1.991s, learning 0.091s)
             Mean action noise std: 2.82
          Mean value_function loss: 128.7877
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 53.5034
                       Mean reward: 835.91
               Mean episode length: 224.24
    Episode_Reward/reaching_object: 1.3169
     Episode_Reward/lifting_object: 170.2312
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 2.08s
                      Time elapsed: 00:53:47
                               ETA: 00:22:29

################################################################################
                     [1m Learning iteration 1411/2000 [0m                     

                       Computation: 46380 steps/s (collection: 2.030s, learning 0.089s)
             Mean action noise std: 2.82
          Mean value_function loss: 130.5952
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.5132
                       Mean reward: 844.67
               Mean episode length: 230.27
    Episode_Reward/reaching_object: 1.3213
     Episode_Reward/lifting_object: 170.3445
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 2.12s
                      Time elapsed: 00:53:49
                               ETA: 00:22:27

################################################################################
                     [1m Learning iteration 1412/2000 [0m                     

                       Computation: 46863 steps/s (collection: 2.006s, learning 0.092s)
             Mean action noise std: 2.82
          Mean value_function loss: 117.8525
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 53.5221
                       Mean reward: 873.37
               Mean episode length: 235.30
    Episode_Reward/reaching_object: 1.3410
     Episode_Reward/lifting_object: 172.5179
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 2.10s
                      Time elapsed: 00:53:51
                               ETA: 00:22:24

################################################################################
                     [1m Learning iteration 1413/2000 [0m                     

                       Computation: 46807 steps/s (collection: 2.004s, learning 0.096s)
             Mean action noise std: 2.82
          Mean value_function loss: 113.4035
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 53.5331
                       Mean reward: 889.31
               Mean episode length: 237.76
    Episode_Reward/reaching_object: 1.3366
     Episode_Reward/lifting_object: 171.5170
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 2.10s
                      Time elapsed: 00:53:54
                               ETA: 00:22:22

################################################################################
                     [1m Learning iteration 1414/2000 [0m                     

                       Computation: 47605 steps/s (collection: 1.969s, learning 0.096s)
             Mean action noise std: 2.82
          Mean value_function loss: 115.1683
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 53.5468
                       Mean reward: 859.85
               Mean episode length: 230.75
    Episode_Reward/reaching_object: 1.3321
     Episode_Reward/lifting_object: 171.4171
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 2.06s
                      Time elapsed: 00:53:56
                               ETA: 00:22:20

################################################################################
                     [1m Learning iteration 1415/2000 [0m                     

                       Computation: 47033 steps/s (collection: 1.993s, learning 0.097s)
             Mean action noise std: 2.82
          Mean value_function loss: 149.8218
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 53.5632
                       Mean reward: 854.88
               Mean episode length: 229.36
    Episode_Reward/reaching_object: 1.3361
     Episode_Reward/lifting_object: 172.0686
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 2.09s
                      Time elapsed: 00:53:58
                               ETA: 00:22:17

################################################################################
                     [1m Learning iteration 1416/2000 [0m                     

                       Computation: 47218 steps/s (collection: 1.991s, learning 0.091s)
             Mean action noise std: 2.82
          Mean value_function loss: 152.7668
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.5701
                       Mean reward: 792.42
               Mean episode length: 216.75
    Episode_Reward/reaching_object: 1.3185
     Episode_Reward/lifting_object: 168.8755
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 2.08s
                      Time elapsed: 00:54:00
                               ETA: 00:22:15

################################################################################
                     [1m Learning iteration 1417/2000 [0m                     

                       Computation: 47059 steps/s (collection: 1.996s, learning 0.093s)
             Mean action noise std: 2.82
          Mean value_function loss: 166.3033
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.5793
                       Mean reward: 849.78
               Mean episode length: 227.98
    Episode_Reward/reaching_object: 1.3006
     Episode_Reward/lifting_object: 166.7792
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 2.09s
                      Time elapsed: 00:54:02
                               ETA: 00:22:13

################################################################################
                     [1m Learning iteration 1418/2000 [0m                     

                       Computation: 46473 steps/s (collection: 2.012s, learning 0.104s)
             Mean action noise std: 2.83
          Mean value_function loss: 140.3475
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 53.5892
                       Mean reward: 878.23
               Mean episode length: 235.44
    Episode_Reward/reaching_object: 1.3380
     Episode_Reward/lifting_object: 172.6055
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 2.12s
                      Time elapsed: 00:54:04
                               ETA: 00:22:10

################################################################################
                     [1m Learning iteration 1419/2000 [0m                     

                       Computation: 46295 steps/s (collection: 2.021s, learning 0.103s)
             Mean action noise std: 2.83
          Mean value_function loss: 177.6880
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 53.6026
                       Mean reward: 804.52
               Mean episode length: 218.80
    Episode_Reward/reaching_object: 1.2947
     Episode_Reward/lifting_object: 165.6405
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 2.12s
                      Time elapsed: 00:54:06
                               ETA: 00:22:08

################################################################################
                     [1m Learning iteration 1420/2000 [0m                     

                       Computation: 46593 steps/s (collection: 2.019s, learning 0.091s)
             Mean action noise std: 2.83
          Mean value_function loss: 151.6759
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.6119
                       Mean reward: 892.86
               Mean episode length: 238.83
    Episode_Reward/reaching_object: 1.3384
     Episode_Reward/lifting_object: 172.2121
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 2.11s
                      Time elapsed: 00:54:08
                               ETA: 00:22:06

################################################################################
                     [1m Learning iteration 1421/2000 [0m                     

                       Computation: 46844 steps/s (collection: 1.988s, learning 0.110s)
             Mean action noise std: 2.83
          Mean value_function loss: 161.5338
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 53.6199
                       Mean reward: 861.19
               Mean episode length: 230.80
    Episode_Reward/reaching_object: 1.2906
     Episode_Reward/lifting_object: 165.7525
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 2.10s
                      Time elapsed: 00:54:10
                               ETA: 00:22:03

################################################################################
                     [1m Learning iteration 1422/2000 [0m                     

                       Computation: 46236 steps/s (collection: 2.040s, learning 0.087s)
             Mean action noise std: 2.83
          Mean value_function loss: 104.2524
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 53.6294
                       Mean reward: 883.14
               Mean episode length: 236.15
    Episode_Reward/reaching_object: 1.3367
     Episode_Reward/lifting_object: 172.1463
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 2.13s
                      Time elapsed: 00:54:12
                               ETA: 00:22:01

################################################################################
                     [1m Learning iteration 1423/2000 [0m                     

                       Computation: 47310 steps/s (collection: 1.985s, learning 0.093s)
             Mean action noise std: 2.83
          Mean value_function loss: 121.2803
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.6361
                       Mean reward: 902.52
               Mean episode length: 240.01
    Episode_Reward/reaching_object: 1.3515
     Episode_Reward/lifting_object: 174.3427
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 2.08s
                      Time elapsed: 00:54:15
                               ETA: 00:21:58

################################################################################
                     [1m Learning iteration 1424/2000 [0m                     

                       Computation: 46705 steps/s (collection: 2.006s, learning 0.099s)
             Mean action noise std: 2.83
          Mean value_function loss: 191.2513
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 53.6443
                       Mean reward: 834.40
               Mean episode length: 225.37
    Episode_Reward/reaching_object: 1.3112
     Episode_Reward/lifting_object: 168.5568
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 2.10s
                      Time elapsed: 00:54:17
                               ETA: 00:21:56

################################################################################
                     [1m Learning iteration 1425/2000 [0m                     

                       Computation: 46139 steps/s (collection: 2.042s, learning 0.089s)
             Mean action noise std: 2.83
          Mean value_function loss: 122.8853
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 53.6485
                       Mean reward: 875.40
               Mean episode length: 233.57
    Episode_Reward/reaching_object: 1.3168
     Episode_Reward/lifting_object: 170.0094
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 2.13s
                      Time elapsed: 00:54:19
                               ETA: 00:21:54

################################################################################
                     [1m Learning iteration 1426/2000 [0m                     

                       Computation: 46375 steps/s (collection: 2.028s, learning 0.092s)
             Mean action noise std: 2.83
          Mean value_function loss: 109.7928
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 53.6557
                       Mean reward: 884.47
               Mean episode length: 236.53
    Episode_Reward/reaching_object: 1.3476
     Episode_Reward/lifting_object: 173.8685
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 2.12s
                      Time elapsed: 00:54:21
                               ETA: 00:21:51

################################################################################
                     [1m Learning iteration 1427/2000 [0m                     

                       Computation: 45669 steps/s (collection: 2.058s, learning 0.094s)
             Mean action noise std: 2.84
          Mean value_function loss: 100.5852
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.6662
                       Mean reward: 892.11
               Mean episode length: 238.88
    Episode_Reward/reaching_object: 1.3642
     Episode_Reward/lifting_object: 176.7993
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 2.15s
                      Time elapsed: 00:54:23
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 1428/2000 [0m                     

                       Computation: 46002 steps/s (collection: 2.044s, learning 0.093s)
             Mean action noise std: 2.84
          Mean value_function loss: 110.2418
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.6751
                       Mean reward: 858.29
               Mean episode length: 231.02
    Episode_Reward/reaching_object: 1.3248
     Episode_Reward/lifting_object: 171.1190
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 2.14s
                      Time elapsed: 00:54:25
                               ETA: 00:21:47

################################################################################
                     [1m Learning iteration 1429/2000 [0m                     

                       Computation: 46449 steps/s (collection: 2.020s, learning 0.097s)
             Mean action noise std: 2.84
          Mean value_function loss: 103.0924
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.6841
                       Mean reward: 889.68
               Mean episode length: 236.88
    Episode_Reward/reaching_object: 1.3455
     Episode_Reward/lifting_object: 174.3400
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 2.12s
                      Time elapsed: 00:54:27
                               ETA: 00:21:44

################################################################################
                     [1m Learning iteration 1430/2000 [0m                     

                       Computation: 46108 steps/s (collection: 2.040s, learning 0.092s)
             Mean action noise std: 2.84
          Mean value_function loss: 86.3537
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 53.6909
                       Mean reward: 892.67
               Mean episode length: 237.57
    Episode_Reward/reaching_object: 1.3586
     Episode_Reward/lifting_object: 176.5315
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 2.13s
                      Time elapsed: 00:54:29
                               ETA: 00:21:42

################################################################################
                     [1m Learning iteration 1431/2000 [0m                     

                       Computation: 45532 steps/s (collection: 2.067s, learning 0.092s)
             Mean action noise std: 2.84
          Mean value_function loss: 141.5646
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 53.7002
                       Mean reward: 861.49
               Mean episode length: 230.55
    Episode_Reward/reaching_object: 1.3340
     Episode_Reward/lifting_object: 173.1433
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 2.16s
                      Time elapsed: 00:54:32
                               ETA: 00:21:40

################################################################################
                     [1m Learning iteration 1432/2000 [0m                     

                       Computation: 45533 steps/s (collection: 2.061s, learning 0.098s)
             Mean action noise std: 2.84
          Mean value_function loss: 110.9358
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.7088
                       Mean reward: 896.09
               Mean episode length: 238.03
    Episode_Reward/reaching_object: 1.3182
     Episode_Reward/lifting_object: 171.3570
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 2.16s
                      Time elapsed: 00:54:34
                               ETA: 00:21:37

################################################################################
                     [1m Learning iteration 1433/2000 [0m                     

                       Computation: 46033 steps/s (collection: 2.036s, learning 0.100s)
             Mean action noise std: 2.84
          Mean value_function loss: 111.4068
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.7196
                       Mean reward: 910.33
               Mean episode length: 241.98
    Episode_Reward/reaching_object: 1.3451
     Episode_Reward/lifting_object: 174.7301
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 2.14s
                      Time elapsed: 00:54:36
                               ETA: 00:21:35

################################################################################
                     [1m Learning iteration 1434/2000 [0m                     

                       Computation: 45532 steps/s (collection: 2.070s, learning 0.089s)
             Mean action noise std: 2.85
          Mean value_function loss: 83.0184
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.7345
                       Mean reward: 897.76
               Mean episode length: 238.70
    Episode_Reward/reaching_object: 1.3537
     Episode_Reward/lifting_object: 175.8036
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 2.16s
                      Time elapsed: 00:54:38
                               ETA: 00:21:33

################################################################################
                     [1m Learning iteration 1435/2000 [0m                     

                       Computation: 45960 steps/s (collection: 2.038s, learning 0.101s)
             Mean action noise std: 2.85
          Mean value_function loss: 93.2962
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 53.7476
                       Mean reward: 903.04
               Mean episode length: 239.90
    Episode_Reward/reaching_object: 1.3523
     Episode_Reward/lifting_object: 176.0071
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 2.14s
                      Time elapsed: 00:54:40
                               ETA: 00:21:30

################################################################################
                     [1m Learning iteration 1436/2000 [0m                     

                       Computation: 46368 steps/s (collection: 2.004s, learning 0.116s)
             Mean action noise std: 2.85
          Mean value_function loss: 123.1827
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 53.7529
                       Mean reward: 840.84
               Mean episode length: 226.70
    Episode_Reward/reaching_object: 1.3131
     Episode_Reward/lifting_object: 170.6779
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 2.12s
                      Time elapsed: 00:54:42
                               ETA: 00:21:28

################################################################################
                     [1m Learning iteration 1437/2000 [0m                     

                       Computation: 46725 steps/s (collection: 2.014s, learning 0.090s)
             Mean action noise std: 2.85
          Mean value_function loss: 121.1386
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.7645
                       Mean reward: 875.86
               Mean episode length: 233.88
    Episode_Reward/reaching_object: 1.3424
     Episode_Reward/lifting_object: 174.3858
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 2.10s
                      Time elapsed: 00:54:44
                               ETA: 00:21:26

################################################################################
                     [1m Learning iteration 1438/2000 [0m                     

                       Computation: 46351 steps/s (collection: 2.021s, learning 0.100s)
             Mean action noise std: 2.85
          Mean value_function loss: 101.6913
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.7757
                       Mean reward: 909.15
               Mean episode length: 242.02
    Episode_Reward/reaching_object: 1.3472
     Episode_Reward/lifting_object: 175.7241
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 2.12s
                      Time elapsed: 00:54:47
                               ETA: 00:21:23

################################################################################
                     [1m Learning iteration 1439/2000 [0m                     

                       Computation: 44744 steps/s (collection: 2.105s, learning 0.092s)
             Mean action noise std: 2.85
          Mean value_function loss: 94.7946
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 53.7860
                       Mean reward: 870.78
               Mean episode length: 234.02
    Episode_Reward/reaching_object: 1.3512
     Episode_Reward/lifting_object: 176.3358
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 2.20s
                      Time elapsed: 00:54:49
                               ETA: 00:21:21

################################################################################
                     [1m Learning iteration 1440/2000 [0m                     

                       Computation: 45554 steps/s (collection: 2.065s, learning 0.093s)
             Mean action noise std: 2.85
          Mean value_function loss: 122.8695
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.7991
                       Mean reward: 897.51
               Mean episode length: 238.41
    Episode_Reward/reaching_object: 1.3367
     Episode_Reward/lifting_object: 174.5745
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 2.16s
                      Time elapsed: 00:54:51
                               ETA: 00:21:19

################################################################################
                     [1m Learning iteration 1441/2000 [0m                     

                       Computation: 46396 steps/s (collection: 2.029s, learning 0.090s)
             Mean action noise std: 2.86
          Mean value_function loss: 95.0641
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.8072
                       Mean reward: 918.48
               Mean episode length: 243.80
    Episode_Reward/reaching_object: 1.3319
     Episode_Reward/lifting_object: 173.6385
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 2.12s
                      Time elapsed: 00:54:53
                               ETA: 00:21:16

################################################################################
                     [1m Learning iteration 1442/2000 [0m                     

                       Computation: 46345 steps/s (collection: 2.010s, learning 0.112s)
             Mean action noise std: 2.86
          Mean value_function loss: 116.4965
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 53.8190
                       Mean reward: 858.46
               Mean episode length: 229.75
    Episode_Reward/reaching_object: 1.3262
     Episode_Reward/lifting_object: 173.1392
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 2.12s
                      Time elapsed: 00:54:55
                               ETA: 00:21:14

################################################################################
                     [1m Learning iteration 1443/2000 [0m                     

                       Computation: 46260 steps/s (collection: 2.017s, learning 0.108s)
             Mean action noise std: 2.86
          Mean value_function loss: 126.3830
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 53.8351
                       Mean reward: 872.77
               Mean episode length: 232.83
    Episode_Reward/reaching_object: 1.3336
     Episode_Reward/lifting_object: 174.1034
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 2.12s
                      Time elapsed: 00:54:57
                               ETA: 00:21:12

################################################################################
                     [1m Learning iteration 1444/2000 [0m                     

                       Computation: 46556 steps/s (collection: 2.018s, learning 0.093s)
             Mean action noise std: 2.86
          Mean value_function loss: 130.9129
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 53.8466
                       Mean reward: 849.22
               Mean episode length: 226.89
    Episode_Reward/reaching_object: 1.3253
     Episode_Reward/lifting_object: 171.9392
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 2.11s
                      Time elapsed: 00:54:59
                               ETA: 00:21:09

################################################################################
                     [1m Learning iteration 1445/2000 [0m                     

                       Computation: 45702 steps/s (collection: 2.064s, learning 0.087s)
             Mean action noise std: 2.86
          Mean value_function loss: 70.4437
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 53.8587
                       Mean reward: 910.93
               Mean episode length: 241.31
    Episode_Reward/reaching_object: 1.3604
     Episode_Reward/lifting_object: 177.2379
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 2.15s
                      Time elapsed: 00:55:02
                               ETA: 00:21:07

################################################################################
                     [1m Learning iteration 1446/2000 [0m                     

                       Computation: 45732 steps/s (collection: 2.048s, learning 0.102s)
             Mean action noise std: 2.86
          Mean value_function loss: 111.5689
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.8722
                       Mean reward: 834.43
               Mean episode length: 224.72
    Episode_Reward/reaching_object: 1.3497
     Episode_Reward/lifting_object: 175.6141
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 2.15s
                      Time elapsed: 00:55:04
                               ETA: 00:21:05

################################################################################
                     [1m Learning iteration 1447/2000 [0m                     

                       Computation: 46258 steps/s (collection: 2.034s, learning 0.091s)
             Mean action noise std: 2.87
          Mean value_function loss: 111.7960
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.8826
                       Mean reward: 857.11
               Mean episode length: 229.82
    Episode_Reward/reaching_object: 1.3555
     Episode_Reward/lifting_object: 175.5003
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 2.13s
                      Time elapsed: 00:55:06
                               ETA: 00:21:02

################################################################################
                     [1m Learning iteration 1448/2000 [0m                     

                       Computation: 45702 steps/s (collection: 2.058s, learning 0.093s)
             Mean action noise std: 2.87
          Mean value_function loss: 92.1660
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.8907
                       Mean reward: 934.12
               Mean episode length: 247.11
    Episode_Reward/reaching_object: 1.3738
     Episode_Reward/lifting_object: 178.2350
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 2.15s
                      Time elapsed: 00:55:08
                               ETA: 00:21:00

################################################################################
                     [1m Learning iteration 1449/2000 [0m                     

                       Computation: 47387 steps/s (collection: 1.986s, learning 0.088s)
             Mean action noise std: 2.87
          Mean value_function loss: 111.3386
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 53.9007
                       Mean reward: 872.66
               Mean episode length: 235.30
    Episode_Reward/reaching_object: 1.3483
     Episode_Reward/lifting_object: 173.6929
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 2.07s
                      Time elapsed: 00:55:10
                               ETA: 00:20:57

################################################################################
                     [1m Learning iteration 1450/2000 [0m                     

                       Computation: 46720 steps/s (collection: 2.014s, learning 0.091s)
             Mean action noise std: 2.87
          Mean value_function loss: 77.3031
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 53.9104
                       Mean reward: 872.00
               Mean episode length: 234.90
    Episode_Reward/reaching_object: 1.3727
     Episode_Reward/lifting_object: 177.7181
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 2.10s
                      Time elapsed: 00:55:12
                               ETA: 00:20:55

################################################################################
                     [1m Learning iteration 1451/2000 [0m                     

                       Computation: 45945 steps/s (collection: 2.042s, learning 0.098s)
             Mean action noise std: 2.87
          Mean value_function loss: 115.0149
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.9253
                       Mean reward: 856.69
               Mean episode length: 230.97
    Episode_Reward/reaching_object: 1.3343
     Episode_Reward/lifting_object: 171.9253
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 2.14s
                      Time elapsed: 00:55:14
                               ETA: 00:20:53

################################################################################
                     [1m Learning iteration 1452/2000 [0m                     

                       Computation: 46330 steps/s (collection: 2.033s, learning 0.089s)
             Mean action noise std: 2.87
          Mean value_function loss: 108.7431
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.9326
                       Mean reward: 880.67
               Mean episode length: 235.59
    Episode_Reward/reaching_object: 1.3368
     Episode_Reward/lifting_object: 171.9826
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 2.12s
                      Time elapsed: 00:55:16
                               ETA: 00:20:50

################################################################################
                     [1m Learning iteration 1453/2000 [0m                     

                       Computation: 46261 steps/s (collection: 2.036s, learning 0.089s)
             Mean action noise std: 2.88
          Mean value_function loss: 126.8871
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 53.9453
                       Mean reward: 848.56
               Mean episode length: 229.56
    Episode_Reward/reaching_object: 1.3570
     Episode_Reward/lifting_object: 173.7831
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 2.12s
                      Time elapsed: 00:55:19
                               ETA: 00:20:48

################################################################################
                     [1m Learning iteration 1454/2000 [0m                     

                       Computation: 46865 steps/s (collection: 2.012s, learning 0.086s)
             Mean action noise std: 2.88
          Mean value_function loss: 145.2092
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 53.9616
                       Mean reward: 858.39
               Mean episode length: 230.51
    Episode_Reward/reaching_object: 1.3564
     Episode_Reward/lifting_object: 173.6534
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 2.10s
                      Time elapsed: 00:55:21
                               ETA: 00:20:46

################################################################################
                     [1m Learning iteration 1455/2000 [0m                     

                       Computation: 47006 steps/s (collection: 1.994s, learning 0.097s)
             Mean action noise std: 2.88
          Mean value_function loss: 80.7709
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 53.9658
                       Mean reward: 901.67
               Mean episode length: 239.89
    Episode_Reward/reaching_object: 1.3681
     Episode_Reward/lifting_object: 175.9328
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 2.09s
                      Time elapsed: 00:55:23
                               ETA: 00:20:43

################################################################################
                     [1m Learning iteration 1456/2000 [0m                     

                       Computation: 46708 steps/s (collection: 2.011s, learning 0.094s)
             Mean action noise std: 2.88
          Mean value_function loss: 83.1165
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.9687
                       Mean reward: 879.87
               Mean episode length: 234.97
    Episode_Reward/reaching_object: 1.3783
     Episode_Reward/lifting_object: 176.7118
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 2.10s
                      Time elapsed: 00:55:25
                               ETA: 00:20:41

################################################################################
                     [1m Learning iteration 1457/2000 [0m                     

                       Computation: 44989 steps/s (collection: 2.076s, learning 0.110s)
             Mean action noise std: 2.88
          Mean value_function loss: 118.4520
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.9739
                       Mean reward: 860.13
               Mean episode length: 231.46
    Episode_Reward/reaching_object: 1.3362
     Episode_Reward/lifting_object: 171.1900
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 2.19s
                      Time elapsed: 00:55:27
                               ETA: 00:20:39

################################################################################
                     [1m Learning iteration 1458/2000 [0m                     

                       Computation: 46148 steps/s (collection: 2.017s, learning 0.113s)
             Mean action noise std: 2.88
          Mean value_function loss: 107.1483
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.9815
                       Mean reward: 885.05
               Mean episode length: 236.69
    Episode_Reward/reaching_object: 1.3516
     Episode_Reward/lifting_object: 173.5220
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 2.13s
                      Time elapsed: 00:55:29
                               ETA: 00:20:36

################################################################################
                     [1m Learning iteration 1459/2000 [0m                     

                       Computation: 45934 steps/s (collection: 2.041s, learning 0.100s)
             Mean action noise std: 2.88
          Mean value_function loss: 118.1215
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 53.9880
                       Mean reward: 835.49
               Mean episode length: 224.44
    Episode_Reward/reaching_object: 1.3278
     Episode_Reward/lifting_object: 170.0264
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 2.14s
                      Time elapsed: 00:55:31
                               ETA: 00:20:34

################################################################################
                     [1m Learning iteration 1460/2000 [0m                     

                       Computation: 45802 steps/s (collection: 2.042s, learning 0.105s)
             Mean action noise std: 2.88
          Mean value_function loss: 104.1854
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 53.9939
                       Mean reward: 875.56
               Mean episode length: 233.93
    Episode_Reward/reaching_object: 1.3513
     Episode_Reward/lifting_object: 173.2825
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 2.15s
                      Time elapsed: 00:55:33
                               ETA: 00:20:32

################################################################################
                     [1m Learning iteration 1461/2000 [0m                     

                       Computation: 46839 steps/s (collection: 1.996s, learning 0.103s)
             Mean action noise std: 2.88
          Mean value_function loss: 116.3419
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.0063
                       Mean reward: 892.79
               Mean episode length: 237.60
    Episode_Reward/reaching_object: 1.3404
     Episode_Reward/lifting_object: 171.2556
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 2.10s
                      Time elapsed: 00:55:35
                               ETA: 00:20:29

################################################################################
                     [1m Learning iteration 1462/2000 [0m                     

                       Computation: 45807 steps/s (collection: 2.055s, learning 0.091s)
             Mean action noise std: 2.88
          Mean value_function loss: 140.2787
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 54.0159
                       Mean reward: 852.55
               Mean episode length: 228.40
    Episode_Reward/reaching_object: 1.3279
     Episode_Reward/lifting_object: 169.9198
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 2.15s
                      Time elapsed: 00:55:38
                               ETA: 00:20:27

################################################################################
                     [1m Learning iteration 1463/2000 [0m                     

                       Computation: 46774 steps/s (collection: 2.005s, learning 0.097s)
             Mean action noise std: 2.88
          Mean value_function loss: 119.1990
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 54.0226
                       Mean reward: 834.23
               Mean episode length: 224.58
    Episode_Reward/reaching_object: 1.3361
     Episode_Reward/lifting_object: 171.0674
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 2.10s
                      Time elapsed: 00:55:40
                               ETA: 00:20:25

################################################################################
                     [1m Learning iteration 1464/2000 [0m                     

                       Computation: 45864 steps/s (collection: 2.052s, learning 0.091s)
             Mean action noise std: 2.89
          Mean value_function loss: 138.5663
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.0336
                       Mean reward: 907.09
               Mean episode length: 240.90
    Episode_Reward/reaching_object: 1.3853
     Episode_Reward/lifting_object: 177.6017
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 2.14s
                      Time elapsed: 00:55:42
                               ETA: 00:20:22

################################################################################
                     [1m Learning iteration 1465/2000 [0m                     

                       Computation: 47095 steps/s (collection: 1.997s, learning 0.090s)
             Mean action noise std: 2.89
          Mean value_function loss: 121.0259
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.0469
                       Mean reward: 859.68
               Mean episode length: 228.76
    Episode_Reward/reaching_object: 1.3327
     Episode_Reward/lifting_object: 170.3719
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 2.09s
                      Time elapsed: 00:55:44
                               ETA: 00:20:20

################################################################################
                     [1m Learning iteration 1466/2000 [0m                     

                       Computation: 47371 steps/s (collection: 1.988s, learning 0.087s)
             Mean action noise std: 2.89
          Mean value_function loss: 136.2355
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.0604
                       Mean reward: 844.87
               Mean episode length: 225.93
    Episode_Reward/reaching_object: 1.3135
     Episode_Reward/lifting_object: 168.2715
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 2.08s
                      Time elapsed: 00:55:46
                               ETA: 00:20:18

################################################################################
                     [1m Learning iteration 1467/2000 [0m                     

                       Computation: 47441 steps/s (collection: 1.982s, learning 0.091s)
             Mean action noise std: 2.89
          Mean value_function loss: 147.7672
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 54.0711
                       Mean reward: 857.17
               Mean episode length: 228.98
    Episode_Reward/reaching_object: 1.3029
     Episode_Reward/lifting_object: 167.2631
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 2.07s
                      Time elapsed: 00:55:48
                               ETA: 00:20:15

################################################################################
                     [1m Learning iteration 1468/2000 [0m                     

                       Computation: 46154 steps/s (collection: 2.041s, learning 0.089s)
             Mean action noise std: 2.89
          Mean value_function loss: 142.1302
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.0746
                       Mean reward: 899.74
               Mean episode length: 239.43
    Episode_Reward/reaching_object: 1.3329
     Episode_Reward/lifting_object: 171.5051
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 2.13s
                      Time elapsed: 00:55:50
                               ETA: 00:20:13

################################################################################
                     [1m Learning iteration 1469/2000 [0m                     

                       Computation: 46519 steps/s (collection: 2.018s, learning 0.096s)
             Mean action noise std: 2.89
          Mean value_function loss: 152.9158
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 54.0821
                       Mean reward: 821.28
               Mean episode length: 220.65
    Episode_Reward/reaching_object: 1.3185
     Episode_Reward/lifting_object: 169.8969
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 2.11s
                      Time elapsed: 00:55:52
                               ETA: 00:20:11

################################################################################
                     [1m Learning iteration 1470/2000 [0m                     

                       Computation: 47156 steps/s (collection: 1.992s, learning 0.093s)
             Mean action noise std: 2.89
          Mean value_function loss: 102.4177
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 54.0910
                       Mean reward: 865.74
               Mean episode length: 231.71
    Episode_Reward/reaching_object: 1.3614
     Episode_Reward/lifting_object: 175.1418
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 2.08s
                      Time elapsed: 00:55:54
                               ETA: 00:20:08

################################################################################
                     [1m Learning iteration 1471/2000 [0m                     

                       Computation: 46165 steps/s (collection: 2.022s, learning 0.108s)
             Mean action noise std: 2.89
          Mean value_function loss: 139.4077
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 54.0938
                       Mean reward: 896.32
               Mean episode length: 238.46
    Episode_Reward/reaching_object: 1.3495
     Episode_Reward/lifting_object: 173.7201
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 2.13s
                      Time elapsed: 00:55:57
                               ETA: 00:20:06

################################################################################
                     [1m Learning iteration 1472/2000 [0m                     

                       Computation: 45706 steps/s (collection: 2.046s, learning 0.105s)
             Mean action noise std: 2.90
          Mean value_function loss: 102.5048
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 54.1015
                       Mean reward: 875.58
               Mean episode length: 235.63
    Episode_Reward/reaching_object: 1.3422
     Episode_Reward/lifting_object: 172.2960
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 2.15s
                      Time elapsed: 00:55:59
                               ETA: 00:20:04

################################################################################
                     [1m Learning iteration 1473/2000 [0m                     

                       Computation: 46214 steps/s (collection: 2.016s, learning 0.111s)
             Mean action noise std: 2.90
          Mean value_function loss: 98.5392
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.1149
                       Mean reward: 887.98
               Mean episode length: 236.40
    Episode_Reward/reaching_object: 1.3828
     Episode_Reward/lifting_object: 178.6294
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 2.13s
                      Time elapsed: 00:56:01
                               ETA: 00:20:01

################################################################################
                     [1m Learning iteration 1474/2000 [0m                     

                       Computation: 45675 steps/s (collection: 2.049s, learning 0.103s)
             Mean action noise std: 2.90
          Mean value_function loss: 139.6851
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 54.1227
                       Mean reward: 862.12
               Mean episode length: 231.07
    Episode_Reward/reaching_object: 1.3590
     Episode_Reward/lifting_object: 175.1048
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 2.15s
                      Time elapsed: 00:56:03
                               ETA: 00:19:59

################################################################################
                     [1m Learning iteration 1475/2000 [0m                     

                       Computation: 46829 steps/s (collection: 1.998s, learning 0.101s)
             Mean action noise std: 2.90
          Mean value_function loss: 124.1652
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 54.1304
                       Mean reward: 892.13
               Mean episode length: 238.87
    Episode_Reward/reaching_object: 1.3491
     Episode_Reward/lifting_object: 173.8097
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 2.10s
                      Time elapsed: 00:56:05
                               ETA: 00:19:57

################################################################################
                     [1m Learning iteration 1476/2000 [0m                     

                       Computation: 47067 steps/s (collection: 1.998s, learning 0.091s)
             Mean action noise std: 2.90
          Mean value_function loss: 128.7097
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 54.1461
                       Mean reward: 871.94
               Mean episode length: 232.64
    Episode_Reward/reaching_object: 1.3310
     Episode_Reward/lifting_object: 171.9046
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 2.09s
                      Time elapsed: 00:56:07
                               ETA: 00:19:54

################################################################################
                     [1m Learning iteration 1477/2000 [0m                     

                       Computation: 46925 steps/s (collection: 1.996s, learning 0.099s)
             Mean action noise std: 2.90
          Mean value_function loss: 113.1161
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.1622
                       Mean reward: 862.59
               Mean episode length: 230.10
    Episode_Reward/reaching_object: 1.3478
     Episode_Reward/lifting_object: 174.1058
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 2.09s
                      Time elapsed: 00:56:09
                               ETA: 00:19:52

################################################################################
                     [1m Learning iteration 1478/2000 [0m                     

                       Computation: 46505 steps/s (collection: 2.017s, learning 0.097s)
             Mean action noise std: 2.90
          Mean value_function loss: 113.4871
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 54.1727
                       Mean reward: 901.01
               Mean episode length: 239.18
    Episode_Reward/reaching_object: 1.3712
     Episode_Reward/lifting_object: 177.4624
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 2.11s
                      Time elapsed: 00:56:11
                               ETA: 00:19:50

################################################################################
                     [1m Learning iteration 1479/2000 [0m                     

                       Computation: 46843 steps/s (collection: 1.994s, learning 0.104s)
             Mean action noise std: 2.91
          Mean value_function loss: 146.4950
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.1791
                       Mean reward: 838.35
               Mean episode length: 224.99
    Episode_Reward/reaching_object: 1.3233
     Episode_Reward/lifting_object: 171.2636
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 2.10s
                      Time elapsed: 00:56:14
                               ETA: 00:19:47

################################################################################
                     [1m Learning iteration 1480/2000 [0m                     

                       Computation: 46278 steps/s (collection: 2.019s, learning 0.106s)
             Mean action noise std: 2.91
          Mean value_function loss: 148.9789
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.1891
                       Mean reward: 866.40
               Mean episode length: 231.88
    Episode_Reward/reaching_object: 1.3071
     Episode_Reward/lifting_object: 169.4720
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 2.12s
                      Time elapsed: 00:56:16
                               ETA: 00:19:45

################################################################################
                     [1m Learning iteration 1481/2000 [0m                     

                       Computation: 46600 steps/s (collection: 2.015s, learning 0.094s)
             Mean action noise std: 2.91
          Mean value_function loss: 114.8308
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.2031
                       Mean reward: 868.42
               Mean episode length: 231.73
    Episode_Reward/reaching_object: 1.3268
     Episode_Reward/lifting_object: 172.5641
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 2.11s
                      Time elapsed: 00:56:18
                               ETA: 00:19:43

################################################################################
                     [1m Learning iteration 1482/2000 [0m                     

                       Computation: 46054 steps/s (collection: 2.037s, learning 0.097s)
             Mean action noise std: 2.91
          Mean value_function loss: 126.1408
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.2212
                       Mean reward: 849.18
               Mean episode length: 230.41
    Episode_Reward/reaching_object: 1.3351
     Episode_Reward/lifting_object: 174.0658
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 2.13s
                      Time elapsed: 00:56:20
                               ETA: 00:19:40

################################################################################
                     [1m Learning iteration 1483/2000 [0m                     

                       Computation: 45649 steps/s (collection: 2.064s, learning 0.090s)
             Mean action noise std: 2.91
          Mean value_function loss: 97.4977
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 54.2332
                       Mean reward: 885.84
               Mean episode length: 238.79
    Episode_Reward/reaching_object: 1.3612
     Episode_Reward/lifting_object: 177.1186
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 2.15s
                      Time elapsed: 00:56:22
                               ETA: 00:19:38

################################################################################
                     [1m Learning iteration 1484/2000 [0m                     

                       Computation: 46648 steps/s (collection: 2.017s, learning 0.091s)
             Mean action noise std: 2.91
          Mean value_function loss: 144.0138
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.2427
                       Mean reward: 870.80
               Mean episode length: 232.45
    Episode_Reward/reaching_object: 1.3252
     Episode_Reward/lifting_object: 172.8172
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 2.11s
                      Time elapsed: 00:56:24
                               ETA: 00:19:36

################################################################################
                     [1m Learning iteration 1485/2000 [0m                     

                       Computation: 46245 steps/s (collection: 2.014s, learning 0.112s)
             Mean action noise std: 2.92
          Mean value_function loss: 109.8260
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.2511
                       Mean reward: 853.73
               Mean episode length: 229.04
    Episode_Reward/reaching_object: 1.3209
     Episode_Reward/lifting_object: 171.0427
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 2.13s
                      Time elapsed: 00:56:26
                               ETA: 00:19:33

################################################################################
                     [1m Learning iteration 1486/2000 [0m                     

                       Computation: 45852 steps/s (collection: 2.032s, learning 0.112s)
             Mean action noise std: 2.92
          Mean value_function loss: 125.3569
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 54.2590
                       Mean reward: 858.58
               Mean episode length: 230.89
    Episode_Reward/reaching_object: 1.3246
     Episode_Reward/lifting_object: 172.4445
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 2.14s
                      Time elapsed: 00:56:28
                               ETA: 00:19:31

################################################################################
                     [1m Learning iteration 1487/2000 [0m                     

                       Computation: 45794 steps/s (collection: 2.042s, learning 0.105s)
             Mean action noise std: 2.92
          Mean value_function loss: 96.3467
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 54.2644
                       Mean reward: 915.66
               Mean episode length: 242.85
    Episode_Reward/reaching_object: 1.3445
     Episode_Reward/lifting_object: 174.6077
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 2.15s
                      Time elapsed: 00:56:31
                               ETA: 00:19:29

################################################################################
                     [1m Learning iteration 1488/2000 [0m                     

                       Computation: 46184 steps/s (collection: 2.036s, learning 0.093s)
             Mean action noise std: 2.92
          Mean value_function loss: 107.9231
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.2722
                       Mean reward: 900.32
               Mean episode length: 240.53
    Episode_Reward/reaching_object: 1.3425
     Episode_Reward/lifting_object: 173.3127
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 2.13s
                      Time elapsed: 00:56:33
                               ETA: 00:19:26

################################################################################
                     [1m Learning iteration 1489/2000 [0m                     

                       Computation: 45603 steps/s (collection: 2.065s, learning 0.091s)
             Mean action noise std: 2.92
          Mean value_function loss: 107.5096
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.2840
                       Mean reward: 896.18
               Mean episode length: 239.20
    Episode_Reward/reaching_object: 1.3591
     Episode_Reward/lifting_object: 177.0039
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 2.16s
                      Time elapsed: 00:56:35
                               ETA: 00:19:24

################################################################################
                     [1m Learning iteration 1490/2000 [0m                     

                       Computation: 46391 steps/s (collection: 2.022s, learning 0.097s)
             Mean action noise std: 2.92
          Mean value_function loss: 79.4806
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 54.2973
                       Mean reward: 877.62
               Mean episode length: 237.16
    Episode_Reward/reaching_object: 1.3436
     Episode_Reward/lifting_object: 173.8160
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 2.12s
                      Time elapsed: 00:56:37
                               ETA: 00:19:22

################################################################################
                     [1m Learning iteration 1491/2000 [0m                     

                       Computation: 41484 steps/s (collection: 2.259s, learning 0.111s)
             Mean action noise std: 2.92
          Mean value_function loss: 101.1199
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.3063
                       Mean reward: 873.69
               Mean episode length: 233.22
    Episode_Reward/reaching_object: 1.3515
     Episode_Reward/lifting_object: 174.9566
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 2.37s
                      Time elapsed: 00:56:39
                               ETA: 00:19:19

################################################################################
                     [1m Learning iteration 1492/2000 [0m                     

                       Computation: 45274 steps/s (collection: 2.069s, learning 0.102s)
             Mean action noise std: 2.92
          Mean value_function loss: 101.2301
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.3124
                       Mean reward: 867.83
               Mean episode length: 232.61
    Episode_Reward/reaching_object: 1.3560
     Episode_Reward/lifting_object: 175.9391
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 2.17s
                      Time elapsed: 00:56:41
                               ETA: 00:19:17

################################################################################
                     [1m Learning iteration 1493/2000 [0m                     

                       Computation: 41399 steps/s (collection: 2.245s, learning 0.130s)
             Mean action noise std: 2.93
          Mean value_function loss: 129.4926
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 54.3256
                       Mean reward: 821.49
               Mean episode length: 222.43
    Episode_Reward/reaching_object: 1.3406
     Episode_Reward/lifting_object: 173.1168
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 2.37s
                      Time elapsed: 00:56:44
                               ETA: 00:19:15

################################################################################
                     [1m Learning iteration 1494/2000 [0m                     

                       Computation: 45886 steps/s (collection: 2.041s, learning 0.101s)
             Mean action noise std: 2.93
          Mean value_function loss: 110.7771
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 54.3433
                       Mean reward: 861.94
               Mean episode length: 231.77
    Episode_Reward/reaching_object: 1.3414
     Episode_Reward/lifting_object: 173.2480
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 2.14s
                      Time elapsed: 00:56:46
                               ETA: 00:19:12

################################################################################
                     [1m Learning iteration 1495/2000 [0m                     

                       Computation: 45629 steps/s (collection: 2.061s, learning 0.094s)
             Mean action noise std: 2.93
          Mean value_function loss: 92.9547
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 54.3516
                       Mean reward: 873.47
               Mean episode length: 232.96
    Episode_Reward/reaching_object: 1.3595
     Episode_Reward/lifting_object: 175.5428
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 2.15s
                      Time elapsed: 00:56:48
                               ETA: 00:19:10

################################################################################
                     [1m Learning iteration 1496/2000 [0m                     

                       Computation: 45312 steps/s (collection: 2.083s, learning 0.086s)
             Mean action noise std: 2.93
          Mean value_function loss: 82.5838
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.3657
                       Mean reward: 890.87
               Mean episode length: 237.98
    Episode_Reward/reaching_object: 1.3787
     Episode_Reward/lifting_object: 178.0888
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 2.17s
                      Time elapsed: 00:56:50
                               ETA: 00:19:08

################################################################################
                     [1m Learning iteration 1497/2000 [0m                     

                       Computation: 45508 steps/s (collection: 2.069s, learning 0.092s)
             Mean action noise std: 2.93
          Mean value_function loss: 128.6334
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 54.3769
                       Mean reward: 852.06
               Mean episode length: 228.56
    Episode_Reward/reaching_object: 1.3135
     Episode_Reward/lifting_object: 169.3670
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 2.16s
                      Time elapsed: 00:56:52
                               ETA: 00:19:06

################################################################################
                     [1m Learning iteration 1498/2000 [0m                     

                       Computation: 45269 steps/s (collection: 2.071s, learning 0.101s)
             Mean action noise std: 2.93
          Mean value_function loss: 101.9180
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.3876
                       Mean reward: 899.07
               Mean episode length: 239.13
    Episode_Reward/reaching_object: 1.3483
     Episode_Reward/lifting_object: 174.9314
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 2.17s
                      Time elapsed: 00:56:55
                               ETA: 00:19:03

################################################################################
                     [1m Learning iteration 1499/2000 [0m                     

                       Computation: 46777 steps/s (collection: 2.013s, learning 0.089s)
             Mean action noise std: 2.94
          Mean value_function loss: 124.3283
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.4005
                       Mean reward: 896.22
               Mean episode length: 238.85
    Episode_Reward/reaching_object: 1.3535
     Episode_Reward/lifting_object: 174.5837
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 2.10s
                      Time elapsed: 00:56:57
                               ETA: 00:19:01

################################################################################
                     [1m Learning iteration 1500/2000 [0m                     

                       Computation: 45797 steps/s (collection: 2.053s, learning 0.094s)
             Mean action noise std: 2.94
          Mean value_function loss: 124.4815
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.4154
                       Mean reward: 855.19
               Mean episode length: 227.59
    Episode_Reward/reaching_object: 1.3292
     Episode_Reward/lifting_object: 172.2701
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 2.15s
                      Time elapsed: 00:56:59
                               ETA: 00:18:59

################################################################################
                     [1m Learning iteration 1501/2000 [0m                     

                       Computation: 44737 steps/s (collection: 2.091s, learning 0.106s)
             Mean action noise std: 2.94
          Mean value_function loss: 126.2921
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.4286
                       Mean reward: 898.05
               Mean episode length: 238.24
    Episode_Reward/reaching_object: 1.3067
     Episode_Reward/lifting_object: 169.7177
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 2.20s
                      Time elapsed: 00:57:01
                               ETA: 00:18:56

################################################################################
                     [1m Learning iteration 1502/2000 [0m                     

                       Computation: 43027 steps/s (collection: 2.154s, learning 0.131s)
             Mean action noise std: 2.94
          Mean value_function loss: 128.3214
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.4413
                       Mean reward: 859.99
               Mean episode length: 231.55
    Episode_Reward/reaching_object: 1.3261
     Episode_Reward/lifting_object: 171.6382
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 2.28s
                      Time elapsed: 00:57:03
                               ETA: 00:18:54

################################################################################
                     [1m Learning iteration 1503/2000 [0m                     

                       Computation: 45744 steps/s (collection: 2.059s, learning 0.090s)
             Mean action noise std: 2.94
          Mean value_function loss: 148.8637
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.4534
                       Mean reward: 783.61
               Mean episode length: 212.14
    Episode_Reward/reaching_object: 1.3185
     Episode_Reward/lifting_object: 170.0357
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 2.15s
                      Time elapsed: 00:57:06
                               ETA: 00:18:52

################################################################################
                     [1m Learning iteration 1504/2000 [0m                     

                       Computation: 44400 steps/s (collection: 2.075s, learning 0.139s)
             Mean action noise std: 2.94
          Mean value_function loss: 126.2641
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.4634
                       Mean reward: 900.85
               Mean episode length: 239.26
    Episode_Reward/reaching_object: 1.3303
     Episode_Reward/lifting_object: 171.4252
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 2.21s
                      Time elapsed: 00:57:08
                               ETA: 00:18:49

################################################################################
                     [1m Learning iteration 1505/2000 [0m                     

                       Computation: 42785 steps/s (collection: 2.120s, learning 0.178s)
             Mean action noise std: 2.94
          Mean value_function loss: 123.5383
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.4705
                       Mean reward: 878.43
               Mean episode length: 233.50
    Episode_Reward/reaching_object: 1.3379
     Episode_Reward/lifting_object: 173.5616
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 2.30s
                      Time elapsed: 00:57:10
                               ETA: 00:18:47

################################################################################
                     [1m Learning iteration 1506/2000 [0m                     

                       Computation: 44128 steps/s (collection: 2.141s, learning 0.087s)
             Mean action noise std: 2.95
          Mean value_function loss: 136.6409
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 54.4801
                       Mean reward: 881.65
               Mean episode length: 233.78
    Episode_Reward/reaching_object: 1.3206
     Episode_Reward/lifting_object: 170.9661
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 2.23s
                      Time elapsed: 00:57:12
                               ETA: 00:18:45

################################################################################
                     [1m Learning iteration 1507/2000 [0m                     

                       Computation: 44998 steps/s (collection: 2.096s, learning 0.089s)
             Mean action noise std: 2.95
          Mean value_function loss: 119.9829
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.4944
                       Mean reward: 859.01
               Mean episode length: 230.88
    Episode_Reward/reaching_object: 1.3451
     Episode_Reward/lifting_object: 174.3806
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 2.18s
                      Time elapsed: 00:57:14
                               ETA: 00:18:42

################################################################################
                     [1m Learning iteration 1508/2000 [0m                     

                       Computation: 42516 steps/s (collection: 2.218s, learning 0.094s)
             Mean action noise std: 2.95
          Mean value_function loss: 99.9224
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 54.5081
                       Mean reward: 894.13
               Mean episode length: 238.83
    Episode_Reward/reaching_object: 1.3615
     Episode_Reward/lifting_object: 175.6799
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 2.31s
                      Time elapsed: 00:57:17
                               ETA: 00:18:40

################################################################################
                     [1m Learning iteration 1509/2000 [0m                     

                       Computation: 43391 steps/s (collection: 2.119s, learning 0.147s)
             Mean action noise std: 2.95
          Mean value_function loss: 111.8091
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.5249
                       Mean reward: 869.09
               Mean episode length: 234.13
    Episode_Reward/reaching_object: 1.3633
     Episode_Reward/lifting_object: 176.1668
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 2.27s
                      Time elapsed: 00:57:19
                               ETA: 00:18:38

################################################################################
                     [1m Learning iteration 1510/2000 [0m                     

                       Computation: 45228 steps/s (collection: 2.082s, learning 0.092s)
             Mean action noise std: 2.95
          Mean value_function loss: 95.1348
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.5375
                       Mean reward: 871.58
               Mean episode length: 232.76
    Episode_Reward/reaching_object: 1.3582
     Episode_Reward/lifting_object: 176.9415
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 2.17s
                      Time elapsed: 00:57:21
                               ETA: 00:18:36

################################################################################
                     [1m Learning iteration 1511/2000 [0m                     

                       Computation: 45091 steps/s (collection: 2.084s, learning 0.097s)
             Mean action noise std: 2.96
          Mean value_function loss: 137.4458
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 54.5455
                       Mean reward: 830.82
               Mean episode length: 224.50
    Episode_Reward/reaching_object: 1.3203
     Episode_Reward/lifting_object: 170.4148
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 2.18s
                      Time elapsed: 00:57:23
                               ETA: 00:18:33

################################################################################
                     [1m Learning iteration 1512/2000 [0m                     

                       Computation: 45742 steps/s (collection: 2.052s, learning 0.098s)
             Mean action noise std: 2.96
          Mean value_function loss: 131.8916
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 54.5587
                       Mean reward: 842.96
               Mean episode length: 225.66
    Episode_Reward/reaching_object: 1.3312
     Episode_Reward/lifting_object: 173.4908
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 2.15s
                      Time elapsed: 00:57:26
                               ETA: 00:18:31

################################################################################
                     [1m Learning iteration 1513/2000 [0m                     

                       Computation: 44428 steps/s (collection: 2.106s, learning 0.107s)
             Mean action noise std: 2.96
          Mean value_function loss: 113.9834
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 54.5738
                       Mean reward: 868.76
               Mean episode length: 233.50
    Episode_Reward/reaching_object: 1.3175
     Episode_Reward/lifting_object: 170.9346
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 2.21s
                      Time elapsed: 00:57:28
                               ETA: 00:18:29

################################################################################
                     [1m Learning iteration 1514/2000 [0m                     

                       Computation: 44778 steps/s (collection: 2.102s, learning 0.093s)
             Mean action noise std: 2.96
          Mean value_function loss: 137.5972
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.5849
                       Mean reward: 828.61
               Mean episode length: 223.12
    Episode_Reward/reaching_object: 1.3261
     Episode_Reward/lifting_object: 171.1428
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 2.20s
                      Time elapsed: 00:57:30
                               ETA: 00:18:26

################################################################################
                     [1m Learning iteration 1515/2000 [0m                     

                       Computation: 46185 steps/s (collection: 2.027s, learning 0.101s)
             Mean action noise std: 2.96
          Mean value_function loss: 108.7428
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 54.5940
                       Mean reward: 888.19
               Mean episode length: 236.69
    Episode_Reward/reaching_object: 1.3586
     Episode_Reward/lifting_object: 177.0968
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 2.13s
                      Time elapsed: 00:57:32
                               ETA: 00:18:24

################################################################################
                     [1m Learning iteration 1516/2000 [0m                     

                       Computation: 45386 steps/s (collection: 2.059s, learning 0.107s)
             Mean action noise std: 2.96
          Mean value_function loss: 137.5224
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.6029
                       Mean reward: 880.38
               Mean episode length: 235.77
    Episode_Reward/reaching_object: 1.3380
     Episode_Reward/lifting_object: 174.2798
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 2.17s
                      Time elapsed: 00:57:34
                               ETA: 00:18:22

################################################################################
                     [1m Learning iteration 1517/2000 [0m                     

                       Computation: 46753 steps/s (collection: 2.011s, learning 0.092s)
             Mean action noise std: 2.96
          Mean value_function loss: 124.2610
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.6077
                       Mean reward: 851.29
               Mean episode length: 227.50
    Episode_Reward/reaching_object: 1.3077
     Episode_Reward/lifting_object: 170.3582
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 2.10s
                      Time elapsed: 00:57:36
                               ETA: 00:18:19

################################################################################
                     [1m Learning iteration 1518/2000 [0m                     

                       Computation: 46601 steps/s (collection: 2.017s, learning 0.092s)
             Mean action noise std: 2.96
          Mean value_function loss: 126.9834
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 54.6173
                       Mean reward: 853.60
               Mean episode length: 228.97
    Episode_Reward/reaching_object: 1.3415
     Episode_Reward/lifting_object: 174.9380
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 2.11s
                      Time elapsed: 00:57:38
                               ETA: 00:18:17

################################################################################
                     [1m Learning iteration 1519/2000 [0m                     

                       Computation: 43590 steps/s (collection: 2.115s, learning 0.141s)
             Mean action noise std: 2.97
          Mean value_function loss: 119.5843
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.6253
                       Mean reward: 879.47
               Mean episode length: 233.99
    Episode_Reward/reaching_object: 1.3485
     Episode_Reward/lifting_object: 175.0888
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 2.26s
                      Time elapsed: 00:57:41
                               ETA: 00:18:15

################################################################################
                     [1m Learning iteration 1520/2000 [0m                     

                       Computation: 44270 steps/s (collection: 2.127s, learning 0.094s)
             Mean action noise std: 2.97
          Mean value_function loss: 93.0155
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.6356
                       Mean reward: 873.51
               Mean episode length: 234.01
    Episode_Reward/reaching_object: 1.3410
     Episode_Reward/lifting_object: 174.4145
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 2.22s
                      Time elapsed: 00:57:43
                               ETA: 00:18:12

################################################################################
                     [1m Learning iteration 1521/2000 [0m                     

                       Computation: 43733 steps/s (collection: 2.042s, learning 0.205s)
             Mean action noise std: 2.97
          Mean value_function loss: 97.8483
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.6466
                       Mean reward: 900.41
               Mean episode length: 239.05
    Episode_Reward/reaching_object: 1.3316
     Episode_Reward/lifting_object: 173.7934
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 2.25s
                      Time elapsed: 00:57:45
                               ETA: 00:18:10

################################################################################
                     [1m Learning iteration 1522/2000 [0m                     

                       Computation: 43086 steps/s (collection: 2.169s, learning 0.113s)
             Mean action noise std: 2.97
          Mean value_function loss: 89.7409
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 54.6548
                       Mean reward: 901.47
               Mean episode length: 239.48
    Episode_Reward/reaching_object: 1.3735
     Episode_Reward/lifting_object: 179.0542
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 2.28s
                      Time elapsed: 00:57:47
                               ETA: 00:18:08

################################################################################
                     [1m Learning iteration 1523/2000 [0m                     

                       Computation: 46957 steps/s (collection: 2.007s, learning 0.086s)
             Mean action noise std: 2.97
          Mean value_function loss: 103.8499
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 54.6661
                       Mean reward: 851.19
               Mean episode length: 231.00
    Episode_Reward/reaching_object: 1.3449
     Episode_Reward/lifting_object: 173.6251
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 2.09s
                      Time elapsed: 00:57:50
                               ETA: 00:18:06

################################################################################
                     [1m Learning iteration 1524/2000 [0m                     

                       Computation: 46345 steps/s (collection: 2.019s, learning 0.102s)
             Mean action noise std: 2.97
          Mean value_function loss: 98.2434
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.6760
                       Mean reward: 902.40
               Mean episode length: 240.04
    Episode_Reward/reaching_object: 1.3671
     Episode_Reward/lifting_object: 177.4498
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 2.12s
                      Time elapsed: 00:57:52
                               ETA: 00:18:03

################################################################################
                     [1m Learning iteration 1525/2000 [0m                     

                       Computation: 45656 steps/s (collection: 2.055s, learning 0.098s)
             Mean action noise std: 2.97
          Mean value_function loss: 100.1114
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 54.6883
                       Mean reward: 862.33
               Mean episode length: 232.01
    Episode_Reward/reaching_object: 1.3442
     Episode_Reward/lifting_object: 173.8673
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 2.15s
                      Time elapsed: 00:57:54
                               ETA: 00:18:01

################################################################################
                     [1m Learning iteration 1526/2000 [0m                     

                       Computation: 45237 steps/s (collection: 2.063s, learning 0.111s)
             Mean action noise std: 2.98
          Mean value_function loss: 107.2014
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.7006
                       Mean reward: 893.01
               Mean episode length: 238.14
    Episode_Reward/reaching_object: 1.3817
     Episode_Reward/lifting_object: 178.5937
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 2.17s
                      Time elapsed: 00:57:56
                               ETA: 00:17:59

################################################################################
                     [1m Learning iteration 1527/2000 [0m                     

                       Computation: 45526 steps/s (collection: 2.028s, learning 0.131s)
             Mean action noise std: 2.98
          Mean value_function loss: 107.3332
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 54.7119
                       Mean reward: 883.32
               Mean episode length: 235.00
    Episode_Reward/reaching_object: 1.3586
     Episode_Reward/lifting_object: 175.7135
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 2.16s
                      Time elapsed: 00:57:58
                               ETA: 00:17:56

################################################################################
                     [1m Learning iteration 1528/2000 [0m                     

                       Computation: 44773 steps/s (collection: 2.064s, learning 0.132s)
             Mean action noise std: 2.98
          Mean value_function loss: 115.2972
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.7219
                       Mean reward: 877.77
               Mean episode length: 234.77
    Episode_Reward/reaching_object: 1.3592
     Episode_Reward/lifting_object: 174.3305
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 2.20s
                      Time elapsed: 00:58:00
                               ETA: 00:17:54

################################################################################
                     [1m Learning iteration 1529/2000 [0m                     

                       Computation: 45646 steps/s (collection: 2.036s, learning 0.118s)
             Mean action noise std: 2.98
          Mean value_function loss: 134.8766
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 54.7286
                       Mean reward: 850.17
               Mean episode length: 228.06
    Episode_Reward/reaching_object: 1.3516
     Episode_Reward/lifting_object: 174.2751
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 2.15s
                      Time elapsed: 00:58:03
                               ETA: 00:17:52

################################################################################
                     [1m Learning iteration 1530/2000 [0m                     

                       Computation: 44792 steps/s (collection: 2.079s, learning 0.116s)
             Mean action noise std: 2.98
          Mean value_function loss: 113.6906
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.7408
                       Mean reward: 837.81
               Mean episode length: 225.63
    Episode_Reward/reaching_object: 1.3309
     Episode_Reward/lifting_object: 171.3611
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 2.19s
                      Time elapsed: 00:58:05
                               ETA: 00:17:49

################################################################################
                     [1m Learning iteration 1531/2000 [0m                     

                       Computation: 42025 steps/s (collection: 2.218s, learning 0.121s)
             Mean action noise std: 2.98
          Mean value_function loss: 127.3191
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.7471
                       Mean reward: 918.81
               Mean episode length: 243.54
    Episode_Reward/reaching_object: 1.3847
     Episode_Reward/lifting_object: 178.6393
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 2.34s
                      Time elapsed: 00:58:07
                               ETA: 00:17:47

################################################################################
                     [1m Learning iteration 1532/2000 [0m                     

                       Computation: 45343 steps/s (collection: 2.076s, learning 0.092s)
             Mean action noise std: 2.98
          Mean value_function loss: 122.0937
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 54.7554
                       Mean reward: 834.78
               Mean episode length: 225.45
    Episode_Reward/reaching_object: 1.3312
     Episode_Reward/lifting_object: 170.1426
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 2.17s
                      Time elapsed: 00:58:09
                               ETA: 00:17:45

################################################################################
                     [1m Learning iteration 1533/2000 [0m                     

                       Computation: 43920 steps/s (collection: 2.091s, learning 0.148s)
             Mean action noise std: 2.99
          Mean value_function loss: 130.1407
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.7672
                       Mean reward: 875.26
               Mean episode length: 233.56
    Episode_Reward/reaching_object: 1.3332
     Episode_Reward/lifting_object: 170.4228
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 2.24s
                      Time elapsed: 00:58:11
                               ETA: 00:17:43

################################################################################
                     [1m Learning iteration 1534/2000 [0m                     

                       Computation: 44942 steps/s (collection: 2.097s, learning 0.090s)
             Mean action noise std: 2.99
          Mean value_function loss: 111.4794
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 54.7797
                       Mean reward: 849.15
               Mean episode length: 227.44
    Episode_Reward/reaching_object: 1.3359
     Episode_Reward/lifting_object: 171.1963
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 2.19s
                      Time elapsed: 00:58:14
                               ETA: 00:17:40

################################################################################
                     [1m Learning iteration 1535/2000 [0m                     

                       Computation: 45685 steps/s (collection: 2.062s, learning 0.090s)
             Mean action noise std: 2.99
          Mean value_function loss: 132.8748
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 54.7921
                       Mean reward: 855.74
               Mean episode length: 228.87
    Episode_Reward/reaching_object: 1.3378
     Episode_Reward/lifting_object: 171.2734
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 2.15s
                      Time elapsed: 00:58:16
                               ETA: 00:17:38

################################################################################
                     [1m Learning iteration 1536/2000 [0m                     

                       Computation: 44250 steps/s (collection: 2.093s, learning 0.129s)
             Mean action noise std: 2.99
          Mean value_function loss: 112.6154
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.7962
                       Mean reward: 834.31
               Mean episode length: 225.24
    Episode_Reward/reaching_object: 1.3462
     Episode_Reward/lifting_object: 171.7197
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 2.22s
                      Time elapsed: 00:58:18
                               ETA: 00:17:36

################################################################################
                     [1m Learning iteration 1537/2000 [0m                     

                       Computation: 42990 steps/s (collection: 2.144s, learning 0.143s)
             Mean action noise std: 2.99
          Mean value_function loss: 115.0207
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.8035
                       Mean reward: 869.82
               Mean episode length: 233.01
    Episode_Reward/reaching_object: 1.3566
     Episode_Reward/lifting_object: 173.2742
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 2.29s
                      Time elapsed: 00:58:20
                               ETA: 00:17:33

################################################################################
                     [1m Learning iteration 1538/2000 [0m                     

                       Computation: 43540 steps/s (collection: 2.154s, learning 0.104s)
             Mean action noise std: 2.99
          Mean value_function loss: 112.0315
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 54.8157
                       Mean reward: 871.61
               Mean episode length: 232.98
    Episode_Reward/reaching_object: 1.3482
     Episode_Reward/lifting_object: 171.5645
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 2.26s
                      Time elapsed: 00:58:23
                               ETA: 00:17:31

################################################################################
                     [1m Learning iteration 1539/2000 [0m                     

                       Computation: 45533 steps/s (collection: 2.058s, learning 0.101s)
             Mean action noise std: 2.99
          Mean value_function loss: 131.7404
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.8228
                       Mean reward: 838.63
               Mean episode length: 226.36
    Episode_Reward/reaching_object: 1.3519
     Episode_Reward/lifting_object: 172.6976
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 2.16s
                      Time elapsed: 00:58:25
                               ETA: 00:17:29

################################################################################
                     [1m Learning iteration 1540/2000 [0m                     

                       Computation: 46039 steps/s (collection: 2.036s, learning 0.100s)
             Mean action noise std: 2.99
          Mean value_function loss: 120.0882
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 54.8315
                       Mean reward: 876.89
               Mean episode length: 233.49
    Episode_Reward/reaching_object: 1.3610
     Episode_Reward/lifting_object: 173.7506
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 2.14s
                      Time elapsed: 00:58:27
                               ETA: 00:17:26

################################################################################
                     [1m Learning iteration 1541/2000 [0m                     

                       Computation: 44378 steps/s (collection: 2.086s, learning 0.129s)
             Mean action noise std: 3.00
          Mean value_function loss: 112.0306
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 54.8425
                       Mean reward: 897.06
               Mean episode length: 237.35
    Episode_Reward/reaching_object: 1.3361
     Episode_Reward/lifting_object: 170.6051
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 2.22s
                      Time elapsed: 00:58:29
                               ETA: 00:17:24

################################################################################
                     [1m Learning iteration 1542/2000 [0m                     

                       Computation: 43964 steps/s (collection: 2.086s, learning 0.150s)
             Mean action noise std: 3.00
          Mean value_function loss: 156.5172
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.8572
                       Mean reward: 846.14
               Mean episode length: 228.35
    Episode_Reward/reaching_object: 1.3341
     Episode_Reward/lifting_object: 169.4595
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 2.24s
                      Time elapsed: 00:58:31
                               ETA: 00:17:22

################################################################################
                     [1m Learning iteration 1543/2000 [0m                     

                       Computation: 44387 steps/s (collection: 2.127s, learning 0.088s)
             Mean action noise std: 3.00
          Mean value_function loss: 135.8518
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 54.8693
                       Mean reward: 847.83
               Mean episode length: 227.62
    Episode_Reward/reaching_object: 1.3261
     Episode_Reward/lifting_object: 168.6331
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 2.21s
                      Time elapsed: 00:58:34
                               ETA: 00:17:20

################################################################################
                     [1m Learning iteration 1544/2000 [0m                     

                       Computation: 45989 steps/s (collection: 2.037s, learning 0.101s)
             Mean action noise std: 3.00
          Mean value_function loss: 120.5437
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 54.8761
                       Mean reward: 889.98
               Mean episode length: 236.05
    Episode_Reward/reaching_object: 1.3747
     Episode_Reward/lifting_object: 176.4778
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 2.14s
                      Time elapsed: 00:58:36
                               ETA: 00:17:17

################################################################################
                     [1m Learning iteration 1545/2000 [0m                     

                       Computation: 45825 steps/s (collection: 2.053s, learning 0.092s)
             Mean action noise std: 3.00
          Mean value_function loss: 108.8612
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.8919
                       Mean reward: 868.25
               Mean episode length: 232.23
    Episode_Reward/reaching_object: 1.3075
     Episode_Reward/lifting_object: 167.5560
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 2.15s
                      Time elapsed: 00:58:38
                               ETA: 00:17:15

################################################################################
                     [1m Learning iteration 1546/2000 [0m                     

                       Computation: 46283 steps/s (collection: 2.024s, learning 0.100s)
             Mean action noise std: 3.01
          Mean value_function loss: 152.4278
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 54.9073
                       Mean reward: 844.60
               Mean episode length: 227.48
    Episode_Reward/reaching_object: 1.3279
     Episode_Reward/lifting_object: 170.2023
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 2.12s
                      Time elapsed: 00:58:40
                               ETA: 00:17:13

################################################################################
                     [1m Learning iteration 1547/2000 [0m                     

                       Computation: 45599 steps/s (collection: 2.065s, learning 0.091s)
             Mean action noise std: 3.01
          Mean value_function loss: 132.1862
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 54.9192
                       Mean reward: 866.02
               Mean episode length: 231.74
    Episode_Reward/reaching_object: 1.3331
     Episode_Reward/lifting_object: 172.0849
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 2.16s
                      Time elapsed: 00:58:42
                               ETA: 00:17:10

################################################################################
                     [1m Learning iteration 1548/2000 [0m                     

                       Computation: 44804 steps/s (collection: 2.083s, learning 0.111s)
             Mean action noise std: 3.01
          Mean value_function loss: 130.3652
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 54.9241
                       Mean reward: 887.05
               Mean episode length: 236.66
    Episode_Reward/reaching_object: 1.3461
     Episode_Reward/lifting_object: 173.6344
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 2.19s
                      Time elapsed: 00:58:44
                               ETA: 00:17:08

################################################################################
                     [1m Learning iteration 1549/2000 [0m                     

                       Computation: 44957 steps/s (collection: 2.073s, learning 0.114s)
             Mean action noise std: 3.01
          Mean value_function loss: 126.5468
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 54.9335
                       Mean reward: 866.07
               Mean episode length: 232.14
    Episode_Reward/reaching_object: 1.3409
     Episode_Reward/lifting_object: 173.3084
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 2.19s
                      Time elapsed: 00:58:46
                               ETA: 00:17:06

################################################################################
                     [1m Learning iteration 1550/2000 [0m                     

                       Computation: 46706 steps/s (collection: 2.014s, learning 0.091s)
             Mean action noise std: 3.01
          Mean value_function loss: 175.7832
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 54.9454
                       Mean reward: 891.31
               Mean episode length: 236.78
    Episode_Reward/reaching_object: 1.3359
     Episode_Reward/lifting_object: 172.5963
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 2.10s
                      Time elapsed: 00:58:49
                               ETA: 00:17:03

################################################################################
                     [1m Learning iteration 1551/2000 [0m                     

                       Computation: 43486 steps/s (collection: 2.142s, learning 0.118s)
             Mean action noise std: 3.01
          Mean value_function loss: 131.3676
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 54.9495
                       Mean reward: 873.47
               Mean episode length: 232.95
    Episode_Reward/reaching_object: 1.3152
     Episode_Reward/lifting_object: 170.4006
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 2.26s
                      Time elapsed: 00:58:51
                               ETA: 00:17:01

################################################################################
                     [1m Learning iteration 1552/2000 [0m                     

                       Computation: 45892 steps/s (collection: 2.053s, learning 0.090s)
             Mean action noise std: 3.01
          Mean value_function loss: 131.8422
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 54.9515
                       Mean reward: 864.04
               Mean episode length: 230.61
    Episode_Reward/reaching_object: 1.3501
     Episode_Reward/lifting_object: 176.1272
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 2.14s
                      Time elapsed: 00:58:53
                               ETA: 00:16:59

################################################################################
                     [1m Learning iteration 1553/2000 [0m                     

                       Computation: 45112 steps/s (collection: 2.080s, learning 0.100s)
             Mean action noise std: 3.01
          Mean value_function loss: 94.7364
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.9541
                       Mean reward: 889.19
               Mean episode length: 237.50
    Episode_Reward/reaching_object: 1.3558
     Episode_Reward/lifting_object: 175.7366
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 2.18s
                      Time elapsed: 00:58:55
                               ETA: 00:16:57

################################################################################
                     [1m Learning iteration 1554/2000 [0m                     

                       Computation: 43665 steps/s (collection: 2.092s, learning 0.160s)
             Mean action noise std: 3.01
          Mean value_function loss: 156.1465
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 54.9609
                       Mean reward: 842.32
               Mean episode length: 226.70
    Episode_Reward/reaching_object: 1.3217
     Episode_Reward/lifting_object: 171.3351
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 2.25s
                      Time elapsed: 00:58:57
                               ETA: 00:16:54

################################################################################
                     [1m Learning iteration 1555/2000 [0m                     

                       Computation: 44550 steps/s (collection: 2.117s, learning 0.090s)
             Mean action noise std: 3.01
          Mean value_function loss: 97.0854
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 54.9692
                       Mean reward: 854.31
               Mean episode length: 228.29
    Episode_Reward/reaching_object: 1.3387
     Episode_Reward/lifting_object: 174.3523
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 2.21s
                      Time elapsed: 00:59:00
                               ETA: 00:16:52

################################################################################
                     [1m Learning iteration 1556/2000 [0m                     

                       Computation: 45467 steps/s (collection: 2.048s, learning 0.115s)
             Mean action noise std: 3.01
          Mean value_function loss: 115.2815
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.9754
                       Mean reward: 833.96
               Mean episode length: 224.62
    Episode_Reward/reaching_object: 1.3092
     Episode_Reward/lifting_object: 169.8639
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 2.16s
                      Time elapsed: 00:59:02
                               ETA: 00:16:50

################################################################################
                     [1m Learning iteration 1557/2000 [0m                     

                       Computation: 45041 steps/s (collection: 2.068s, learning 0.115s)
             Mean action noise std: 3.02
          Mean value_function loss: 108.0173
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.9831
                       Mean reward: 875.89
               Mean episode length: 234.42
    Episode_Reward/reaching_object: 1.3461
     Episode_Reward/lifting_object: 174.7666
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 2.18s
                      Time elapsed: 00:59:04
                               ETA: 00:16:47

################################################################################
                     [1m Learning iteration 1558/2000 [0m                     

                       Computation: 45703 steps/s (collection: 2.052s, learning 0.099s)
             Mean action noise std: 3.02
          Mean value_function loss: 139.2944
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.9901
                       Mean reward: 850.86
               Mean episode length: 228.73
    Episode_Reward/reaching_object: 1.3045
     Episode_Reward/lifting_object: 170.2390
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 2.15s
                      Time elapsed: 00:59:06
                               ETA: 00:16:45

################################################################################
                     [1m Learning iteration 1559/2000 [0m                     

                       Computation: 43788 steps/s (collection: 2.118s, learning 0.127s)
             Mean action noise std: 3.02
          Mean value_function loss: 99.0667
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 54.9971
                       Mean reward: 886.14
               Mean episode length: 236.98
    Episode_Reward/reaching_object: 1.3573
     Episode_Reward/lifting_object: 176.5594
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 2.24s
                      Time elapsed: 00:59:08
                               ETA: 00:16:43

################################################################################
                     [1m Learning iteration 1560/2000 [0m                     

                       Computation: 45224 steps/s (collection: 2.068s, learning 0.106s)
             Mean action noise std: 3.02
          Mean value_function loss: 160.4155
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.0055
                       Mean reward: 822.37
               Mean episode length: 221.53
    Episode_Reward/reaching_object: 1.3047
     Episode_Reward/lifting_object: 169.3979
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 2.17s
                      Time elapsed: 00:59:11
                               ETA: 00:16:40

################################################################################
                     [1m Learning iteration 1561/2000 [0m                     

                       Computation: 45505 steps/s (collection: 2.064s, learning 0.096s)
             Mean action noise std: 3.02
          Mean value_function loss: 153.6678
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 55.0168
                       Mean reward: 854.58
               Mean episode length: 228.14
    Episode_Reward/reaching_object: 1.3215
     Episode_Reward/lifting_object: 171.7876
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 2.16s
                      Time elapsed: 00:59:13
                               ETA: 00:16:38

################################################################################
                     [1m Learning iteration 1562/2000 [0m                     

                       Computation: 45245 steps/s (collection: 2.071s, learning 0.102s)
             Mean action noise std: 3.02
          Mean value_function loss: 178.2066
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.0271
                       Mean reward: 891.53
               Mean episode length: 238.27
    Episode_Reward/reaching_object: 1.3313
     Episode_Reward/lifting_object: 172.8779
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 2.17s
                      Time elapsed: 00:59:15
                               ETA: 00:16:36

################################################################################
                     [1m Learning iteration 1563/2000 [0m                     

                       Computation: 45258 steps/s (collection: 2.080s, learning 0.092s)
             Mean action noise std: 3.02
          Mean value_function loss: 156.8676
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 55.0326
                       Mean reward: 863.57
               Mean episode length: 230.93
    Episode_Reward/reaching_object: 1.2998
     Episode_Reward/lifting_object: 168.6877
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 2.17s
                      Time elapsed: 00:59:17
                               ETA: 00:16:34

################################################################################
                     [1m Learning iteration 1564/2000 [0m                     

                       Computation: 44676 steps/s (collection: 2.103s, learning 0.097s)
             Mean action noise std: 3.02
          Mean value_function loss: 128.5089
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.0426
                       Mean reward: 847.97
               Mean episode length: 229.21
    Episode_Reward/reaching_object: 1.2995
     Episode_Reward/lifting_object: 169.5694
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 2.20s
                      Time elapsed: 00:59:19
                               ETA: 00:16:31

################################################################################
                     [1m Learning iteration 1565/2000 [0m                     

                       Computation: 42325 steps/s (collection: 2.210s, learning 0.112s)
             Mean action noise std: 3.03
          Mean value_function loss: 137.2142
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.0521
                       Mean reward: 853.45
               Mean episode length: 228.75
    Episode_Reward/reaching_object: 1.3035
     Episode_Reward/lifting_object: 169.6649
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 2.32s
                      Time elapsed: 00:59:22
                               ETA: 00:16:29

################################################################################
                     [1m Learning iteration 1566/2000 [0m                     

                       Computation: 44903 steps/s (collection: 2.099s, learning 0.090s)
             Mean action noise std: 3.03
          Mean value_function loss: 136.7491
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.0624
                       Mean reward: 863.88
               Mean episode length: 230.21
    Episode_Reward/reaching_object: 1.3129
     Episode_Reward/lifting_object: 171.7707
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 2.19s
                      Time elapsed: 00:59:24
                               ETA: 00:16:27

################################################################################
                     [1m Learning iteration 1567/2000 [0m                     

                       Computation: 45505 steps/s (collection: 2.065s, learning 0.095s)
             Mean action noise std: 3.03
          Mean value_function loss: 128.1489
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 55.0710
                       Mean reward: 850.20
               Mean episode length: 226.88
    Episode_Reward/reaching_object: 1.3297
     Episode_Reward/lifting_object: 173.6718
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 2.16s
                      Time elapsed: 00:59:26
                               ETA: 00:16:24

################################################################################
                     [1m Learning iteration 1568/2000 [0m                     

                       Computation: 42580 steps/s (collection: 2.119s, learning 0.190s)
             Mean action noise std: 3.03
          Mean value_function loss: 123.7300
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 55.0849
                       Mean reward: 870.44
               Mean episode length: 231.70
    Episode_Reward/reaching_object: 1.3171
     Episode_Reward/lifting_object: 172.5151
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 2.31s
                      Time elapsed: 00:59:28
                               ETA: 00:16:22

################################################################################
                     [1m Learning iteration 1569/2000 [0m                     

                       Computation: 41988 steps/s (collection: 2.236s, learning 0.106s)
             Mean action noise std: 3.03
          Mean value_function loss: 136.9923
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.0966
                       Mean reward: 898.38
               Mean episode length: 238.68
    Episode_Reward/reaching_object: 1.3156
     Episode_Reward/lifting_object: 172.9140
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 2.34s
                      Time elapsed: 00:59:31
                               ETA: 00:16:20

################################################################################
                     [1m Learning iteration 1570/2000 [0m                     

                       Computation: 43342 steps/s (collection: 2.163s, learning 0.105s)
             Mean action noise std: 3.03
          Mean value_function loss: 124.0473
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 55.1063
                       Mean reward: 877.34
               Mean episode length: 234.69
    Episode_Reward/reaching_object: 1.3174
     Episode_Reward/lifting_object: 172.5109
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 2.27s
                      Time elapsed: 00:59:33
                               ETA: 00:16:18

################################################################################
                     [1m Learning iteration 1571/2000 [0m                     

                       Computation: 43941 steps/s (collection: 2.121s, learning 0.116s)
             Mean action noise std: 3.03
          Mean value_function loss: 147.0416
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.1116
                       Mean reward: 863.24
               Mean episode length: 231.72
    Episode_Reward/reaching_object: 1.3254
     Episode_Reward/lifting_object: 173.5610
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 2.24s
                      Time elapsed: 00:59:35
                               ETA: 00:16:15

################################################################################
                     [1m Learning iteration 1572/2000 [0m                     

                       Computation: 44995 steps/s (collection: 2.089s, learning 0.096s)
             Mean action noise std: 3.04
          Mean value_function loss: 117.0785
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 55.1174
                       Mean reward: 881.59
               Mean episode length: 234.17
    Episode_Reward/reaching_object: 1.3143
     Episode_Reward/lifting_object: 171.5300
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 2.18s
                      Time elapsed: 00:59:37
                               ETA: 00:16:13

################################################################################
                     [1m Learning iteration 1573/2000 [0m                     

                       Computation: 44156 steps/s (collection: 2.086s, learning 0.141s)
             Mean action noise std: 3.04
          Mean value_function loss: 149.2299
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 55.1211
                       Mean reward: 854.65
               Mean episode length: 228.43
    Episode_Reward/reaching_object: 1.2995
     Episode_Reward/lifting_object: 169.5293
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 2.23s
                      Time elapsed: 00:59:39
                               ETA: 00:16:11

################################################################################
                     [1m Learning iteration 1574/2000 [0m                     

                       Computation: 43782 steps/s (collection: 2.136s, learning 0.109s)
             Mean action noise std: 3.04
          Mean value_function loss: 118.8563
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 55.1244
                       Mean reward: 868.59
               Mean episode length: 232.30
    Episode_Reward/reaching_object: 1.3106
     Episode_Reward/lifting_object: 170.4216
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 2.25s
                      Time elapsed: 00:59:42
                               ETA: 00:16:08

################################################################################
                     [1m Learning iteration 1575/2000 [0m                     

                       Computation: 44724 steps/s (collection: 2.094s, learning 0.104s)
             Mean action noise std: 3.04
          Mean value_function loss: 117.0196
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 55.1310
                       Mean reward: 887.34
               Mean episode length: 235.04
    Episode_Reward/reaching_object: 1.3414
     Episode_Reward/lifting_object: 174.9398
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 2.20s
                      Time elapsed: 00:59:44
                               ETA: 00:16:06

################################################################################
                     [1m Learning iteration 1576/2000 [0m                     

                       Computation: 44209 steps/s (collection: 2.116s, learning 0.108s)
             Mean action noise std: 3.04
          Mean value_function loss: 128.3431
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.1385
                       Mean reward: 882.47
               Mean episode length: 236.42
    Episode_Reward/reaching_object: 1.3253
     Episode_Reward/lifting_object: 173.0232
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 2.22s
                      Time elapsed: 00:59:46
                               ETA: 00:16:04

################################################################################
                     [1m Learning iteration 1577/2000 [0m                     

                       Computation: 44432 steps/s (collection: 2.115s, learning 0.097s)
             Mean action noise std: 3.04
          Mean value_function loss: 122.9849
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.1507
                       Mean reward: 891.10
               Mean episode length: 236.71
    Episode_Reward/reaching_object: 1.3398
     Episode_Reward/lifting_object: 175.1509
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 2.21s
                      Time elapsed: 00:59:48
                               ETA: 00:16:02

################################################################################
                     [1m Learning iteration 1578/2000 [0m                     

                       Computation: 44743 steps/s (collection: 2.106s, learning 0.092s)
             Mean action noise std: 3.04
          Mean value_function loss: 134.9203
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 55.1626
                       Mean reward: 858.66
               Mean episode length: 231.13
    Episode_Reward/reaching_object: 1.3059
     Episode_Reward/lifting_object: 169.7204
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 2.20s
                      Time elapsed: 00:59:51
                               ETA: 00:15:59

################################################################################
                     [1m Learning iteration 1579/2000 [0m                     

                       Computation: 44007 steps/s (collection: 2.137s, learning 0.097s)
             Mean action noise std: 3.04
          Mean value_function loss: 118.4079
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 55.1695
                       Mean reward: 835.69
               Mean episode length: 225.07
    Episode_Reward/reaching_object: 1.3271
     Episode_Reward/lifting_object: 173.4393
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 2.23s
                      Time elapsed: 00:59:53
                               ETA: 00:15:57

################################################################################
                     [1m Learning iteration 1580/2000 [0m                     

                       Computation: 43285 steps/s (collection: 2.172s, learning 0.099s)
             Mean action noise std: 3.04
          Mean value_function loss: 113.8963
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.1744
                       Mean reward: 863.20
               Mean episode length: 231.52
    Episode_Reward/reaching_object: 1.3386
     Episode_Reward/lifting_object: 174.4581
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 2.27s
                      Time elapsed: 00:59:55
                               ETA: 00:15:55

################################################################################
                     [1m Learning iteration 1581/2000 [0m                     

                       Computation: 43361 steps/s (collection: 2.162s, learning 0.105s)
             Mean action noise std: 3.05
          Mean value_function loss: 159.2638
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 55.1837
                       Mean reward: 880.01
               Mean episode length: 235.95
    Episode_Reward/reaching_object: 1.3298
     Episode_Reward/lifting_object: 174.2243
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 155516928
                    Iteration time: 2.27s
                      Time elapsed: 00:59:57
                               ETA: 00:15:52

################################################################################
                     [1m Learning iteration 1582/2000 [0m                     

                       Computation: 42122 steps/s (collection: 2.181s, learning 0.153s)
             Mean action noise std: 3.05
          Mean value_function loss: 130.1864
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.1954
                       Mean reward: 856.78
               Mean episode length: 229.36
    Episode_Reward/reaching_object: 1.3108
     Episode_Reward/lifting_object: 171.4023
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 155615232
                    Iteration time: 2.33s
                      Time elapsed: 01:00:00
                               ETA: 00:15:50

################################################################################
                     [1m Learning iteration 1583/2000 [0m                     

                       Computation: 43920 steps/s (collection: 2.125s, learning 0.113s)
             Mean action noise std: 3.05
          Mean value_function loss: 129.2144
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 55.2049
                       Mean reward: 902.14
               Mean episode length: 239.94
    Episode_Reward/reaching_object: 1.3273
     Episode_Reward/lifting_object: 173.3479
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 155713536
                    Iteration time: 2.24s
                      Time elapsed: 01:00:02
                               ETA: 00:15:48

################################################################################
                     [1m Learning iteration 1584/2000 [0m                     

                       Computation: 44055 steps/s (collection: 2.118s, learning 0.114s)
             Mean action noise std: 3.05
          Mean value_function loss: 124.6807
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 55.2208
                       Mean reward: 882.28
               Mean episode length: 235.84
    Episode_Reward/reaching_object: 1.2999
     Episode_Reward/lifting_object: 170.0895
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 155811840
                    Iteration time: 2.23s
                      Time elapsed: 01:00:04
                               ETA: 00:15:46

################################################################################
                     [1m Learning iteration 1585/2000 [0m                     

                       Computation: 44671 steps/s (collection: 2.108s, learning 0.093s)
             Mean action noise std: 3.05
          Mean value_function loss: 142.4141
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.2290
                       Mean reward: 877.99
               Mean episode length: 235.20
    Episode_Reward/reaching_object: 1.3179
     Episode_Reward/lifting_object: 171.6738
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 155910144
                    Iteration time: 2.20s
                      Time elapsed: 01:00:06
                               ETA: 00:15:43

################################################################################
                     [1m Learning iteration 1586/2000 [0m                     

                       Computation: 42929 steps/s (collection: 2.143s, learning 0.147s)
             Mean action noise std: 3.05
          Mean value_function loss: 114.8906
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 55.2373
                       Mean reward: 868.83
               Mean episode length: 232.64
    Episode_Reward/reaching_object: 1.3031
     Episode_Reward/lifting_object: 169.5788
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 156008448
                    Iteration time: 2.29s
                      Time elapsed: 01:00:09
                               ETA: 00:15:41

################################################################################
                     [1m Learning iteration 1587/2000 [0m                     

                       Computation: 43559 steps/s (collection: 2.139s, learning 0.118s)
             Mean action noise std: 3.06
          Mean value_function loss: 117.1630
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.2520
                       Mean reward: 880.73
               Mean episode length: 235.13
    Episode_Reward/reaching_object: 1.3232
     Episode_Reward/lifting_object: 172.5588
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 156106752
                    Iteration time: 2.26s
                      Time elapsed: 01:00:11
                               ETA: 00:15:39

################################################################################
                     [1m Learning iteration 1588/2000 [0m                     

                       Computation: 43756 steps/s (collection: 2.129s, learning 0.118s)
             Mean action noise std: 3.06
          Mean value_function loss: 137.3722
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.2664
                       Mean reward: 854.74
               Mean episode length: 228.52
    Episode_Reward/reaching_object: 1.3069
     Episode_Reward/lifting_object: 170.5384
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 156205056
                    Iteration time: 2.25s
                      Time elapsed: 01:00:13
                               ETA: 00:15:36

################################################################################
                     [1m Learning iteration 1589/2000 [0m                     

                       Computation: 41899 steps/s (collection: 2.246s, learning 0.100s)
             Mean action noise std: 3.06
          Mean value_function loss: 133.7094
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 55.2749
                       Mean reward: 867.36
               Mean episode length: 231.88
    Episode_Reward/reaching_object: 1.3177
     Episode_Reward/lifting_object: 170.5323
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 156303360
                    Iteration time: 2.35s
                      Time elapsed: 01:00:15
                               ETA: 00:15:34

################################################################################
                     [1m Learning iteration 1590/2000 [0m                     

                       Computation: 41637 steps/s (collection: 2.251s, learning 0.110s)
             Mean action noise std: 3.06
          Mean value_function loss: 108.1302
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.2872
                       Mean reward: 868.16
               Mean episode length: 231.34
    Episode_Reward/reaching_object: 1.3159
     Episode_Reward/lifting_object: 171.0040
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 156401664
                    Iteration time: 2.36s
                      Time elapsed: 01:00:18
                               ETA: 00:15:32

################################################################################
                     [1m Learning iteration 1591/2000 [0m                     

                       Computation: 39854 steps/s (collection: 2.319s, learning 0.148s)
             Mean action noise std: 3.06
          Mean value_function loss: 91.4864
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.2975
                       Mean reward: 871.01
               Mean episode length: 232.89
    Episode_Reward/reaching_object: 1.3506
     Episode_Reward/lifting_object: 175.4774
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 156499968
                    Iteration time: 2.47s
                      Time elapsed: 01:00:20
                               ETA: 00:15:30

################################################################################
                     [1m Learning iteration 1592/2000 [0m                     

                       Computation: 41942 steps/s (collection: 2.205s, learning 0.139s)
             Mean action noise std: 3.06
          Mean value_function loss: 94.4087
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 55.3082
                       Mean reward: 915.81
               Mean episode length: 242.68
    Episode_Reward/reaching_object: 1.3634
     Episode_Reward/lifting_object: 177.0726
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 156598272
                    Iteration time: 2.34s
                      Time elapsed: 01:00:23
                               ETA: 00:15:27

################################################################################
                     [1m Learning iteration 1593/2000 [0m                     

                       Computation: 42893 steps/s (collection: 2.196s, learning 0.096s)
             Mean action noise std: 3.06
          Mean value_function loss: 134.5729
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 55.3105
                       Mean reward: 859.08
               Mean episode length: 229.85
    Episode_Reward/reaching_object: 1.3431
     Episode_Reward/lifting_object: 174.2192
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 156696576
                    Iteration time: 2.29s
                      Time elapsed: 01:00:25
                               ETA: 00:15:25

################################################################################
                     [1m Learning iteration 1594/2000 [0m                     

                       Computation: 42222 steps/s (collection: 2.201s, learning 0.128s)
             Mean action noise std: 3.06
          Mean value_function loss: 145.1518
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 55.3174
                       Mean reward: 851.15
               Mean episode length: 227.74
    Episode_Reward/reaching_object: 1.3300
     Episode_Reward/lifting_object: 172.7845
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 156794880
                    Iteration time: 2.33s
                      Time elapsed: 01:00:27
                               ETA: 00:15:23

################################################################################
                     [1m Learning iteration 1595/2000 [0m                     

                       Computation: 41237 steps/s (collection: 2.272s, learning 0.112s)
             Mean action noise std: 3.07
          Mean value_function loss: 93.6146
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 55.3262
                       Mean reward: 889.63
               Mean episode length: 236.65
    Episode_Reward/reaching_object: 1.3369
     Episode_Reward/lifting_object: 172.8161
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 156893184
                    Iteration time: 2.38s
                      Time elapsed: 01:00:30
                               ETA: 00:15:21

################################################################################
                     [1m Learning iteration 1596/2000 [0m                     

                       Computation: 41352 steps/s (collection: 2.250s, learning 0.127s)
             Mean action noise std: 3.07
          Mean value_function loss: 103.0724
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 55.3342
                       Mean reward: 898.46
               Mean episode length: 239.93
    Episode_Reward/reaching_object: 1.3570
     Episode_Reward/lifting_object: 175.8718
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 156991488
                    Iteration time: 2.38s
                      Time elapsed: 01:00:32
                               ETA: 00:15:18

################################################################################
                     [1m Learning iteration 1597/2000 [0m                     

                       Computation: 43125 steps/s (collection: 2.159s, learning 0.121s)
             Mean action noise std: 3.07
          Mean value_function loss: 97.4676
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.3449
                       Mean reward: 850.40
               Mean episode length: 226.36
    Episode_Reward/reaching_object: 1.3541
     Episode_Reward/lifting_object: 176.2619
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 157089792
                    Iteration time: 2.28s
                      Time elapsed: 01:00:34
                               ETA: 00:15:16

################################################################################
                     [1m Learning iteration 1598/2000 [0m                     

                       Computation: 43255 steps/s (collection: 2.154s, learning 0.119s)
             Mean action noise std: 3.07
          Mean value_function loss: 137.4480
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.3568
                       Mean reward: 889.18
               Mean episode length: 236.37
    Episode_Reward/reaching_object: 1.3595
     Episode_Reward/lifting_object: 177.2939
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 157188096
                    Iteration time: 2.27s
                      Time elapsed: 01:00:37
                               ETA: 00:15:14

################################################################################
                     [1m Learning iteration 1599/2000 [0m                     

                       Computation: 43164 steps/s (collection: 2.173s, learning 0.105s)
             Mean action noise std: 3.07
          Mean value_function loss: 110.1945
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.3707
                       Mean reward: 859.26
               Mean episode length: 230.33
    Episode_Reward/reaching_object: 1.3326
     Episode_Reward/lifting_object: 172.7850
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 157286400
                    Iteration time: 2.28s
                      Time elapsed: 01:00:39
                               ETA: 00:15:12

################################################################################
                     [1m Learning iteration 1600/2000 [0m                     

                       Computation: 41915 steps/s (collection: 2.223s, learning 0.122s)
             Mean action noise std: 3.07
          Mean value_function loss: 116.1966
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 55.3805
                       Mean reward: 876.48
               Mean episode length: 233.73
    Episode_Reward/reaching_object: 1.3424
     Episode_Reward/lifting_object: 174.5542
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 157384704
                    Iteration time: 2.35s
                      Time elapsed: 01:00:41
                               ETA: 00:15:09

################################################################################
                     [1m Learning iteration 1601/2000 [0m                     

                       Computation: 39692 steps/s (collection: 2.306s, learning 0.171s)
             Mean action noise std: 3.07
          Mean value_function loss: 103.9887
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 55.3916
                       Mean reward: 826.07
               Mean episode length: 222.84
    Episode_Reward/reaching_object: 1.3047
     Episode_Reward/lifting_object: 169.0060
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 157483008
                    Iteration time: 2.48s
                      Time elapsed: 01:00:44
                               ETA: 00:15:07

################################################################################
                     [1m Learning iteration 1602/2000 [0m                     

                       Computation: 42115 steps/s (collection: 2.203s, learning 0.131s)
             Mean action noise std: 3.08
          Mean value_function loss: 103.1771
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.4027
                       Mean reward: 894.45
               Mean episode length: 237.36
    Episode_Reward/reaching_object: 1.3557
     Episode_Reward/lifting_object: 176.2693
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 157581312
                    Iteration time: 2.33s
                      Time elapsed: 01:00:46
                               ETA: 00:15:05

################################################################################
                     [1m Learning iteration 1603/2000 [0m                     

                       Computation: 43459 steps/s (collection: 2.160s, learning 0.102s)
             Mean action noise std: 3.08
          Mean value_function loss: 121.1581
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.4137
                       Mean reward: 874.38
               Mean episode length: 233.33
    Episode_Reward/reaching_object: 1.3446
     Episode_Reward/lifting_object: 173.2092
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 157679616
                    Iteration time: 2.26s
                      Time elapsed: 01:00:48
                               ETA: 00:15:03

################################################################################
                     [1m Learning iteration 1604/2000 [0m                     

                       Computation: 43766 steps/s (collection: 2.144s, learning 0.102s)
             Mean action noise std: 3.08
          Mean value_function loss: 117.4057
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 55.4241
                       Mean reward: 873.00
               Mean episode length: 231.62
    Episode_Reward/reaching_object: 1.3338
     Episode_Reward/lifting_object: 174.0149
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 157777920
                    Iteration time: 2.25s
                      Time elapsed: 01:00:51
                               ETA: 00:15:00

################################################################################
                     [1m Learning iteration 1605/2000 [0m                     

                       Computation: 41780 steps/s (collection: 2.165s, learning 0.188s)
             Mean action noise std: 3.08
          Mean value_function loss: 144.3717
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.4362
                       Mean reward: 857.96
               Mean episode length: 228.14
    Episode_Reward/reaching_object: 1.3137
     Episode_Reward/lifting_object: 170.4460
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 157876224
                    Iteration time: 2.35s
                      Time elapsed: 01:00:53
                               ETA: 00:14:58

################################################################################
                     [1m Learning iteration 1606/2000 [0m                     

                       Computation: 42340 steps/s (collection: 2.219s, learning 0.103s)
             Mean action noise std: 3.08
          Mean value_function loss: 121.6446
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.4460
                       Mean reward: 883.44
               Mean episode length: 234.63
    Episode_Reward/reaching_object: 1.3221
     Episode_Reward/lifting_object: 172.0927
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 157974528
                    Iteration time: 2.32s
                      Time elapsed: 01:00:55
                               ETA: 00:14:56

################################################################################
                     [1m Learning iteration 1607/2000 [0m                     

                       Computation: 42699 steps/s (collection: 2.201s, learning 0.102s)
             Mean action noise std: 3.08
          Mean value_function loss: 141.0696
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 55.4553
                       Mean reward: 863.93
               Mean episode length: 229.72
    Episode_Reward/reaching_object: 1.3264
     Episode_Reward/lifting_object: 172.3932
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 158072832
                    Iteration time: 2.30s
                      Time elapsed: 01:00:57
                               ETA: 00:14:54

################################################################################
                     [1m Learning iteration 1608/2000 [0m                     

                       Computation: 42870 steps/s (collection: 2.148s, learning 0.145s)
             Mean action noise std: 3.09
          Mean value_function loss: 124.8466
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.4654
                       Mean reward: 873.66
               Mean episode length: 232.13
    Episode_Reward/reaching_object: 1.3151
     Episode_Reward/lifting_object: 171.1855
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 158171136
                    Iteration time: 2.29s
                      Time elapsed: 01:01:00
                               ETA: 00:14:51

################################################################################
                     [1m Learning iteration 1609/2000 [0m                     

                       Computation: 42425 steps/s (collection: 2.200s, learning 0.118s)
             Mean action noise std: 3.09
          Mean value_function loss: 111.2275
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.4719
                       Mean reward: 872.56
               Mean episode length: 232.01
    Episode_Reward/reaching_object: 1.3258
     Episode_Reward/lifting_object: 173.0876
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 158269440
                    Iteration time: 2.32s
                      Time elapsed: 01:01:02
                               ETA: 00:14:49

################################################################################
                     [1m Learning iteration 1610/2000 [0m                     

                       Computation: 41794 steps/s (collection: 2.259s, learning 0.093s)
             Mean action noise std: 3.09
          Mean value_function loss: 123.1596
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.4761
                       Mean reward: 864.82
               Mean episode length: 230.79
    Episode_Reward/reaching_object: 1.3396
     Episode_Reward/lifting_object: 174.9872
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 158367744
                    Iteration time: 2.35s
                      Time elapsed: 01:01:04
                               ETA: 00:14:47

################################################################################
                     [1m Learning iteration 1611/2000 [0m                     

                       Computation: 41389 steps/s (collection: 2.249s, learning 0.126s)
             Mean action noise std: 3.09
          Mean value_function loss: 116.4129
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.4808
                       Mean reward: 878.00
               Mean episode length: 232.60
    Episode_Reward/reaching_object: 1.3442
     Episode_Reward/lifting_object: 175.2118
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 158466048
                    Iteration time: 2.38s
                      Time elapsed: 01:01:07
                               ETA: 00:14:44

################################################################################
                     [1m Learning iteration 1612/2000 [0m                     

                       Computation: 43471 steps/s (collection: 2.164s, learning 0.097s)
             Mean action noise std: 3.09
          Mean value_function loss: 99.1749
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 55.4916
                       Mean reward: 896.29
               Mean episode length: 237.89
    Episode_Reward/reaching_object: 1.3396
     Episode_Reward/lifting_object: 174.8408
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 158564352
                    Iteration time: 2.26s
                      Time elapsed: 01:01:09
                               ETA: 00:14:42

################################################################################
                     [1m Learning iteration 1613/2000 [0m                     

                       Computation: 42785 steps/s (collection: 2.193s, learning 0.105s)
             Mean action noise std: 3.09
          Mean value_function loss: 119.7608
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 55.5066
                       Mean reward: 863.92
               Mean episode length: 230.21
    Episode_Reward/reaching_object: 1.3520
     Episode_Reward/lifting_object: 176.1219
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 158662656
                    Iteration time: 2.30s
                      Time elapsed: 01:01:11
                               ETA: 00:14:40

################################################################################
                     [1m Learning iteration 1614/2000 [0m                     

                       Computation: 42453 steps/s (collection: 2.195s, learning 0.121s)
             Mean action noise std: 3.09
          Mean value_function loss: 115.1764
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.5156
                       Mean reward: 836.06
               Mean episode length: 223.68
    Episode_Reward/reaching_object: 1.3319
     Episode_Reward/lifting_object: 173.9471
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 158760960
                    Iteration time: 2.32s
                      Time elapsed: 01:01:14
                               ETA: 00:14:38

################################################################################
                     [1m Learning iteration 1615/2000 [0m                     

                       Computation: 41763 steps/s (collection: 2.257s, learning 0.097s)
             Mean action noise std: 3.09
          Mean value_function loss: 138.2333
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 55.5215
                       Mean reward: 866.25
               Mean episode length: 230.32
    Episode_Reward/reaching_object: 1.3017
     Episode_Reward/lifting_object: 169.8487
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 158859264
                    Iteration time: 2.35s
                      Time elapsed: 01:01:16
                               ETA: 00:14:35

################################################################################
                     [1m Learning iteration 1616/2000 [0m                     

                       Computation: 43339 steps/s (collection: 2.148s, learning 0.120s)
             Mean action noise std: 3.09
          Mean value_function loss: 116.8654
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.5254
                       Mean reward: 875.72
               Mean episode length: 232.99
    Episode_Reward/reaching_object: 1.3229
     Episode_Reward/lifting_object: 172.3964
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 158957568
                    Iteration time: 2.27s
                      Time elapsed: 01:01:18
                               ETA: 00:14:33

################################################################################
                     [1m Learning iteration 1617/2000 [0m                     

                       Computation: 41118 steps/s (collection: 2.220s, learning 0.171s)
             Mean action noise std: 3.09
          Mean value_function loss: 148.2248
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 55.5327
                       Mean reward: 849.18
               Mean episode length: 226.90
    Episode_Reward/reaching_object: 1.3328
     Episode_Reward/lifting_object: 173.3448
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 159055872
                    Iteration time: 2.39s
                      Time elapsed: 01:01:21
                               ETA: 00:14:31

################################################################################
                     [1m Learning iteration 1618/2000 [0m                     

                       Computation: 40593 steps/s (collection: 2.268s, learning 0.154s)
             Mean action noise std: 3.10
          Mean value_function loss: 105.7867
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 55.5375
                       Mean reward: 853.37
               Mean episode length: 227.30
    Episode_Reward/reaching_object: 1.3190
     Episode_Reward/lifting_object: 172.2590
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 159154176
                    Iteration time: 2.42s
                      Time elapsed: 01:01:23
                               ETA: 00:14:29

################################################################################
                     [1m Learning iteration 1619/2000 [0m                     

                       Computation: 41877 steps/s (collection: 2.249s, learning 0.099s)
             Mean action noise std: 3.10
          Mean value_function loss: 106.1364
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.5430
                       Mean reward: 887.68
               Mean episode length: 235.66
    Episode_Reward/reaching_object: 1.3432
     Episode_Reward/lifting_object: 176.0442
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 159252480
                    Iteration time: 2.35s
                      Time elapsed: 01:01:25
                               ETA: 00:14:26

################################################################################
                     [1m Learning iteration 1620/2000 [0m                     

                       Computation: 42862 steps/s (collection: 2.163s, learning 0.130s)
             Mean action noise std: 3.10
          Mean value_function loss: 131.5088
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 55.5532
                       Mean reward: 871.88
               Mean episode length: 231.78
    Episode_Reward/reaching_object: 1.3066
     Episode_Reward/lifting_object: 170.4889
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 159350784
                    Iteration time: 2.29s
                      Time elapsed: 01:01:28
                               ETA: 00:14:24

################################################################################
                     [1m Learning iteration 1621/2000 [0m                     

                       Computation: 42088 steps/s (collection: 2.206s, learning 0.130s)
             Mean action noise std: 3.10
          Mean value_function loss: 113.9229
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 55.5619
                       Mean reward: 859.39
               Mean episode length: 229.94
    Episode_Reward/reaching_object: 1.3283
     Episode_Reward/lifting_object: 173.5304
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 159449088
                    Iteration time: 2.34s
                      Time elapsed: 01:01:30
                               ETA: 00:14:22

################################################################################
                     [1m Learning iteration 1622/2000 [0m                     

                       Computation: 43951 steps/s (collection: 2.138s, learning 0.099s)
             Mean action noise std: 3.10
          Mean value_function loss: 119.1030
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.5740
                       Mean reward: 881.61
               Mean episode length: 234.48
    Episode_Reward/reaching_object: 1.3117
     Episode_Reward/lifting_object: 171.0972
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 159547392
                    Iteration time: 2.24s
                      Time elapsed: 01:01:32
                               ETA: 00:14:20

################################################################################
                     [1m Learning iteration 1623/2000 [0m                     

                       Computation: 42876 steps/s (collection: 2.182s, learning 0.110s)
             Mean action noise std: 3.10
          Mean value_function loss: 112.7279
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 55.5910
                       Mean reward: 908.92
               Mean episode length: 240.44
    Episode_Reward/reaching_object: 1.3365
     Episode_Reward/lifting_object: 174.4772
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 159645696
                    Iteration time: 2.29s
                      Time elapsed: 01:01:35
                               ETA: 00:14:17

################################################################################
                     [1m Learning iteration 1624/2000 [0m                     

                       Computation: 39489 steps/s (collection: 2.302s, learning 0.188s)
             Mean action noise std: 3.11
          Mean value_function loss: 100.7619
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.6035
                       Mean reward: 895.93
               Mean episode length: 237.68
    Episode_Reward/reaching_object: 1.3485
     Episode_Reward/lifting_object: 176.6882
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 159744000
                    Iteration time: 2.49s
                      Time elapsed: 01:01:37
                               ETA: 00:14:15

################################################################################
                     [1m Learning iteration 1625/2000 [0m                     

                       Computation: 41194 steps/s (collection: 2.186s, learning 0.201s)
             Mean action noise std: 3.11
          Mean value_function loss: 114.2213
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.6137
                       Mean reward: 882.32
               Mean episode length: 236.74
    Episode_Reward/reaching_object: 1.3305
     Episode_Reward/lifting_object: 173.0772
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 159842304
                    Iteration time: 2.39s
                      Time elapsed: 01:01:40
                               ETA: 00:14:13

################################################################################
                     [1m Learning iteration 1626/2000 [0m                     

                       Computation: 42777 steps/s (collection: 2.203s, learning 0.095s)
             Mean action noise std: 3.11
          Mean value_function loss: 131.6971
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.6227
                       Mean reward: 854.54
               Mean episode length: 229.04
    Episode_Reward/reaching_object: 1.3258
     Episode_Reward/lifting_object: 173.3919
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 159940608
                    Iteration time: 2.30s
                      Time elapsed: 01:01:42
                               ETA: 00:14:11

################################################################################
                     [1m Learning iteration 1627/2000 [0m                     

                       Computation: 43690 steps/s (collection: 2.135s, learning 0.115s)
             Mean action noise std: 3.11
          Mean value_function loss: 102.2909
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.6315
                       Mean reward: 894.60
               Mean episode length: 237.07
    Episode_Reward/reaching_object: 1.3431
     Episode_Reward/lifting_object: 174.6764
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 160038912
                    Iteration time: 2.25s
                      Time elapsed: 01:01:44
                               ETA: 00:14:08

################################################################################
                     [1m Learning iteration 1628/2000 [0m                     

                       Computation: 42827 steps/s (collection: 2.199s, learning 0.096s)
             Mean action noise std: 3.11
          Mean value_function loss: 116.6478
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.6392
                       Mean reward: 892.88
               Mean episode length: 237.17
    Episode_Reward/reaching_object: 1.3375
     Episode_Reward/lifting_object: 175.4764
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 160137216
                    Iteration time: 2.30s
                      Time elapsed: 01:01:46
                               ETA: 00:14:06

################################################################################
                     [1m Learning iteration 1629/2000 [0m                     

                       Computation: 40861 steps/s (collection: 2.261s, learning 0.145s)
             Mean action noise std: 3.11
          Mean value_function loss: 150.3071
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.6459
                       Mean reward: 798.81
               Mean episode length: 218.01
    Episode_Reward/reaching_object: 1.3099
     Episode_Reward/lifting_object: 169.7996
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 160235520
                    Iteration time: 2.41s
                      Time elapsed: 01:01:49
                               ETA: 00:14:04

################################################################################
                     [1m Learning iteration 1630/2000 [0m                     

                       Computation: 42554 steps/s (collection: 2.178s, learning 0.132s)
             Mean action noise std: 3.11
          Mean value_function loss: 122.0340
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 55.6516
                       Mean reward: 875.36
               Mean episode length: 233.30
    Episode_Reward/reaching_object: 1.3248
     Episode_Reward/lifting_object: 172.3014
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 160333824
                    Iteration time: 2.31s
                      Time elapsed: 01:01:51
                               ETA: 00:14:01

################################################################################
                     [1m Learning iteration 1631/2000 [0m                     

                       Computation: 43531 steps/s (collection: 2.157s, learning 0.102s)
             Mean action noise std: 3.11
          Mean value_function loss: 127.1216
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 55.6590
                       Mean reward: 836.81
               Mean episode length: 223.65
    Episode_Reward/reaching_object: 1.3094
     Episode_Reward/lifting_object: 170.6985
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 160432128
                    Iteration time: 2.26s
                      Time elapsed: 01:01:53
                               ETA: 00:13:59

################################################################################
                     [1m Learning iteration 1632/2000 [0m                     

                       Computation: 43408 steps/s (collection: 2.172s, learning 0.093s)
             Mean action noise std: 3.11
          Mean value_function loss: 131.2185
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 55.6627
                       Mean reward: 879.31
               Mean episode length: 233.96
    Episode_Reward/reaching_object: 1.3272
     Episode_Reward/lifting_object: 171.5443
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 160530432
                    Iteration time: 2.26s
                      Time elapsed: 01:01:56
                               ETA: 00:13:57

################################################################################
                     [1m Learning iteration 1633/2000 [0m                     

                       Computation: 43420 steps/s (collection: 2.155s, learning 0.109s)
             Mean action noise std: 3.12
          Mean value_function loss: 129.0619
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 55.6716
                       Mean reward: 848.16
               Mean episode length: 227.65
    Episode_Reward/reaching_object: 1.3166
     Episode_Reward/lifting_object: 170.4478
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 160628736
                    Iteration time: 2.26s
                      Time elapsed: 01:01:58
                               ETA: 00:13:55

################################################################################
                     [1m Learning iteration 1634/2000 [0m                     

                       Computation: 43080 steps/s (collection: 2.173s, learning 0.109s)
             Mean action noise std: 3.12
          Mean value_function loss: 120.3187
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.6848
                       Mean reward: 874.45
               Mean episode length: 232.42
    Episode_Reward/reaching_object: 1.3130
     Episode_Reward/lifting_object: 170.8783
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 160727040
                    Iteration time: 2.28s
                      Time elapsed: 01:02:00
                               ETA: 00:13:52

################################################################################
                     [1m Learning iteration 1635/2000 [0m                     

                       Computation: 42261 steps/s (collection: 2.210s, learning 0.117s)
             Mean action noise std: 3.12
          Mean value_function loss: 128.2985
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.6985
                       Mean reward: 841.58
               Mean episode length: 225.45
    Episode_Reward/reaching_object: 1.3210
     Episode_Reward/lifting_object: 170.9873
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 160825344
                    Iteration time: 2.33s
                      Time elapsed: 01:02:02
                               ETA: 00:13:50

################################################################################
                     [1m Learning iteration 1636/2000 [0m                     

                       Computation: 40884 steps/s (collection: 2.299s, learning 0.105s)
             Mean action noise std: 3.12
          Mean value_function loss: 99.1130
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 55.7140
                       Mean reward: 896.55
               Mean episode length: 236.83
    Episode_Reward/reaching_object: 1.3494
     Episode_Reward/lifting_object: 174.8908
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 160923648
                    Iteration time: 2.40s
                      Time elapsed: 01:02:05
                               ETA: 00:13:48

################################################################################
                     [1m Learning iteration 1637/2000 [0m                     

                       Computation: 40574 steps/s (collection: 2.259s, learning 0.164s)
             Mean action noise std: 3.12
          Mean value_function loss: 130.0116
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 55.7177
                       Mean reward: 800.48
               Mean episode length: 213.99
    Episode_Reward/reaching_object: 1.3034
     Episode_Reward/lifting_object: 169.1825
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 161021952
                    Iteration time: 2.42s
                      Time elapsed: 01:02:07
                               ETA: 00:13:46

################################################################################
                     [1m Learning iteration 1638/2000 [0m                     

                       Computation: 42655 steps/s (collection: 2.172s, learning 0.133s)
             Mean action noise std: 3.12
          Mean value_function loss: 108.8370
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 55.7201
                       Mean reward: 897.95
               Mean episode length: 237.57
    Episode_Reward/reaching_object: 1.3459
     Episode_Reward/lifting_object: 173.8479
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 161120256
                    Iteration time: 2.30s
                      Time elapsed: 01:02:10
                               ETA: 00:13:43

################################################################################
                     [1m Learning iteration 1639/2000 [0m                     

                       Computation: 44188 steps/s (collection: 2.128s, learning 0.097s)
             Mean action noise std: 3.12
          Mean value_function loss: 132.0687
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.7229
                       Mean reward: 904.66
               Mean episode length: 240.10
    Episode_Reward/reaching_object: 1.3598
     Episode_Reward/lifting_object: 176.1179
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 161218560
                    Iteration time: 2.22s
                      Time elapsed: 01:02:12
                               ETA: 00:13:41

################################################################################
                     [1m Learning iteration 1640/2000 [0m                     

                       Computation: 42827 steps/s (collection: 2.139s, learning 0.156s)
             Mean action noise std: 3.12
          Mean value_function loss: 106.2408
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 55.7312
                       Mean reward: 895.11
               Mean episode length: 238.57
    Episode_Reward/reaching_object: 1.3314
     Episode_Reward/lifting_object: 172.6802
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 161316864
                    Iteration time: 2.30s
                      Time elapsed: 01:02:14
                               ETA: 00:13:39

################################################################################
                     [1m Learning iteration 1641/2000 [0m                     

                       Computation: 42523 steps/s (collection: 2.196s, learning 0.116s)
             Mean action noise std: 3.13
          Mean value_function loss: 103.9507
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.7494
                       Mean reward: 899.52
               Mean episode length: 240.18
    Episode_Reward/reaching_object: 1.3449
     Episode_Reward/lifting_object: 174.5692
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 161415168
                    Iteration time: 2.31s
                      Time elapsed: 01:02:16
                               ETA: 00:13:37

################################################################################
                     [1m Learning iteration 1642/2000 [0m                     

                       Computation: 43272 steps/s (collection: 2.164s, learning 0.108s)
             Mean action noise std: 3.13
          Mean value_function loss: 129.5232
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 55.7612
                       Mean reward: 847.08
               Mean episode length: 226.77
    Episode_Reward/reaching_object: 1.3464
     Episode_Reward/lifting_object: 174.6947
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 161513472
                    Iteration time: 2.27s
                      Time elapsed: 01:02:19
                               ETA: 00:13:34

################################################################################
                     [1m Learning iteration 1643/2000 [0m                     

                       Computation: 43279 steps/s (collection: 2.167s, learning 0.104s)
             Mean action noise std: 3.13
          Mean value_function loss: 139.0560
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 55.7654
                       Mean reward: 855.60
               Mean episode length: 228.53
    Episode_Reward/reaching_object: 1.3195
     Episode_Reward/lifting_object: 170.8197
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 161611776
                    Iteration time: 2.27s
                      Time elapsed: 01:02:21
                               ETA: 00:13:32

################################################################################
                     [1m Learning iteration 1644/2000 [0m                     

                       Computation: 40601 steps/s (collection: 2.274s, learning 0.148s)
             Mean action noise std: 3.13
          Mean value_function loss: 99.3221
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.7729
                       Mean reward: 840.53
               Mean episode length: 224.85
    Episode_Reward/reaching_object: 1.3526
     Episode_Reward/lifting_object: 175.4866
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 161710080
                    Iteration time: 2.42s
                      Time elapsed: 01:02:23
                               ETA: 00:13:30

################################################################################
                     [1m Learning iteration 1645/2000 [0m                     

                       Computation: 42266 steps/s (collection: 2.219s, learning 0.107s)
             Mean action noise std: 3.13
          Mean value_function loss: 123.8916
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 55.7819
                       Mean reward: 878.48
               Mean episode length: 233.72
    Episode_Reward/reaching_object: 1.3288
     Episode_Reward/lifting_object: 172.5925
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 161808384
                    Iteration time: 2.33s
                      Time elapsed: 01:02:26
                               ETA: 00:13:27

################################################################################
                     [1m Learning iteration 1646/2000 [0m                     

                       Computation: 40349 steps/s (collection: 2.272s, learning 0.164s)
             Mean action noise std: 3.13
          Mean value_function loss: 141.8307
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.7946
                       Mean reward: 811.49
               Mean episode length: 219.36
    Episode_Reward/reaching_object: 1.3335
     Episode_Reward/lifting_object: 172.4844
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 161906688
                    Iteration time: 2.44s
                      Time elapsed: 01:02:28
                               ETA: 00:13:25

################################################################################
                     [1m Learning iteration 1647/2000 [0m                     

                       Computation: 43950 steps/s (collection: 2.109s, learning 0.128s)
             Mean action noise std: 3.13
          Mean value_function loss: 96.0520
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.8057
                       Mean reward: 848.71
               Mean episode length: 226.99
    Episode_Reward/reaching_object: 1.3392
     Episode_Reward/lifting_object: 173.7397
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 162004992
                    Iteration time: 2.24s
                      Time elapsed: 01:02:30
                               ETA: 00:13:23

################################################################################
                     [1m Learning iteration 1648/2000 [0m                     

                       Computation: 43860 steps/s (collection: 2.140s, learning 0.101s)
             Mean action noise std: 3.14
          Mean value_function loss: 99.8852
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.8158
                       Mean reward: 862.34
               Mean episode length: 229.70
    Episode_Reward/reaching_object: 1.3474
     Episode_Reward/lifting_object: 174.2812
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 162103296
                    Iteration time: 2.24s
                      Time elapsed: 01:02:33
                               ETA: 00:13:21

################################################################################
                     [1m Learning iteration 1649/2000 [0m                     

                       Computation: 43698 steps/s (collection: 2.153s, learning 0.096s)
             Mean action noise std: 3.14
          Mean value_function loss: 97.9051
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 55.8306
                       Mean reward: 894.87
               Mean episode length: 238.97
    Episode_Reward/reaching_object: 1.3790
     Episode_Reward/lifting_object: 178.9997
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 162201600
                    Iteration time: 2.25s
                      Time elapsed: 01:02:35
                               ETA: 00:13:18

################################################################################
                     [1m Learning iteration 1650/2000 [0m                     

                       Computation: 43953 steps/s (collection: 2.130s, learning 0.107s)
             Mean action noise std: 3.14
          Mean value_function loss: 118.7279
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 55.8376
                       Mean reward: 886.24
               Mean episode length: 235.55
    Episode_Reward/reaching_object: 1.3317
     Episode_Reward/lifting_object: 172.3525
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 162299904
                    Iteration time: 2.24s
                      Time elapsed: 01:02:37
                               ETA: 00:13:16

################################################################################
                     [1m Learning iteration 1651/2000 [0m                     

                       Computation: 39758 steps/s (collection: 2.325s, learning 0.147s)
             Mean action noise std: 3.14
          Mean value_function loss: 126.1365
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 55.8494
                       Mean reward: 868.57
               Mean episode length: 231.89
    Episode_Reward/reaching_object: 1.3294
     Episode_Reward/lifting_object: 172.2997
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 162398208
                    Iteration time: 2.47s
                      Time elapsed: 01:02:40
                               ETA: 00:13:14

################################################################################
                     [1m Learning iteration 1652/2000 [0m                     

                       Computation: 43949 steps/s (collection: 2.136s, learning 0.101s)
             Mean action noise std: 3.14
          Mean value_function loss: 91.8088
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.8608
                       Mean reward: 846.70
               Mean episode length: 228.26
    Episode_Reward/reaching_object: 1.3382
     Episode_Reward/lifting_object: 172.2013
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 162496512
                    Iteration time: 2.24s
                      Time elapsed: 01:02:42
                               ETA: 00:13:12

################################################################################
                     [1m Learning iteration 1653/2000 [0m                     

                       Computation: 41351 steps/s (collection: 2.199s, learning 0.178s)
             Mean action noise std: 3.14
          Mean value_function loss: 119.6416
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 55.8710
                       Mean reward: 911.12
               Mean episode length: 241.47
    Episode_Reward/reaching_object: 1.3471
     Episode_Reward/lifting_object: 174.8221
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 162594816
                    Iteration time: 2.38s
                      Time elapsed: 01:02:44
                               ETA: 00:13:09

################################################################################
                     [1m Learning iteration 1654/2000 [0m                     

                       Computation: 42648 steps/s (collection: 2.204s, learning 0.101s)
             Mean action noise std: 3.15
          Mean value_function loss: 119.9484
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 55.8776
                       Mean reward: 911.97
               Mean episode length: 242.07
    Episode_Reward/reaching_object: 1.3495
     Episode_Reward/lifting_object: 174.4250
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 162693120
                    Iteration time: 2.30s
                      Time elapsed: 01:02:47
                               ETA: 00:13:07

################################################################################
                     [1m Learning iteration 1655/2000 [0m                     

                       Computation: 43959 steps/s (collection: 2.138s, learning 0.099s)
             Mean action noise std: 3.15
          Mean value_function loss: 103.5307
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 55.8867
                       Mean reward: 908.11
               Mean episode length: 241.07
    Episode_Reward/reaching_object: 1.3399
     Episode_Reward/lifting_object: 172.4112
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 162791424
                    Iteration time: 2.24s
                      Time elapsed: 01:02:49
                               ETA: 00:13:05

################################################################################
                     [1m Learning iteration 1656/2000 [0m                     

                       Computation: 43190 steps/s (collection: 2.173s, learning 0.103s)
             Mean action noise std: 3.15
          Mean value_function loss: 105.4001
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.8965
                       Mean reward: 867.26
               Mean episode length: 232.10
    Episode_Reward/reaching_object: 1.3471
     Episode_Reward/lifting_object: 174.6763
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 162889728
                    Iteration time: 2.28s
                      Time elapsed: 01:02:51
                               ETA: 00:13:02

################################################################################
                     [1m Learning iteration 1657/2000 [0m                     

                       Computation: 41941 steps/s (collection: 2.189s, learning 0.155s)
             Mean action noise std: 3.15
          Mean value_function loss: 111.7882
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.9048
                       Mean reward: 883.50
               Mean episode length: 233.92
    Episode_Reward/reaching_object: 1.3595
     Episode_Reward/lifting_object: 175.5607
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 162988032
                    Iteration time: 2.34s
                      Time elapsed: 01:02:53
                               ETA: 00:13:00

################################################################################
                     [1m Learning iteration 1658/2000 [0m                     

                       Computation: 40805 steps/s (collection: 2.307s, learning 0.103s)
             Mean action noise std: 3.15
          Mean value_function loss: 96.0003
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.9148
                       Mean reward: 902.63
               Mean episode length: 240.34
    Episode_Reward/reaching_object: 1.3610
     Episode_Reward/lifting_object: 176.2159
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 163086336
                    Iteration time: 2.41s
                      Time elapsed: 01:02:56
                               ETA: 00:12:58

################################################################################
                     [1m Learning iteration 1659/2000 [0m                     

                       Computation: 42753 steps/s (collection: 2.135s, learning 0.164s)
             Mean action noise std: 3.15
          Mean value_function loss: 126.9552
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.9243
                       Mean reward: 919.56
               Mean episode length: 244.00
    Episode_Reward/reaching_object: 1.3787
     Episode_Reward/lifting_object: 178.5108
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 163184640
                    Iteration time: 2.30s
                      Time elapsed: 01:02:58
                               ETA: 00:12:56

################################################################################
                     [1m Learning iteration 1660/2000 [0m                     

                       Computation: 42413 steps/s (collection: 2.220s, learning 0.098s)
             Mean action noise std: 3.15
          Mean value_function loss: 141.6117
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.9324
                       Mean reward: 880.92
               Mean episode length: 234.78
    Episode_Reward/reaching_object: 1.3447
     Episode_Reward/lifting_object: 174.0692
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163282944
                    Iteration time: 2.32s
                      Time elapsed: 01:03:00
                               ETA: 00:12:53

################################################################################
                     [1m Learning iteration 1661/2000 [0m                     

                       Computation: 44279 steps/s (collection: 2.121s, learning 0.100s)
             Mean action noise std: 3.15
          Mean value_function loss: 148.2786
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.9397
                       Mean reward: 890.23
               Mean episode length: 237.00
    Episode_Reward/reaching_object: 1.3347
     Episode_Reward/lifting_object: 172.5558
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 163381248
                    Iteration time: 2.22s
                      Time elapsed: 01:03:03
                               ETA: 00:12:51

################################################################################
                     [1m Learning iteration 1662/2000 [0m                     

                       Computation: 44657 steps/s (collection: 2.097s, learning 0.104s)
             Mean action noise std: 3.16
          Mean value_function loss: 93.4752
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.9490
                       Mean reward: 920.92
               Mean episode length: 244.38
    Episode_Reward/reaching_object: 1.3779
     Episode_Reward/lifting_object: 178.4061
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 163479552
                    Iteration time: 2.20s
                      Time elapsed: 01:03:05
                               ETA: 00:12:49

################################################################################
                     [1m Learning iteration 1663/2000 [0m                     

                       Computation: 44985 steps/s (collection: 2.091s, learning 0.095s)
             Mean action noise std: 3.16
          Mean value_function loss: 121.5775
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 55.9626
                       Mean reward: 890.89
               Mean episode length: 236.88
    Episode_Reward/reaching_object: 1.3566
     Episode_Reward/lifting_object: 176.0010
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 163577856
                    Iteration time: 2.19s
                      Time elapsed: 01:03:07
                               ETA: 00:12:47

################################################################################
                     [1m Learning iteration 1664/2000 [0m                     

                       Computation: 44526 steps/s (collection: 2.113s, learning 0.095s)
             Mean action noise std: 3.16
          Mean value_function loss: 107.0750
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 55.9740
                       Mean reward: 838.47
               Mean episode length: 225.56
    Episode_Reward/reaching_object: 1.3520
     Episode_Reward/lifting_object: 174.3691
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163676160
                    Iteration time: 2.21s
                      Time elapsed: 01:03:09
                               ETA: 00:12:44

################################################################################
                     [1m Learning iteration 1665/2000 [0m                     

                       Computation: 43967 steps/s (collection: 2.126s, learning 0.110s)
             Mean action noise std: 3.16
          Mean value_function loss: 119.7983
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.9867
                       Mean reward: 864.66
               Mean episode length: 230.50
    Episode_Reward/reaching_object: 1.3468
     Episode_Reward/lifting_object: 174.0581
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 163774464
                    Iteration time: 2.24s
                      Time elapsed: 01:03:11
                               ETA: 00:12:42

################################################################################
                     [1m Learning iteration 1666/2000 [0m                     

                       Computation: 26292 steps/s (collection: 3.619s, learning 0.120s)
             Mean action noise std: 3.16
          Mean value_function loss: 111.0434
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.0014
                       Mean reward: 904.92
               Mean episode length: 239.74
    Episode_Reward/reaching_object: 1.3512
     Episode_Reward/lifting_object: 173.9768
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 163872768
                    Iteration time: 3.74s
                      Time elapsed: 01:03:15
                               ETA: 00:12:40

################################################################################
                     [1m Learning iteration 1667/2000 [0m                     

                       Computation: 14084 steps/s (collection: 6.842s, learning 0.138s)
             Mean action noise std: 3.16
          Mean value_function loss: 128.9807
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.0087
                       Mean reward: 887.96
               Mean episode length: 237.12
    Episode_Reward/reaching_object: 1.3394
     Episode_Reward/lifting_object: 172.8692
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163971072
                    Iteration time: 6.98s
                      Time elapsed: 01:03:22
                               ETA: 00:12:39

################################################################################
                     [1m Learning iteration 1668/2000 [0m                     

                       Computation: 13990 steps/s (collection: 6.896s, learning 0.131s)
             Mean action noise std: 3.17
          Mean value_function loss: 95.8028
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.0147
                       Mean reward: 888.98
               Mean episode length: 235.88
    Episode_Reward/reaching_object: 1.3725
     Episode_Reward/lifting_object: 177.1556
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 164069376
                    Iteration time: 7.03s
                      Time elapsed: 01:03:29
                               ETA: 00:12:37

################################################################################
                     [1m Learning iteration 1669/2000 [0m                     

                       Computation: 13740 steps/s (collection: 7.019s, learning 0.136s)
             Mean action noise std: 3.17
          Mean value_function loss: 93.3504
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.0207
                       Mean reward: 882.67
               Mean episode length: 235.55
    Episode_Reward/reaching_object: 1.3703
     Episode_Reward/lifting_object: 176.8200
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 164167680
                    Iteration time: 7.15s
                      Time elapsed: 01:03:36
                               ETA: 00:12:36

################################################################################
                     [1m Learning iteration 1670/2000 [0m                     

                       Computation: 14013 steps/s (collection: 6.893s, learning 0.122s)
             Mean action noise std: 3.17
          Mean value_function loss: 123.4768
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 56.0297
                       Mean reward: 849.40
               Mean episode length: 227.04
    Episode_Reward/reaching_object: 1.3327
     Episode_Reward/lifting_object: 170.9741
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 164265984
                    Iteration time: 7.01s
                      Time elapsed: 01:03:43
                               ETA: 00:12:35

################################################################################
                     [1m Learning iteration 1671/2000 [0m                     

                       Computation: 13979 steps/s (collection: 6.909s, learning 0.123s)
             Mean action noise std: 3.17
          Mean value_function loss: 112.5404
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 56.0373
                       Mean reward: 891.16
               Mean episode length: 237.00
    Episode_Reward/reaching_object: 1.3390
     Episode_Reward/lifting_object: 172.1640
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 164364288
                    Iteration time: 7.03s
                      Time elapsed: 01:03:50
                               ETA: 00:12:33

################################################################################
                     [1m Learning iteration 1672/2000 [0m                     

                       Computation: 13910 steps/s (collection: 6.935s, learning 0.132s)
             Mean action noise std: 3.17
          Mean value_function loss: 122.8439
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.0459
                       Mean reward: 889.66
               Mean episode length: 235.97
    Episode_Reward/reaching_object: 1.3672
     Episode_Reward/lifting_object: 176.0467
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 164462592
                    Iteration time: 7.07s
                      Time elapsed: 01:03:57
                               ETA: 00:12:32

################################################################################
                     [1m Learning iteration 1673/2000 [0m                     

                       Computation: 13863 steps/s (collection: 6.980s, learning 0.111s)
             Mean action noise std: 3.17
          Mean value_function loss: 121.7467
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.0553
                       Mean reward: 858.18
               Mean episode length: 229.13
    Episode_Reward/reaching_object: 1.3367
     Episode_Reward/lifting_object: 172.1158
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 164560896
                    Iteration time: 7.09s
                      Time elapsed: 01:04:05
                               ETA: 00:12:31

################################################################################
                     [1m Learning iteration 1674/2000 [0m                     

                       Computation: 13677 steps/s (collection: 7.062s, learning 0.126s)
             Mean action noise std: 3.17
          Mean value_function loss: 198.3847
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.0650
                       Mean reward: 839.90
               Mean episode length: 225.51
    Episode_Reward/reaching_object: 1.3201
     Episode_Reward/lifting_object: 170.1219
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 164659200
                    Iteration time: 7.19s
                      Time elapsed: 01:04:12
                               ETA: 00:12:29

################################################################################
                     [1m Learning iteration 1675/2000 [0m                     

                       Computation: 21016 steps/s (collection: 4.579s, learning 0.098s)
             Mean action noise std: 3.17
          Mean value_function loss: 180.1847
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 56.0712
                       Mean reward: 825.33
               Mean episode length: 227.84
    Episode_Reward/reaching_object: 1.3366
     Episode_Reward/lifting_object: 172.1573
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 164757504
                    Iteration time: 4.68s
                      Time elapsed: 01:04:16
                               ETA: 00:12:27

################################################################################
                     [1m Learning iteration 1676/2000 [0m                     

                       Computation: 46377 steps/s (collection: 2.025s, learning 0.095s)
             Mean action noise std: 3.18
          Mean value_function loss: 175.7834
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 56.0777
                       Mean reward: 854.44
               Mean episode length: 228.50
    Episode_Reward/reaching_object: 1.3315
     Episode_Reward/lifting_object: 172.3245
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 164855808
                    Iteration time: 2.12s
                      Time elapsed: 01:04:19
                               ETA: 00:12:25

################################################################################
                     [1m Learning iteration 1677/2000 [0m                     

                       Computation: 45384 steps/s (collection: 2.038s, learning 0.128s)
             Mean action noise std: 3.18
          Mean value_function loss: 195.8186
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 56.0884
                       Mean reward: 870.82
               Mean episode length: 233.01
    Episode_Reward/reaching_object: 1.2857
     Episode_Reward/lifting_object: 167.2561
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 164954112
                    Iteration time: 2.17s
                      Time elapsed: 01:04:21
                               ETA: 00:12:23

################################################################################
                     [1m Learning iteration 1678/2000 [0m                     

                       Computation: 46225 steps/s (collection: 2.022s, learning 0.105s)
             Mean action noise std: 3.18
          Mean value_function loss: 143.6441
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 56.1032
                       Mean reward: 903.31
               Mean episode length: 239.43
    Episode_Reward/reaching_object: 1.3213
     Episode_Reward/lifting_object: 172.1784
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 165052416
                    Iteration time: 2.13s
                      Time elapsed: 01:04:23
                               ETA: 00:12:20

################################################################################
                     [1m Learning iteration 1679/2000 [0m                     

                       Computation: 45086 steps/s (collection: 2.093s, learning 0.087s)
             Mean action noise std: 3.18
          Mean value_function loss: 157.8169
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 56.1122
                       Mean reward: 836.21
               Mean episode length: 227.74
    Episode_Reward/reaching_object: 1.2954
     Episode_Reward/lifting_object: 167.7491
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 165150720
                    Iteration time: 2.18s
                      Time elapsed: 01:04:25
                               ETA: 00:12:18

################################################################################
                     [1m Learning iteration 1680/2000 [0m                     

                       Computation: 46170 steps/s (collection: 2.017s, learning 0.113s)
             Mean action noise std: 3.18
          Mean value_function loss: 146.2079
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.1180
                       Mean reward: 894.97
               Mean episode length: 236.55
    Episode_Reward/reaching_object: 1.3501
     Episode_Reward/lifting_object: 175.5088
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 165249024
                    Iteration time: 2.13s
                      Time elapsed: 01:04:27
                               ETA: 00:12:16

################################################################################
                     [1m Learning iteration 1681/2000 [0m                     

                       Computation: 45114 steps/s (collection: 2.067s, learning 0.112s)
             Mean action noise std: 3.18
          Mean value_function loss: 149.3028
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.1261
                       Mean reward: 842.91
               Mean episode length: 227.10
    Episode_Reward/reaching_object: 1.3162
     Episode_Reward/lifting_object: 170.0333
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 165347328
                    Iteration time: 2.18s
                      Time elapsed: 01:04:29
                               ETA: 00:12:13

################################################################################
                     [1m Learning iteration 1682/2000 [0m                     

                       Computation: 46879 steps/s (collection: 2.004s, learning 0.093s)
             Mean action noise std: 3.18
          Mean value_function loss: 126.5679
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.1330
                       Mean reward: 873.56
               Mean episode length: 232.01
    Episode_Reward/reaching_object: 1.3172
     Episode_Reward/lifting_object: 170.8412
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 165445632
                    Iteration time: 2.10s
                      Time elapsed: 01:04:31
                               ETA: 00:12:11

################################################################################
                     [1m Learning iteration 1683/2000 [0m                     

                       Computation: 46158 steps/s (collection: 2.007s, learning 0.123s)
             Mean action noise std: 3.18
          Mean value_function loss: 160.4659
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 56.1392
                       Mean reward: 879.87
               Mean episode length: 234.47
    Episode_Reward/reaching_object: 1.3295
     Episode_Reward/lifting_object: 172.7298
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 165543936
                    Iteration time: 2.13s
                      Time elapsed: 01:04:34
                               ETA: 00:12:09

################################################################################
                     [1m Learning iteration 1684/2000 [0m                     

                       Computation: 47064 steps/s (collection: 1.999s, learning 0.090s)
             Mean action noise std: 3.19
          Mean value_function loss: 151.2271
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.1492
                       Mean reward: 861.93
               Mean episode length: 230.58
    Episode_Reward/reaching_object: 1.3352
     Episode_Reward/lifting_object: 172.7470
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 165642240
                    Iteration time: 2.09s
                      Time elapsed: 01:04:36
                               ETA: 00:12:06

################################################################################
                     [1m Learning iteration 1685/2000 [0m                     

                       Computation: 46990 steps/s (collection: 1.987s, learning 0.105s)
             Mean action noise std: 3.19
          Mean value_function loss: 136.4334
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 56.1598
                       Mean reward: 873.83
               Mean episode length: 233.19
    Episode_Reward/reaching_object: 1.3256
     Episode_Reward/lifting_object: 171.6163
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 165740544
                    Iteration time: 2.09s
                      Time elapsed: 01:04:38
                               ETA: 00:12:04

################################################################################
                     [1m Learning iteration 1686/2000 [0m                     

                       Computation: 46773 steps/s (collection: 2.017s, learning 0.085s)
             Mean action noise std: 3.19
          Mean value_function loss: 140.6132
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.1741
                       Mean reward: 905.42
               Mean episode length: 240.83
    Episode_Reward/reaching_object: 1.3532
     Episode_Reward/lifting_object: 175.4082
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 165838848
                    Iteration time: 2.10s
                      Time elapsed: 01:04:40
                               ETA: 00:12:02

################################################################################
                     [1m Learning iteration 1687/2000 [0m                     

                       Computation: 46410 steps/s (collection: 2.003s, learning 0.116s)
             Mean action noise std: 3.19
          Mean value_function loss: 115.1983
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 56.1870
                       Mean reward: 887.99
               Mean episode length: 235.85
    Episode_Reward/reaching_object: 1.3618
     Episode_Reward/lifting_object: 175.6523
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 165937152
                    Iteration time: 2.12s
                      Time elapsed: 01:04:42
                               ETA: 00:11:59

################################################################################
                     [1m Learning iteration 1688/2000 [0m                     

                       Computation: 45442 steps/s (collection: 2.056s, learning 0.108s)
             Mean action noise std: 3.19
          Mean value_function loss: 102.9350
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.2007
                       Mean reward: 900.11
               Mean episode length: 239.08
    Episode_Reward/reaching_object: 1.3592
     Episode_Reward/lifting_object: 175.9085
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0521
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 166035456
                    Iteration time: 2.16s
                      Time elapsed: 01:04:44
                               ETA: 00:11:57

################################################################################
                     [1m Learning iteration 1689/2000 [0m                     

                       Computation: 46674 steps/s (collection: 2.004s, learning 0.103s)
             Mean action noise std: 3.19
          Mean value_function loss: 149.2063
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.2067
                       Mean reward: 812.55
               Mean episode length: 218.76
    Episode_Reward/reaching_object: 1.3169
     Episode_Reward/lifting_object: 169.8379
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 166133760
                    Iteration time: 2.11s
                      Time elapsed: 01:04:46
                               ETA: 00:11:55

################################################################################
                     [1m Learning iteration 1690/2000 [0m                     

                       Computation: 47709 steps/s (collection: 1.963s, learning 0.098s)
             Mean action noise std: 3.20
          Mean value_function loss: 116.6039
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 56.2180
                       Mean reward: 836.16
               Mean episode length: 223.98
    Episode_Reward/reaching_object: 1.3381
     Episode_Reward/lifting_object: 172.6629
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 166232064
                    Iteration time: 2.06s
                      Time elapsed: 01:04:48
                               ETA: 00:11:52

################################################################################
                     [1m Learning iteration 1691/2000 [0m                     

                       Computation: 47048 steps/s (collection: 1.982s, learning 0.107s)
             Mean action noise std: 3.20
          Mean value_function loss: 111.7091
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 56.2270
                       Mean reward: 866.51
               Mean episode length: 231.03
    Episode_Reward/reaching_object: 1.3437
     Episode_Reward/lifting_object: 173.1478
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 166330368
                    Iteration time: 2.09s
                      Time elapsed: 01:04:50
                               ETA: 00:11:50

################################################################################
                     [1m Learning iteration 1692/2000 [0m                     

                       Computation: 46387 steps/s (collection: 2.009s, learning 0.111s)
             Mean action noise std: 3.20
          Mean value_function loss: 140.6849
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 56.2391
                       Mean reward: 871.91
               Mean episode length: 230.98
    Episode_Reward/reaching_object: 1.3298
     Episode_Reward/lifting_object: 171.0598
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 166428672
                    Iteration time: 2.12s
                      Time elapsed: 01:04:52
                               ETA: 00:11:48

################################################################################
                     [1m Learning iteration 1693/2000 [0m                     

                       Computation: 46646 steps/s (collection: 1.984s, learning 0.124s)
             Mean action noise std: 3.20
          Mean value_function loss: 91.7413
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 56.2517
                       Mean reward: 889.78
               Mean episode length: 237.63
    Episode_Reward/reaching_object: 1.3626
     Episode_Reward/lifting_object: 175.0697
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 166526976
                    Iteration time: 2.11s
                      Time elapsed: 01:04:55
                               ETA: 00:11:45

################################################################################
                     [1m Learning iteration 1694/2000 [0m                     

                       Computation: 45208 steps/s (collection: 2.037s, learning 0.138s)
             Mean action noise std: 3.20
          Mean value_function loss: 107.6093
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.2563
                       Mean reward: 897.66
               Mean episode length: 238.97
    Episode_Reward/reaching_object: 1.3610
     Episode_Reward/lifting_object: 175.5860
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 166625280
                    Iteration time: 2.17s
                      Time elapsed: 01:04:57
                               ETA: 00:11:43

################################################################################
                     [1m Learning iteration 1695/2000 [0m                     

                       Computation: 46997 steps/s (collection: 2.000s, learning 0.092s)
             Mean action noise std: 3.20
          Mean value_function loss: 137.0965
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.2621
                       Mean reward: 832.38
               Mean episode length: 223.27
    Episode_Reward/reaching_object: 1.3436
     Episode_Reward/lifting_object: 172.9293
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 166723584
                    Iteration time: 2.09s
                      Time elapsed: 01:04:59
                               ETA: 00:11:41

################################################################################
                     [1m Learning iteration 1696/2000 [0m                     

                       Computation: 46484 steps/s (collection: 2.000s, learning 0.115s)
             Mean action noise std: 3.20
          Mean value_function loss: 142.5586
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.2717
                       Mean reward: 870.38
               Mean episode length: 231.77
    Episode_Reward/reaching_object: 1.3506
     Episode_Reward/lifting_object: 174.5909
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0521
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 166821888
                    Iteration time: 2.11s
                      Time elapsed: 01:05:01
                               ETA: 00:11:38

################################################################################
                     [1m Learning iteration 1697/2000 [0m                     

                       Computation: 45196 steps/s (collection: 2.065s, learning 0.110s)
             Mean action noise std: 3.21
          Mean value_function loss: 123.6929
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 56.2802
                       Mean reward: 870.49
               Mean episode length: 233.60
    Episode_Reward/reaching_object: 1.3506
     Episode_Reward/lifting_object: 174.7589
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166920192
                    Iteration time: 2.18s
                      Time elapsed: 01:05:03
                               ETA: 00:11:36

################################################################################
                     [1m Learning iteration 1698/2000 [0m                     

                       Computation: 46740 steps/s (collection: 1.990s, learning 0.114s)
             Mean action noise std: 3.21
          Mean value_function loss: 129.0291
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.2879
                       Mean reward: 855.55
               Mean episode length: 229.08
    Episode_Reward/reaching_object: 1.3179
     Episode_Reward/lifting_object: 170.7866
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 167018496
                    Iteration time: 2.10s
                      Time elapsed: 01:05:05
                               ETA: 00:11:34

################################################################################
                     [1m Learning iteration 1699/2000 [0m                     

                       Computation: 46926 steps/s (collection: 2.004s, learning 0.091s)
             Mean action noise std: 3.21
          Mean value_function loss: 144.9556
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.2970
                       Mean reward: 891.08
               Mean episode length: 236.15
    Episode_Reward/reaching_object: 1.3261
     Episode_Reward/lifting_object: 173.3204
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 167116800
                    Iteration time: 2.09s
                      Time elapsed: 01:05:07
                               ETA: 00:11:31

################################################################################
                     [1m Learning iteration 1700/2000 [0m                     

                       Computation: 45820 steps/s (collection: 2.053s, learning 0.092s)
             Mean action noise std: 3.21
          Mean value_function loss: 134.0175
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 56.3086
                       Mean reward: 866.44
               Mean episode length: 231.96
    Episode_Reward/reaching_object: 1.3190
     Episode_Reward/lifting_object: 171.0219
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 167215104
                    Iteration time: 2.15s
                      Time elapsed: 01:05:09
                               ETA: 00:11:29

################################################################################
                     [1m Learning iteration 1701/2000 [0m                     

                       Computation: 45193 steps/s (collection: 2.069s, learning 0.107s)
             Mean action noise std: 3.21
          Mean value_function loss: 102.2692
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.3233
                       Mean reward: 874.90
               Mean episode length: 232.93
    Episode_Reward/reaching_object: 1.3409
     Episode_Reward/lifting_object: 174.3490
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 167313408
                    Iteration time: 2.18s
                      Time elapsed: 01:05:12
                               ETA: 00:11:27

################################################################################
                     [1m Learning iteration 1702/2000 [0m                     

                       Computation: 46943 steps/s (collection: 1.996s, learning 0.099s)
             Mean action noise std: 3.21
          Mean value_function loss: 111.8842
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 56.3358
                       Mean reward: 855.75
               Mean episode length: 229.72
    Episode_Reward/reaching_object: 1.3238
     Episode_Reward/lifting_object: 171.7440
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 167411712
                    Iteration time: 2.09s
                      Time elapsed: 01:05:14
                               ETA: 00:11:24

################################################################################
                     [1m Learning iteration 1703/2000 [0m                     

                       Computation: 46098 steps/s (collection: 2.024s, learning 0.108s)
             Mean action noise std: 3.22
          Mean value_function loss: 77.7409
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.3446
                       Mean reward: 894.42
               Mean episode length: 238.25
    Episode_Reward/reaching_object: 1.3712
     Episode_Reward/lifting_object: 177.7444
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 167510016
                    Iteration time: 2.13s
                      Time elapsed: 01:05:16
                               ETA: 00:11:22

################################################################################
                     [1m Learning iteration 1704/2000 [0m                     

                       Computation: 46855 steps/s (collection: 2.003s, learning 0.095s)
             Mean action noise std: 3.22
          Mean value_function loss: 126.6666
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 56.3494
                       Mean reward: 846.38
               Mean episode length: 225.92
    Episode_Reward/reaching_object: 1.3605
     Episode_Reward/lifting_object: 176.6008
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 167608320
                    Iteration time: 2.10s
                      Time elapsed: 01:05:18
                               ETA: 00:11:20

################################################################################
                     [1m Learning iteration 1705/2000 [0m                     

                       Computation: 45835 steps/s (collection: 2.035s, learning 0.110s)
             Mean action noise std: 3.22
          Mean value_function loss: 147.7862
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.3580
                       Mean reward: 835.61
               Mean episode length: 225.46
    Episode_Reward/reaching_object: 1.3434
     Episode_Reward/lifting_object: 173.7680
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 167706624
                    Iteration time: 2.14s
                      Time elapsed: 01:05:20
                               ETA: 00:11:17

################################################################################
                     [1m Learning iteration 1706/2000 [0m                     

                       Computation: 46430 steps/s (collection: 2.026s, learning 0.091s)
             Mean action noise std: 3.22
          Mean value_function loss: 144.6015
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.3706
                       Mean reward: 878.65
               Mean episode length: 233.63
    Episode_Reward/reaching_object: 1.3459
     Episode_Reward/lifting_object: 174.3623
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 167804928
                    Iteration time: 2.12s
                      Time elapsed: 01:05:22
                               ETA: 00:11:15

################################################################################
                     [1m Learning iteration 1707/2000 [0m                     

                       Computation: 47034 steps/s (collection: 2.003s, learning 0.087s)
             Mean action noise std: 3.22
          Mean value_function loss: 150.5834
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.3786
                       Mean reward: 873.58
               Mean episode length: 234.47
    Episode_Reward/reaching_object: 1.3279
     Episode_Reward/lifting_object: 171.2872
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 167903232
                    Iteration time: 2.09s
                      Time elapsed: 01:05:24
                               ETA: 00:11:13

################################################################################
                     [1m Learning iteration 1708/2000 [0m                     

                       Computation: 46519 steps/s (collection: 2.012s, learning 0.101s)
             Mean action noise std: 3.22
          Mean value_function loss: 132.6816
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 56.3912
                       Mean reward: 851.67
               Mean episode length: 236.57
    Episode_Reward/reaching_object: 1.3194
     Episode_Reward/lifting_object: 168.5247
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 168001536
                    Iteration time: 2.11s
                      Time elapsed: 01:05:26
                               ETA: 00:11:10

################################################################################
                     [1m Learning iteration 1709/2000 [0m                     

                       Computation: 44571 steps/s (collection: 2.094s, learning 0.111s)
             Mean action noise std: 3.22
          Mean value_function loss: 160.1508
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 56.4004
                       Mean reward: 829.77
               Mean episode length: 221.72
    Episode_Reward/reaching_object: 1.3261
     Episode_Reward/lifting_object: 170.3114
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 168099840
                    Iteration time: 2.21s
                      Time elapsed: 01:05:29
                               ETA: 00:11:08

################################################################################
                     [1m Learning iteration 1710/2000 [0m                     

                       Computation: 45339 steps/s (collection: 2.037s, learning 0.131s)
             Mean action noise std: 3.22
          Mean value_function loss: 134.7122
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.4084
                       Mean reward: 869.09
               Mean episode length: 230.86
    Episode_Reward/reaching_object: 1.3216
     Episode_Reward/lifting_object: 170.4063
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 168198144
                    Iteration time: 2.17s
                      Time elapsed: 01:05:31
                               ETA: 00:11:06

################################################################################
                     [1m Learning iteration 1711/2000 [0m                     

                       Computation: 46576 steps/s (collection: 2.013s, learning 0.098s)
             Mean action noise std: 3.23
          Mean value_function loss: 113.5336
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.4143
                       Mean reward: 871.74
               Mean episode length: 232.41
    Episode_Reward/reaching_object: 1.3154
     Episode_Reward/lifting_object: 169.0994
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 168296448
                    Iteration time: 2.11s
                      Time elapsed: 01:05:33
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 1712/2000 [0m                     

                       Computation: 46912 steps/s (collection: 2.009s, learning 0.086s)
             Mean action noise std: 3.23
          Mean value_function loss: 126.2702
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 56.4174
                       Mean reward: 828.23
               Mean episode length: 222.36
    Episode_Reward/reaching_object: 1.3406
     Episode_Reward/lifting_object: 172.6490
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168394752
                    Iteration time: 2.10s
                      Time elapsed: 01:05:35
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 1713/2000 [0m                     

                       Computation: 46886 steps/s (collection: 2.003s, learning 0.094s)
             Mean action noise std: 3.23
          Mean value_function loss: 123.5479
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 56.4232
                       Mean reward: 904.15
               Mean episode length: 240.44
    Episode_Reward/reaching_object: 1.3462
     Episode_Reward/lifting_object: 173.5123
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 168493056
                    Iteration time: 2.10s
                      Time elapsed: 01:05:37
                               ETA: 00:10:59

################################################################################
                     [1m Learning iteration 1714/2000 [0m                     

                       Computation: 46972 steps/s (collection: 2.005s, learning 0.088s)
             Mean action noise std: 3.23
          Mean value_function loss: 155.5439
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 56.4299
                       Mean reward: 846.49
               Mean episode length: 227.27
    Episode_Reward/reaching_object: 1.3440
     Episode_Reward/lifting_object: 173.2309
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 168591360
                    Iteration time: 2.09s
                      Time elapsed: 01:05:39
                               ETA: 00:10:57

################################################################################
                     [1m Learning iteration 1715/2000 [0m                     

                       Computation: 47063 steps/s (collection: 1.994s, learning 0.095s)
             Mean action noise std: 3.23
          Mean value_function loss: 126.1176
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 56.4411
                       Mean reward: 856.28
               Mean episode length: 229.00
    Episode_Reward/reaching_object: 1.3524
     Episode_Reward/lifting_object: 173.9081
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 168689664
                    Iteration time: 2.09s
                      Time elapsed: 01:05:41
                               ETA: 00:10:54

################################################################################
                     [1m Learning iteration 1716/2000 [0m                     

                       Computation: 46216 steps/s (collection: 2.033s, learning 0.094s)
             Mean action noise std: 3.23
          Mean value_function loss: 117.6993
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 56.4513
                       Mean reward: 876.67
               Mean episode length: 234.82
    Episode_Reward/reaching_object: 1.3695
     Episode_Reward/lifting_object: 175.7900
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 168787968
                    Iteration time: 2.13s
                      Time elapsed: 01:05:43
                               ETA: 00:10:52

################################################################################
                     [1m Learning iteration 1717/2000 [0m                     

                       Computation: 46567 steps/s (collection: 2.014s, learning 0.097s)
             Mean action noise std: 3.23
          Mean value_function loss: 121.2375
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.4581
                       Mean reward: 872.59
               Mean episode length: 232.89
    Episode_Reward/reaching_object: 1.3524
     Episode_Reward/lifting_object: 174.2479
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 168886272
                    Iteration time: 2.11s
                      Time elapsed: 01:05:46
                               ETA: 00:10:50

################################################################################
                     [1m Learning iteration 1718/2000 [0m                     

                       Computation: 47428 steps/s (collection: 1.979s, learning 0.094s)
             Mean action noise std: 3.23
          Mean value_function loss: 108.2170
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.4693
                       Mean reward: 894.04
               Mean episode length: 237.85
    Episode_Reward/reaching_object: 1.3464
     Episode_Reward/lifting_object: 173.4711
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 168984576
                    Iteration time: 2.07s
                      Time elapsed: 01:05:48
                               ETA: 00:10:47

################################################################################
                     [1m Learning iteration 1719/2000 [0m                     

                       Computation: 47362 steps/s (collection: 1.986s, learning 0.090s)
             Mean action noise std: 3.24
          Mean value_function loss: 126.6015
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 56.4841
                       Mean reward: 855.34
               Mean episode length: 227.58
    Episode_Reward/reaching_object: 1.3507
     Episode_Reward/lifting_object: 173.6797
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 169082880
                    Iteration time: 2.08s
                      Time elapsed: 01:05:50
                               ETA: 00:10:45

################################################################################
                     [1m Learning iteration 1720/2000 [0m                     

                       Computation: 46336 steps/s (collection: 2.013s, learning 0.109s)
             Mean action noise std: 3.24
          Mean value_function loss: 134.6075
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 56.4947
                       Mean reward: 876.01
               Mean episode length: 233.26
    Episode_Reward/reaching_object: 1.3588
     Episode_Reward/lifting_object: 174.3335
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0530
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 169181184
                    Iteration time: 2.12s
                      Time elapsed: 01:05:52
                               ETA: 00:10:43

################################################################################
                     [1m Learning iteration 1721/2000 [0m                     

                       Computation: 45946 steps/s (collection: 2.049s, learning 0.091s)
             Mean action noise std: 3.24
          Mean value_function loss: 129.6122
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.5052
                       Mean reward: 865.35
               Mean episode length: 234.21
    Episode_Reward/reaching_object: 1.3532
     Episode_Reward/lifting_object: 173.2370
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0530
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 169279488
                    Iteration time: 2.14s
                      Time elapsed: 01:05:54
                               ETA: 00:10:40

################################################################################
                     [1m Learning iteration 1722/2000 [0m                     

                       Computation: 45403 steps/s (collection: 2.063s, learning 0.103s)
             Mean action noise std: 3.24
          Mean value_function loss: 128.7327
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 56.5161
                       Mean reward: 919.88
               Mean episode length: 242.92
    Episode_Reward/reaching_object: 1.3496
     Episode_Reward/lifting_object: 173.3082
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 169377792
                    Iteration time: 2.17s
                      Time elapsed: 01:05:56
                               ETA: 00:10:38

################################################################################
                     [1m Learning iteration 1723/2000 [0m                     

                       Computation: 46497 steps/s (collection: 2.012s, learning 0.102s)
             Mean action noise std: 3.24
          Mean value_function loss: 99.5455
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.5271
                       Mean reward: 893.87
               Mean episode length: 237.95
    Episode_Reward/reaching_object: 1.3589
     Episode_Reward/lifting_object: 174.7580
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0533
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 169476096
                    Iteration time: 2.11s
                      Time elapsed: 01:05:58
                               ETA: 00:10:36

################################################################################
                     [1m Learning iteration 1724/2000 [0m                     

                       Computation: 46022 steps/s (collection: 2.028s, learning 0.108s)
             Mean action noise std: 3.24
          Mean value_function loss: 132.6816
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 56.5323
                       Mean reward: 874.74
               Mean episode length: 233.55
    Episode_Reward/reaching_object: 1.3287
     Episode_Reward/lifting_object: 171.0735
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 169574400
                    Iteration time: 2.14s
                      Time elapsed: 01:06:00
                               ETA: 00:10:33

################################################################################
                     [1m Learning iteration 1725/2000 [0m                     

                       Computation: 45753 steps/s (collection: 2.064s, learning 0.084s)
             Mean action noise std: 3.24
          Mean value_function loss: 118.4013
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.5399
                       Mean reward: 883.82
               Mean episode length: 235.90
    Episode_Reward/reaching_object: 1.3587
     Episode_Reward/lifting_object: 174.3812
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0533
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 169672704
                    Iteration time: 2.15s
                      Time elapsed: 01:06:03
                               ETA: 00:10:31

################################################################################
                     [1m Learning iteration 1726/2000 [0m                     

                       Computation: 46633 steps/s (collection: 2.010s, learning 0.098s)
             Mean action noise std: 3.25
          Mean value_function loss: 106.1339
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.5491
                       Mean reward: 896.48
               Mean episode length: 237.46
    Episode_Reward/reaching_object: 1.3817
     Episode_Reward/lifting_object: 177.6110
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 169771008
                    Iteration time: 2.11s
                      Time elapsed: 01:06:05
                               ETA: 00:10:29

################################################################################
                     [1m Learning iteration 1727/2000 [0m                     

                       Computation: 47157 steps/s (collection: 1.998s, learning 0.087s)
             Mean action noise std: 3.25
          Mean value_function loss: 145.9984
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.5580
                       Mean reward: 880.26
               Mean episode length: 237.15
    Episode_Reward/reaching_object: 1.3333
     Episode_Reward/lifting_object: 170.9466
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0530
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 169869312
                    Iteration time: 2.08s
                      Time elapsed: 01:06:07
                               ETA: 00:10:26

################################################################################
                     [1m Learning iteration 1728/2000 [0m                     

                       Computation: 45595 steps/s (collection: 2.040s, learning 0.116s)
             Mean action noise std: 3.25
          Mean value_function loss: 129.4651
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.5665
                       Mean reward: 884.58
               Mean episode length: 235.40
    Episode_Reward/reaching_object: 1.3475
     Episode_Reward/lifting_object: 173.0538
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 169967616
                    Iteration time: 2.16s
                      Time elapsed: 01:06:09
                               ETA: 00:10:24

################################################################################
                     [1m Learning iteration 1729/2000 [0m                     

                       Computation: 45265 steps/s (collection: 2.048s, learning 0.124s)
             Mean action noise std: 3.25
          Mean value_function loss: 150.4937
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.5763
                       Mean reward: 837.78
               Mean episode length: 225.49
    Episode_Reward/reaching_object: 1.3432
     Episode_Reward/lifting_object: 172.6690
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 170065920
                    Iteration time: 2.17s
                      Time elapsed: 01:06:11
                               ETA: 00:10:22

################################################################################
                     [1m Learning iteration 1730/2000 [0m                     

                       Computation: 46666 steps/s (collection: 2.010s, learning 0.096s)
             Mean action noise std: 3.25
          Mean value_function loss: 169.0232
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 56.5843
                       Mean reward: 814.33
               Mean episode length: 220.18
    Episode_Reward/reaching_object: 1.3098
     Episode_Reward/lifting_object: 167.8172
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 170164224
                    Iteration time: 2.11s
                      Time elapsed: 01:06:13
                               ETA: 00:10:19

################################################################################
                     [1m Learning iteration 1731/2000 [0m                     

                       Computation: 46863 steps/s (collection: 2.001s, learning 0.097s)
             Mean action noise std: 3.25
          Mean value_function loss: 106.7844
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.5948
                       Mean reward: 897.64
               Mean episode length: 239.13
    Episode_Reward/reaching_object: 1.3716
     Episode_Reward/lifting_object: 175.6293
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 170262528
                    Iteration time: 2.10s
                      Time elapsed: 01:06:15
                               ETA: 00:10:17

################################################################################
                     [1m Learning iteration 1732/2000 [0m                     

                       Computation: 46377 steps/s (collection: 2.023s, learning 0.097s)
             Mean action noise std: 3.26
          Mean value_function loss: 117.0937
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 56.6059
                       Mean reward: 905.77
               Mean episode length: 241.96
    Episode_Reward/reaching_object: 1.3497
     Episode_Reward/lifting_object: 173.3358
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0670
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 170360832
                    Iteration time: 2.12s
                      Time elapsed: 01:06:17
                               ETA: 00:10:15

################################################################################
                     [1m Learning iteration 1733/2000 [0m                     

                       Computation: 46541 steps/s (collection: 2.023s, learning 0.089s)
             Mean action noise std: 3.26
          Mean value_function loss: 88.6158
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 56.6160
                       Mean reward: 887.56
               Mean episode length: 236.37
    Episode_Reward/reaching_object: 1.3818
     Episode_Reward/lifting_object: 177.3346
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 170459136
                    Iteration time: 2.11s
                      Time elapsed: 01:06:19
                               ETA: 00:10:12

################################################################################
                     [1m Learning iteration 1734/2000 [0m                     

                       Computation: 45757 steps/s (collection: 2.039s, learning 0.109s)
             Mean action noise std: 3.26
          Mean value_function loss: 109.5928
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.6308
                       Mean reward: 888.78
               Mean episode length: 236.06
    Episode_Reward/reaching_object: 1.3508
     Episode_Reward/lifting_object: 172.8840
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 170557440
                    Iteration time: 2.15s
                      Time elapsed: 01:06:22
                               ETA: 00:10:10

################################################################################
                     [1m Learning iteration 1735/2000 [0m                     

                       Computation: 46997 steps/s (collection: 2.006s, learning 0.086s)
             Mean action noise std: 3.26
          Mean value_function loss: 127.8562
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 56.6438
                       Mean reward: 905.04
               Mean episode length: 239.46
    Episode_Reward/reaching_object: 1.3635
     Episode_Reward/lifting_object: 174.6804
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 170655744
                    Iteration time: 2.09s
                      Time elapsed: 01:06:24
                               ETA: 00:10:08

################################################################################
                     [1m Learning iteration 1736/2000 [0m                     

                       Computation: 45263 steps/s (collection: 2.070s, learning 0.102s)
             Mean action noise std: 3.26
          Mean value_function loss: 93.5149
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 56.6574
                       Mean reward: 871.19
               Mean episode length: 230.42
    Episode_Reward/reaching_object: 1.3709
     Episode_Reward/lifting_object: 176.0443
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 170754048
                    Iteration time: 2.17s
                      Time elapsed: 01:06:26
                               ETA: 00:10:05

################################################################################
                     [1m Learning iteration 1737/2000 [0m                     

                       Computation: 42778 steps/s (collection: 2.180s, learning 0.118s)
             Mean action noise std: 3.26
          Mean value_function loss: 1271252929.6355
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 56.6614
                       Mean reward: 878.63
               Mean episode length: 233.78
    Episode_Reward/reaching_object: 1.3668
     Episode_Reward/lifting_object: 174.7481
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -3.7835
          Episode_Reward/joint_vel: -2257.3882
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 170852352
                    Iteration time: 2.30s
                      Time elapsed: 01:06:28
                               ETA: 00:10:03

################################################################################
                     [1m Learning iteration 1738/2000 [0m                     

                       Computation: 46728 steps/s (collection: 2.015s, learning 0.089s)
             Mean action noise std: 3.26
          Mean value_function loss: 119.4105
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.6659
                       Mean reward: 886.55
               Mean episode length: 234.87
    Episode_Reward/reaching_object: 1.3775
     Episode_Reward/lifting_object: 174.5426
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 170950656
                    Iteration time: 2.10s
                      Time elapsed: 01:06:30
                               ETA: 00:10:01

################################################################################
                     [1m Learning iteration 1739/2000 [0m                     

                       Computation: 44425 steps/s (collection: 2.119s, learning 0.094s)
             Mean action noise std: 3.27
          Mean value_function loss: 86.4077
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.6764
                       Mean reward: 929.70
               Mean episode length: 245.54
    Episode_Reward/reaching_object: 1.3702
     Episode_Reward/lifting_object: 174.5998
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 171048960
                    Iteration time: 2.21s
                      Time elapsed: 01:06:33
                               ETA: 00:09:58

################################################################################
                     [1m Learning iteration 1740/2000 [0m                     

                       Computation: 46324 steps/s (collection: 2.024s, learning 0.098s)
             Mean action noise std: 3.27
          Mean value_function loss: 114.4594
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 56.6869
                       Mean reward: 915.81
               Mean episode length: 243.82
    Episode_Reward/reaching_object: 1.3558
     Episode_Reward/lifting_object: 174.1838
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 171147264
                    Iteration time: 2.12s
                      Time elapsed: 01:06:35
                               ETA: 00:09:56

################################################################################
                     [1m Learning iteration 1741/2000 [0m                     

                       Computation: 46521 steps/s (collection: 2.010s, learning 0.103s)
             Mean action noise std: 3.27
          Mean value_function loss: 102.5264
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 56.6974
                       Mean reward: 896.97
               Mean episode length: 240.21
    Episode_Reward/reaching_object: 1.3701
     Episode_Reward/lifting_object: 175.5679
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 171245568
                    Iteration time: 2.11s
                      Time elapsed: 01:06:37
                               ETA: 00:09:54

################################################################################
                     [1m Learning iteration 1742/2000 [0m                     

                       Computation: 46147 steps/s (collection: 2.043s, learning 0.088s)
             Mean action noise std: 3.27
          Mean value_function loss: 89.3580
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 56.7060
                       Mean reward: 891.58
               Mean episode length: 236.81
    Episode_Reward/reaching_object: 1.3647
     Episode_Reward/lifting_object: 174.4379
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 171343872
                    Iteration time: 2.13s
                      Time elapsed: 01:06:39
                               ETA: 00:09:51

################################################################################
                     [1m Learning iteration 1743/2000 [0m                     

                       Computation: 46677 steps/s (collection: 2.012s, learning 0.095s)
             Mean action noise std: 3.27
          Mean value_function loss: 115.9395
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 56.7108
                       Mean reward: 877.29
               Mean episode length: 234.56
    Episode_Reward/reaching_object: 1.3673
     Episode_Reward/lifting_object: 173.9204
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0700
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 171442176
                    Iteration time: 2.11s
                      Time elapsed: 01:06:41
                               ETA: 00:09:49

################################################################################
                     [1m Learning iteration 1744/2000 [0m                     

                       Computation: 46122 steps/s (collection: 2.026s, learning 0.105s)
             Mean action noise std: 3.27
          Mean value_function loss: 83.2659
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.7157
                       Mean reward: 891.71
               Mean episode length: 237.79
    Episode_Reward/reaching_object: 1.3815
     Episode_Reward/lifting_object: 177.3434
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0692
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 171540480
                    Iteration time: 2.13s
                      Time elapsed: 01:06:43
                               ETA: 00:09:47

################################################################################
                     [1m Learning iteration 1745/2000 [0m                     

                       Computation: 46963 steps/s (collection: 1.999s, learning 0.094s)
             Mean action noise std: 3.27
          Mean value_function loss: 138.9745
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 56.7255
                       Mean reward: 895.61
               Mean episode length: 238.40
    Episode_Reward/reaching_object: 1.3415
     Episode_Reward/lifting_object: 171.5634
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 171638784
                    Iteration time: 2.09s
                      Time elapsed: 01:06:45
                               ETA: 00:09:45

################################################################################
                     [1m Learning iteration 1746/2000 [0m                     

                       Computation: 46366 steps/s (collection: 2.033s, learning 0.087s)
             Mean action noise std: 3.28
          Mean value_function loss: 146.1564
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 56.7386
                       Mean reward: 868.13
               Mean episode length: 231.04
    Episode_Reward/reaching_object: 1.3589
     Episode_Reward/lifting_object: 173.7997
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 171737088
                    Iteration time: 2.12s
                      Time elapsed: 01:06:47
                               ETA: 00:09:42

################################################################################
                     [1m Learning iteration 1747/2000 [0m                     

                       Computation: 45090 steps/s (collection: 2.080s, learning 0.100s)
             Mean action noise std: 3.28
          Mean value_function loss: 182.3026
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 56.7524
                       Mean reward: 839.72
               Mean episode length: 224.40
    Episode_Reward/reaching_object: 1.2965
     Episode_Reward/lifting_object: 164.7048
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 171835392
                    Iteration time: 2.18s
                      Time elapsed: 01:06:49
                               ETA: 00:09:40

################################################################################
                     [1m Learning iteration 1748/2000 [0m                     

                       Computation: 46305 steps/s (collection: 2.035s, learning 0.088s)
             Mean action noise std: 3.28
          Mean value_function loss: 101.0983
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 56.7670
                       Mean reward: 921.24
               Mean episode length: 242.77
    Episode_Reward/reaching_object: 1.3754
     Episode_Reward/lifting_object: 176.1364
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 171933696
                    Iteration time: 2.12s
                      Time elapsed: 01:06:52
                               ETA: 00:09:38

################################################################################
                     [1m Learning iteration 1749/2000 [0m                     

                       Computation: 45954 steps/s (collection: 2.055s, learning 0.085s)
             Mean action noise std: 3.28
          Mean value_function loss: 131.0407
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.7790
                       Mean reward: 868.47
               Mean episode length: 232.06
    Episode_Reward/reaching_object: 1.3625
     Episode_Reward/lifting_object: 174.7008
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 172032000
                    Iteration time: 2.14s
                      Time elapsed: 01:06:54
                               ETA: 00:09:35

################################################################################
                     [1m Learning iteration 1750/2000 [0m                     

                       Computation: 46152 steps/s (collection: 2.036s, learning 0.094s)
             Mean action noise std: 3.28
          Mean value_function loss: 87.1418
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 56.7888
                       Mean reward: 901.51
               Mean episode length: 239.43
    Episode_Reward/reaching_object: 1.3896
     Episode_Reward/lifting_object: 178.3134
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 172130304
                    Iteration time: 2.13s
                      Time elapsed: 01:06:56
                               ETA: 00:09:33

################################################################################
                     [1m Learning iteration 1751/2000 [0m                     

                       Computation: 44399 steps/s (collection: 2.097s, learning 0.118s)
             Mean action noise std: 3.28
          Mean value_function loss: 112.5528
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.8007
                       Mean reward: 852.47
               Mean episode length: 228.95
    Episode_Reward/reaching_object: 1.3623
     Episode_Reward/lifting_object: 174.1966
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0549
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 172228608
                    Iteration time: 2.21s
                      Time elapsed: 01:06:58
                               ETA: 00:09:31

################################################################################
                     [1m Learning iteration 1752/2000 [0m                     

                       Computation: 45764 steps/s (collection: 2.026s, learning 0.122s)
             Mean action noise std: 3.29
          Mean value_function loss: 160.6289
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.8088
                       Mean reward: 854.29
               Mean episode length: 234.41
    Episode_Reward/reaching_object: 1.3551
     Episode_Reward/lifting_object: 172.5612
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 172326912
                    Iteration time: 2.15s
                      Time elapsed: 01:07:00
                               ETA: 00:09:28

################################################################################
                     [1m Learning iteration 1753/2000 [0m                     

                       Computation: 46678 steps/s (collection: 2.018s, learning 0.088s)
             Mean action noise std: 3.29
          Mean value_function loss: 131.1665
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 56.8175
                       Mean reward: 852.39
               Mean episode length: 226.12
    Episode_Reward/reaching_object: 1.3344
     Episode_Reward/lifting_object: 170.6848
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 172425216
                    Iteration time: 2.11s
                      Time elapsed: 01:07:02
                               ETA: 00:09:26

################################################################################
                     [1m Learning iteration 1754/2000 [0m                     

                       Computation: 46625 steps/s (collection: 2.018s, learning 0.091s)
             Mean action noise std: 3.29
          Mean value_function loss: 145.2014
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 56.8273
                       Mean reward: 834.93
               Mean episode length: 223.58
    Episode_Reward/reaching_object: 1.3325
     Episode_Reward/lifting_object: 170.5906
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 172523520
                    Iteration time: 2.11s
                      Time elapsed: 01:07:04
                               ETA: 00:09:24

################################################################################
                     [1m Learning iteration 1755/2000 [0m                     

                       Computation: 46356 steps/s (collection: 2.023s, learning 0.098s)
             Mean action noise std: 3.29
          Mean value_function loss: 154.9253
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.8394
                       Mean reward: 856.90
               Mean episode length: 229.90
    Episode_Reward/reaching_object: 1.3310
     Episode_Reward/lifting_object: 170.4944
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 172621824
                    Iteration time: 2.12s
                      Time elapsed: 01:07:07
                               ETA: 00:09:21

################################################################################
                     [1m Learning iteration 1756/2000 [0m                     

                       Computation: 46635 steps/s (collection: 2.020s, learning 0.088s)
             Mean action noise std: 3.29
          Mean value_function loss: 136.9712
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 56.8502
                       Mean reward: 833.44
               Mean episode length: 224.40
    Episode_Reward/reaching_object: 1.3302
     Episode_Reward/lifting_object: 170.5689
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 172720128
                    Iteration time: 2.11s
                      Time elapsed: 01:07:09
                               ETA: 00:09:19

################################################################################
                     [1m Learning iteration 1757/2000 [0m                     

                       Computation: 45781 steps/s (collection: 2.054s, learning 0.094s)
             Mean action noise std: 3.29
          Mean value_function loss: 152.0561
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 56.8621
                       Mean reward: 859.46
               Mean episode length: 230.18
    Episode_Reward/reaching_object: 1.3567
     Episode_Reward/lifting_object: 173.6487
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 172818432
                    Iteration time: 2.15s
                      Time elapsed: 01:07:11
                               ETA: 00:09:17

################################################################################
                     [1m Learning iteration 1758/2000 [0m                     

                       Computation: 46412 steps/s (collection: 2.029s, learning 0.089s)
             Mean action noise std: 3.30
          Mean value_function loss: 191.0201
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 56.8674
                       Mean reward: 875.78
               Mean episode length: 232.79
    Episode_Reward/reaching_object: 1.3495
     Episode_Reward/lifting_object: 172.9812
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0549
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 172916736
                    Iteration time: 2.12s
                      Time elapsed: 01:07:13
                               ETA: 00:09:14

################################################################################
                     [1m Learning iteration 1759/2000 [0m                     

                       Computation: 46130 steps/s (collection: 2.042s, learning 0.089s)
             Mean action noise std: 3.30
          Mean value_function loss: 148.2974
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.8745
                       Mean reward: 873.20
               Mean episode length: 233.34
    Episode_Reward/reaching_object: 1.3503
     Episode_Reward/lifting_object: 173.2722
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 173015040
                    Iteration time: 2.13s
                      Time elapsed: 01:07:15
                               ETA: 00:09:12

################################################################################
                     [1m Learning iteration 1760/2000 [0m                     

                       Computation: 46402 steps/s (collection: 2.011s, learning 0.108s)
             Mean action noise std: 3.30
          Mean value_function loss: 129.0209
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.8861
                       Mean reward: 872.15
               Mean episode length: 231.49
    Episode_Reward/reaching_object: 1.3494
     Episode_Reward/lifting_object: 172.2525
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0690
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 173113344
                    Iteration time: 2.12s
                      Time elapsed: 01:07:17
                               ETA: 00:09:10

################################################################################
                     [1m Learning iteration 1761/2000 [0m                     

                       Computation: 45734 steps/s (collection: 2.054s, learning 0.095s)
             Mean action noise std: 3.30
          Mean value_function loss: 101.2399
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 56.8958
                       Mean reward: 862.87
               Mean episode length: 229.98
    Episode_Reward/reaching_object: 1.3545
     Episode_Reward/lifting_object: 173.6266
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.0703
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 173211648
                    Iteration time: 2.15s
                      Time elapsed: 01:07:19
                               ETA: 00:09:07

################################################################################
                     [1m Learning iteration 1762/2000 [0m                     

                       Computation: 46734 steps/s (collection: 2.016s, learning 0.088s)
             Mean action noise std: 3.30
          Mean value_function loss: 98.5553
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 56.9049
                       Mean reward: 893.08
               Mean episode length: 238.06
    Episode_Reward/reaching_object: 1.3703
     Episode_Reward/lifting_object: 175.8999
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0701
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 173309952
                    Iteration time: 2.10s
                      Time elapsed: 01:07:21
                               ETA: 00:09:05

################################################################################
                     [1m Learning iteration 1763/2000 [0m                     

                       Computation: 46990 steps/s (collection: 2.004s, learning 0.088s)
             Mean action noise std: 3.30
          Mean value_function loss: 115.2114
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.9167
                       Mean reward: 890.54
               Mean episode length: 236.17
    Episode_Reward/reaching_object: 1.3403
     Episode_Reward/lifting_object: 171.6142
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0695
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 173408256
                    Iteration time: 2.09s
                      Time elapsed: 01:07:24
                               ETA: 00:09:03

################################################################################
                     [1m Learning iteration 1764/2000 [0m                     

                       Computation: 45945 steps/s (collection: 2.037s, learning 0.102s)
             Mean action noise std: 3.30
          Mean value_function loss: 125.6780
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 56.9245
                       Mean reward: 818.44
               Mean episode length: 219.86
    Episode_Reward/reaching_object: 1.3572
     Episode_Reward/lifting_object: 173.5447
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.0715
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 173506560
                    Iteration time: 2.14s
                      Time elapsed: 01:07:26
                               ETA: 00:09:01

################################################################################
                     [1m Learning iteration 1765/2000 [0m                     

                       Computation: 44027 steps/s (collection: 2.124s, learning 0.109s)
             Mean action noise std: 3.31
          Mean value_function loss: 126.5098
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 56.9324
                       Mean reward: 872.69
               Mean episode length: 233.62
    Episode_Reward/reaching_object: 1.3575
     Episode_Reward/lifting_object: 173.0592
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.0722
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173604864
                    Iteration time: 2.23s
                      Time elapsed: 01:07:28
                               ETA: 00:08:58

################################################################################
                     [1m Learning iteration 1766/2000 [0m                     

                       Computation: 45723 steps/s (collection: 2.059s, learning 0.091s)
             Mean action noise std: 3.31
          Mean value_function loss: 143.0560
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.9470
                       Mean reward: 841.05
               Mean episode length: 225.82
    Episode_Reward/reaching_object: 1.3263
     Episode_Reward/lifting_object: 168.9828
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 173703168
                    Iteration time: 2.15s
                      Time elapsed: 01:07:30
                               ETA: 00:08:56

################################################################################
                     [1m Learning iteration 1767/2000 [0m                     

                       Computation: 45719 steps/s (collection: 2.040s, learning 0.110s)
             Mean action noise std: 3.31
          Mean value_function loss: 115.4261
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 56.9571
                       Mean reward: 897.98
               Mean episode length: 239.41
    Episode_Reward/reaching_object: 1.3703
     Episode_Reward/lifting_object: 174.3471
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0730
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 173801472
                    Iteration time: 2.15s
                      Time elapsed: 01:07:32
                               ETA: 00:08:54

################################################################################
                     [1m Learning iteration 1768/2000 [0m                     

                       Computation: 44961 steps/s (collection: 2.076s, learning 0.111s)
             Mean action noise std: 3.31
          Mean value_function loss: 94.8734
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 56.9642
                       Mean reward: 854.69
               Mean episode length: 227.54
    Episode_Reward/reaching_object: 1.3829
     Episode_Reward/lifting_object: 175.7837
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0739
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 173899776
                    Iteration time: 2.19s
                      Time elapsed: 01:07:34
                               ETA: 00:08:51

################################################################################
                     [1m Learning iteration 1769/2000 [0m                     

                       Computation: 46315 steps/s (collection: 2.033s, learning 0.090s)
             Mean action noise std: 3.31
          Mean value_function loss: 96.7610
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.9679
                       Mean reward: 918.41
               Mean episode length: 243.92
    Episode_Reward/reaching_object: 1.3766
     Episode_Reward/lifting_object: 174.5352
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173998080
                    Iteration time: 2.12s
                      Time elapsed: 01:07:37
                               ETA: 00:08:49

################################################################################
                     [1m Learning iteration 1770/2000 [0m                     

                       Computation: 46376 steps/s (collection: 2.029s, learning 0.091s)
             Mean action noise std: 3.31
          Mean value_function loss: 126.0290
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 56.9730
                       Mean reward: 881.91
               Mean episode length: 235.11
    Episode_Reward/reaching_object: 1.3872
     Episode_Reward/lifting_object: 176.8857
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0735
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 174096384
                    Iteration time: 2.12s
                      Time elapsed: 01:07:39
                               ETA: 00:08:47

################################################################################
                     [1m Learning iteration 1771/2000 [0m                     

                       Computation: 46104 steps/s (collection: 2.043s, learning 0.089s)
             Mean action noise std: 3.31
          Mean value_function loss: 117.4201
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.9793
                       Mean reward: 882.85
               Mean episode length: 235.17
    Episode_Reward/reaching_object: 1.3529
     Episode_Reward/lifting_object: 172.2267
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.0725
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 174194688
                    Iteration time: 2.13s
                      Time elapsed: 01:07:41
                               ETA: 00:08:44

################################################################################
                     [1m Learning iteration 1772/2000 [0m                     

                       Computation: 46069 steps/s (collection: 2.046s, learning 0.088s)
             Mean action noise std: 3.31
          Mean value_function loss: 100.3179
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 56.9851
                       Mean reward: 892.11
               Mean episode length: 237.24
    Episode_Reward/reaching_object: 1.3928
     Episode_Reward/lifting_object: 177.1380
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0737
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 174292992
                    Iteration time: 2.13s
                      Time elapsed: 01:07:43
                               ETA: 00:08:42

################################################################################
                     [1m Learning iteration 1773/2000 [0m                     

                       Computation: 45903 steps/s (collection: 2.040s, learning 0.102s)
             Mean action noise std: 3.32
          Mean value_function loss: 100.0841
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 56.9914
                       Mean reward: 853.01
               Mean episode length: 228.17
    Episode_Reward/reaching_object: 1.3669
     Episode_Reward/lifting_object: 173.7236
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0730
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 174391296
                    Iteration time: 2.14s
                      Time elapsed: 01:07:45
                               ETA: 00:08:40

################################################################################
                     [1m Learning iteration 1774/2000 [0m                     

                       Computation: 46380 steps/s (collection: 2.031s, learning 0.088s)
             Mean action noise std: 3.32
          Mean value_function loss: 87.2808
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 57.0066
                       Mean reward: 901.10
               Mean episode length: 238.33
    Episode_Reward/reaching_object: 1.3846
     Episode_Reward/lifting_object: 176.5859
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0720
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 174489600
                    Iteration time: 2.12s
                      Time elapsed: 01:07:47
                               ETA: 00:08:37

################################################################################
                     [1m Learning iteration 1775/2000 [0m                     

                       Computation: 46670 steps/s (collection: 1.997s, learning 0.110s)
             Mean action noise std: 3.32
          Mean value_function loss: 94.6706
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 57.0178
                       Mean reward: 844.12
               Mean episode length: 226.44
    Episode_Reward/reaching_object: 1.3977
     Episode_Reward/lifting_object: 178.4077
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0717
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 174587904
                    Iteration time: 2.11s
                      Time elapsed: 01:07:49
                               ETA: 00:08:35

################################################################################
                     [1m Learning iteration 1776/2000 [0m                     

                       Computation: 46274 steps/s (collection: 2.016s, learning 0.108s)
             Mean action noise std: 3.32
          Mean value_function loss: 107.7165
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 57.0238
                       Mean reward: 905.76
               Mean episode length: 239.64
    Episode_Reward/reaching_object: 1.3832
     Episode_Reward/lifting_object: 176.4279
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0713
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 174686208
                    Iteration time: 2.12s
                      Time elapsed: 01:07:51
                               ETA: 00:08:33

################################################################################
                     [1m Learning iteration 1777/2000 [0m                     

                       Computation: 46018 steps/s (collection: 2.036s, learning 0.100s)
             Mean action noise std: 3.32
          Mean value_function loss: 107.2265
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 57.0342
                       Mean reward: 868.46
               Mean episode length: 230.95
    Episode_Reward/reaching_object: 1.3545
     Episode_Reward/lifting_object: 172.7682
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 174784512
                    Iteration time: 2.14s
                      Time elapsed: 01:07:54
                               ETA: 00:08:30

################################################################################
                     [1m Learning iteration 1778/2000 [0m                     

                       Computation: 46620 steps/s (collection: 2.023s, learning 0.086s)
             Mean action noise std: 3.32
          Mean value_function loss: 82.0333
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.0463
                       Mean reward: 909.48
               Mean episode length: 239.78
    Episode_Reward/reaching_object: 1.4175
     Episode_Reward/lifting_object: 180.8720
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0577
          Episode_Reward/joint_vel: -0.0708
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 174882816
                    Iteration time: 2.11s
                      Time elapsed: 01:07:56
                               ETA: 00:08:28

################################################################################
                     [1m Learning iteration 1779/2000 [0m                     

                       Computation: 46098 steps/s (collection: 2.023s, learning 0.109s)
             Mean action noise std: 3.32
          Mean value_function loss: 144.3127
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.0557
                       Mean reward: 839.46
               Mean episode length: 225.41
    Episode_Reward/reaching_object: 1.3651
     Episode_Reward/lifting_object: 173.5345
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 174981120
                    Iteration time: 2.13s
                      Time elapsed: 01:07:58
                               ETA: 00:08:26

################################################################################
                     [1m Learning iteration 1780/2000 [0m                     

                       Computation: 46731 steps/s (collection: 2.016s, learning 0.088s)
             Mean action noise std: 3.33
          Mean value_function loss: 144.8546
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 57.0681
                       Mean reward: 859.72
               Mean episode length: 229.33
    Episode_Reward/reaching_object: 1.3667
     Episode_Reward/lifting_object: 173.2637
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 175079424
                    Iteration time: 2.10s
                      Time elapsed: 01:08:00
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 1781/2000 [0m                     

                       Computation: 46612 steps/s (collection: 2.011s, learning 0.098s)
             Mean action noise std: 3.33
          Mean value_function loss: 132.8510
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 57.0798
                       Mean reward: 867.24
               Mean episode length: 232.25
    Episode_Reward/reaching_object: 1.3704
     Episode_Reward/lifting_object: 173.4541
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 175177728
                    Iteration time: 2.11s
                      Time elapsed: 01:08:02
                               ETA: 00:08:21

################################################################################
                     [1m Learning iteration 1782/2000 [0m                     

                       Computation: 46528 steps/s (collection: 2.022s, learning 0.091s)
             Mean action noise std: 3.33
          Mean value_function loss: 102.2699
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 57.0902
                       Mean reward: 887.54
               Mean episode length: 235.31
    Episode_Reward/reaching_object: 1.3725
     Episode_Reward/lifting_object: 174.1304
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0690
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 175276032
                    Iteration time: 2.11s
                      Time elapsed: 01:08:04
                               ETA: 00:08:19

################################################################################
                     [1m Learning iteration 1783/2000 [0m                     

                       Computation: 46123 steps/s (collection: 2.043s, learning 0.089s)
             Mean action noise std: 3.33
          Mean value_function loss: 122.1436
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 57.1082
                       Mean reward: 856.58
               Mean episode length: 228.76
    Episode_Reward/reaching_object: 1.3358
     Episode_Reward/lifting_object: 168.8601
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 175374336
                    Iteration time: 2.13s
                      Time elapsed: 01:08:06
                               ETA: 00:08:17

################################################################################
                     [1m Learning iteration 1784/2000 [0m                     

                       Computation: 45380 steps/s (collection: 2.056s, learning 0.111s)
             Mean action noise std: 3.33
          Mean value_function loss: 109.6959
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.1151
                       Mean reward: 915.78
               Mean episode length: 243.70
    Episode_Reward/reaching_object: 1.4138
     Episode_Reward/lifting_object: 180.0020
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0714
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 175472640
                    Iteration time: 2.17s
                      Time elapsed: 01:08:08
                               ETA: 00:08:14

################################################################################
                     [1m Learning iteration 1785/2000 [0m                     

                       Computation: 46181 steps/s (collection: 2.025s, learning 0.104s)
             Mean action noise std: 3.33
          Mean value_function loss: 96.5847
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 57.1248
                       Mean reward: 891.10
               Mean episode length: 237.19
    Episode_Reward/reaching_object: 1.3839
     Episode_Reward/lifting_object: 174.8817
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0713
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 175570944
                    Iteration time: 2.13s
                      Time elapsed: 01:08:11
                               ETA: 00:08:12

################################################################################
                     [1m Learning iteration 1786/2000 [0m                     

                       Computation: 46862 steps/s (collection: 2.012s, learning 0.086s)
             Mean action noise std: 3.34
          Mean value_function loss: 96.4082
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 57.1309
                       Mean reward: 874.21
               Mean episode length: 233.36
    Episode_Reward/reaching_object: 1.3952
     Episode_Reward/lifting_object: 177.0842
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0572
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 175669248
                    Iteration time: 2.10s
                      Time elapsed: 01:08:13
                               ETA: 00:08:10

################################################################################
                     [1m Learning iteration 1787/2000 [0m                     

                       Computation: 46876 steps/s (collection: 2.010s, learning 0.087s)
             Mean action noise std: 3.34
          Mean value_function loss: 82.0147
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 57.1362
                       Mean reward: 893.95
               Mean episode length: 237.31
    Episode_Reward/reaching_object: 1.4312
     Episode_Reward/lifting_object: 181.6644
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0735
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 175767552
                    Iteration time: 2.10s
                      Time elapsed: 01:08:15
                               ETA: 00:08:07

################################################################################
                     [1m Learning iteration 1788/2000 [0m                     

                       Computation: 45506 steps/s (collection: 2.069s, learning 0.091s)
             Mean action noise std: 3.34
          Mean value_function loss: 95.3421
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 57.1456
                       Mean reward: 853.51
               Mean episode length: 228.53
    Episode_Reward/reaching_object: 1.3866
     Episode_Reward/lifting_object: 175.5258
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0716
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 175865856
                    Iteration time: 2.16s
                      Time elapsed: 01:08:17
                               ETA: 00:08:05

################################################################################
                     [1m Learning iteration 1789/2000 [0m                     

                       Computation: 45456 steps/s (collection: 2.055s, learning 0.108s)
             Mean action noise std: 3.34
          Mean value_function loss: 91.5675
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 57.1566
                       Mean reward: 882.41
               Mean episode length: 236.49
    Episode_Reward/reaching_object: 1.3827
     Episode_Reward/lifting_object: 174.7898
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0572
          Episode_Reward/joint_vel: -0.0728
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 175964160
                    Iteration time: 2.16s
                      Time elapsed: 01:08:19
                               ETA: 00:08:03

################################################################################
                     [1m Learning iteration 1790/2000 [0m                     

                       Computation: 44357 steps/s (collection: 2.116s, learning 0.101s)
             Mean action noise std: 3.34
          Mean value_function loss: 100.4239
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 57.1669
                       Mean reward: 890.34
               Mean episode length: 236.67
    Episode_Reward/reaching_object: 1.3986
     Episode_Reward/lifting_object: 176.9507
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0575
          Episode_Reward/joint_vel: -0.0726
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 176062464
                    Iteration time: 2.22s
                      Time elapsed: 01:08:21
                               ETA: 00:08:00

################################################################################
                     [1m Learning iteration 1791/2000 [0m                     

                       Computation: 45290 steps/s (collection: 2.061s, learning 0.110s)
             Mean action noise std: 3.34
          Mean value_function loss: 124.5748
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.1746
                       Mean reward: 849.58
               Mean episode length: 228.01
    Episode_Reward/reaching_object: 1.3489
     Episode_Reward/lifting_object: 169.5648
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.0715
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 176160768
                    Iteration time: 2.17s
                      Time elapsed: 01:08:23
                               ETA: 00:07:58

################################################################################
                     [1m Learning iteration 1792/2000 [0m                     

                       Computation: 45953 steps/s (collection: 2.054s, learning 0.086s)
             Mean action noise std: 3.34
          Mean value_function loss: 94.5401
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 57.1848
                       Mean reward: 891.77
               Mean episode length: 237.22
    Episode_Reward/reaching_object: 1.3818
     Episode_Reward/lifting_object: 174.1317
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.0727
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 176259072
                    Iteration time: 2.14s
                      Time elapsed: 01:08:26
                               ETA: 00:07:56

################################################################################
                     [1m Learning iteration 1793/2000 [0m                     

                       Computation: 44587 steps/s (collection: 2.119s, learning 0.086s)
             Mean action noise std: 3.35
          Mean value_function loss: 82.9804
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.1948
                       Mean reward: 865.20
               Mean episode length: 230.67
    Episode_Reward/reaching_object: 1.3887
     Episode_Reward/lifting_object: 175.2582
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0721
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 176357376
                    Iteration time: 2.20s
                      Time elapsed: 01:08:28
                               ETA: 00:07:54

################################################################################
                     [1m Learning iteration 1794/2000 [0m                     

                       Computation: 46128 steps/s (collection: 2.027s, learning 0.104s)
             Mean action noise std: 3.35
          Mean value_function loss: 99.4181
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.2054
                       Mean reward: 889.21
               Mean episode length: 236.42
    Episode_Reward/reaching_object: 1.4070
     Episode_Reward/lifting_object: 177.8366
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0727
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 176455680
                    Iteration time: 2.13s
                      Time elapsed: 01:08:30
                               ETA: 00:07:51

################################################################################
                     [1m Learning iteration 1795/2000 [0m                     

                       Computation: 46064 steps/s (collection: 2.042s, learning 0.092s)
             Mean action noise std: 3.35
          Mean value_function loss: 83.6658
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 57.2158
                       Mean reward: 890.17
               Mean episode length: 237.36
    Episode_Reward/reaching_object: 1.4024
     Episode_Reward/lifting_object: 177.1504
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0717
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 176553984
                    Iteration time: 2.13s
                      Time elapsed: 01:08:32
                               ETA: 00:07:49

################################################################################
                     [1m Learning iteration 1796/2000 [0m                     

                       Computation: 46422 steps/s (collection: 2.028s, learning 0.090s)
             Mean action noise std: 3.35
          Mean value_function loss: 77.4939
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 57.2290
                       Mean reward: 904.12
               Mean episode length: 239.28
    Episode_Reward/reaching_object: 1.4094
     Episode_Reward/lifting_object: 178.5340
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0724
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 176652288
                    Iteration time: 2.12s
                      Time elapsed: 01:08:34
                               ETA: 00:07:47

################################################################################
                     [1m Learning iteration 1797/2000 [0m                     

                       Computation: 46754 steps/s (collection: 2.015s, learning 0.088s)
             Mean action noise std: 3.35
          Mean value_function loss: 75.2381
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.2437
                       Mean reward: 911.27
               Mean episode length: 240.89
    Episode_Reward/reaching_object: 1.4048
     Episode_Reward/lifting_object: 177.8550
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0710
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 176750592
                    Iteration time: 2.10s
                      Time elapsed: 01:08:36
                               ETA: 00:07:44

################################################################################
                     [1m Learning iteration 1798/2000 [0m                     

                       Computation: 46339 steps/s (collection: 2.033s, learning 0.089s)
             Mean action noise std: 3.35
          Mean value_function loss: 91.3475
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.2511
                       Mean reward: 917.32
               Mean episode length: 242.41
    Episode_Reward/reaching_object: 1.4181
     Episode_Reward/lifting_object: 179.6644
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0584
          Episode_Reward/joint_vel: -0.0709
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 176848896
                    Iteration time: 2.12s
                      Time elapsed: 01:08:38
                               ETA: 00:07:42

################################################################################
                     [1m Learning iteration 1799/2000 [0m                     

                       Computation: 45313 steps/s (collection: 2.063s, learning 0.107s)
             Mean action noise std: 3.36
          Mean value_function loss: 105.8257
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 57.2582
                       Mean reward: 885.44
               Mean episode length: 234.53
    Episode_Reward/reaching_object: 1.3615
     Episode_Reward/lifting_object: 171.3438
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 176947200
                    Iteration time: 2.17s
                      Time elapsed: 01:08:41
                               ETA: 00:07:40

################################################################################
                     [1m Learning iteration 1800/2000 [0m                     

                       Computation: 45387 steps/s (collection: 2.057s, learning 0.109s)
             Mean action noise std: 3.36
          Mean value_function loss: 160.7825
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.2627
                       Mean reward: 888.09
               Mean episode length: 236.28
    Episode_Reward/reaching_object: 1.3814
     Episode_Reward/lifting_object: 174.0627
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 177045504
                    Iteration time: 2.17s
                      Time elapsed: 01:08:43
                               ETA: 00:07:37

################################################################################
                     [1m Learning iteration 1801/2000 [0m                     

                       Computation: 44690 steps/s (collection: 2.105s, learning 0.095s)
             Mean action noise std: 3.36
          Mean value_function loss: 118.3500
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 57.2725
                       Mean reward: 829.16
               Mean episode length: 223.88
    Episode_Reward/reaching_object: 1.3698
     Episode_Reward/lifting_object: 171.8564
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0690
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 177143808
                    Iteration time: 2.20s
                      Time elapsed: 01:08:45
                               ETA: 00:07:35

################################################################################
                     [1m Learning iteration 1802/2000 [0m                     

                       Computation: 46546 steps/s (collection: 2.013s, learning 0.099s)
             Mean action noise std: 3.36
          Mean value_function loss: 89.4762
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 57.2828
                       Mean reward: 886.31
               Mean episode length: 234.86
    Episode_Reward/reaching_object: 1.4022
     Episode_Reward/lifting_object: 176.5510
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0707
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 177242112
                    Iteration time: 2.11s
                      Time elapsed: 01:08:47
                               ETA: 00:07:33

################################################################################
                     [1m Learning iteration 1803/2000 [0m                     

                       Computation: 46366 steps/s (collection: 2.033s, learning 0.087s)
             Mean action noise std: 3.36
          Mean value_function loss: 117.6959
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 57.2869
                       Mean reward: 864.67
               Mean episode length: 231.31
    Episode_Reward/reaching_object: 1.3689
     Episode_Reward/lifting_object: 172.1564
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0690
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 177340416
                    Iteration time: 2.12s
                      Time elapsed: 01:08:49
                               ETA: 00:07:30

################################################################################
                     [1m Learning iteration 1804/2000 [0m                     

                       Computation: 46580 steps/s (collection: 2.015s, learning 0.096s)
             Mean action noise std: 3.36
          Mean value_function loss: 71.0719
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.2969
                       Mean reward: 871.73
               Mean episode length: 232.36
    Episode_Reward/reaching_object: 1.3912
     Episode_Reward/lifting_object: 175.3739
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0576
          Episode_Reward/joint_vel: -0.0699
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 177438720
                    Iteration time: 2.11s
                      Time elapsed: 01:08:51
                               ETA: 00:07:28

################################################################################
                     [1m Learning iteration 1805/2000 [0m                     

                       Computation: 45697 steps/s (collection: 2.058s, learning 0.093s)
             Mean action noise std: 3.36
          Mean value_function loss: 96.0003
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 57.3081
                       Mean reward: 868.18
               Mean episode length: 230.38
    Episode_Reward/reaching_object: 1.3792
     Episode_Reward/lifting_object: 173.9725
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0699
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 177537024
                    Iteration time: 2.15s
                      Time elapsed: 01:08:53
                               ETA: 00:07:26

################################################################################
                     [1m Learning iteration 1806/2000 [0m                     

                       Computation: 46128 steps/s (collection: 2.035s, learning 0.096s)
             Mean action noise std: 3.37
          Mean value_function loss: 97.4774
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.3176
                       Mean reward: 854.48
               Mean episode length: 228.92
    Episode_Reward/reaching_object: 1.3695
     Episode_Reward/lifting_object: 172.5198
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0692
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 177635328
                    Iteration time: 2.13s
                      Time elapsed: 01:08:56
                               ETA: 00:07:24

################################################################################
                     [1m Learning iteration 1807/2000 [0m                     

                       Computation: 45707 steps/s (collection: 2.028s, learning 0.123s)
             Mean action noise std: 3.37
          Mean value_function loss: 71.6214
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 57.3299
                       Mean reward: 898.30
               Mean episode length: 238.11
    Episode_Reward/reaching_object: 1.4290
     Episode_Reward/lifting_object: 180.0481
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0591
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 177733632
                    Iteration time: 2.15s
                      Time elapsed: 01:08:58
                               ETA: 00:07:21

################################################################################
                     [1m Learning iteration 1808/2000 [0m                     

                       Computation: 46124 steps/s (collection: 2.044s, learning 0.087s)
             Mean action noise std: 3.37
          Mean value_function loss: 85.4853
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 57.3385
                       Mean reward: 862.59
               Mean episode length: 230.45
    Episode_Reward/reaching_object: 1.3892
     Episode_Reward/lifting_object: 174.7740
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0700
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 177831936
                    Iteration time: 2.13s
                      Time elapsed: 01:09:00
                               ETA: 00:07:19

################################################################################
                     [1m Learning iteration 1809/2000 [0m                     

                       Computation: 46986 steps/s (collection: 1.999s, learning 0.094s)
             Mean action noise std: 3.37
          Mean value_function loss: 87.8787
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.3563
                       Mean reward: 889.31
               Mean episode length: 235.96
    Episode_Reward/reaching_object: 1.4040
     Episode_Reward/lifting_object: 176.8718
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0709
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 177930240
                    Iteration time: 2.09s
                      Time elapsed: 01:09:02
                               ETA: 00:07:17

################################################################################
                     [1m Learning iteration 1810/2000 [0m                     

                       Computation: 45805 steps/s (collection: 2.025s, learning 0.121s)
             Mean action noise std: 3.37
          Mean value_function loss: 96.5490
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 57.3719
                       Mean reward: 881.99
               Mean episode length: 234.98
    Episode_Reward/reaching_object: 1.3893
     Episode_Reward/lifting_object: 175.4702
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0582
          Episode_Reward/joint_vel: -0.0701
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 178028544
                    Iteration time: 2.15s
                      Time elapsed: 01:09:04
                               ETA: 00:07:14

################################################################################
                     [1m Learning iteration 1811/2000 [0m                     

                       Computation: 46576 steps/s (collection: 2.015s, learning 0.096s)
             Mean action noise std: 3.38
          Mean value_function loss: 103.0871
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.3823
                       Mean reward: 887.75
               Mean episode length: 236.51
    Episode_Reward/reaching_object: 1.4159
     Episode_Reward/lifting_object: 178.5430
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0704
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 178126848
                    Iteration time: 2.11s
                      Time elapsed: 01:09:06
                               ETA: 00:07:12

################################################################################
                     [1m Learning iteration 1812/2000 [0m                     

                       Computation: 45465 steps/s (collection: 2.065s, learning 0.097s)
             Mean action noise std: 3.38
          Mean value_function loss: 95.8953
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 57.3935
                       Mean reward: 891.77
               Mean episode length: 237.24
    Episode_Reward/reaching_object: 1.4048
     Episode_Reward/lifting_object: 177.9873
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0591
          Episode_Reward/joint_vel: -0.0701
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 178225152
                    Iteration time: 2.16s
                      Time elapsed: 01:09:08
                               ETA: 00:07:10

################################################################################
                     [1m Learning iteration 1813/2000 [0m                     

                       Computation: 46153 steps/s (collection: 2.039s, learning 0.091s)
             Mean action noise std: 3.38
          Mean value_function loss: 125.4713
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 57.4046
                       Mean reward: 847.83
               Mean episode length: 226.35
    Episode_Reward/reaching_object: 1.3746
     Episode_Reward/lifting_object: 173.4684
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 178323456
                    Iteration time: 2.13s
                      Time elapsed: 01:09:10
                               ETA: 00:07:07

################################################################################
                     [1m Learning iteration 1814/2000 [0m                     

                       Computation: 45697 steps/s (collection: 2.022s, learning 0.130s)
             Mean action noise std: 3.38
          Mean value_function loss: 99.7538
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 57.4137
                       Mean reward: 877.73
               Mean episode length: 234.25
    Episode_Reward/reaching_object: 1.3892
     Episode_Reward/lifting_object: 175.2472
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 178421760
                    Iteration time: 2.15s
                      Time elapsed: 01:09:13
                               ETA: 00:07:05

################################################################################
                     [1m Learning iteration 1815/2000 [0m                     

                       Computation: 45138 steps/s (collection: 2.051s, learning 0.127s)
             Mean action noise std: 3.38
          Mean value_function loss: 106.3159
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 57.4175
                       Mean reward: 905.91
               Mean episode length: 239.94
    Episode_Reward/reaching_object: 1.3696
     Episode_Reward/lifting_object: 172.8858
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 178520064
                    Iteration time: 2.18s
                      Time elapsed: 01:09:15
                               ETA: 00:07:03

################################################################################
                     [1m Learning iteration 1816/2000 [0m                     

                       Computation: 45834 steps/s (collection: 2.053s, learning 0.092s)
             Mean action noise std: 3.38
          Mean value_function loss: 123.8881
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.4252
                       Mean reward: 867.70
               Mean episode length: 230.97
    Episode_Reward/reaching_object: 1.3818
     Episode_Reward/lifting_object: 174.4189
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 178618368
                    Iteration time: 2.14s
                      Time elapsed: 01:09:17
                               ETA: 00:07:01

################################################################################
                     [1m Learning iteration 1817/2000 [0m                     

                       Computation: 45837 steps/s (collection: 2.053s, learning 0.092s)
             Mean action noise std: 3.38
          Mean value_function loss: 101.3274
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 57.4370
                       Mean reward: 878.82
               Mean episode length: 233.32
    Episode_Reward/reaching_object: 1.3761
     Episode_Reward/lifting_object: 173.3639
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 178716672
                    Iteration time: 2.14s
                      Time elapsed: 01:09:19
                               ETA: 00:06:58

################################################################################
                     [1m Learning iteration 1818/2000 [0m                     

                       Computation: 45592 steps/s (collection: 2.049s, learning 0.107s)
             Mean action noise std: 3.38
          Mean value_function loss: 110.3994
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 57.4438
                       Mean reward: 874.01
               Mean episode length: 231.77
    Episode_Reward/reaching_object: 1.3891
     Episode_Reward/lifting_object: 174.8312
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 178814976
                    Iteration time: 2.16s
                      Time elapsed: 01:09:21
                               ETA: 00:06:56

################################################################################
                     [1m Learning iteration 1819/2000 [0m                     

                       Computation: 46497 steps/s (collection: 2.025s, learning 0.089s)
             Mean action noise std: 3.39
          Mean value_function loss: 122.8480
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.4512
                       Mean reward: 861.92
               Mean episode length: 231.00
    Episode_Reward/reaching_object: 1.3922
     Episode_Reward/lifting_object: 175.1870
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 178913280
                    Iteration time: 2.11s
                      Time elapsed: 01:09:23
                               ETA: 00:06:54

################################################################################
                     [1m Learning iteration 1820/2000 [0m                     

                       Computation: 46318 steps/s (collection: 2.026s, learning 0.096s)
             Mean action noise std: 3.39
          Mean value_function loss: 109.3326
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 57.4601
                       Mean reward: 873.98
               Mean episode length: 232.28
    Episode_Reward/reaching_object: 1.3918
     Episode_Reward/lifting_object: 174.7378
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 179011584
                    Iteration time: 2.12s
                      Time elapsed: 01:09:25
                               ETA: 00:06:51

################################################################################
                     [1m Learning iteration 1821/2000 [0m                     

                       Computation: 43179 steps/s (collection: 2.143s, learning 0.134s)
             Mean action noise std: 3.39
          Mean value_function loss: 75.8995
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 57.4716
                       Mean reward: 904.90
               Mean episode length: 240.46
    Episode_Reward/reaching_object: 1.4009
     Episode_Reward/lifting_object: 176.0957
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 18.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 179109888
                    Iteration time: 2.28s
                      Time elapsed: 01:09:28
                               ETA: 00:06:49

################################################################################
                     [1m Learning iteration 1822/2000 [0m                     

                       Computation: 42461 steps/s (collection: 2.174s, learning 0.142s)
             Mean action noise std: 3.39
          Mean value_function loss: 116.9846
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.4837
                       Mean reward: 874.80
               Mean episode length: 234.57
    Episode_Reward/reaching_object: 1.3870
     Episode_Reward/lifting_object: 173.2426
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 179208192
                    Iteration time: 2.32s
                      Time elapsed: 01:09:30
                               ETA: 00:06:47

################################################################################
                     [1m Learning iteration 1823/2000 [0m                     

                       Computation: 43711 steps/s (collection: 2.132s, learning 0.117s)
             Mean action noise std: 3.39
          Mean value_function loss: 131.4472
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.4965
                       Mean reward: 899.39
               Mean episode length: 239.64
    Episode_Reward/reaching_object: 1.4189
     Episode_Reward/lifting_object: 177.8458
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0599
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 179306496
                    Iteration time: 2.25s
                      Time elapsed: 01:09:32
                               ETA: 00:06:44

################################################################################
                     [1m Learning iteration 1824/2000 [0m                     

                       Computation: 45898 steps/s (collection: 2.042s, learning 0.100s)
             Mean action noise std: 3.40
          Mean value_function loss: 90.8685
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 57.5115
                       Mean reward: 902.24
               Mean episode length: 239.00
    Episode_Reward/reaching_object: 1.4049
     Episode_Reward/lifting_object: 176.2107
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 179404800
                    Iteration time: 2.14s
                      Time elapsed: 01:09:34
                               ETA: 00:06:42

################################################################################
                     [1m Learning iteration 1825/2000 [0m                     

                       Computation: 45895 steps/s (collection: 2.052s, learning 0.090s)
             Mean action noise std: 3.40
          Mean value_function loss: 131.5467
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 57.5218
                       Mean reward: 870.47
               Mean episode length: 233.48
    Episode_Reward/reaching_object: 1.3845
     Episode_Reward/lifting_object: 173.0580
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 179503104
                    Iteration time: 2.14s
                      Time elapsed: 01:09:37
                               ETA: 00:06:40

################################################################################
                     [1m Learning iteration 1826/2000 [0m                     

                       Computation: 45901 steps/s (collection: 2.051s, learning 0.091s)
             Mean action noise std: 3.40
          Mean value_function loss: 104.8646
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 57.5303
                       Mean reward: 907.55
               Mean episode length: 240.38
    Episode_Reward/reaching_object: 1.3878
     Episode_Reward/lifting_object: 173.5081
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0590
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 179601408
                    Iteration time: 2.14s
                      Time elapsed: 01:09:39
                               ETA: 00:06:38

################################################################################
                     [1m Learning iteration 1827/2000 [0m                     

                       Computation: 45026 steps/s (collection: 2.061s, learning 0.123s)
             Mean action noise std: 3.40
          Mean value_function loss: 108.9410
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.5396
                       Mean reward: 867.76
               Mean episode length: 231.48
    Episode_Reward/reaching_object: 1.4024
     Episode_Reward/lifting_object: 175.9855
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 179699712
                    Iteration time: 2.18s
                      Time elapsed: 01:09:41
                               ETA: 00:06:35

################################################################################
                     [1m Learning iteration 1828/2000 [0m                     

                       Computation: 44955 steps/s (collection: 2.090s, learning 0.097s)
             Mean action noise std: 3.40
          Mean value_function loss: 81.8383
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 57.5458
                       Mean reward: 902.55
               Mean episode length: 239.98
    Episode_Reward/reaching_object: 1.3996
     Episode_Reward/lifting_object: 175.4455
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0596
          Episode_Reward/joint_vel: -0.0694
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 179798016
                    Iteration time: 2.19s
                      Time elapsed: 01:09:43
                               ETA: 00:06:33

################################################################################
                     [1m Learning iteration 1829/2000 [0m                     

                       Computation: 46355 steps/s (collection: 2.029s, learning 0.092s)
             Mean action noise std: 3.40
          Mean value_function loss: 125.1718
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 57.5533
                       Mean reward: 866.39
               Mean episode length: 231.10
    Episode_Reward/reaching_object: 1.3865
     Episode_Reward/lifting_object: 173.4205
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0591
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 179896320
                    Iteration time: 2.12s
                      Time elapsed: 01:09:45
                               ETA: 00:06:31

################################################################################
                     [1m Learning iteration 1830/2000 [0m                     

                       Computation: 43561 steps/s (collection: 2.169s, learning 0.088s)
             Mean action noise std: 3.40
          Mean value_function loss: 124.7620
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.5574
                       Mean reward: 886.91
               Mean episode length: 238.73
    Episode_Reward/reaching_object: 1.4187
     Episode_Reward/lifting_object: 177.8134
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0702
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 179994624
                    Iteration time: 2.26s
                      Time elapsed: 01:09:48
                               ETA: 00:06:28

################################################################################
                     [1m Learning iteration 1831/2000 [0m                     

                       Computation: 44535 steps/s (collection: 2.109s, learning 0.098s)
             Mean action noise std: 3.40
          Mean value_function loss: 107.7023
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 57.5608
                       Mean reward: 847.49
               Mean episode length: 226.79
    Episode_Reward/reaching_object: 1.3930
     Episode_Reward/lifting_object: 174.5926
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0596
          Episode_Reward/joint_vel: -0.0686
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 180092928
                    Iteration time: 2.21s
                      Time elapsed: 01:09:50
                               ETA: 00:06:26

################################################################################
                     [1m Learning iteration 1832/2000 [0m                     

                       Computation: 45547 steps/s (collection: 2.047s, learning 0.112s)
             Mean action noise std: 3.41
          Mean value_function loss: 120.6323
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.5680
                       Mean reward: 855.81
               Mean episode length: 228.83
    Episode_Reward/reaching_object: 1.3734
     Episode_Reward/lifting_object: 172.5964
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 180191232
                    Iteration time: 2.16s
                      Time elapsed: 01:09:52
                               ETA: 00:06:24

################################################################################
                     [1m Learning iteration 1833/2000 [0m                     

                       Computation: 44663 steps/s (collection: 2.073s, learning 0.128s)
             Mean action noise std: 3.41
          Mean value_function loss: 98.7074
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 57.5777
                       Mean reward: 906.73
               Mean episode length: 240.98
    Episode_Reward/reaching_object: 1.4228
     Episode_Reward/lifting_object: 178.6769
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 180289536
                    Iteration time: 2.20s
                      Time elapsed: 01:09:54
                               ETA: 00:06:21

################################################################################
                     [1m Learning iteration 1834/2000 [0m                     

                       Computation: 45737 steps/s (collection: 2.052s, learning 0.098s)
             Mean action noise std: 3.41
          Mean value_function loss: 82.7324
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 57.5838
                       Mean reward: 890.48
               Mean episode length: 237.30
    Episode_Reward/reaching_object: 1.4145
     Episode_Reward/lifting_object: 177.6304
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0604
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 180387840
                    Iteration time: 2.15s
                      Time elapsed: 01:09:56
                               ETA: 00:06:19

################################################################################
                     [1m Learning iteration 1835/2000 [0m                     

                       Computation: 45956 steps/s (collection: 2.037s, learning 0.102s)
             Mean action noise std: 3.41
          Mean value_function loss: 109.4716
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.5870
                       Mean reward: 872.23
               Mean episode length: 232.31
    Episode_Reward/reaching_object: 1.3902
     Episode_Reward/lifting_object: 174.7532
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 180486144
                    Iteration time: 2.14s
                      Time elapsed: 01:09:58
                               ETA: 00:06:17

################################################################################
                     [1m Learning iteration 1836/2000 [0m                     

                       Computation: 45645 steps/s (collection: 2.027s, learning 0.127s)
             Mean action noise std: 3.41
          Mean value_function loss: 480270390.4000
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.5911
                       Mean reward: 911.50
               Mean episode length: 242.49
    Episode_Reward/reaching_object: 1.4158
     Episode_Reward/lifting_object: 178.0763
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -1.8875
          Episode_Reward/joint_vel: -1242.2587
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 180584448
                    Iteration time: 2.15s
                      Time elapsed: 01:10:01
                               ETA: 00:06:15

################################################################################
                     [1m Learning iteration 1837/2000 [0m                     

                       Computation: 46463 steps/s (collection: 2.031s, learning 0.085s)
             Mean action noise std: 3.41
          Mean value_function loss: 93.9852
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.5942
                       Mean reward: 896.34
               Mean episode length: 237.83
    Episode_Reward/reaching_object: 1.4031
     Episode_Reward/lifting_object: 175.7616
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0599
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 180682752
                    Iteration time: 2.12s
                      Time elapsed: 01:10:03
                               ETA: 00:06:12

################################################################################
                     [1m Learning iteration 1838/2000 [0m                     

                       Computation: 46525 steps/s (collection: 2.027s, learning 0.086s)
             Mean action noise std: 3.41
          Mean value_function loss: 123.5013
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 57.5993
                       Mean reward: 846.95
               Mean episode length: 227.40
    Episode_Reward/reaching_object: 1.3733
     Episode_Reward/lifting_object: 172.0426
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 180781056
                    Iteration time: 2.11s
                      Time elapsed: 01:10:05
                               ETA: 00:06:10

################################################################################
                     [1m Learning iteration 1839/2000 [0m                     

                       Computation: 45696 steps/s (collection: 2.056s, learning 0.096s)
             Mean action noise std: 3.41
          Mean value_function loss: 129.2866
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.6057
                       Mean reward: 868.78
               Mean episode length: 230.17
    Episode_Reward/reaching_object: 1.3454
     Episode_Reward/lifting_object: 168.5809
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0575
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 180879360
                    Iteration time: 2.15s
                      Time elapsed: 01:10:07
                               ETA: 00:06:08

################################################################################
                     [1m Learning iteration 1840/2000 [0m                     

                       Computation: 45131 steps/s (collection: 2.048s, learning 0.131s)
             Mean action noise std: 3.41
          Mean value_function loss: 141.9931
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.6153
                       Mean reward: 852.88
               Mean episode length: 229.34
    Episode_Reward/reaching_object: 1.3661
     Episode_Reward/lifting_object: 171.1585
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 180977664
                    Iteration time: 2.18s
                      Time elapsed: 01:10:09
                               ETA: 00:06:05

################################################################################
                     [1m Learning iteration 1841/2000 [0m                     

                       Computation: 45838 steps/s (collection: 2.055s, learning 0.090s)
             Mean action noise std: 3.41
          Mean value_function loss: 132.4140
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 57.6224
                       Mean reward: 890.76
               Mean episode length: 236.90
    Episode_Reward/reaching_object: 1.3642
     Episode_Reward/lifting_object: 171.8017
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 181075968
                    Iteration time: 2.14s
                      Time elapsed: 01:10:11
                               ETA: 00:06:03

################################################################################
                     [1m Learning iteration 1842/2000 [0m                     

                       Computation: 45172 steps/s (collection: 2.077s, learning 0.100s)
             Mean action noise std: 3.42
          Mean value_function loss: 120.6451
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 57.6313
                       Mean reward: 864.32
               Mean episode length: 231.27
    Episode_Reward/reaching_object: 1.3667
     Episode_Reward/lifting_object: 170.3600
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 181174272
                    Iteration time: 2.18s
                      Time elapsed: 01:10:13
                               ETA: 00:06:01

################################################################################
                     [1m Learning iteration 1843/2000 [0m                     

                       Computation: 46469 steps/s (collection: 2.026s, learning 0.089s)
             Mean action noise std: 3.42
          Mean value_function loss: 162.6797
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 57.6366
                       Mean reward: 794.27
               Mean episode length: 215.33
    Episode_Reward/reaching_object: 1.3423
     Episode_Reward/lifting_object: 168.0337
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0577
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 181272576
                    Iteration time: 2.12s
                      Time elapsed: 01:10:16
                               ETA: 00:05:58

################################################################################
                     [1m Learning iteration 1844/2000 [0m                     

                       Computation: 47020 steps/s (collection: 1.999s, learning 0.092s)
             Mean action noise std: 3.42
          Mean value_function loss: 103.1832
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 57.6392
                       Mean reward: 901.14
               Mean episode length: 239.08
    Episode_Reward/reaching_object: 1.3587
     Episode_Reward/lifting_object: 170.6223
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0582
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 181370880
                    Iteration time: 2.09s
                      Time elapsed: 01:10:18
                               ETA: 00:05:56

################################################################################
                     [1m Learning iteration 1845/2000 [0m                     

                       Computation: 45464 steps/s (collection: 2.067s, learning 0.095s)
             Mean action noise std: 3.42
          Mean value_function loss: 119.3453
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.6450
                       Mean reward: 866.65
               Mean episode length: 230.82
    Episode_Reward/reaching_object: 1.3836
     Episode_Reward/lifting_object: 174.1520
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 181469184
                    Iteration time: 2.16s
                      Time elapsed: 01:10:20
                               ETA: 00:05:54

################################################################################
                     [1m Learning iteration 1846/2000 [0m                     

                       Computation: 45707 steps/s (collection: 2.029s, learning 0.122s)
             Mean action noise std: 3.42
          Mean value_function loss: 139.7631
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.6588
                       Mean reward: 835.24
               Mean episode length: 223.49
    Episode_Reward/reaching_object: 1.3885
     Episode_Reward/lifting_object: 174.3193
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 181567488
                    Iteration time: 2.15s
                      Time elapsed: 01:10:22
                               ETA: 00:05:52

################################################################################
                     [1m Learning iteration 1847/2000 [0m                     

                       Computation: 45535 steps/s (collection: 2.050s, learning 0.109s)
             Mean action noise std: 3.42
          Mean value_function loss: 136.6844
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 57.6770
                       Mean reward: 858.50
               Mean episode length: 229.36
    Episode_Reward/reaching_object: 1.3662
     Episode_Reward/lifting_object: 171.3954
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 181665792
                    Iteration time: 2.16s
                      Time elapsed: 01:10:24
                               ETA: 00:05:49

################################################################################
                     [1m Learning iteration 1848/2000 [0m                     

                       Computation: 43601 steps/s (collection: 2.127s, learning 0.128s)
             Mean action noise std: 3.43
          Mean value_function loss: 118.9556
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 57.6936
                       Mean reward: 864.02
               Mean episode length: 230.38
    Episode_Reward/reaching_object: 1.3748
     Episode_Reward/lifting_object: 172.6936
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 181764096
                    Iteration time: 2.25s
                      Time elapsed: 01:10:26
                               ETA: 00:05:47

################################################################################
                     [1m Learning iteration 1849/2000 [0m                     

                       Computation: 45623 steps/s (collection: 2.043s, learning 0.112s)
             Mean action noise std: 3.43
          Mean value_function loss: 127.1637
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 57.7054
                       Mean reward: 875.20
               Mean episode length: 237.68
    Episode_Reward/reaching_object: 1.3791
     Episode_Reward/lifting_object: 172.1991
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 181862400
                    Iteration time: 2.15s
                      Time elapsed: 01:10:28
                               ETA: 00:05:45

################################################################################
                     [1m Learning iteration 1850/2000 [0m                     

                       Computation: 46136 steps/s (collection: 2.045s, learning 0.086s)
             Mean action noise std: 3.43
          Mean value_function loss: 110.3254
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 57.7162
                       Mean reward: 894.85
               Mean episode length: 236.96
    Episode_Reward/reaching_object: 1.3829
     Episode_Reward/lifting_object: 173.6870
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 181960704
                    Iteration time: 2.13s
                      Time elapsed: 01:10:31
                               ETA: 00:05:42

################################################################################
                     [1m Learning iteration 1851/2000 [0m                     

                       Computation: 45093 steps/s (collection: 2.070s, learning 0.110s)
             Mean action noise std: 3.43
          Mean value_function loss: 108.1276
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 57.7232
                       Mean reward: 913.99
               Mean episode length: 241.90
    Episode_Reward/reaching_object: 1.4121
     Episode_Reward/lifting_object: 177.4045
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0603
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 182059008
                    Iteration time: 2.18s
                      Time elapsed: 01:10:33
                               ETA: 00:05:40

################################################################################
                     [1m Learning iteration 1852/2000 [0m                     

                       Computation: 44826 steps/s (collection: 2.069s, learning 0.124s)
             Mean action noise std: 3.43
          Mean value_function loss: 107.0939
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 57.7253
                       Mean reward: 895.00
               Mean episode length: 237.20
    Episode_Reward/reaching_object: 1.4068
     Episode_Reward/lifting_object: 177.0048
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0604
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 182157312
                    Iteration time: 2.19s
                      Time elapsed: 01:10:35
                               ETA: 00:05:38

################################################################################
                     [1m Learning iteration 1853/2000 [0m                     

                       Computation: 45265 steps/s (collection: 2.076s, learning 0.096s)
             Mean action noise std: 3.43
          Mean value_function loss: 131.7141
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 57.7283
                       Mean reward: 851.80
               Mean episode length: 226.87
    Episode_Reward/reaching_object: 1.3581
     Episode_Reward/lifting_object: 170.4788
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 182255616
                    Iteration time: 2.17s
                      Time elapsed: 01:10:37
                               ETA: 00:05:35

################################################################################
                     [1m Learning iteration 1854/2000 [0m                     

                       Computation: 44574 steps/s (collection: 2.114s, learning 0.091s)
             Mean action noise std: 3.43
          Mean value_function loss: 166.3476
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 57.7366
                       Mean reward: 831.31
               Mean episode length: 222.36
    Episode_Reward/reaching_object: 1.3590
     Episode_Reward/lifting_object: 170.1488
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 182353920
                    Iteration time: 2.21s
                      Time elapsed: 01:10:39
                               ETA: 00:05:33

################################################################################
                     [1m Learning iteration 1855/2000 [0m                     

                       Computation: 44968 steps/s (collection: 2.096s, learning 0.091s)
             Mean action noise std: 3.43
          Mean value_function loss: 132.9794
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 57.7441
                       Mean reward: 858.67
               Mean episode length: 228.47
    Episode_Reward/reaching_object: 1.3477
     Episode_Reward/lifting_object: 168.0145
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0584
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 182452224
                    Iteration time: 2.19s
                      Time elapsed: 01:10:42
                               ETA: 00:05:31

################################################################################
                     [1m Learning iteration 1856/2000 [0m                     

                       Computation: 43885 steps/s (collection: 2.138s, learning 0.102s)
             Mean action noise std: 3.43
          Mean value_function loss: 124.0263
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 57.7509
                       Mean reward: 887.25
               Mean episode length: 236.17
    Episode_Reward/reaching_object: 1.3670
     Episode_Reward/lifting_object: 171.3929
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 182550528
                    Iteration time: 2.24s
                      Time elapsed: 01:10:44
                               ETA: 00:05:29

################################################################################
                     [1m Learning iteration 1857/2000 [0m                     

                       Computation: 44469 steps/s (collection: 2.094s, learning 0.117s)
             Mean action noise std: 3.44
          Mean value_function loss: 104.5164
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 57.7648
                       Mean reward: 828.69
               Mean episode length: 224.06
    Episode_Reward/reaching_object: 1.3520
     Episode_Reward/lifting_object: 169.4603
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 182648832
                    Iteration time: 2.21s
                      Time elapsed: 01:10:46
                               ETA: 00:05:26

################################################################################
                     [1m Learning iteration 1858/2000 [0m                     

                       Computation: 44977 steps/s (collection: 2.088s, learning 0.098s)
             Mean action noise std: 3.44
          Mean value_function loss: 113.9694
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 57.7809
                       Mean reward: 861.00
               Mean episode length: 228.41
    Episode_Reward/reaching_object: 1.3523
     Episode_Reward/lifting_object: 169.6857
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 182747136
                    Iteration time: 2.19s
                      Time elapsed: 01:10:48
                               ETA: 00:05:24

################################################################################
                     [1m Learning iteration 1859/2000 [0m                     

                       Computation: 45197 steps/s (collection: 2.080s, learning 0.095s)
             Mean action noise std: 3.44
          Mean value_function loss: 139.6687
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 57.7897
                       Mean reward: 904.28
               Mean episode length: 240.30
    Episode_Reward/reaching_object: 1.4047
     Episode_Reward/lifting_object: 176.6756
      Episode_Reward/object_height: 0.0284
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182845440
                    Iteration time: 2.17s
                      Time elapsed: 01:10:50
                               ETA: 00:05:22

################################################################################
                     [1m Learning iteration 1860/2000 [0m                     

                       Computation: 44800 steps/s (collection: 2.103s, learning 0.092s)
             Mean action noise std: 3.44
          Mean value_function loss: 3522167244.8000
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 57.7922
                       Mean reward: 894.32
               Mean episode length: 238.60
    Episode_Reward/reaching_object: 1.3708
     Episode_Reward/lifting_object: 172.2714
      Episode_Reward/object_height: 0.0282
        Episode_Reward/action_rate: -13.3901
          Episode_Reward/joint_vel: -2920.9644
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182943744
                    Iteration time: 2.19s
                      Time elapsed: 01:10:53
                               ETA: 00:05:19

################################################################################
                     [1m Learning iteration 1861/2000 [0m                     

                       Computation: 44990 steps/s (collection: 2.078s, learning 0.107s)
             Mean action noise std: 3.44
          Mean value_function loss: 145.3746
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.7950
                       Mean reward: 817.76
               Mean episode length: 220.66
    Episode_Reward/reaching_object: 1.3611
     Episode_Reward/lifting_object: 170.8644
      Episode_Reward/object_height: 0.0281
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 183042048
                    Iteration time: 2.18s
                      Time elapsed: 01:10:55
                               ETA: 00:05:17

################################################################################
                     [1m Learning iteration 1862/2000 [0m                     

                       Computation: 45027 steps/s (collection: 2.085s, learning 0.099s)
             Mean action noise std: 3.44
          Mean value_function loss: 154.5932
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.8062
                       Mean reward: 868.85
               Mean episode length: 230.76
    Episode_Reward/reaching_object: 1.3583
     Episode_Reward/lifting_object: 170.5885
      Episode_Reward/object_height: 0.0282
        Episode_Reward/action_rate: -0.0591
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 183140352
                    Iteration time: 2.18s
                      Time elapsed: 01:10:57
                               ETA: 00:05:15

################################################################################
                     [1m Learning iteration 1863/2000 [0m                     

                       Computation: 44136 steps/s (collection: 2.091s, learning 0.136s)
             Mean action noise std: 3.45
          Mean value_function loss: 135.9899
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 57.8203
                       Mean reward: 853.31
               Mean episode length: 227.92
    Episode_Reward/reaching_object: 1.3973
     Episode_Reward/lifting_object: 175.4155
      Episode_Reward/object_height: 0.0290
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 183238656
                    Iteration time: 2.23s
                      Time elapsed: 01:10:59
                               ETA: 00:05:13

################################################################################
                     [1m Learning iteration 1864/2000 [0m                     

                       Computation: 45498 steps/s (collection: 2.066s, learning 0.095s)
             Mean action noise std: 3.45
          Mean value_function loss: 161.2726
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 57.8287
                       Mean reward: 860.85
               Mean episode length: 228.29
    Episode_Reward/reaching_object: 1.3344
     Episode_Reward/lifting_object: 167.4427
      Episode_Reward/object_height: 0.0279
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 183336960
                    Iteration time: 2.16s
                      Time elapsed: 01:11:01
                               ETA: 00:05:10

################################################################################
                     [1m Learning iteration 1865/2000 [0m                     

                       Computation: 44599 steps/s (collection: 2.106s, learning 0.098s)
             Mean action noise std: 3.45
          Mean value_function loss: 127.4109
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 57.8349
                       Mean reward: 890.17
               Mean episode length: 235.35
    Episode_Reward/reaching_object: 1.3791
     Episode_Reward/lifting_object: 173.0219
      Episode_Reward/object_height: 0.0284
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 183435264
                    Iteration time: 2.20s
                      Time elapsed: 01:11:04
                               ETA: 00:05:08

################################################################################
                     [1m Learning iteration 1866/2000 [0m                     

                       Computation: 44589 steps/s (collection: 2.087s, learning 0.118s)
             Mean action noise std: 3.45
          Mean value_function loss: 178.3522
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.8456
                       Mean reward: 863.29
               Mean episode length: 229.57
    Episode_Reward/reaching_object: 1.3523
     Episode_Reward/lifting_object: 169.9605
      Episode_Reward/object_height: 0.0278
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 183533568
                    Iteration time: 2.20s
                      Time elapsed: 01:11:06
                               ETA: 00:05:06

################################################################################
                     [1m Learning iteration 1867/2000 [0m                     

                       Computation: 44620 steps/s (collection: 2.110s, learning 0.093s)
             Mean action noise std: 3.45
          Mean value_function loss: 133.1933
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 57.8528
                       Mean reward: 857.11
               Mean episode length: 228.25
    Episode_Reward/reaching_object: 1.3296
     Episode_Reward/lifting_object: 166.9591
      Episode_Reward/object_height: 0.0271
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 183631872
                    Iteration time: 2.20s
                      Time elapsed: 01:11:08
                               ETA: 00:05:03

################################################################################
                     [1m Learning iteration 1868/2000 [0m                     

                       Computation: 44401 steps/s (collection: 2.102s, learning 0.112s)
             Mean action noise std: 3.45
          Mean value_function loss: 140.0993
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.8609
                       Mean reward: 892.68
               Mean episode length: 237.37
    Episode_Reward/reaching_object: 1.3909
     Episode_Reward/lifting_object: 174.9802
      Episode_Reward/object_height: 0.0277
        Episode_Reward/action_rate: -0.0604
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 183730176
                    Iteration time: 2.21s
                      Time elapsed: 01:11:10
                               ETA: 00:05:01

################################################################################
                     [1m Learning iteration 1869/2000 [0m                     

                       Computation: 44331 steps/s (collection: 2.126s, learning 0.092s)
             Mean action noise std: 3.45
          Mean value_function loss: 116.3066
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.8690
                       Mean reward: 846.55
               Mean episode length: 225.51
    Episode_Reward/reaching_object: 1.3724
     Episode_Reward/lifting_object: 172.4359
      Episode_Reward/object_height: 0.0272
        Episode_Reward/action_rate: -0.0599
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 183828480
                    Iteration time: 2.22s
                      Time elapsed: 01:11:12
                               ETA: 00:04:59

################################################################################
                     [1m Learning iteration 1870/2000 [0m                     

                       Computation: 44892 steps/s (collection: 2.098s, learning 0.092s)
             Mean action noise std: 3.46
          Mean value_function loss: 90.6452
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.8773
                       Mean reward: 869.19
               Mean episode length: 231.24
    Episode_Reward/reaching_object: 1.4034
     Episode_Reward/lifting_object: 175.4096
      Episode_Reward/object_height: 0.0278
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 183926784
                    Iteration time: 2.19s
                      Time elapsed: 01:11:15
                               ETA: 00:04:57

################################################################################
                     [1m Learning iteration 1871/2000 [0m                     

                       Computation: 44854 steps/s (collection: 2.099s, learning 0.093s)
             Mean action noise std: 3.46
          Mean value_function loss: 126.2263
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 57.8923
                       Mean reward: 852.95
               Mean episode length: 227.75
    Episode_Reward/reaching_object: 1.3784
     Episode_Reward/lifting_object: 171.9385
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 184025088
                    Iteration time: 2.19s
                      Time elapsed: 01:11:17
                               ETA: 00:04:54

################################################################################
                     [1m Learning iteration 1872/2000 [0m                     

                       Computation: 44954 steps/s (collection: 2.089s, learning 0.098s)
             Mean action noise std: 3.46
          Mean value_function loss: 96.5073
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 57.9044
                       Mean reward: 869.42
               Mean episode length: 232.12
    Episode_Reward/reaching_object: 1.3685
     Episode_Reward/lifting_object: 171.3309
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0599
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 184123392
                    Iteration time: 2.19s
                      Time elapsed: 01:11:19
                               ETA: 00:04:52

################################################################################
                     [1m Learning iteration 1873/2000 [0m                     

                       Computation: 42597 steps/s (collection: 2.152s, learning 0.156s)
             Mean action noise std: 3.46
          Mean value_function loss: 95.0417
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.9150
                       Mean reward: 904.79
               Mean episode length: 240.05
    Episode_Reward/reaching_object: 1.4009
     Episode_Reward/lifting_object: 175.7004
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0612
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 184221696
                    Iteration time: 2.31s
                      Time elapsed: 01:11:21
                               ETA: 00:04:50

################################################################################
                     [1m Learning iteration 1874/2000 [0m                     

                       Computation: 45026 steps/s (collection: 2.083s, learning 0.100s)
             Mean action noise std: 3.46
          Mean value_function loss: 100.9532
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 57.9224
                       Mean reward: 903.53
               Mean episode length: 240.29
    Episode_Reward/reaching_object: 1.3956
     Episode_Reward/lifting_object: 175.0638
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0608
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 184320000
                    Iteration time: 2.18s
                      Time elapsed: 01:11:23
                               ETA: 00:04:47

################################################################################
                     [1m Learning iteration 1875/2000 [0m                     

                       Computation: 43139 steps/s (collection: 2.165s, learning 0.114s)
             Mean action noise std: 3.46
          Mean value_function loss: 116.3324
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 57.9305
                       Mean reward: 862.43
               Mean episode length: 229.69
    Episode_Reward/reaching_object: 1.3917
     Episode_Reward/lifting_object: 175.1978
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0607
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 184418304
                    Iteration time: 2.28s
                      Time elapsed: 01:11:26
                               ETA: 00:04:45

################################################################################
                     [1m Learning iteration 1876/2000 [0m                     

                       Computation: 44625 steps/s (collection: 2.104s, learning 0.099s)
             Mean action noise std: 3.47
          Mean value_function loss: 111.5111
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.9406
                       Mean reward: 897.79
               Mean episode length: 238.23
    Episode_Reward/reaching_object: 1.3916
     Episode_Reward/lifting_object: 174.6510
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 184516608
                    Iteration time: 2.20s
                      Time elapsed: 01:11:28
                               ETA: 00:04:43

################################################################################
                     [1m Learning iteration 1877/2000 [0m                     

                       Computation: 45235 steps/s (collection: 2.082s, learning 0.091s)
             Mean action noise std: 3.47
          Mean value_function loss: 106.6318
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 57.9502
                       Mean reward: 882.97
               Mean episode length: 235.15
    Episode_Reward/reaching_object: 1.3752
     Episode_Reward/lifting_object: 172.7424
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0603
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 184614912
                    Iteration time: 2.17s
                      Time elapsed: 01:11:30
                               ETA: 00:04:41

################################################################################
                     [1m Learning iteration 1878/2000 [0m                     

                       Computation: 42861 steps/s (collection: 2.126s, learning 0.168s)
             Mean action noise std: 3.47
          Mean value_function loss: 109.7430
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.9579
                       Mean reward: 840.98
               Mean episode length: 225.78
    Episode_Reward/reaching_object: 1.3860
     Episode_Reward/lifting_object: 174.0297
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0608
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 184713216
                    Iteration time: 2.29s
                      Time elapsed: 01:11:32
                               ETA: 00:04:38

################################################################################
                     [1m Learning iteration 1879/2000 [0m                     

                       Computation: 44833 steps/s (collection: 2.086s, learning 0.107s)
             Mean action noise std: 3.47
          Mean value_function loss: 94.1943
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 57.9641
                       Mean reward: 860.17
               Mean episode length: 229.33
    Episode_Reward/reaching_object: 1.3825
     Episode_Reward/lifting_object: 173.8636
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0607
          Episode_Reward/joint_vel: -0.0670
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 184811520
                    Iteration time: 2.19s
                      Time elapsed: 01:11:35
                               ETA: 00:04:36

################################################################################
                     [1m Learning iteration 1880/2000 [0m                     

                       Computation: 43464 steps/s (collection: 2.165s, learning 0.097s)
             Mean action noise std: 3.47
          Mean value_function loss: 110.8562
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.9727
                       Mean reward: 904.45
               Mean episode length: 241.04
    Episode_Reward/reaching_object: 1.3977
     Episode_Reward/lifting_object: 175.4014
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0613
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 184909824
                    Iteration time: 2.26s
                      Time elapsed: 01:11:37
                               ETA: 00:04:34

################################################################################
                     [1m Learning iteration 1881/2000 [0m                     

                       Computation: 44268 steps/s (collection: 2.121s, learning 0.100s)
             Mean action noise std: 3.47
          Mean value_function loss: 104.9488
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.9806
                       Mean reward: 838.17
               Mean episode length: 226.59
    Episode_Reward/reaching_object: 1.3806
     Episode_Reward/lifting_object: 172.8492
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0608
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 185008128
                    Iteration time: 2.22s
                      Time elapsed: 01:11:39
                               ETA: 00:04:31

################################################################################
                     [1m Learning iteration 1882/2000 [0m                     

                       Computation: 44157 steps/s (collection: 2.132s, learning 0.094s)
             Mean action noise std: 3.47
          Mean value_function loss: 93.4113
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 57.9897
                       Mean reward: 867.63
               Mean episode length: 231.11
    Episode_Reward/reaching_object: 1.3880
     Episode_Reward/lifting_object: 174.5111
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 185106432
                    Iteration time: 2.23s
                      Time elapsed: 01:11:41
                               ETA: 00:04:29

################################################################################
                     [1m Learning iteration 1883/2000 [0m                     

                       Computation: 45152 steps/s (collection: 2.074s, learning 0.104s)
             Mean action noise std: 3.47
          Mean value_function loss: 100.4525
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 57.9984
                       Mean reward: 893.68
               Mean episode length: 236.95
    Episode_Reward/reaching_object: 1.4118
     Episode_Reward/lifting_object: 177.6434
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 185204736
                    Iteration time: 2.18s
                      Time elapsed: 01:11:43
                               ETA: 00:04:27

################################################################################
                     [1m Learning iteration 1884/2000 [0m                     

                       Computation: 43602 steps/s (collection: 2.107s, learning 0.148s)
             Mean action noise std: 3.47
          Mean value_function loss: 108.5765
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 58.0022
                       Mean reward: 901.31
               Mean episode length: 240.25
    Episode_Reward/reaching_object: 1.3985
     Episode_Reward/lifting_object: 175.2162
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0614
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 185303040
                    Iteration time: 2.25s
                      Time elapsed: 01:11:46
                               ETA: 00:04:24

################################################################################
                     [1m Learning iteration 1885/2000 [0m                     

                       Computation: 43589 steps/s (collection: 2.132s, learning 0.124s)
             Mean action noise std: 3.48
          Mean value_function loss: 100.0121
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 58.0049
                       Mean reward: 868.37
               Mean episode length: 231.68
    Episode_Reward/reaching_object: 1.3984
     Episode_Reward/lifting_object: 175.4519
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0614
          Episode_Reward/joint_vel: -0.0690
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 185401344
                    Iteration time: 2.26s
                      Time elapsed: 01:11:48
                               ETA: 00:04:22

################################################################################
                     [1m Learning iteration 1886/2000 [0m                     

                       Computation: 44475 steps/s (collection: 2.114s, learning 0.096s)
             Mean action noise std: 3.48
          Mean value_function loss: 86.1524
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 58.0088
                       Mean reward: 899.21
               Mean episode length: 238.98
    Episode_Reward/reaching_object: 1.3996
     Episode_Reward/lifting_object: 175.8029
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0615
          Episode_Reward/joint_vel: -0.0700
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 185499648
                    Iteration time: 2.21s
                      Time elapsed: 01:11:50
                               ETA: 00:04:20

################################################################################
                     [1m Learning iteration 1887/2000 [0m                     

                       Computation: 44678 steps/s (collection: 2.095s, learning 0.105s)
             Mean action noise std: 3.48
          Mean value_function loss: 85.1584
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 58.0166
                       Mean reward: 912.68
               Mean episode length: 241.85
    Episode_Reward/reaching_object: 1.4123
     Episode_Reward/lifting_object: 177.8285
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 185597952
                    Iteration time: 2.20s
                      Time elapsed: 01:11:52
                               ETA: 00:04:18

################################################################################
                     [1m Learning iteration 1888/2000 [0m                     

                       Computation: 42093 steps/s (collection: 2.169s, learning 0.166s)
             Mean action noise std: 3.48
          Mean value_function loss: 64.6237
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 58.0254
                       Mean reward: 911.80
               Mean episode length: 242.64
    Episode_Reward/reaching_object: 1.4059
     Episode_Reward/lifting_object: 176.7149
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0700
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 185696256
                    Iteration time: 2.34s
                      Time elapsed: 01:11:55
                               ETA: 00:04:15

################################################################################
                     [1m Learning iteration 1889/2000 [0m                     

                       Computation: 44267 steps/s (collection: 2.125s, learning 0.096s)
             Mean action noise std: 3.48
          Mean value_function loss: 102.8583
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.0331
                       Mean reward: 912.39
               Mean episode length: 241.57
    Episode_Reward/reaching_object: 1.3956
     Episode_Reward/lifting_object: 176.0505
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 185794560
                    Iteration time: 2.22s
                      Time elapsed: 01:11:57
                               ETA: 00:04:13

################################################################################
                     [1m Learning iteration 1890/2000 [0m                     

                       Computation: 42617 steps/s (collection: 2.189s, learning 0.118s)
             Mean action noise std: 3.48
          Mean value_function loss: 74.6753
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 58.0396
                       Mean reward: 901.08
               Mean episode length: 238.25
    Episode_Reward/reaching_object: 1.4171
     Episode_Reward/lifting_object: 179.0779
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0626
          Episode_Reward/joint_vel: -0.0702
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 185892864
                    Iteration time: 2.31s
                      Time elapsed: 01:11:59
                               ETA: 00:04:11

################################################################################
                     [1m Learning iteration 1891/2000 [0m                     

                       Computation: 44300 steps/s (collection: 2.125s, learning 0.094s)
             Mean action noise std: 3.48
          Mean value_function loss: 110.0355
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 58.0495
                       Mean reward: 871.12
               Mean episode length: 231.81
    Episode_Reward/reaching_object: 1.3983
     Episode_Reward/lifting_object: 176.2809
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 185991168
                    Iteration time: 2.22s
                      Time elapsed: 01:12:01
                               ETA: 00:04:08

################################################################################
                     [1m Learning iteration 1892/2000 [0m                     

                       Computation: 43895 steps/s (collection: 2.121s, learning 0.119s)
             Mean action noise std: 3.48
          Mean value_function loss: 83.6915
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 58.0580
                       Mean reward: 900.40
               Mean episode length: 237.81
    Episode_Reward/reaching_object: 1.3950
     Episode_Reward/lifting_object: 175.5214
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 186089472
                    Iteration time: 2.24s
                      Time elapsed: 01:12:04
                               ETA: 00:04:06

################################################################################
                     [1m Learning iteration 1893/2000 [0m                     

                       Computation: 43543 steps/s (collection: 2.159s, learning 0.099s)
             Mean action noise std: 3.49
          Mean value_function loss: 84.6288
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 58.0646
                       Mean reward: 888.01
               Mean episode length: 235.44
    Episode_Reward/reaching_object: 1.4082
     Episode_Reward/lifting_object: 177.8523
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0625
          Episode_Reward/joint_vel: -0.0707
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 186187776
                    Iteration time: 2.26s
                      Time elapsed: 01:12:06
                               ETA: 00:04:04

################################################################################
                     [1m Learning iteration 1894/2000 [0m                     

                       Computation: 43927 steps/s (collection: 2.138s, learning 0.100s)
             Mean action noise std: 3.49
          Mean value_function loss: 87.0710
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 58.0739
                       Mean reward: 884.64
               Mean episode length: 236.38
    Episode_Reward/reaching_object: 1.3982
     Episode_Reward/lifting_object: 176.3263
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0622
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 186286080
                    Iteration time: 2.24s
                      Time elapsed: 01:12:08
                               ETA: 00:04:02

################################################################################
                     [1m Learning iteration 1895/2000 [0m                     

                       Computation: 44792 steps/s (collection: 2.097s, learning 0.098s)
             Mean action noise std: 3.49
          Mean value_function loss: 101.7110
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.0856
                       Mean reward: 909.29
               Mean episode length: 241.16
    Episode_Reward/reaching_object: 1.3845
     Episode_Reward/lifting_object: 174.4474
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0614
          Episode_Reward/joint_vel: -0.0693
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 186384384
                    Iteration time: 2.19s
                      Time elapsed: 01:12:10
                               ETA: 00:03:59

################################################################################
                     [1m Learning iteration 1896/2000 [0m                     

                       Computation: 43716 steps/s (collection: 2.151s, learning 0.097s)
             Mean action noise std: 3.49
          Mean value_function loss: 94.3788
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.0950
                       Mean reward: 876.77
               Mean episode length: 235.45
    Episode_Reward/reaching_object: 1.4212
     Episode_Reward/lifting_object: 179.1961
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 18.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 186482688
                    Iteration time: 2.25s
                      Time elapsed: 01:12:13
                               ETA: 00:03:57

################################################################################
                     [1m Learning iteration 1897/2000 [0m                     

                       Computation: 44097 steps/s (collection: 2.135s, learning 0.094s)
             Mean action noise std: 3.49
          Mean value_function loss: 80.8338
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.1024
                       Mean reward: 895.97
               Mean episode length: 238.42
    Episode_Reward/reaching_object: 1.4140
     Episode_Reward/lifting_object: 177.7551
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0704
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 186580992
                    Iteration time: 2.23s
                      Time elapsed: 01:12:15
                               ETA: 00:03:55

################################################################################
                     [1m Learning iteration 1898/2000 [0m                     

                       Computation: 44257 steps/s (collection: 2.124s, learning 0.097s)
             Mean action noise std: 3.49
          Mean value_function loss: 84.2248
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 58.1099
                       Mean reward: 923.65
               Mean episode length: 244.15
    Episode_Reward/reaching_object: 1.4013
     Episode_Reward/lifting_object: 175.7299
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 186679296
                    Iteration time: 2.22s
                      Time elapsed: 01:12:17
                               ETA: 00:03:52

################################################################################
                     [1m Learning iteration 1899/2000 [0m                     

                       Computation: 43599 steps/s (collection: 2.141s, learning 0.114s)
             Mean action noise std: 3.49
          Mean value_function loss: 82.1612
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.1188
                       Mean reward: 874.88
               Mean episode length: 233.79
    Episode_Reward/reaching_object: 1.3850
     Episode_Reward/lifting_object: 173.4889
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 186777600
                    Iteration time: 2.25s
                      Time elapsed: 01:12:19
                               ETA: 00:03:50

################################################################################
                     [1m Learning iteration 1900/2000 [0m                     

                       Computation: 42707 steps/s (collection: 2.149s, learning 0.153s)
             Mean action noise std: 3.50
          Mean value_function loss: 91.9266
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 58.1246
                       Mean reward: 897.44
               Mean episode length: 238.18
    Episode_Reward/reaching_object: 1.4220
     Episode_Reward/lifting_object: 178.5506
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0690
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 186875904
                    Iteration time: 2.30s
                      Time elapsed: 01:12:22
                               ETA: 00:03:48

################################################################################
                     [1m Learning iteration 1901/2000 [0m                     

                       Computation: 42320 steps/s (collection: 2.227s, learning 0.096s)
             Mean action noise std: 3.50
          Mean value_function loss: 92.7018
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 58.1353
                       Mean reward: 880.56
               Mean episode length: 235.45
    Episode_Reward/reaching_object: 1.3878
     Episode_Reward/lifting_object: 173.5202
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 186974208
                    Iteration time: 2.32s
                      Time elapsed: 01:12:24
                               ETA: 00:03:46

################################################################################
                     [1m Learning iteration 1902/2000 [0m                     

                       Computation: 43128 steps/s (collection: 2.187s, learning 0.092s)
             Mean action noise std: 3.50
          Mean value_function loss: 105.9751
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 58.1477
                       Mean reward: 872.83
               Mean episode length: 233.54
    Episode_Reward/reaching_object: 1.3971
     Episode_Reward/lifting_object: 175.2003
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0625
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 187072512
                    Iteration time: 2.28s
                      Time elapsed: 01:12:26
                               ETA: 00:03:43

################################################################################
                     [1m Learning iteration 1903/2000 [0m                     

                       Computation: 43363 steps/s (collection: 2.159s, learning 0.108s)
             Mean action noise std: 3.50
          Mean value_function loss: 100.5162
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.1518
                       Mean reward: 898.24
               Mean episode length: 237.87
    Episode_Reward/reaching_object: 1.3796
     Episode_Reward/lifting_object: 173.1748
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0616
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 187170816
                    Iteration time: 2.27s
                      Time elapsed: 01:12:28
                               ETA: 00:03:41

################################################################################
                     [1m Learning iteration 1904/2000 [0m                     

                       Computation: 42037 steps/s (collection: 2.187s, learning 0.151s)
             Mean action noise std: 3.50
          Mean value_function loss: 94.0456
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 58.1552
                       Mean reward: 869.09
               Mean episode length: 232.26
    Episode_Reward/reaching_object: 1.3853
     Episode_Reward/lifting_object: 173.9334
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 187269120
                    Iteration time: 2.34s
                      Time elapsed: 01:12:31
                               ETA: 00:03:39

################################################################################
                     [1m Learning iteration 1905/2000 [0m                     

                       Computation: 42589 steps/s (collection: 2.215s, learning 0.093s)
             Mean action noise std: 3.50
          Mean value_function loss: 111.1569
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.1630
                       Mean reward: 857.62
               Mean episode length: 229.72
    Episode_Reward/reaching_object: 1.4000
     Episode_Reward/lifting_object: 175.3491
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 187367424
                    Iteration time: 2.31s
                      Time elapsed: 01:12:33
                               ETA: 00:03:36

################################################################################
                     [1m Learning iteration 1906/2000 [0m                     

                       Computation: 43057 steps/s (collection: 2.194s, learning 0.089s)
             Mean action noise std: 3.50
          Mean value_function loss: 105.8246
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.1714
                       Mean reward: 892.03
               Mean episode length: 236.75
    Episode_Reward/reaching_object: 1.4108
     Episode_Reward/lifting_object: 177.3265
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 187465728
                    Iteration time: 2.28s
                      Time elapsed: 01:12:35
                               ETA: 00:03:34

################################################################################
                     [1m Learning iteration 1907/2000 [0m                     

                       Computation: 43188 steps/s (collection: 2.152s, learning 0.125s)
             Mean action noise std: 3.50
          Mean value_function loss: 84.4462
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.1785
                       Mean reward: 915.21
               Mean episode length: 243.85
    Episode_Reward/reaching_object: 1.4143
     Episode_Reward/lifting_object: 177.8814
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 187564032
                    Iteration time: 2.28s
                      Time elapsed: 01:12:38
                               ETA: 00:03:32

################################################################################
                     [1m Learning iteration 1908/2000 [0m                     

                       Computation: 43202 steps/s (collection: 2.146s, learning 0.129s)
             Mean action noise std: 3.51
          Mean value_function loss: 96.4190
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 58.1861
                       Mean reward: 856.99
               Mean episode length: 227.73
    Episode_Reward/reaching_object: 1.3833
     Episode_Reward/lifting_object: 174.4006
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 187662336
                    Iteration time: 2.28s
                      Time elapsed: 01:12:40
                               ETA: 00:03:30

################################################################################
                     [1m Learning iteration 1909/2000 [0m                     

                       Computation: 43809 steps/s (collection: 2.138s, learning 0.106s)
             Mean action noise std: 3.51
          Mean value_function loss: 64.0836
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.1932
                       Mean reward: 914.54
               Mean episode length: 241.54
    Episode_Reward/reaching_object: 1.4144
     Episode_Reward/lifting_object: 178.4583
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0633
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 187760640
                    Iteration time: 2.24s
                      Time elapsed: 01:12:42
                               ETA: 00:03:27

################################################################################
                     [1m Learning iteration 1910/2000 [0m                     

                       Computation: 44217 steps/s (collection: 2.123s, learning 0.100s)
             Mean action noise std: 3.51
          Mean value_function loss: 108.6128
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 58.1994
                       Mean reward: 882.38
               Mean episode length: 235.26
    Episode_Reward/reaching_object: 1.3867
     Episode_Reward/lifting_object: 174.7701
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 187858944
                    Iteration time: 2.22s
                      Time elapsed: 01:12:44
                               ETA: 00:03:25

################################################################################
                     [1m Learning iteration 1911/2000 [0m                     

                       Computation: 43156 steps/s (collection: 2.157s, learning 0.121s)
             Mean action noise std: 3.51
          Mean value_function loss: 105.2218
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 58.2045
                       Mean reward: 913.48
               Mean episode length: 241.47
    Episode_Reward/reaching_object: 1.4193
     Episode_Reward/lifting_object: 178.6054
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 187957248
                    Iteration time: 2.28s
                      Time elapsed: 01:12:47
                               ETA: 00:03:23

################################################################################
                     [1m Learning iteration 1912/2000 [0m                     

                       Computation: 43434 steps/s (collection: 2.171s, learning 0.093s)
             Mean action noise std: 3.51
          Mean value_function loss: 109.6094
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 58.2066
                       Mean reward: 881.24
               Mean episode length: 233.91
    Episode_Reward/reaching_object: 1.4010
     Episode_Reward/lifting_object: 176.3685
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 188055552
                    Iteration time: 2.26s
                      Time elapsed: 01:12:49
                               ETA: 00:03:21

################################################################################
                     [1m Learning iteration 1913/2000 [0m                     

                       Computation: 43348 steps/s (collection: 2.161s, learning 0.107s)
             Mean action noise std: 3.51
          Mean value_function loss: 85.6778
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.2104
                       Mean reward: 886.80
               Mean episode length: 236.03
    Episode_Reward/reaching_object: 1.3968
     Episode_Reward/lifting_object: 175.3195
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 188153856
                    Iteration time: 2.27s
                      Time elapsed: 01:12:51
                               ETA: 00:03:18

################################################################################
                     [1m Learning iteration 1914/2000 [0m                     

                       Computation: 44206 steps/s (collection: 2.130s, learning 0.094s)
             Mean action noise std: 3.51
          Mean value_function loss: 111.4045
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 58.2179
                       Mean reward: 879.11
               Mean episode length: 233.21
    Episode_Reward/reaching_object: 1.4016
     Episode_Reward/lifting_object: 176.4087
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0629
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 188252160
                    Iteration time: 2.22s
                      Time elapsed: 01:12:53
                               ETA: 00:03:16

################################################################################
                     [1m Learning iteration 1915/2000 [0m                     

                       Computation: 43409 steps/s (collection: 2.171s, learning 0.093s)
             Mean action noise std: 3.51
          Mean value_function loss: 106.8323
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 58.2263
                       Mean reward: 852.08
               Mean episode length: 228.22
    Episode_Reward/reaching_object: 1.4126
     Episode_Reward/lifting_object: 177.4180
      Episode_Reward/object_height: 0.0273
        Episode_Reward/action_rate: -0.0634
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 188350464
                    Iteration time: 2.26s
                      Time elapsed: 01:12:56
                               ETA: 00:03:14

################################################################################
                     [1m Learning iteration 1916/2000 [0m                     

                       Computation: 43555 steps/s (collection: 2.143s, learning 0.114s)
             Mean action noise std: 3.51
          Mean value_function loss: 90.7246
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 58.2324
                       Mean reward: 883.75
               Mean episode length: 234.66
    Episode_Reward/reaching_object: 1.3931
     Episode_Reward/lifting_object: 175.0245
      Episode_Reward/object_height: 0.0271
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0694
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 188448768
                    Iteration time: 2.26s
                      Time elapsed: 01:12:58
                               ETA: 00:03:11

################################################################################
                     [1m Learning iteration 1917/2000 [0m                     

                       Computation: 41733 steps/s (collection: 2.236s, learning 0.120s)
             Mean action noise std: 3.51
          Mean value_function loss: 101.1522
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.2420
                       Mean reward: 873.71
               Mean episode length: 234.92
    Episode_Reward/reaching_object: 1.4145
     Episode_Reward/lifting_object: 175.4247
      Episode_Reward/object_height: 0.0277
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0694
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 188547072
                    Iteration time: 2.36s
                      Time elapsed: 01:13:00
                               ETA: 00:03:09

################################################################################
                     [1m Learning iteration 1918/2000 [0m                     

                       Computation: 43320 steps/s (collection: 2.161s, learning 0.109s)
             Mean action noise std: 3.52
          Mean value_function loss: 106.2058
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 58.2489
                       Mean reward: 874.52
               Mean episode length: 232.34
    Episode_Reward/reaching_object: 1.3971
     Episode_Reward/lifting_object: 175.2372
      Episode_Reward/object_height: 0.0271
        Episode_Reward/action_rate: -0.0629
          Episode_Reward/joint_vel: -0.0686
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 188645376
                    Iteration time: 2.27s
                      Time elapsed: 01:13:03
                               ETA: 00:03:07

################################################################################
                     [1m Learning iteration 1919/2000 [0m                     

                       Computation: 42978 steps/s (collection: 2.158s, learning 0.130s)
             Mean action noise std: 3.52
          Mean value_function loss: 107.3183
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.2538
                       Mean reward: 868.56
               Mean episode length: 231.24
    Episode_Reward/reaching_object: 1.3997
     Episode_Reward/lifting_object: 175.4139
      Episode_Reward/object_height: 0.0271
        Episode_Reward/action_rate: -0.0633
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 188743680
                    Iteration time: 2.29s
                      Time elapsed: 01:13:05
                               ETA: 00:03:05

################################################################################
                     [1m Learning iteration 1920/2000 [0m                     

                       Computation: 43499 steps/s (collection: 2.167s, learning 0.093s)
             Mean action noise std: 3.52
          Mean value_function loss: 94.0676
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.2591
                       Mean reward: 889.07
               Mean episode length: 235.30
    Episode_Reward/reaching_object: 1.4117
     Episode_Reward/lifting_object: 177.2496
      Episode_Reward/object_height: 0.0275
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0686
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 188841984
                    Iteration time: 2.26s
                      Time elapsed: 01:13:07
                               ETA: 00:03:02

################################################################################
                     [1m Learning iteration 1921/2000 [0m                     

                       Computation: 42770 steps/s (collection: 2.176s, learning 0.122s)
             Mean action noise std: 3.52
          Mean value_function loss: 88.2351
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 58.2707
                       Mean reward: 901.15
               Mean episode length: 240.28
    Episode_Reward/reaching_object: 1.3990
     Episode_Reward/lifting_object: 175.3321
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 188940288
                    Iteration time: 2.30s
                      Time elapsed: 01:13:09
                               ETA: 00:03:00

################################################################################
                     [1m Learning iteration 1922/2000 [0m                     

                       Computation: 42726 steps/s (collection: 2.180s, learning 0.121s)
             Mean action noise std: 3.52
          Mean value_function loss: 96.4303
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 58.2833
                       Mean reward: 914.82
               Mean episode length: 242.36
    Episode_Reward/reaching_object: 1.4069
     Episode_Reward/lifting_object: 176.2722
      Episode_Reward/object_height: 0.0276
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 189038592
                    Iteration time: 2.30s
                      Time elapsed: 01:13:12
                               ETA: 00:02:58

################################################################################
                     [1m Learning iteration 1923/2000 [0m                     

                       Computation: 42819 steps/s (collection: 2.186s, learning 0.110s)
             Mean action noise std: 3.52
          Mean value_function loss: 105.4232
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 58.2953
                       Mean reward: 890.90
               Mean episode length: 237.06
    Episode_Reward/reaching_object: 1.4157
     Episode_Reward/lifting_object: 177.3183
      Episode_Reward/object_height: 0.0285
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 189136896
                    Iteration time: 2.30s
                      Time elapsed: 01:13:14
                               ETA: 00:02:55

################################################################################
                     [1m Learning iteration 1924/2000 [0m                     

                       Computation: 43934 steps/s (collection: 2.142s, learning 0.095s)
             Mean action noise std: 3.53
          Mean value_function loss: 65.4108
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.3055
                       Mean reward: 927.15
               Mean episode length: 245.30
    Episode_Reward/reaching_object: 1.4250
     Episode_Reward/lifting_object: 178.4501
      Episode_Reward/object_height: 0.0286
        Episode_Reward/action_rate: -0.0643
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 189235200
                    Iteration time: 2.24s
                      Time elapsed: 01:13:16
                               ETA: 00:02:53

################################################################################
                     [1m Learning iteration 1925/2000 [0m                     

                       Computation: 43517 steps/s (collection: 2.159s, learning 0.100s)
             Mean action noise std: 3.53
          Mean value_function loss: 103.1617
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.3190
                       Mean reward: 882.28
               Mean episode length: 234.70
    Episode_Reward/reaching_object: 1.4130
     Episode_Reward/lifting_object: 177.0197
      Episode_Reward/object_height: 0.0288
        Episode_Reward/action_rate: -0.0637
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 189333504
                    Iteration time: 2.26s
                      Time elapsed: 01:13:19
                               ETA: 00:02:51

################################################################################
                     [1m Learning iteration 1926/2000 [0m                     

                       Computation: 40740 steps/s (collection: 2.276s, learning 0.137s)
             Mean action noise std: 3.53
          Mean value_function loss: 110.4358
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 58.3289
                       Mean reward: 908.91
               Mean episode length: 241.01
    Episode_Reward/reaching_object: 1.4297
     Episode_Reward/lifting_object: 179.2673
      Episode_Reward/object_height: 0.0293
        Episode_Reward/action_rate: -0.0645
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 189431808
                    Iteration time: 2.41s
                      Time elapsed: 01:13:21
                               ETA: 00:02:49

################################################################################
                     [1m Learning iteration 1927/2000 [0m                     

                       Computation: 40837 steps/s (collection: 2.304s, learning 0.103s)
             Mean action noise std: 3.53
          Mean value_function loss: 94.7801
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 58.3344
                       Mean reward: 884.32
               Mean episode length: 235.66
    Episode_Reward/reaching_object: 1.3976
     Episode_Reward/lifting_object: 174.2117
      Episode_Reward/object_height: 0.0289
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0670
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 189530112
                    Iteration time: 2.41s
                      Time elapsed: 01:13:23
                               ETA: 00:02:46

################################################################################
                     [1m Learning iteration 1928/2000 [0m                     

                       Computation: 42507 steps/s (collection: 2.205s, learning 0.108s)
             Mean action noise std: 3.53
          Mean value_function loss: 114.2083
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 58.3423
                       Mean reward: 880.79
               Mean episode length: 235.53
    Episode_Reward/reaching_object: 1.4072
     Episode_Reward/lifting_object: 174.7235
      Episode_Reward/object_height: 0.0294
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 189628416
                    Iteration time: 2.31s
                      Time elapsed: 01:13:26
                               ETA: 00:02:44

################################################################################
                     [1m Learning iteration 1929/2000 [0m                     

                       Computation: 42504 steps/s (collection: 2.200s, learning 0.113s)
             Mean action noise std: 3.53
          Mean value_function loss: 101.5904
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.3532
                       Mean reward: 915.42
               Mean episode length: 242.00
    Episode_Reward/reaching_object: 1.3815
     Episode_Reward/lifting_object: 172.7803
      Episode_Reward/object_height: 0.0286
        Episode_Reward/action_rate: -0.0626
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 189726720
                    Iteration time: 2.31s
                      Time elapsed: 01:13:28
                               ETA: 00:02:42

################################################################################
                     [1m Learning iteration 1930/2000 [0m                     

                       Computation: 43986 steps/s (collection: 2.131s, learning 0.104s)
             Mean action noise std: 3.53
          Mean value_function loss: 92.7325
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.3631
                       Mean reward: 888.42
               Mean episode length: 237.20
    Episode_Reward/reaching_object: 1.4122
     Episode_Reward/lifting_object: 177.0698
      Episode_Reward/object_height: 0.0292
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 189825024
                    Iteration time: 2.23s
                      Time elapsed: 01:13:30
                               ETA: 00:02:39

################################################################################
                     [1m Learning iteration 1931/2000 [0m                     

                       Computation: 37857 steps/s (collection: 2.485s, learning 0.111s)
             Mean action noise std: 3.53
          Mean value_function loss: 55759882649.6000
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 58.3682
                       Mean reward: 872.54
               Mean episode length: 232.29
    Episode_Reward/reaching_object: 1.4060
     Episode_Reward/lifting_object: 176.0445
      Episode_Reward/object_height: 0.0287
        Episode_Reward/action_rate: -27.1911
          Episode_Reward/joint_vel: -12737.9805
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 189923328
                    Iteration time: 2.60s
                      Time elapsed: 01:13:33
                               ETA: 00:02:37

################################################################################
                     [1m Learning iteration 1932/2000 [0m                     

                       Computation: 40213 steps/s (collection: 2.312s, learning 0.132s)
             Mean action noise std: 3.54
          Mean value_function loss: 89.6574
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.3714
                       Mean reward: 891.46
               Mean episode length: 237.43
    Episode_Reward/reaching_object: 1.3753
     Episode_Reward/lifting_object: 172.1554
      Episode_Reward/object_height: 0.0280
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 190021632
                    Iteration time: 2.44s
                      Time elapsed: 01:13:35
                               ETA: 00:02:35

################################################################################
                     [1m Learning iteration 1933/2000 [0m                     

                       Computation: 28961 steps/s (collection: 3.216s, learning 0.178s)
             Mean action noise std: 3.54
          Mean value_function loss: 103.0961
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.3820
                       Mean reward: 877.62
               Mean episode length: 234.72
    Episode_Reward/reaching_object: 1.3676
     Episode_Reward/lifting_object: 170.7989
      Episode_Reward/object_height: 0.0274
        Episode_Reward/action_rate: -0.0626
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 190119936
                    Iteration time: 3.39s
                      Time elapsed: 01:13:39
                               ETA: 00:02:33

################################################################################
                     [1m Learning iteration 1934/2000 [0m                     

                       Computation: 33802 steps/s (collection: 2.804s, learning 0.105s)
             Mean action noise std: 3.54
          Mean value_function loss: 97.1464
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.3876
                       Mean reward: 884.39
               Mean episode length: 236.02
    Episode_Reward/reaching_object: 1.4082
     Episode_Reward/lifting_object: 176.8529
      Episode_Reward/object_height: 0.0282
        Episode_Reward/action_rate: -0.0643
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 190218240
                    Iteration time: 2.91s
                      Time elapsed: 01:13:42
                               ETA: 00:02:30

################################################################################
                     [1m Learning iteration 1935/2000 [0m                     

                       Computation: 31070 steps/s (collection: 2.988s, learning 0.176s)
             Mean action noise std: 3.54
          Mean value_function loss: 86.1705
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 58.3918
                       Mean reward: 893.46
               Mean episode length: 238.18
    Episode_Reward/reaching_object: 1.4036
     Episode_Reward/lifting_object: 175.9232
      Episode_Reward/object_height: 0.0280
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 190316544
                    Iteration time: 3.16s
                      Time elapsed: 01:13:45
                               ETA: 00:02:28

################################################################################
                     [1m Learning iteration 1936/2000 [0m                     

                       Computation: 36433 steps/s (collection: 2.590s, learning 0.109s)
             Mean action noise std: 3.54
          Mean value_function loss: 101.9333
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.3968
                       Mean reward: 929.48
               Mean episode length: 246.03
    Episode_Reward/reaching_object: 1.4321
     Episode_Reward/lifting_object: 179.9458
      Episode_Reward/object_height: 0.0290
        Episode_Reward/action_rate: -0.0649
          Episode_Reward/joint_vel: -0.0690
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 190414848
                    Iteration time: 2.70s
                      Time elapsed: 01:13:47
                               ETA: 00:02:26

################################################################################
                     [1m Learning iteration 1937/2000 [0m                     

                       Computation: 40201 steps/s (collection: 2.323s, learning 0.123s)
             Mean action noise std: 3.54
          Mean value_function loss: 99.5341
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.4057
                       Mean reward: 889.85
               Mean episode length: 237.03
    Episode_Reward/reaching_object: 1.3826
     Episode_Reward/lifting_object: 173.0973
      Episode_Reward/object_height: 0.0279
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0670
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 190513152
                    Iteration time: 2.45s
                      Time elapsed: 01:13:50
                               ETA: 00:02:24

################################################################################
                     [1m Learning iteration 1938/2000 [0m                     

                       Computation: 37603 steps/s (collection: 2.512s, learning 0.103s)
             Mean action noise std: 3.54
          Mean value_function loss: 81.2836
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.4132
                       Mean reward: 870.06
               Mean episode length: 232.40
    Episode_Reward/reaching_object: 1.3966
     Episode_Reward/lifting_object: 175.0790
      Episode_Reward/object_height: 0.0283
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 190611456
                    Iteration time: 2.61s
                      Time elapsed: 01:13:53
                               ETA: 00:02:21

################################################################################
                     [1m Learning iteration 1939/2000 [0m                     

                       Computation: 39347 steps/s (collection: 2.342s, learning 0.156s)
             Mean action noise std: 3.54
          Mean value_function loss: 101.7880
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 58.4234
                       Mean reward: 874.57
               Mean episode length: 231.52
    Episode_Reward/reaching_object: 1.3998
     Episode_Reward/lifting_object: 175.9583
      Episode_Reward/object_height: 0.0288
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 190709760
                    Iteration time: 2.50s
                      Time elapsed: 01:13:55
                               ETA: 00:02:19

################################################################################
                     [1m Learning iteration 1940/2000 [0m                     

                       Computation: 40938 steps/s (collection: 2.296s, learning 0.105s)
             Mean action noise std: 3.55
          Mean value_function loss: 84.3958
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 58.4305
                       Mean reward: 901.84
               Mean episode length: 238.75
    Episode_Reward/reaching_object: 1.4120
     Episode_Reward/lifting_object: 177.4281
      Episode_Reward/object_height: 0.0288
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 190808064
                    Iteration time: 2.40s
                      Time elapsed: 01:13:57
                               ETA: 00:02:17

################################################################################
                     [1m Learning iteration 1941/2000 [0m                     

                       Computation: 36601 steps/s (collection: 2.526s, learning 0.160s)
             Mean action noise std: 3.55
          Mean value_function loss: 86.2369
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.4383
                       Mean reward: 884.41
               Mean episode length: 234.74
    Episode_Reward/reaching_object: 1.4099
     Episode_Reward/lifting_object: 177.0478
      Episode_Reward/object_height: 0.0291
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0670
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 190906368
                    Iteration time: 2.69s
                      Time elapsed: 01:14:00
                               ETA: 00:02:14

################################################################################
                     [1m Learning iteration 1942/2000 [0m                     

                       Computation: 41075 steps/s (collection: 2.291s, learning 0.102s)
             Mean action noise std: 3.55
          Mean value_function loss: 106.0957
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 58.4486
                       Mean reward: 877.57
               Mean episode length: 233.46
    Episode_Reward/reaching_object: 1.3846
     Episode_Reward/lifting_object: 173.3416
      Episode_Reward/object_height: 0.0289
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 191004672
                    Iteration time: 2.39s
                      Time elapsed: 01:14:02
                               ETA: 00:02:12

################################################################################
                     [1m Learning iteration 1943/2000 [0m                     

                       Computation: 31632 steps/s (collection: 2.818s, learning 0.290s)
             Mean action noise std: 3.55
          Mean value_function loss: 85.2389
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.4613
                       Mean reward: 874.09
               Mean episode length: 233.08
    Episode_Reward/reaching_object: 1.4151
     Episode_Reward/lifting_object: 177.1936
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.0642
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 191102976
                    Iteration time: 3.11s
                      Time elapsed: 01:14:06
                               ETA: 00:02:10

################################################################################
                     [1m Learning iteration 1944/2000 [0m                     

                       Computation: 32784 steps/s (collection: 2.838s, learning 0.160s)
             Mean action noise std: 3.55
          Mean value_function loss: 136.5188
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.4692
                       Mean reward: 919.09
               Mean episode length: 242.88
    Episode_Reward/reaching_object: 1.4000
     Episode_Reward/lifting_object: 175.4800
      Episode_Reward/object_height: 0.0291
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 191201280
                    Iteration time: 3.00s
                      Time elapsed: 01:14:09
                               ETA: 00:02:08

################################################################################
                     [1m Learning iteration 1945/2000 [0m                     

                       Computation: 39512 steps/s (collection: 2.389s, learning 0.099s)
             Mean action noise std: 3.55
          Mean value_function loss: 84.9292
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 58.4778
                       Mean reward: 883.88
               Mean episode length: 234.97
    Episode_Reward/reaching_object: 1.4080
     Episode_Reward/lifting_object: 175.8636
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 191299584
                    Iteration time: 2.49s
                      Time elapsed: 01:14:11
                               ETA: 00:02:05

################################################################################
                     [1m Learning iteration 1946/2000 [0m                     

                       Computation: 42729 steps/s (collection: 2.204s, learning 0.097s)
             Mean action noise std: 3.55
          Mean value_function loss: 81.4471
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 58.4830
                       Mean reward: 888.16
               Mean episode length: 235.78
    Episode_Reward/reaching_object: 1.4151
     Episode_Reward/lifting_object: 177.5378
      Episode_Reward/object_height: 0.0299
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 191397888
                    Iteration time: 2.30s
                      Time elapsed: 01:14:13
                               ETA: 00:02:03

################################################################################
                     [1m Learning iteration 1947/2000 [0m                     

                       Computation: 42834 steps/s (collection: 2.182s, learning 0.113s)
             Mean action noise std: 3.56
          Mean value_function loss: 94.1046
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 58.4884
                       Mean reward: 889.48
               Mean episode length: 236.58
    Episode_Reward/reaching_object: 1.4002
     Episode_Reward/lifting_object: 174.9129
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 191496192
                    Iteration time: 2.29s
                      Time elapsed: 01:14:16
                               ETA: 00:02:01

################################################################################
                     [1m Learning iteration 1948/2000 [0m                     

                       Computation: 41467 steps/s (collection: 2.258s, learning 0.113s)
             Mean action noise std: 3.56
          Mean value_function loss: 120.0130
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.4952
                       Mean reward: 926.15
               Mean episode length: 244.31
    Episode_Reward/reaching_object: 1.3939
     Episode_Reward/lifting_object: 174.3066
      Episode_Reward/object_height: 0.0299
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 191594496
                    Iteration time: 2.37s
                      Time elapsed: 01:14:18
                               ETA: 00:01:58

################################################################################
                     [1m Learning iteration 1949/2000 [0m                     

                       Computation: 40541 steps/s (collection: 2.322s, learning 0.103s)
             Mean action noise std: 3.56
          Mean value_function loss: 128.8071
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 58.5029
                       Mean reward: 882.13
               Mean episode length: 235.98
    Episode_Reward/reaching_object: 1.3701
     Episode_Reward/lifting_object: 171.0157
      Episode_Reward/object_height: 0.0292
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 191692800
                    Iteration time: 2.42s
                      Time elapsed: 01:14:20
                               ETA: 00:01:56

################################################################################
                     [1m Learning iteration 1950/2000 [0m                     

                       Computation: 43328 steps/s (collection: 2.174s, learning 0.095s)
             Mean action noise std: 3.56
          Mean value_function loss: 30258.3250
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 58.5078
                       Mean reward: 898.25
               Mean episode length: 237.83
    Episode_Reward/reaching_object: 1.4014
     Episode_Reward/lifting_object: 175.3581
      Episode_Reward/object_height: 0.0298
        Episode_Reward/action_rate: -0.0644
          Episode_Reward/joint_vel: -0.0701
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 191791104
                    Iteration time: 2.27s
                      Time elapsed: 01:14:23
                               ETA: 00:01:54

################################################################################
                     [1m Learning iteration 1951/2000 [0m                     

                       Computation: 39640 steps/s (collection: 2.347s, learning 0.133s)
             Mean action noise std: 3.56
          Mean value_function loss: 1087927362244313088.0000
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.5086
                       Mean reward: 883.39
               Mean episode length: 235.39
    Episode_Reward/reaching_object: 1.4023
     Episode_Reward/lifting_object: 175.6645
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -101287.8672
          Episode_Reward/joint_vel: -60250748.0000
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 191889408
                    Iteration time: 2.48s
                      Time elapsed: 01:14:25
                               ETA: 00:01:52

################################################################################
                     [1m Learning iteration 1952/2000 [0m                     

                       Computation: 40095 steps/s (collection: 2.342s, learning 0.110s)
             Mean action noise std: 3.56
          Mean value_function loss: 97.4006
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 58.5106
                       Mean reward: 874.40
               Mean episode length: 233.06
    Episode_Reward/reaching_object: 1.4079
     Episode_Reward/lifting_object: 176.0188
      Episode_Reward/object_height: 0.0299
        Episode_Reward/action_rate: -0.0650
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 191987712
                    Iteration time: 2.45s
                      Time elapsed: 01:14:28
                               ETA: 00:01:49

################################################################################
                     [1m Learning iteration 1953/2000 [0m                     

                       Computation: 41162 steps/s (collection: 2.260s, learning 0.129s)
             Mean action noise std: 3.56
          Mean value_function loss: 112.8737
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 58.5192
                       Mean reward: 898.54
               Mean episode length: 239.37
    Episode_Reward/reaching_object: 1.4005
     Episode_Reward/lifting_object: 175.3575
      Episode_Reward/object_height: 0.0301
        Episode_Reward/action_rate: -0.0650
          Episode_Reward/joint_vel: -0.0715
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 192086016
                    Iteration time: 2.39s
                      Time elapsed: 01:14:30
                               ETA: 00:01:47

################################################################################
                     [1m Learning iteration 1954/2000 [0m                     

                       Computation: 31522 steps/s (collection: 2.744s, learning 0.375s)
             Mean action noise std: 3.56
          Mean value_function loss: 130.6477
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.5317
                       Mean reward: 872.32
               Mean episode length: 232.50
    Episode_Reward/reaching_object: 1.3880
     Episode_Reward/lifting_object: 173.1817
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0716
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 192184320
                    Iteration time: 3.12s
                      Time elapsed: 01:14:33
                               ETA: 00:01:45

################################################################################
                     [1m Learning iteration 1955/2000 [0m                     

                       Computation: 31062 steps/s (collection: 2.927s, learning 0.238s)
             Mean action noise std: 3.56
          Mean value_function loss: 120.7813
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.5399
                       Mean reward: 853.10
               Mean episode length: 229.12
    Episode_Reward/reaching_object: 1.3750
     Episode_Reward/lifting_object: 171.4530
      Episode_Reward/object_height: 0.0298
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.0704
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 192282624
                    Iteration time: 3.16s
                      Time elapsed: 01:14:36
                               ETA: 00:01:42

################################################################################
                     [1m Learning iteration 1956/2000 [0m                     

                       Computation: 39696 steps/s (collection: 2.379s, learning 0.098s)
             Mean action noise std: 3.57
          Mean value_function loss: 131.3767
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.5476
                       Mean reward: 868.35
               Mean episode length: 232.08
    Episode_Reward/reaching_object: 1.3872
     Episode_Reward/lifting_object: 173.2508
      Episode_Reward/object_height: 0.0304
        Episode_Reward/action_rate: -0.0644
          Episode_Reward/joint_vel: -0.0704
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 192380928
                    Iteration time: 2.48s
                      Time elapsed: 01:14:39
                               ETA: 00:01:40

################################################################################
                     [1m Learning iteration 1957/2000 [0m                     

                       Computation: 42139 steps/s (collection: 2.216s, learning 0.117s)
             Mean action noise std: 3.57
          Mean value_function loss: 119.7887
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 58.5539
                       Mean reward: 852.80
               Mean episode length: 231.32
    Episode_Reward/reaching_object: 1.3767
     Episode_Reward/lifting_object: 171.7611
      Episode_Reward/object_height: 0.0300
        Episode_Reward/action_rate: -0.0644
          Episode_Reward/joint_vel: -0.0701
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 192479232
                    Iteration time: 2.33s
                      Time elapsed: 01:14:41
                               ETA: 00:01:38

################################################################################
                     [1m Learning iteration 1958/2000 [0m                     

                       Computation: 42617 steps/s (collection: 2.192s, learning 0.115s)
             Mean action noise std: 3.57
          Mean value_function loss: 98.0424
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.5586
                       Mean reward: 915.03
               Mean episode length: 242.07
    Episode_Reward/reaching_object: 1.4017
     Episode_Reward/lifting_object: 175.6794
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.0653
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 192577536
                    Iteration time: 2.31s
                      Time elapsed: 01:14:43
                               ETA: 00:01:36

################################################################################
                     [1m Learning iteration 1959/2000 [0m                     

                       Computation: 43918 steps/s (collection: 2.125s, learning 0.113s)
             Mean action noise std: 3.57
          Mean value_function loss: 126.5890
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 58.5660
                       Mean reward: 889.60
               Mean episode length: 236.64
    Episode_Reward/reaching_object: 1.4030
     Episode_Reward/lifting_object: 175.9065
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.0653
          Episode_Reward/joint_vel: -0.0708
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 192675840
                    Iteration time: 2.24s
                      Time elapsed: 01:14:46
                               ETA: 00:01:33

################################################################################
                     [1m Learning iteration 1960/2000 [0m                     

                       Computation: 43452 steps/s (collection: 2.167s, learning 0.095s)
             Mean action noise std: 3.57
          Mean value_function loss: 85.2317
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 58.5726
                       Mean reward: 908.86
               Mean episode length: 240.33
    Episode_Reward/reaching_object: 1.4083
     Episode_Reward/lifting_object: 176.4753
      Episode_Reward/object_height: 0.0313
        Episode_Reward/action_rate: -0.0656
          Episode_Reward/joint_vel: -0.0714
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 192774144
                    Iteration time: 2.26s
                      Time elapsed: 01:14:48
                               ETA: 00:01:31

################################################################################
                     [1m Learning iteration 1961/2000 [0m                     

                       Computation: 43925 steps/s (collection: 2.134s, learning 0.104s)
             Mean action noise std: 3.57
          Mean value_function loss: 80.9374
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.5755
                       Mean reward: 900.21
               Mean episode length: 239.25
    Episode_Reward/reaching_object: 1.4236
     Episode_Reward/lifting_object: 178.5602
      Episode_Reward/object_height: 0.0318
        Episode_Reward/action_rate: -0.0665
          Episode_Reward/joint_vel: -0.0731
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 192872448
                    Iteration time: 2.24s
                      Time elapsed: 01:14:50
                               ETA: 00:01:29

################################################################################
                     [1m Learning iteration 1962/2000 [0m                     

                       Computation: 41201 steps/s (collection: 2.277s, learning 0.109s)
             Mean action noise std: 3.57
          Mean value_function loss: 100.8356
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.5812
                       Mean reward: 858.94
               Mean episode length: 230.22
    Episode_Reward/reaching_object: 1.3998
     Episode_Reward/lifting_object: 175.4754
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.0720
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 192970752
                    Iteration time: 2.39s
                      Time elapsed: 01:14:53
                               ETA: 00:01:26

################################################################################
                     [1m Learning iteration 1963/2000 [0m                     

                       Computation: 40729 steps/s (collection: 2.295s, learning 0.119s)
             Mean action noise std: 3.57
          Mean value_function loss: 130.6364
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 58.5887
                       Mean reward: 839.91
               Mean episode length: 224.19
    Episode_Reward/reaching_object: 1.3953
     Episode_Reward/lifting_object: 174.8381
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.0651
          Episode_Reward/joint_vel: -0.0703
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 193069056
                    Iteration time: 2.41s
                      Time elapsed: 01:14:55
                               ETA: 00:01:24

################################################################################
                     [1m Learning iteration 1964/2000 [0m                     

                       Computation: 31713 steps/s (collection: 2.943s, learning 0.157s)
             Mean action noise std: 3.57
          Mean value_function loss: 97.3684
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 58.5964
                       Mean reward: 899.36
               Mean episode length: 238.29
    Episode_Reward/reaching_object: 1.3909
     Episode_Reward/lifting_object: 174.0367
      Episode_Reward/object_height: 0.0303
        Episode_Reward/action_rate: -0.0651
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 193167360
                    Iteration time: 3.10s
                      Time elapsed: 01:14:58
                               ETA: 00:01:22

################################################################################
                     [1m Learning iteration 1965/2000 [0m                     

                       Computation: 29886 steps/s (collection: 3.038s, learning 0.251s)
             Mean action noise std: 3.58
          Mean value_function loss: 120.6640
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.6033
                       Mean reward: 832.32
               Mean episode length: 224.25
    Episode_Reward/reaching_object: 1.3627
     Episode_Reward/lifting_object: 170.0076
      Episode_Reward/object_height: 0.0293
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0704
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 193265664
                    Iteration time: 3.29s
                      Time elapsed: 01:15:01
                               ETA: 00:01:20

################################################################################
                     [1m Learning iteration 1966/2000 [0m                     

                       Computation: 43167 steps/s (collection: 2.174s, learning 0.104s)
             Mean action noise std: 3.58
          Mean value_function loss: 128.3369
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.6134
                       Mean reward: 845.21
               Mean episode length: 226.32
    Episode_Reward/reaching_object: 1.3968
     Episode_Reward/lifting_object: 174.4830
      Episode_Reward/object_height: 0.0294
        Episode_Reward/action_rate: -0.0654
          Episode_Reward/joint_vel: -0.0705
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 193363968
                    Iteration time: 2.28s
                      Time elapsed: 01:15:04
                               ETA: 00:01:17

################################################################################
                     [1m Learning iteration 1967/2000 [0m                     

                       Computation: 43315 steps/s (collection: 2.165s, learning 0.104s)
             Mean action noise std: 3.58
          Mean value_function loss: 76.6765
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 58.6223
                       Mean reward: 884.88
               Mean episode length: 234.28
    Episode_Reward/reaching_object: 1.4025
     Episode_Reward/lifting_object: 175.2460
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.0700
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 193462272
                    Iteration time: 2.27s
                      Time elapsed: 01:15:06
                               ETA: 00:01:15

################################################################################
                     [1m Learning iteration 1968/2000 [0m                     

                       Computation: 44028 steps/s (collection: 2.133s, learning 0.100s)
             Mean action noise std: 3.58
          Mean value_function loss: 87.0119
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.6284
                       Mean reward: 874.88
               Mean episode length: 233.11
    Episode_Reward/reaching_object: 1.4171
     Episode_Reward/lifting_object: 177.1631
      Episode_Reward/object_height: 0.0294
        Episode_Reward/action_rate: -0.0660
          Episode_Reward/joint_vel: -0.0703
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 193560576
                    Iteration time: 2.23s
                      Time elapsed: 01:15:08
                               ETA: 00:01:13

################################################################################
                     [1m Learning iteration 1969/2000 [0m                     

                       Computation: 43927 steps/s (collection: 2.130s, learning 0.108s)
             Mean action noise std: 3.58
          Mean value_function loss: 108.7352
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.6405
                       Mean reward: 912.20
               Mean episode length: 241.37
    Episode_Reward/reaching_object: 1.4096
     Episode_Reward/lifting_object: 176.2701
      Episode_Reward/object_height: 0.0287
        Episode_Reward/action_rate: -0.0657
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 193658880
                    Iteration time: 2.24s
                      Time elapsed: 01:15:10
                               ETA: 00:01:10

################################################################################
                     [1m Learning iteration 1970/2000 [0m                     

                       Computation: 41172 steps/s (collection: 2.276s, learning 0.112s)
             Mean action noise std: 3.58
          Mean value_function loss: 99.4589
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 58.6531
                       Mean reward: 899.24
               Mean episode length: 238.63
    Episode_Reward/reaching_object: 1.3980
     Episode_Reward/lifting_object: 174.5056
      Episode_Reward/object_height: 0.0281
        Episode_Reward/action_rate: -0.0651
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 193757184
                    Iteration time: 2.39s
                      Time elapsed: 01:15:13
                               ETA: 00:01:08

################################################################################
                     [1m Learning iteration 1971/2000 [0m                     

                       Computation: 42163 steps/s (collection: 2.221s, learning 0.111s)
             Mean action noise std: 3.58
          Mean value_function loss: 122.8449
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 58.6615
                       Mean reward: 927.56
               Mean episode length: 245.16
    Episode_Reward/reaching_object: 1.4266
     Episode_Reward/lifting_object: 178.1717
      Episode_Reward/object_height: 0.0279
        Episode_Reward/action_rate: -0.0664
          Episode_Reward/joint_vel: -0.0691
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 193855488
                    Iteration time: 2.33s
                      Time elapsed: 01:15:15
                               ETA: 00:01:06

################################################################################
                     [1m Learning iteration 1972/2000 [0m                     

                       Computation: 43695 steps/s (collection: 2.131s, learning 0.119s)
             Mean action noise std: 3.59
          Mean value_function loss: 159.7767
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 58.6642
                       Mean reward: 894.63
               Mean episode length: 239.28
    Episode_Reward/reaching_object: 1.4196
     Episode_Reward/lifting_object: 177.0845
      Episode_Reward/object_height: 0.0274
        Episode_Reward/action_rate: -0.0658
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 193953792
                    Iteration time: 2.25s
                      Time elapsed: 01:15:17
                               ETA: 00:01:04

################################################################################
                     [1m Learning iteration 1973/2000 [0m                     

                       Computation: 42142 steps/s (collection: 2.237s, learning 0.096s)
             Mean action noise std: 3.59
          Mean value_function loss: 126.1481
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 58.6665
                       Mean reward: 885.07
               Mean episode length: 234.34
    Episode_Reward/reaching_object: 1.3930
     Episode_Reward/lifting_object: 174.0085
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 194052096
                    Iteration time: 2.33s
                      Time elapsed: 01:15:20
                               ETA: 00:01:01

################################################################################
                     [1m Learning iteration 1974/2000 [0m                     

                       Computation: 41003 steps/s (collection: 2.294s, learning 0.104s)
             Mean action noise std: 3.59
          Mean value_function loss: 116.9166
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 58.6700
                       Mean reward: 855.00
               Mean episode length: 228.40
    Episode_Reward/reaching_object: 1.3855
     Episode_Reward/lifting_object: 172.9411
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0645
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 194150400
                    Iteration time: 2.40s
                      Time elapsed: 01:15:22
                               ETA: 00:00:59

################################################################################
                     [1m Learning iteration 1975/2000 [0m                     

                       Computation: 39234 steps/s (collection: 2.289s, learning 0.217s)
             Mean action noise std: 3.59
          Mean value_function loss: 125.6215
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 58.6718
                       Mean reward: 889.61
               Mean episode length: 235.58
    Episode_Reward/reaching_object: 1.3927
     Episode_Reward/lifting_object: 173.8466
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 194248704
                    Iteration time: 2.51s
                      Time elapsed: 01:15:25
                               ETA: 00:00:57

################################################################################
                     [1m Learning iteration 1976/2000 [0m                     

                       Computation: 33039 steps/s (collection: 2.716s, learning 0.260s)
             Mean action noise std: 3.59
          Mean value_function loss: 105.2392
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.6785
                       Mean reward: 850.62
               Mean episode length: 226.62
    Episode_Reward/reaching_object: 1.4074
     Episode_Reward/lifting_object: 175.6648
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.0653
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 194347008
                    Iteration time: 2.98s
                      Time elapsed: 01:15:28
                               ETA: 00:00:54

################################################################################
                     [1m Learning iteration 1977/2000 [0m                     

                       Computation: 32043 steps/s (collection: 2.905s, learning 0.163s)
             Mean action noise std: 3.59
          Mean value_function loss: 122.7779
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.6861
                       Mean reward: 915.52
               Mean episode length: 243.03
    Episode_Reward/reaching_object: 1.4042
     Episode_Reward/lifting_object: 175.5107
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0652
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 194445312
                    Iteration time: 3.07s
                      Time elapsed: 01:15:31
                               ETA: 00:00:52

################################################################################
                     [1m Learning iteration 1978/2000 [0m                     

                       Computation: 37913 steps/s (collection: 2.465s, learning 0.128s)
             Mean action noise std: 3.59
          Mean value_function loss: 101.9386
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 58.6941
                       Mean reward: 909.74
               Mean episode length: 241.37
    Episode_Reward/reaching_object: 1.4271
     Episode_Reward/lifting_object: 178.3226
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.0660
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 194543616
                    Iteration time: 2.59s
                      Time elapsed: 01:15:33
                               ETA: 00:00:50

################################################################################
                     [1m Learning iteration 1979/2000 [0m                     

                       Computation: 42724 steps/s (collection: 2.179s, learning 0.122s)
             Mean action noise std: 3.59
          Mean value_function loss: 120.1891
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.7019
                       Mean reward: 856.84
               Mean episode length: 229.34
    Episode_Reward/reaching_object: 1.3828
     Episode_Reward/lifting_object: 172.2485
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0642
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 194641920
                    Iteration time: 2.30s
                      Time elapsed: 01:15:36
                               ETA: 00:00:48

################################################################################
                     [1m Learning iteration 1980/2000 [0m                     

                       Computation: 42693 steps/s (collection: 2.194s, learning 0.109s)
             Mean action noise std: 3.59
          Mean value_function loss: 137.0944
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.7085
                       Mean reward: 882.79
               Mean episode length: 233.87
    Episode_Reward/reaching_object: 1.3713
     Episode_Reward/lifting_object: 170.6594
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 194740224
                    Iteration time: 2.30s
                      Time elapsed: 01:15:38
                               ETA: 00:00:45

################################################################################
                     [1m Learning iteration 1981/2000 [0m                     

                       Computation: 44220 steps/s (collection: 2.129s, learning 0.094s)
             Mean action noise std: 3.59
          Mean value_function loss: 123.5358
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.7144
                       Mean reward: 858.99
               Mean episode length: 232.48
    Episode_Reward/reaching_object: 1.3951
     Episode_Reward/lifting_object: 173.5641
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0650
          Episode_Reward/joint_vel: -0.0693
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 194838528
                    Iteration time: 2.22s
                      Time elapsed: 01:15:40
                               ETA: 00:00:43

################################################################################
                     [1m Learning iteration 1982/2000 [0m                     

                       Computation: 43408 steps/s (collection: 2.157s, learning 0.108s)
             Mean action noise std: 3.60
          Mean value_function loss: 147.3325
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 58.7212
                       Mean reward: 867.64
               Mean episode length: 231.00
    Episode_Reward/reaching_object: 1.3633
     Episode_Reward/lifting_object: 170.0996
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 194936832
                    Iteration time: 2.26s
                      Time elapsed: 01:15:42
                               ETA: 00:00:41

################################################################################
                     [1m Learning iteration 1983/2000 [0m                     

                       Computation: 44893 steps/s (collection: 2.073s, learning 0.117s)
             Mean action noise std: 3.60
          Mean value_function loss: 102.9866
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 58.7277
                       Mean reward: 881.67
               Mean episode length: 234.65
    Episode_Reward/reaching_object: 1.4205
     Episode_Reward/lifting_object: 177.0650
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0662
          Episode_Reward/joint_vel: -0.0700
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 195035136
                    Iteration time: 2.19s
                      Time elapsed: 01:15:45
                               ETA: 00:00:38

################################################################################
                     [1m Learning iteration 1984/2000 [0m                     

                       Computation: 44579 steps/s (collection: 2.086s, learning 0.120s)
             Mean action noise std: 3.60
          Mean value_function loss: 134.1240
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 58.7399
                       Mean reward: 868.14
               Mean episode length: 232.19
    Episode_Reward/reaching_object: 1.4144
     Episode_Reward/lifting_object: 176.3711
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0657
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 195133440
                    Iteration time: 2.21s
                      Time elapsed: 01:15:47
                               ETA: 00:00:36

################################################################################
                     [1m Learning iteration 1985/2000 [0m                     

                       Computation: 45924 steps/s (collection: 2.040s, learning 0.101s)
             Mean action noise std: 3.60
          Mean value_function loss: 114.9271
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 58.7541
                       Mean reward: 916.09
               Mean episode length: 242.43
    Episode_Reward/reaching_object: 1.4022
     Episode_Reward/lifting_object: 174.9466
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0653
          Episode_Reward/joint_vel: -0.0695
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 195231744
                    Iteration time: 2.14s
                      Time elapsed: 01:15:49
                               ETA: 00:00:34

################################################################################
                     [1m Learning iteration 1986/2000 [0m                     

                       Computation: 44700 steps/s (collection: 2.089s, learning 0.110s)
             Mean action noise std: 3.60
          Mean value_function loss: 107.5010
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.7615
                       Mean reward: 886.68
               Mean episode length: 237.21
    Episode_Reward/reaching_object: 1.3962
     Episode_Reward/lifting_object: 173.8604
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0653
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 195330048
                    Iteration time: 2.20s
                      Time elapsed: 01:15:51
                               ETA: 00:00:32

################################################################################
                     [1m Learning iteration 1987/2000 [0m                     

                       Computation: 36256 steps/s (collection: 2.577s, learning 0.134s)
             Mean action noise std: 3.60
          Mean value_function loss: 107.1578
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.7672
                       Mean reward: 871.59
               Mean episode length: 233.27
    Episode_Reward/reaching_object: 1.4081
     Episode_Reward/lifting_object: 174.7394
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0657
          Episode_Reward/joint_vel: -0.0693
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 195428352
                    Iteration time: 2.71s
                      Time elapsed: 01:15:54
                               ETA: 00:00:29

################################################################################
                     [1m Learning iteration 1988/2000 [0m                     

                       Computation: 39762 steps/s (collection: 2.342s, learning 0.131s)
             Mean action noise std: 3.60
          Mean value_function loss: 115.2450
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 58.7740
                       Mean reward: 896.17
               Mean episode length: 237.08
    Episode_Reward/reaching_object: 1.3989
     Episode_Reward/lifting_object: 173.9984
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0652
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 195526656
                    Iteration time: 2.47s
                      Time elapsed: 01:15:56
                               ETA: 00:00:27

################################################################################
                     [1m Learning iteration 1989/2000 [0m                     

                       Computation: 43002 steps/s (collection: 2.197s, learning 0.089s)
             Mean action noise std: 3.61
          Mean value_function loss: 103.7499
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.7834
                       Mean reward: 875.87
               Mean episode length: 232.61
    Episode_Reward/reaching_object: 1.4142
     Episode_Reward/lifting_object: 176.2947
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0660
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 195624960
                    Iteration time: 2.29s
                      Time elapsed: 01:15:59
                               ETA: 00:00:25

################################################################################
                     [1m Learning iteration 1990/2000 [0m                     

                       Computation: 46235 steps/s (collection: 2.017s, learning 0.109s)
             Mean action noise std: 3.61
          Mean value_function loss: 100.9251
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 58.7944
                       Mean reward: 901.72
               Mean episode length: 238.00
    Episode_Reward/reaching_object: 1.4145
     Episode_Reward/lifting_object: 174.5082
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0661
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 195723264
                    Iteration time: 2.13s
                      Time elapsed: 01:16:01
                               ETA: 00:00:22

################################################################################
                     [1m Learning iteration 1991/2000 [0m                     

                       Computation: 46116 steps/s (collection: 2.021s, learning 0.111s)
             Mean action noise std: 3.61
          Mean value_function loss: 142.5065
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 58.8001
                       Mean reward: 878.55
               Mean episode length: 235.04
    Episode_Reward/reaching_object: 1.4094
     Episode_Reward/lifting_object: 175.3651
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0656
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 195821568
                    Iteration time: 2.13s
                      Time elapsed: 01:16:03
                               ETA: 00:00:20

################################################################################
                     [1m Learning iteration 1992/2000 [0m                     

                       Computation: 46871 steps/s (collection: 1.993s, learning 0.104s)
             Mean action noise std: 3.61
          Mean value_function loss: 113.4661
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 58.8076
                       Mean reward: 883.02
               Mean episode length: 234.57
    Episode_Reward/reaching_object: 1.3963
     Episode_Reward/lifting_object: 172.9339
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0653
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 195919872
                    Iteration time: 2.10s
                      Time elapsed: 01:16:05
                               ETA: 00:00:18

################################################################################
                     [1m Learning iteration 1993/2000 [0m                     

                       Computation: 46460 steps/s (collection: 2.029s, learning 0.087s)
             Mean action noise std: 3.61
          Mean value_function loss: 120.6378
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.8155
                       Mean reward: 921.52
               Mean episode length: 243.21
    Episode_Reward/reaching_object: 1.3899
     Episode_Reward/lifting_object: 172.6206
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0650
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 196018176
                    Iteration time: 2.12s
                      Time elapsed: 01:16:07
                               ETA: 00:00:16

################################################################################
                     [1m Learning iteration 1994/2000 [0m                     

                       Computation: 47408 steps/s (collection: 1.987s, learning 0.087s)
             Mean action noise std: 3.61
          Mean value_function loss: 99.5954
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 58.8237
                       Mean reward: 875.79
               Mean episode length: 233.38
    Episode_Reward/reaching_object: 1.4317
     Episode_Reward/lifting_object: 178.1091
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0668
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 196116480
                    Iteration time: 2.07s
                      Time elapsed: 01:16:09
                               ETA: 00:00:13

################################################################################
                     [1m Learning iteration 1995/2000 [0m                     

                       Computation: 44743 steps/s (collection: 2.108s, learning 0.089s)
             Mean action noise std: 3.61
          Mean value_function loss: 106.1936
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.8302
                       Mean reward: 858.40
               Mean episode length: 229.70
    Episode_Reward/reaching_object: 1.4069
     Episode_Reward/lifting_object: 174.8128
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0660
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 196214784
                    Iteration time: 2.20s
                      Time elapsed: 01:16:11
                               ETA: 00:00:11

################################################################################
                     [1m Learning iteration 1996/2000 [0m                     

                       Computation: 47031 steps/s (collection: 1.993s, learning 0.098s)
             Mean action noise std: 3.61
          Mean value_function loss: 97.8586
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 58.8362
                       Mean reward: 909.80
               Mean episode length: 240.54
    Episode_Reward/reaching_object: 1.3891
     Episode_Reward/lifting_object: 172.6315
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0650
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 196313088
                    Iteration time: 2.09s
                      Time elapsed: 01:16:13
                               ETA: 00:00:09

################################################################################
                     [1m Learning iteration 1997/2000 [0m                     

                       Computation: 46741 steps/s (collection: 1.992s, learning 0.112s)
             Mean action noise std: 3.62
          Mean value_function loss: 70.8224
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.8463
                       Mean reward: 886.42
               Mean episode length: 235.17
    Episode_Reward/reaching_object: 1.4300
     Episode_Reward/lifting_object: 177.2528
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0675
          Episode_Reward/joint_vel: -0.0695
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 196411392
                    Iteration time: 2.10s
                      Time elapsed: 01:16:15
                               ETA: 00:00:06

################################################################################
                     [1m Learning iteration 1998/2000 [0m                     

                       Computation: 46698 steps/s (collection: 1.982s, learning 0.123s)
             Mean action noise std: 3.62
          Mean value_function loss: 113.1767
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 58.8547
                       Mean reward: 862.13
               Mean episode length: 229.71
    Episode_Reward/reaching_object: 1.4069
     Episode_Reward/lifting_object: 175.0633
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0663
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 196509696
                    Iteration time: 2.11s
                      Time elapsed: 01:16:18
                               ETA: 00:00:04

################################################################################
                     [1m Learning iteration 1999/2000 [0m                     

                       Computation: 47091 steps/s (collection: 1.995s, learning 0.092s)
             Mean action noise std: 3.62
          Mean value_function loss: 98.6681
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 58.8629
                       Mean reward: 877.53
               Mean episode length: 232.31
    Episode_Reward/reaching_object: 1.4215
     Episode_Reward/lifting_object: 177.4541
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0670
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 196608000
                    Iteration time: 2.09s
                      Time elapsed: 01:16:20
                               ETA: 00:00:02

