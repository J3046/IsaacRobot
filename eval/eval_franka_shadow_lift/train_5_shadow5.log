################################################################################
                      [1m Learning iteration 0/2000 [0m                       

                       Computation: 10364 steps/s (collection: 9.169s, learning 0.316s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0050
               Mean surrogate loss: -0.0069
                 Mean entropy loss: 44.0318
                       Mean reward: 0.01
               Mean episode length: 21.40
    Episode_Reward/reaching_object: 0.0014
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0001
        Episode_Reward/action_rate: -0.0003
          Episode_Reward/joint_vel: -0.0005
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 9.48s
                      Time elapsed: 00:00:09
                               ETA: 05:16:09

################################################################################
                      [1m Learning iteration 1/2000 [0m                       

                       Computation: 14074 steps/s (collection: 6.846s, learning 0.139s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0007
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 44.1456
                       Mean reward: 0.01
               Mean episode length: 45.09
    Episode_Reward/reaching_object: 0.0040
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0003
        Episode_Reward/action_rate: -0.0009
          Episode_Reward/joint_vel: -0.0014
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 6.98s
                      Time elapsed: 00:00:16
                               ETA: 04:34:21

################################################################################
                      [1m Learning iteration 2/2000 [0m                       

                       Computation: 14473 steps/s (collection: 6.657s, learning 0.135s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 44.2754
                       Mean reward: 0.02
               Mean episode length: 69.30
    Episode_Reward/reaching_object: 0.0070
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0005
        Episode_Reward/action_rate: -0.0015
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 6.79s
                      Time elapsed: 00:00:23
                               ETA: 04:18:12

################################################################################
                      [1m Learning iteration 3/2000 [0m                       

                       Computation: 14282 steps/s (collection: 6.729s, learning 0.154s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 44.3822
                       Mean reward: 0.04
               Mean episode length: 93.06
    Episode_Reward/reaching_object: 0.0108
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0007
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0033
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 6.88s
                      Time elapsed: 00:00:30
                               ETA: 04:10:49

################################################################################
                      [1m Learning iteration 4/2000 [0m                       

                       Computation: 13972 steps/s (collection: 6.888s, learning 0.147s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 44.4298
                       Mean reward: 0.05
               Mean episode length: 117.24
    Episode_Reward/reaching_object: 0.0144
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0043
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 7.04s
                      Time elapsed: 00:00:37
                               ETA: 04:07:22

################################################################################
                      [1m Learning iteration 5/2000 [0m                       

                       Computation: 13826 steps/s (collection: 6.982s, learning 0.128s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 44.4620
                       Mean reward: 0.06
               Mean episode length: 141.16
    Episode_Reward/reaching_object: 0.0190
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0053
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 7.11s
                      Time elapsed: 00:00:44
                               ETA: 04:05:26

################################################################################
                      [1m Learning iteration 6/2000 [0m                       

                       Computation: 14050 steps/s (collection: 6.847s, learning 0.150s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 44.4984
                       Mean reward: 0.09
               Mean episode length: 165.69
    Episode_Reward/reaching_object: 0.0244
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0062
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 7.00s
                      Time elapsed: 00:00:51
                               ETA: 04:03:29

################################################################################
                      [1m Learning iteration 7/2000 [0m                       

                       Computation: 14189 steps/s (collection: 6.768s, learning 0.160s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 44.5681
                       Mean reward: 0.13
               Mean episode length: 189.43
    Episode_Reward/reaching_object: 0.0316
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0072
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 6.93s
                      Time elapsed: 00:00:58
                               ETA: 04:01:42

################################################################################
                      [1m Learning iteration 8/2000 [0m                       

                       Computation: 17555 steps/s (collection: 5.489s, learning 0.111s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 44.5571
                       Mean reward: 0.16
               Mean episode length: 213.24
    Episode_Reward/reaching_object: 0.0400
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0082
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 5.60s
                      Time elapsed: 00:01:03
                               ETA: 03:55:24

################################################################################
                      [1m Learning iteration 9/2000 [0m                       

                       Computation: 56700 steps/s (collection: 1.648s, learning 0.086s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 44.5895
                       Mean reward: 0.24
               Mean episode length: 237.77
    Episode_Reward/reaching_object: 0.0543
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 1.73s
                      Time elapsed: 00:01:05
                               ETA: 03:37:30

################################################################################
                      [1m Learning iteration 10/2000 [0m                      

                       Computation: 51613 steps/s (collection: 1.808s, learning 0.097s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0076
                 Mean entropy loss: 44.6419
                       Mean reward: 0.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0637
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 1.90s
                      Time elapsed: 00:01:07
                               ETA: 03:23:22

################################################################################
                      [1m Learning iteration 11/2000 [0m                      

                       Computation: 57555 steps/s (collection: 1.601s, learning 0.107s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.6113
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 44.7397
                       Mean reward: 0.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0794
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 1.71s
                      Time elapsed: 00:01:09
                               ETA: 03:11:03

################################################################################
                      [1m Learning iteration 12/2000 [0m                      

                       Computation: 56238 steps/s (collection: 1.647s, learning 0.101s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 44.8167
                       Mean reward: 0.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0882
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 1.75s
                      Time elapsed: 00:01:10
                               ETA: 03:00:43

################################################################################
                      [1m Learning iteration 13/2000 [0m                      

                       Computation: 53766 steps/s (collection: 1.714s, learning 0.114s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.3387
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 44.9473
                       Mean reward: 0.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1167
     Episode_Reward/lifting_object: -0.0833
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 1.83s
                      Time elapsed: 00:01:12
                               ETA: 02:52:03

################################################################################
                      [1m Learning iteration 14/2000 [0m                      

                       Computation: 54423 steps/s (collection: 1.701s, learning 0.106s)
             Mean action noise std: 1.04
          Mean value_function loss: 0.6020
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 45.0263
                       Mean reward: 0.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1301
     Episode_Reward/lifting_object: -0.0819
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 1.81s
                      Time elapsed: 00:01:14
                               ETA: 02:44:29

################################################################################
                      [1m Learning iteration 15/2000 [0m                      

                       Computation: 55564 steps/s (collection: 1.678s, learning 0.091s)
             Mean action noise std: 1.05
          Mean value_function loss: 6.5053
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 45.2732
                       Mean reward: 0.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1533
     Episode_Reward/lifting_object: -0.0813
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 1.77s
                      Time elapsed: 00:01:16
                               ETA: 02:37:47

################################################################################
                      [1m Learning iteration 16/2000 [0m                      

                       Computation: 55754 steps/s (collection: 1.668s, learning 0.096s)
             Mean action noise std: 1.05
          Mean value_function loss: 1.7791
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 45.4498
                       Mean reward: -0.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1780
     Episode_Reward/lifting_object: -0.4894
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 1.76s
                      Time elapsed: 00:01:18
                               ETA: 02:31:51

################################################################################
                      [1m Learning iteration 17/2000 [0m                      

                       Computation: 56370 steps/s (collection: 1.657s, learning 0.087s)
             Mean action noise std: 1.06
          Mean value_function loss: 0.3515
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 45.6369
                       Mean reward: 0.27
               Mean episode length: 249.86
    Episode_Reward/reaching_object: 0.1890
     Episode_Reward/lifting_object: -0.1209
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 1.74s
                      Time elapsed: 00:01:19
                               ETA: 02:26:33

################################################################################
                      [1m Learning iteration 18/2000 [0m                      

                       Computation: 52328 steps/s (collection: 1.763s, learning 0.116s)
             Mean action noise std: 1.06
          Mean value_function loss: 6.5491
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 45.7931
                       Mean reward: 0.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1955
     Episode_Reward/lifting_object: -0.1389
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 1.88s
                      Time elapsed: 00:01:21
                               ETA: 02:22:02

################################################################################
                      [1m Learning iteration 19/2000 [0m                      

                       Computation: 55063 steps/s (collection: 1.688s, learning 0.098s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.4561
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 45.9966
                       Mean reward: 0.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2021
     Episode_Reward/lifting_object: -0.2263
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 1.79s
                      Time elapsed: 00:01:23
                               ETA: 02:17:48

################################################################################
                      [1m Learning iteration 20/2000 [0m                      

                       Computation: 55712 steps/s (collection: 1.658s, learning 0.107s)
             Mean action noise std: 1.08
          Mean value_function loss: 5.5385
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 46.1797
                       Mean reward: -0.71
               Mean episode length: 249.99
    Episode_Reward/reaching_object: 0.2172
     Episode_Reward/lifting_object: -0.1620
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 1.76s
                      Time elapsed: 00:01:25
                               ETA: 02:13:57

################################################################################
                      [1m Learning iteration 21/2000 [0m                      

                       Computation: 55972 steps/s (collection: 1.662s, learning 0.094s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.3545
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 46.3323
                       Mean reward: 0.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1766
     Episode_Reward/lifting_object: -0.2381
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 1.76s
                      Time elapsed: 00:01:27
                               ETA: 02:10:26

################################################################################
                      [1m Learning iteration 22/2000 [0m                      

                       Computation: 55510 steps/s (collection: 1.680s, learning 0.091s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.5233
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 46.4757
                       Mean reward: 0.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1672
     Episode_Reward/lifting_object: -0.0763
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 1.77s
                      Time elapsed: 00:01:28
                               ETA: 02:07:14

################################################################################
                      [1m Learning iteration 23/2000 [0m                      

                       Computation: 57338 steps/s (collection: 1.628s, learning 0.087s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0837
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 46.6406
                       Mean reward: 0.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1536
     Episode_Reward/lifting_object: -0.0208
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 1.71s
                      Time elapsed: 00:01:30
                               ETA: 02:04:14

################################################################################
                      [1m Learning iteration 24/2000 [0m                      

                       Computation: 56417 steps/s (collection: 1.629s, learning 0.113s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.8541
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 46.7979
                       Mean reward: -0.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1500
     Episode_Reward/lifting_object: -0.0778
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 1.74s
                      Time elapsed: 00:01:32
                               ETA: 02:01:29

################################################################################
                      [1m Learning iteration 25/2000 [0m                      

                       Computation: 57377 steps/s (collection: 1.626s, learning 0.088s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 46.9505
                       Mean reward: 0.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1401
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 1.71s
                      Time elapsed: 00:01:33
                               ETA: 01:58:56

################################################################################
                      [1m Learning iteration 26/2000 [0m                      

                       Computation: 56015 steps/s (collection: 1.636s, learning 0.119s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0007
               Mean surrogate loss: -0.0078
                 Mean entropy loss: 47.1509
                       Mean reward: 0.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1249
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 1.75s
                      Time elapsed: 00:01:35
                               ETA: 01:56:36

################################################################################
                      [1m Learning iteration 27/2000 [0m                      

                       Computation: 56492 steps/s (collection: 1.627s, learning 0.114s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 47.2639
                       Mean reward: 0.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1303
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 1.74s
                      Time elapsed: 00:01:37
                               ETA: 01:54:26

################################################################################
                      [1m Learning iteration 28/2000 [0m                      

                       Computation: 57044 steps/s (collection: 1.604s, learning 0.119s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 47.3574
                       Mean reward: 0.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1341
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 1.72s
                      Time elapsed: 00:01:39
                               ETA: 01:52:23

################################################################################
                      [1m Learning iteration 29/2000 [0m                      

                       Computation: 55924 steps/s (collection: 1.649s, learning 0.109s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.0007
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 47.4559
                       Mean reward: 0.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1294
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 1.76s
                      Time elapsed: 00:01:40
                               ETA: 01:50:30

################################################################################
                      [1m Learning iteration 30/2000 [0m                      

                       Computation: 57126 steps/s (collection: 1.634s, learning 0.087s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.2259
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 47.5731
                       Mean reward: 0.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1318
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 1.72s
                      Time elapsed: 00:01:42
                               ETA: 01:48:42

################################################################################
                      [1m Learning iteration 31/2000 [0m                      

                       Computation: 56818 steps/s (collection: 1.623s, learning 0.108s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.0562
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.6290
                       Mean reward: 0.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1317
     Episode_Reward/lifting_object: -0.0357
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0122
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 1.73s
                      Time elapsed: 00:01:44
                               ETA: 01:47:02

################################################################################
                      [1m Learning iteration 32/2000 [0m                      

                       Computation: 56783 steps/s (collection: 1.630s, learning 0.101s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.0570
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 47.7653
                       Mean reward: 0.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1360
     Episode_Reward/lifting_object: -0.0088
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 1.73s
                      Time elapsed: 00:01:46
                               ETA: 01:45:27

################################################################################
                      [1m Learning iteration 33/2000 [0m                      

                       Computation: 54493 steps/s (collection: 1.695s, learning 0.109s)
             Mean action noise std: 1.14
          Mean value_function loss: 1.9045
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 47.8884
                       Mean reward: 0.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1525
     Episode_Reward/lifting_object: -0.0787
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 1.80s
                      Time elapsed: 00:01:47
                               ETA: 01:44:02

################################################################################
                      [1m Learning iteration 34/2000 [0m                      

                       Computation: 55319 steps/s (collection: 1.676s, learning 0.101s)
             Mean action noise std: 1.14
          Mean value_function loss: 1.6577
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 47.9969
                       Mean reward: 0.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1772
     Episode_Reward/lifting_object: -0.2059
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 1.78s
                      Time elapsed: 00:01:49
                               ETA: 01:42:41

################################################################################
                      [1m Learning iteration 35/2000 [0m                      

                       Computation: 49751 steps/s (collection: 1.822s, learning 0.154s)
             Mean action noise std: 1.14
          Mean value_function loss: 5.4339
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 48.1140
                       Mean reward: -2.92
               Mean episode length: 249.93
    Episode_Reward/reaching_object: 0.1915
     Episode_Reward/lifting_object: -0.2368
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 1.98s
                      Time elapsed: 00:01:51
                               ETA: 01:41:34

################################################################################
                      [1m Learning iteration 36/2000 [0m                      

                       Computation: 51259 steps/s (collection: 1.829s, learning 0.089s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.4564
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 48.2020
                       Mean reward: 0.49
               Mean episode length: 249.91
    Episode_Reward/reaching_object: 0.1999
     Episode_Reward/lifting_object: -0.0861
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 1.92s
                      Time elapsed: 00:01:53
                               ETA: 01:40:28

################################################################################
                      [1m Learning iteration 37/2000 [0m                      

                       Computation: 53661 steps/s (collection: 1.733s, learning 0.099s)
             Mean action noise std: 1.15
          Mean value_function loss: 3.4409
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 48.3135
                       Mean reward: 0.91
               Mean episode length: 249.63
    Episode_Reward/reaching_object: 0.2281
     Episode_Reward/lifting_object: -0.1140
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 1.83s
                      Time elapsed: 00:01:55
                               ETA: 01:39:21

################################################################################
                      [1m Learning iteration 38/2000 [0m                      

                       Computation: 54318 steps/s (collection: 1.693s, learning 0.117s)
             Mean action noise std: 1.16
          Mean value_function loss: 2.8167
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 48.4096
                       Mean reward: 1.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2259
     Episode_Reward/lifting_object: -0.4340
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 1.81s
                      Time elapsed: 00:01:57
                               ETA: 01:38:17

################################################################################
                      [1m Learning iteration 39/2000 [0m                      

                       Computation: 53151 steps/s (collection: 1.741s, learning 0.109s)
             Mean action noise std: 1.16
          Mean value_function loss: 2.7562
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 48.5223
                       Mean reward: 0.66
               Mean episode length: 249.85
    Episode_Reward/reaching_object: 0.2509
     Episode_Reward/lifting_object: -0.3340
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 1.85s
                      Time elapsed: 00:01:59
                               ETA: 01:37:17

################################################################################
                      [1m Learning iteration 40/2000 [0m                      

                       Computation: 54405 steps/s (collection: 1.718s, learning 0.089s)
             Mean action noise std: 1.16
          Mean value_function loss: 1.3588
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 48.6068
                       Mean reward: 0.67
               Mean episode length: 249.85
    Episode_Reward/reaching_object: 0.2482
     Episode_Reward/lifting_object: -0.1725
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0131
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 1.81s
                      Time elapsed: 00:02:00
                               ETA: 01:36:18

################################################################################
                      [1m Learning iteration 41/2000 [0m                      

                       Computation: 51013 steps/s (collection: 1.781s, learning 0.146s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.4226
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 48.7096
                       Mean reward: 0.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2395
     Episode_Reward/lifting_object: -0.1342
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0132
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 1.93s
                      Time elapsed: 00:02:02
                               ETA: 01:35:27

################################################################################
                      [1m Learning iteration 42/2000 [0m                      

                       Computation: 54787 steps/s (collection: 1.704s, learning 0.090s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.0018
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 48.7912
                       Mean reward: 1.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2314
     Episode_Reward/lifting_object: -0.0267
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0133
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 1.79s
                      Time elapsed: 00:02:04
                               ETA: 01:34:33

################################################################################
                      [1m Learning iteration 43/2000 [0m                      

                       Computation: 49892 steps/s (collection: 1.877s, learning 0.094s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.1930
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 48.9085
                       Mean reward: 0.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2036
     Episode_Reward/lifting_object: -0.0385
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 1.97s
                      Time elapsed: 00:02:06
                               ETA: 01:33:49

################################################################################
                      [1m Learning iteration 44/2000 [0m                      

                       Computation: 54740 steps/s (collection: 1.694s, learning 0.102s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 48.9643
                       Mean reward: 0.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1864
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 1.80s
                      Time elapsed: 00:02:08
                               ETA: 01:32:59

################################################################################
                      [1m Learning iteration 45/2000 [0m                      

                       Computation: 55626 steps/s (collection: 1.659s, learning 0.108s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0270
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 49.0887
                       Mean reward: 0.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1744
     Episode_Reward/lifting_object: -0.0104
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 1.77s
                      Time elapsed: 00:02:10
                               ETA: 01:32:10

################################################################################
                      [1m Learning iteration 46/2000 [0m                      

                       Computation: 53972 steps/s (collection: 1.687s, learning 0.135s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0084
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 49.1466
                       Mean reward: 0.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1563
     Episode_Reward/lifting_object: -0.0079
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 1.82s
                      Time elapsed: 00:02:11
                               ETA: 01:31:25

################################################################################
                      [1m Learning iteration 47/2000 [0m                      

                       Computation: 56076 steps/s (collection: 1.657s, learning 0.096s)
             Mean action noise std: 1.19
          Mean value_function loss: 1.8545
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 49.3401
                       Mean reward: -1.94
               Mean episode length: 249.93
    Episode_Reward/reaching_object: 0.1551
     Episode_Reward/lifting_object: -0.1518
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 1.75s
                      Time elapsed: 00:02:13
                               ETA: 01:30:40

################################################################################
                      [1m Learning iteration 48/2000 [0m                      

                       Computation: 53492 steps/s (collection: 1.719s, learning 0.119s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 49.3932
                       Mean reward: 0.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1518
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 1.84s
                      Time elapsed: 00:02:15
                               ETA: 01:29:59

################################################################################
                      [1m Learning iteration 49/2000 [0m                      

                       Computation: 54420 steps/s (collection: 1.671s, learning 0.136s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.1665
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 49.4811
                       Mean reward: 0.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1570
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 1.81s
                      Time elapsed: 00:02:17
                               ETA: 01:29:19

################################################################################
                      [1m Learning iteration 50/2000 [0m                      

                       Computation: 54392 steps/s (collection: 1.714s, learning 0.093s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.7390
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 49.5377
                       Mean reward: -0.54
               Mean episode length: 249.98
    Episode_Reward/reaching_object: 0.1793
     Episode_Reward/lifting_object: -0.1548
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 1.81s
                      Time elapsed: 00:02:19
                               ETA: 01:28:40

################################################################################
                      [1m Learning iteration 51/2000 [0m                      

                       Computation: 52094 steps/s (collection: 1.786s, learning 0.101s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.6385
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 49.6653
                       Mean reward: 0.76
               Mean episode length: 249.92
    Episode_Reward/reaching_object: 0.1700
     Episode_Reward/lifting_object: -0.0238
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 1.89s
                      Time elapsed: 00:02:21
                               ETA: 01:28:06

################################################################################
                      [1m Learning iteration 52/2000 [0m                      

                       Computation: 54185 steps/s (collection: 1.710s, learning 0.105s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 49.7603
                       Mean reward: 0.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1826
     Episode_Reward/lifting_object: -0.0595
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 1.81s
                      Time elapsed: 00:02:22
                               ETA: 01:27:30

################################################################################
                      [1m Learning iteration 53/2000 [0m                      

                       Computation: 55215 steps/s (collection: 1.692s, learning 0.088s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.3499
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 49.8297
                       Mean reward: 0.89
               Mean episode length: 249.45
    Episode_Reward/reaching_object: 0.1973
     Episode_Reward/lifting_object: -0.0536
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 1.78s
                      Time elapsed: 00:02:24
                               ETA: 01:26:54

################################################################################
                      [1m Learning iteration 54/2000 [0m                      

                       Computation: 48953 steps/s (collection: 1.884s, learning 0.124s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.4093
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 49.8759
                       Mean reward: 0.55
               Mean episode length: 249.92
    Episode_Reward/reaching_object: 0.2072
     Episode_Reward/lifting_object: -0.1611
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 2.01s
                      Time elapsed: 00:02:26
                               ETA: 01:26:28

################################################################################
                      [1m Learning iteration 55/2000 [0m                      

                       Computation: 54174 steps/s (collection: 1.697s, learning 0.118s)
             Mean action noise std: 1.21
          Mean value_function loss: 4.1261
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 49.9689
                       Mean reward: 0.83
               Mean episode length: 249.62
    Episode_Reward/reaching_object: 0.2175
     Episode_Reward/lifting_object: -0.1090
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 1.81s
                      Time elapsed: 00:02:28
                               ETA: 01:25:56

################################################################################
                      [1m Learning iteration 56/2000 [0m                      

                       Computation: 53638 steps/s (collection: 1.724s, learning 0.109s)
             Mean action noise std: 1.22
          Mean value_function loss: 4.6263
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 50.0448
                       Mean reward: 1.02
               Mean episode length: 249.98
    Episode_Reward/reaching_object: 0.2276
     Episode_Reward/lifting_object: -0.4460
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 1.83s
                      Time elapsed: 00:02:30
                               ETA: 01:25:25

################################################################################
                      [1m Learning iteration 57/2000 [0m                      

                       Computation: 53395 steps/s (collection: 1.725s, learning 0.116s)
             Mean action noise std: 1.22
          Mean value_function loss: 1.9121
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 50.1247
                       Mean reward: -0.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2355
     Episode_Reward/lifting_object: -0.4413
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 1.84s
                      Time elapsed: 00:02:32
                               ETA: 01:24:56

################################################################################
                      [1m Learning iteration 58/2000 [0m                      

                       Computation: 53181 steps/s (collection: 1.752s, learning 0.097s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.9321
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.1957
                       Mean reward: 0.39
               Mean episode length: 249.54
    Episode_Reward/reaching_object: 0.2434
     Episode_Reward/lifting_object: -0.1025
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 1.85s
                      Time elapsed: 00:02:33
                               ETA: 01:24:28

################################################################################
                      [1m Learning iteration 59/2000 [0m                      

                       Computation: 55150 steps/s (collection: 1.687s, learning 0.096s)
             Mean action noise std: 1.23
          Mean value_function loss: 1.2557
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 50.2772
                       Mean reward: 1.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2373
     Episode_Reward/lifting_object: -0.1917
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 1.78s
                      Time elapsed: 00:02:35
                               ETA: 01:23:58

################################################################################
                      [1m Learning iteration 60/2000 [0m                      

                       Computation: 52718 steps/s (collection: 1.753s, learning 0.112s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.6262
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 50.3751
                       Mean reward: -0.07
               Mean episode length: 249.90
    Episode_Reward/reaching_object: 0.2324
     Episode_Reward/lifting_object: -0.1373
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 1.86s
                      Time elapsed: 00:02:37
                               ETA: 01:23:33

################################################################################
                      [1m Learning iteration 61/2000 [0m                      

                       Computation: 54290 steps/s (collection: 1.703s, learning 0.108s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.6625
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 50.4713
                       Mean reward: -0.05
               Mean episode length: 248.95
    Episode_Reward/reaching_object: 0.2249
     Episode_Reward/lifting_object: -0.0849
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 1.81s
                      Time elapsed: 00:02:39
                               ETA: 01:23:06

################################################################################
                      [1m Learning iteration 62/2000 [0m                      

                       Computation: 53198 steps/s (collection: 1.760s, learning 0.088s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.1205
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 50.5542
                       Mean reward: 0.66
               Mean episode length: 249.90
    Episode_Reward/reaching_object: 0.2156
     Episode_Reward/lifting_object: -0.0409
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 1.85s
                      Time elapsed: 00:02:41
                               ETA: 01:22:41

################################################################################
                      [1m Learning iteration 63/2000 [0m                      

                       Computation: 54929 steps/s (collection: 1.691s, learning 0.099s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 50.6307
                       Mean reward: 0.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2117
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 1.79s
                      Time elapsed: 00:02:43
                               ETA: 01:22:15

################################################################################
                      [1m Learning iteration 64/2000 [0m                      

                       Computation: 54091 steps/s (collection: 1.676s, learning 0.141s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 50.7479
                       Mean reward: 0.91
               Mean episode length: 249.90
    Episode_Reward/reaching_object: 0.2037
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 1.82s
                      Time elapsed: 00:02:44
                               ETA: 01:21:51

################################################################################
                      [1m Learning iteration 65/2000 [0m                      

                       Computation: 55881 steps/s (collection: 1.666s, learning 0.094s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0511
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 50.8252
                       Mean reward: 0.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1867
     Episode_Reward/lifting_object: -0.0390
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 1.76s
                      Time elapsed: 00:02:46
                               ETA: 01:21:26

################################################################################
                      [1m Learning iteration 66/2000 [0m                      

                       Computation: 54438 steps/s (collection: 1.706s, learning 0.100s)
             Mean action noise std: 1.25
          Mean value_function loss: 1.2769
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.8659
                       Mean reward: 0.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1900
     Episode_Reward/lifting_object: -0.0457
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 1.81s
                      Time elapsed: 00:02:48
                               ETA: 01:21:02

################################################################################
                      [1m Learning iteration 67/2000 [0m                      

                       Computation: 53547 steps/s (collection: 1.716s, learning 0.120s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.1998
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 50.9516
                       Mean reward: 0.16
               Mean episode length: 249.72
    Episode_Reward/reaching_object: 0.1729
     Episode_Reward/lifting_object: -0.1424
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 1.84s
                      Time elapsed: 00:02:50
                               ETA: 01:20:40

################################################################################
                      [1m Learning iteration 68/2000 [0m                      

                       Computation: 50088 steps/s (collection: 1.847s, learning 0.116s)
             Mean action noise std: 1.26
          Mean value_function loss: 3.7637
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 51.0352
                       Mean reward: -0.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2007
     Episode_Reward/lifting_object: -0.0872
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 1.96s
                      Time elapsed: 00:02:52
                               ETA: 01:20:23

################################################################################
                      [1m Learning iteration 69/2000 [0m                      

                       Computation: 52958 steps/s (collection: 1.764s, learning 0.092s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0202
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 51.1060
                       Mean reward: 0.81
               Mean episode length: 249.90
    Episode_Reward/reaching_object: 0.1843
     Episode_Reward/lifting_object: -0.1738
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 1.86s
                      Time elapsed: 00:02:54
                               ETA: 01:20:03

################################################################################
                      [1m Learning iteration 70/2000 [0m                      

                       Computation: 51817 steps/s (collection: 1.785s, learning 0.112s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.9329
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 51.2776
                       Mean reward: 0.60
               Mean episode length: 248.93
    Episode_Reward/reaching_object: 0.1954
     Episode_Reward/lifting_object: -0.0639
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 1.90s
                      Time elapsed: 00:02:56
                               ETA: 01:19:44

################################################################################
                      [1m Learning iteration 71/2000 [0m                      

                       Computation: 54136 steps/s (collection: 1.697s, learning 0.119s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 51.3322
                       Mean reward: 0.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2061
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 1.82s
                      Time elapsed: 00:02:57
                               ETA: 01:19:24

################################################################################
                      [1m Learning iteration 72/2000 [0m                      

                       Computation: 52075 steps/s (collection: 1.716s, learning 0.172s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.3347
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 51.4316
                       Mean reward: 1.16
               Mean episode length: 248.97
    Episode_Reward/reaching_object: 0.2294
     Episode_Reward/lifting_object: -0.0588
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 1.89s
                      Time elapsed: 00:02:59
                               ETA: 01:19:06

################################################################################
                      [1m Learning iteration 73/2000 [0m                      

                       Computation: 53643 steps/s (collection: 1.724s, learning 0.109s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.0503
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 51.4880
                       Mean reward: 1.06
               Mean episode length: 249.04
    Episode_Reward/reaching_object: 0.2338
     Episode_Reward/lifting_object: -0.0413
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 1.83s
                      Time elapsed: 00:03:01
                               ETA: 01:18:47

################################################################################
                      [1m Learning iteration 74/2000 [0m                      

                       Computation: 53502 steps/s (collection: 1.751s, learning 0.086s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.7549
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.6936
                       Mean reward: 1.13
               Mean episode length: 248.73
    Episode_Reward/reaching_object: 0.2683
     Episode_Reward/lifting_object: -0.0226
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 1.84s
                      Time elapsed: 00:03:03
                               ETA: 01:18:29

################################################################################
                      [1m Learning iteration 75/2000 [0m                      

                       Computation: 53587 steps/s (collection: 1.746s, learning 0.088s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.3264
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.7620
                       Mean reward: 1.30
               Mean episode length: 249.31
    Episode_Reward/reaching_object: 0.2737
     Episode_Reward/lifting_object: -0.1688
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 1.83s
                      Time elapsed: 00:03:05
                               ETA: 01:18:11

################################################################################
                      [1m Learning iteration 76/2000 [0m                      

                       Computation: 49857 steps/s (collection: 1.882s, learning 0.090s)
             Mean action noise std: 1.29
          Mean value_function loss: 1.3562
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 51.8436
                       Mean reward: -1.33
               Mean episode length: 247.34
    Episode_Reward/reaching_object: 0.2762
     Episode_Reward/lifting_object: -0.2377
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 1.97s
                      Time elapsed: 00:03:07
                               ETA: 01:17:57

################################################################################
                      [1m Learning iteration 77/2000 [0m                      

                       Computation: 48281 steps/s (collection: 1.892s, learning 0.145s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.1226
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 51.9039
                       Mean reward: 1.23
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.2595
     Episode_Reward/lifting_object: -0.0272
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 2.04s
                      Time elapsed: 00:03:09
                               ETA: 01:17:45

################################################################################
                      [1m Learning iteration 78/2000 [0m                      

                       Computation: 51070 steps/s (collection: 1.833s, learning 0.092s)
             Mean action noise std: 1.30
          Mean value_function loss: 1.0907
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.9847
                       Mean reward: 0.67
               Mean episode length: 246.25
    Episode_Reward/reaching_object: 0.2768
     Episode_Reward/lifting_object: -0.1313
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 1.92s
                      Time elapsed: 00:03:11
                               ETA: 01:17:30

################################################################################
                      [1m Learning iteration 79/2000 [0m                      

                       Computation: 53125 steps/s (collection: 1.761s, learning 0.090s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.4285
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.0452
                       Mean reward: 0.78
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 0.2708
     Episode_Reward/lifting_object: -0.0672
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 1.85s
                      Time elapsed: 00:03:13
                               ETA: 01:17:14

################################################################################
                      [1m Learning iteration 80/2000 [0m                      

                       Computation: 48385 steps/s (collection: 1.938s, learning 0.094s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.2094
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 52.1232
                       Mean reward: 0.87
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.2996
     Episode_Reward/lifting_object: -0.0637
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 2.03s
                      Time elapsed: 00:03:15
                               ETA: 01:17:03

################################################################################
                      [1m Learning iteration 81/2000 [0m                      

                       Computation: 53529 steps/s (collection: 1.732s, learning 0.105s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.0888
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.1816
                       Mean reward: 0.79
               Mean episode length: 247.62
    Episode_Reward/reaching_object: 0.3062
     Episode_Reward/lifting_object: -0.0312
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 1.84s
                      Time elapsed: 00:03:16
                               ETA: 01:16:47

################################################################################
                      [1m Learning iteration 82/2000 [0m                      

                       Computation: 52721 steps/s (collection: 1.742s, learning 0.123s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.3887
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 52.2430
                       Mean reward: 0.64
               Mean episode length: 247.21
    Episode_Reward/reaching_object: 0.2880
     Episode_Reward/lifting_object: -0.1259
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 1.86s
                      Time elapsed: 00:03:18
                               ETA: 01:16:32

################################################################################
                      [1m Learning iteration 83/2000 [0m                      

                       Computation: 49419 steps/s (collection: 1.861s, learning 0.129s)
             Mean action noise std: 1.31
          Mean value_function loss: 1.0547
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.2936
                       Mean reward: 0.69
               Mean episode length: 245.48
    Episode_Reward/reaching_object: 0.2959
     Episode_Reward/lifting_object: -0.1467
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 1.99s
                      Time elapsed: 00:03:20
                               ETA: 01:16:20

################################################################################
                      [1m Learning iteration 84/2000 [0m                      

                       Computation: 54390 steps/s (collection: 1.702s, learning 0.105s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.2895
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 52.3648
                       Mean reward: 0.61
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.3062
     Episode_Reward/lifting_object: -0.0606
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 1.81s
                      Time elapsed: 00:03:22
                               ETA: 01:16:05

################################################################################
                      [1m Learning iteration 85/2000 [0m                      

                       Computation: 49918 steps/s (collection: 1.829s, learning 0.140s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.0400
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 52.4379
                       Mean reward: 1.24
               Mean episode length: 247.51
    Episode_Reward/reaching_object: 0.2817
     Episode_Reward/lifting_object: -0.0216
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 1.97s
                      Time elapsed: 00:03:24
                               ETA: 01:15:53

################################################################################
                      [1m Learning iteration 86/2000 [0m                      

                       Computation: 51875 steps/s (collection: 1.778s, learning 0.117s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.1157
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 52.5948
                       Mean reward: 1.27
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.2904
     Episode_Reward/lifting_object: -0.0283
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 1.89s
                      Time elapsed: 00:03:26
                               ETA: 01:15:40

################################################################################
                      [1m Learning iteration 87/2000 [0m                      

                       Computation: 52468 steps/s (collection: 1.763s, learning 0.111s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.0594
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 52.6383
                       Mean reward: 1.06
               Mean episode length: 246.71
    Episode_Reward/reaching_object: 0.2824
     Episode_Reward/lifting_object: -0.0167
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 1.87s
                      Time elapsed: 00:03:28
                               ETA: 01:15:27

################################################################################
                      [1m Learning iteration 88/2000 [0m                      

                       Computation: 48333 steps/s (collection: 1.918s, learning 0.116s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.1537
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 52.6988
                       Mean reward: 1.05
               Mean episode length: 246.56
    Episode_Reward/reaching_object: 0.2868
     Episode_Reward/lifting_object: -0.0122
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 2.03s
                      Time elapsed: 00:03:30
                               ETA: 01:15:17

################################################################################
                      [1m Learning iteration 89/2000 [0m                      

                       Computation: 53433 steps/s (collection: 1.743s, learning 0.097s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.0319
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 52.7560
                       Mean reward: 1.38
               Mean episode length: 245.25
    Episode_Reward/reaching_object: 0.2964
     Episode_Reward/lifting_object: -0.0336
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 1.84s
                      Time elapsed: 00:03:32
                               ETA: 01:15:04

################################################################################
                      [1m Learning iteration 90/2000 [0m                      

                       Computation: 53590 steps/s (collection: 1.747s, learning 0.087s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.3268
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 52.8379
                       Mean reward: 0.96
               Mean episode length: 246.40
    Episode_Reward/reaching_object: 0.2773
     Episode_Reward/lifting_object: -0.0576
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 1.83s
                      Time elapsed: 00:03:33
                               ETA: 01:14:51

################################################################################
                      [1m Learning iteration 91/2000 [0m                      

                       Computation: 53902 steps/s (collection: 1.722s, learning 0.102s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.3430
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.8736
                       Mean reward: 0.50
               Mean episode length: 245.11
    Episode_Reward/reaching_object: 0.2898
     Episode_Reward/lifting_object: -0.0874
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 1.82s
                      Time elapsed: 00:03:35
                               ETA: 01:14:37

################################################################################
                      [1m Learning iteration 92/2000 [0m                      

                       Computation: 51900 steps/s (collection: 1.737s, learning 0.158s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.6011
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 52.9456
                       Mean reward: 1.14
               Mean episode length: 243.44
    Episode_Reward/reaching_object: 0.3020
     Episode_Reward/lifting_object: -0.0556
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 1.89s
                      Time elapsed: 00:03:37
                               ETA: 01:14:26

################################################################################
                      [1m Learning iteration 93/2000 [0m                      

                       Computation: 45693 steps/s (collection: 2.016s, learning 0.136s)
             Mean action noise std: 1.34
          Mean value_function loss: 1.9105
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.0278
                       Mean reward: 1.07
               Mean episode length: 238.65
    Episode_Reward/reaching_object: 0.3130
     Episode_Reward/lifting_object: -0.2337
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 2.15s
                      Time elapsed: 00:03:39
                               ETA: 01:14:20

################################################################################
                      [1m Learning iteration 94/2000 [0m                      

                       Computation: 53711 steps/s (collection: 1.740s, learning 0.091s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.1247
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 53.0939
                       Mean reward: 1.35
               Mean episode length: 237.04
    Episode_Reward/reaching_object: 0.3077
     Episode_Reward/lifting_object: -0.0235
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 1.83s
                      Time elapsed: 00:03:41
                               ETA: 01:14:07

################################################################################
                      [1m Learning iteration 95/2000 [0m                      

                       Computation: 53691 steps/s (collection: 1.744s, learning 0.087s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.1960
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 53.1580
                       Mean reward: 1.52
               Mean episode length: 241.03
    Episode_Reward/reaching_object: 0.3144
     Episode_Reward/lifting_object: -0.0370
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 1.83s
                      Time elapsed: 00:03:43
                               ETA: 01:13:55

################################################################################
                      [1m Learning iteration 96/2000 [0m                      

                       Computation: 50649 steps/s (collection: 1.830s, learning 0.111s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0045
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 53.2016
                       Mean reward: 1.47
               Mean episode length: 233.37
    Episode_Reward/reaching_object: 0.3119
     Episode_Reward/lifting_object: 0.0067
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 1.94s
                      Time elapsed: 00:03:45
                               ETA: 01:13:45

################################################################################
                      [1m Learning iteration 97/2000 [0m                      

                       Computation: 54066 steps/s (collection: 1.733s, learning 0.085s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.3135
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 53.2806
                       Mean reward: 1.60
               Mean episode length: 237.62
    Episode_Reward/reaching_object: 0.3242
     Episode_Reward/lifting_object: -0.0449
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 1.82s
                      Time elapsed: 00:03:47
                               ETA: 01:13:33

################################################################################
                      [1m Learning iteration 98/2000 [0m                      

                       Computation: 52967 steps/s (collection: 1.752s, learning 0.104s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0326
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.3202
                       Mean reward: 1.27
               Mean episode length: 235.52
    Episode_Reward/reaching_object: 0.3282
     Episode_Reward/lifting_object: -0.0072
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 1.86s
                      Time elapsed: 00:03:49
                               ETA: 01:13:21

################################################################################
                      [1m Learning iteration 99/2000 [0m                      

                       Computation: 53342 steps/s (collection: 1.754s, learning 0.089s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0876
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 53.4331
                       Mean reward: 1.30
               Mean episode length: 236.06
    Episode_Reward/reaching_object: 0.3360
     Episode_Reward/lifting_object: -0.0177
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 1.84s
                      Time elapsed: 00:03:50
                               ETA: 01:13:10

################################################################################
                     [1m Learning iteration 100/2000 [0m                      

                       Computation: 51703 steps/s (collection: 1.724s, learning 0.177s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0357
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 53.4720
                       Mean reward: 1.55
               Mean episode length: 231.36
    Episode_Reward/reaching_object: 0.3382
     Episode_Reward/lifting_object: -0.0315
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 1.90s
                      Time elapsed: 00:03:52
                               ETA: 01:13:00

################################################################################
                     [1m Learning iteration 101/2000 [0m                      

                       Computation: 52511 steps/s (collection: 1.759s, learning 0.113s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.1943
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 53.5813
                       Mean reward: 1.33
               Mean episode length: 227.65
    Episode_Reward/reaching_object: 0.3378
     Episode_Reward/lifting_object: -0.0336
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 1.87s
                      Time elapsed: 00:03:54
                               ETA: 01:12:50

################################################################################
                     [1m Learning iteration 102/2000 [0m                      

                       Computation: 52712 steps/s (collection: 1.766s, learning 0.099s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.5485
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.6204
                       Mean reward: 0.55
               Mean episode length: 229.34
    Episode_Reward/reaching_object: 0.3449
     Episode_Reward/lifting_object: -0.1211
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 1.86s
                      Time elapsed: 00:03:56
                               ETA: 01:12:39

################################################################################
                     [1m Learning iteration 103/2000 [0m                      

                       Computation: 52892 steps/s (collection: 1.755s, learning 0.104s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.2558
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 53.6735
                       Mean reward: 1.41
               Mean episode length: 231.53
    Episode_Reward/reaching_object: 0.3443
     Episode_Reward/lifting_object: -0.0634
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 1.86s
                      Time elapsed: 00:03:58
                               ETA: 01:12:29

################################################################################
                     [1m Learning iteration 104/2000 [0m                      

                       Computation: 50619 steps/s (collection: 1.824s, learning 0.118s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.5553
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 53.7392
                       Mean reward: 0.91
               Mean episode length: 231.97
    Episode_Reward/reaching_object: 0.3671
     Episode_Reward/lifting_object: -0.0867
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 1.94s
                      Time elapsed: 00:04:00
                               ETA: 01:12:21

################################################################################
                     [1m Learning iteration 105/2000 [0m                      

                       Computation: 52671 steps/s (collection: 1.754s, learning 0.113s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.0262
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.7973
                       Mean reward: 1.75
               Mean episode length: 229.93
    Episode_Reward/reaching_object: 0.3665
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 1.87s
                      Time elapsed: 00:04:02
                               ETA: 01:12:11

################################################################################
                     [1m Learning iteration 106/2000 [0m                      

                       Computation: 52534 steps/s (collection: 1.768s, learning 0.104s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.2844
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 53.9298
                       Mean reward: 1.75
               Mean episode length: 234.42
    Episode_Reward/reaching_object: 0.3745
     Episode_Reward/lifting_object: -0.0577
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 1.87s
                      Time elapsed: 00:04:04
                               ETA: 01:12:01

################################################################################
                     [1m Learning iteration 107/2000 [0m                      

                       Computation: 51726 steps/s (collection: 1.778s, learning 0.122s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.0052
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 53.9676
                       Mean reward: 1.64
               Mean episode length: 230.58
    Episode_Reward/reaching_object: 0.3609
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.2917
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 1.90s
                      Time elapsed: 00:04:06
                               ETA: 01:11:52

################################################################################
                     [1m Learning iteration 108/2000 [0m                      

                       Computation: 52790 steps/s (collection: 1.741s, learning 0.122s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.0844
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 54.0460
                       Mean reward: 1.58
               Mean episode length: 225.88
    Episode_Reward/reaching_object: 0.3444
     Episode_Reward/lifting_object: -0.0076
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 1.86s
                      Time elapsed: 00:04:07
                               ETA: 01:11:43

################################################################################
                     [1m Learning iteration 109/2000 [0m                      

                       Computation: 52397 steps/s (collection: 1.751s, learning 0.125s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.0091
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.0830
                       Mean reward: 1.58
               Mean episode length: 229.48
    Episode_Reward/reaching_object: 0.3559
     Episode_Reward/lifting_object: -0.0292
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 1.88s
                      Time elapsed: 00:04:09
                               ETA: 01:11:33

################################################################################
                     [1m Learning iteration 110/2000 [0m                      

                       Computation: 50392 steps/s (collection: 1.860s, learning 0.091s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.1100
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 54.1729
                       Mean reward: 1.19
               Mean episode length: 232.40
    Episode_Reward/reaching_object: 0.3503
     Episode_Reward/lifting_object: -0.0369
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 1.95s
                      Time elapsed: 00:04:11
                               ETA: 01:11:26

################################################################################
                     [1m Learning iteration 111/2000 [0m                      

                       Computation: 52419 steps/s (collection: 1.747s, learning 0.129s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.0050
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 54.2101
                       Mean reward: 1.57
               Mean episode length: 230.98
    Episode_Reward/reaching_object: 0.3551
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 1.88s
                      Time elapsed: 00:04:13
                               ETA: 01:11:17

################################################################################
                     [1m Learning iteration 112/2000 [0m                      

                       Computation: 52584 steps/s (collection: 1.762s, learning 0.108s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.6320
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 54.2876
                       Mean reward: 1.62
               Mean episode length: 226.17
    Episode_Reward/reaching_object: 0.3521
     Episode_Reward/lifting_object: -0.0744
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 1.87s
                      Time elapsed: 00:04:15
                               ETA: 01:11:08

################################################################################
                     [1m Learning iteration 113/2000 [0m                      

                       Computation: 52372 steps/s (collection: 1.764s, learning 0.113s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0216
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.3227
                       Mean reward: 1.62
               Mean episode length: 229.33
    Episode_Reward/reaching_object: 0.3590
     Episode_Reward/lifting_object: -0.0381
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 1.88s
                      Time elapsed: 00:04:17
                               ETA: 01:10:59

################################################################################
                     [1m Learning iteration 114/2000 [0m                      

                       Computation: 53476 steps/s (collection: 1.731s, learning 0.107s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.2396
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 54.4310
                       Mean reward: 0.83
               Mean episode length: 228.28
    Episode_Reward/reaching_object: 0.3558
     Episode_Reward/lifting_object: -0.0370
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 1.84s
                      Time elapsed: 00:04:19
                               ETA: 01:10:50

################################################################################
                     [1m Learning iteration 115/2000 [0m                      

                       Computation: 53493 steps/s (collection: 1.750s, learning 0.088s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0622
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.4699
                       Mean reward: 1.46
               Mean episode length: 226.50
    Episode_Reward/reaching_object: 0.3498
     Episode_Reward/lifting_object: -0.0083
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 1.84s
                      Time elapsed: 00:04:21
                               ETA: 01:10:41

################################################################################
                     [1m Learning iteration 116/2000 [0m                      

                       Computation: 53150 steps/s (collection: 1.743s, learning 0.107s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.0053
               Mean surrogate loss: -0.0094
                 Mean entropy loss: 54.5904
                       Mean reward: 1.59
               Mean episode length: 221.81
    Episode_Reward/reaching_object: 0.3356
     Episode_Reward/lifting_object: -0.0079
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.7500
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 1.85s
                      Time elapsed: 00:04:22
                               ETA: 01:10:33

################################################################################
                     [1m Learning iteration 117/2000 [0m                      

                       Computation: 53513 steps/s (collection: 1.741s, learning 0.096s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.1495
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 54.6722
                       Mean reward: 1.28
               Mean episode length: 223.16
    Episode_Reward/reaching_object: 0.3329
     Episode_Reward/lifting_object: -0.0069
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 1.84s
                      Time elapsed: 00:04:24
                               ETA: 01:10:24

################################################################################
                     [1m Learning iteration 118/2000 [0m                      

                       Computation: 53453 steps/s (collection: 1.733s, learning 0.106s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0057
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 54.7027
                       Mean reward: 1.52
               Mean episode length: 225.55
    Episode_Reward/reaching_object: 0.3388
     Episode_Reward/lifting_object: -0.0152
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 1.84s
                      Time elapsed: 00:04:26
                               ETA: 01:10:15

################################################################################
                     [1m Learning iteration 119/2000 [0m                      

                       Computation: 53774 steps/s (collection: 1.737s, learning 0.091s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.1985
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 54.7839
                       Mean reward: 1.57
               Mean episode length: 224.04
    Episode_Reward/reaching_object: 0.3474
     Episode_Reward/lifting_object: -0.0312
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 5.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 1.83s
                      Time elapsed: 00:04:28
                               ETA: 01:10:06

################################################################################
                     [1m Learning iteration 120/2000 [0m                      

                       Computation: 53564 steps/s (collection: 1.739s, learning 0.097s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0052
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 54.8105
                       Mean reward: 1.54
               Mean episode length: 225.44
    Episode_Reward/reaching_object: 0.3365
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 1.84s
                      Time elapsed: 00:04:30
                               ETA: 01:09:58

################################################################################
                     [1m Learning iteration 121/2000 [0m                      

                       Computation: 53518 steps/s (collection: 1.744s, learning 0.093s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.1281
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 54.8645
                       Mean reward: 1.15
               Mean episode length: 218.84
    Episode_Reward/reaching_object: 0.3461
     Episode_Reward/lifting_object: -0.0128
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 5.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 1.84s
                      Time elapsed: 00:04:32
                               ETA: 01:09:50

################################################################################
                     [1m Learning iteration 122/2000 [0m                      

                       Computation: 53579 steps/s (collection: 1.745s, learning 0.089s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0348
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.8959
                       Mean reward: 1.39
               Mean episode length: 222.17
    Episode_Reward/reaching_object: 0.3515
     Episode_Reward/lifting_object: -0.0145
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 6.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 1.83s
                      Time elapsed: 00:04:33
                               ETA: 01:09:41

################################################################################
                     [1m Learning iteration 123/2000 [0m                      

                       Computation: 53370 steps/s (collection: 1.749s, learning 0.093s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.0319
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 55.0204
                       Mean reward: 1.57
               Mean episode length: 223.10
    Episode_Reward/reaching_object: 0.3515
     Episode_Reward/lifting_object: -0.0079
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 1.84s
                      Time elapsed: 00:04:35
                               ETA: 01:09:33

################################################################################
                     [1m Learning iteration 124/2000 [0m                      

                       Computation: 51856 steps/s (collection: 1.767s, learning 0.129s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.0369
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.1480
                       Mean reward: 1.67
               Mean episode length: 213.78
    Episode_Reward/reaching_object: 0.3495
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 5.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.3333
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 1.90s
                      Time elapsed: 00:04:37
                               ETA: 01:09:26

################################################################################
                     [1m Learning iteration 125/2000 [0m                      

                       Computation: 51619 steps/s (collection: 1.783s, learning 0.121s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.0327
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 55.2334
                       Mean reward: 1.71
               Mean episode length: 216.93
    Episode_Reward/reaching_object: 0.3665
     Episode_Reward/lifting_object: -0.0064
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 5.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.4583
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 1.90s
                      Time elapsed: 00:04:39
                               ETA: 01:09:19

################################################################################
                     [1m Learning iteration 126/2000 [0m                      

                       Computation: 52065 steps/s (collection: 1.790s, learning 0.098s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.3788
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 55.3577
                       Mean reward: 1.50
               Mean episode length: 219.45
    Episode_Reward/reaching_object: 0.3712
     Episode_Reward/lifting_object: -0.0514
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.2500
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 1.89s
                      Time elapsed: 00:04:41
                               ETA: 01:09:12

################################################################################
                     [1m Learning iteration 127/2000 [0m                      

                       Computation: 52675 steps/s (collection: 1.755s, learning 0.111s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.0549
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.3944
                       Mean reward: 1.69
               Mean episode length: 211.04
    Episode_Reward/reaching_object: 0.3659
     Episode_Reward/lifting_object: -0.0340
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.7083
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 1.87s
                      Time elapsed: 00:04:43
                               ETA: 01:09:05

################################################################################
                     [1m Learning iteration 128/2000 [0m                      

                       Computation: 52601 steps/s (collection: 1.783s, learning 0.086s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.9169
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 55.4860
                       Mean reward: 1.70
               Mean episode length: 212.79
    Episode_Reward/reaching_object: 0.3792
     Episode_Reward/lifting_object: -0.0987
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.4583
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 1.87s
                      Time elapsed: 00:04:45
                               ETA: 01:08:58

################################################################################
                     [1m Learning iteration 129/2000 [0m                      

                       Computation: 52772 steps/s (collection: 1.770s, learning 0.093s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.2442
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.5155
                       Mean reward: 1.74
               Mean episode length: 205.68
    Episode_Reward/reaching_object: 0.3847
     Episode_Reward/lifting_object: -0.0661
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 1.86s
                      Time elapsed: 00:04:47
                               ETA: 01:08:50

################################################################################
                     [1m Learning iteration 130/2000 [0m                      

                       Computation: 52690 steps/s (collection: 1.774s, learning 0.092s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.8090
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 55.5583
                       Mean reward: 0.84
               Mean episode length: 205.46
    Episode_Reward/reaching_object: 0.3825
     Episode_Reward/lifting_object: -0.1139
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 1.87s
                      Time elapsed: 00:04:48
                               ETA: 01:08:43

################################################################################
                     [1m Learning iteration 131/2000 [0m                      

                       Computation: 51094 steps/s (collection: 1.825s, learning 0.099s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.7914
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.6025
                       Mean reward: 1.68
               Mean episode length: 211.73
    Episode_Reward/reaching_object: 0.3984
     Episode_Reward/lifting_object: -0.0939
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 1.92s
                      Time elapsed: 00:04:50
                               ETA: 01:08:37

################################################################################
                     [1m Learning iteration 132/2000 [0m                      

                       Computation: 51767 steps/s (collection: 1.805s, learning 0.094s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.8626
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 55.6875
                       Mean reward: 1.85
               Mean episode length: 206.08
    Episode_Reward/reaching_object: 0.4020
     Episode_Reward/lifting_object: -0.0941
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 1.90s
                      Time elapsed: 00:04:52
                               ETA: 01:08:31

################################################################################
                     [1m Learning iteration 133/2000 [0m                      

                       Computation: 52328 steps/s (collection: 1.783s, learning 0.096s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.0328
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 55.8623
                       Mean reward: 1.87
               Mean episode length: 209.34
    Episode_Reward/reaching_object: 0.4183
     Episode_Reward/lifting_object: -0.0104
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 1.88s
                      Time elapsed: 00:04:54
                               ETA: 01:08:24

################################################################################
                     [1m Learning iteration 134/2000 [0m                      

                       Computation: 53516 steps/s (collection: 1.747s, learning 0.090s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.0413
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.9878
                       Mean reward: 2.04
               Mean episode length: 219.00
    Episode_Reward/reaching_object: 0.4084
     Episode_Reward/lifting_object: -0.0119
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 1.84s
                      Time elapsed: 00:04:56
                               ETA: 01:08:17

################################################################################
                     [1m Learning iteration 135/2000 [0m                      

                       Computation: 52802 steps/s (collection: 1.776s, learning 0.086s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.0788
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 56.0884
                       Mean reward: 1.40
               Mean episode length: 214.99
    Episode_Reward/reaching_object: 0.3950
     Episode_Reward/lifting_object: -0.0310
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.3750
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 1.86s
                      Time elapsed: 00:04:58
                               ETA: 01:08:10

################################################################################
                     [1m Learning iteration 136/2000 [0m                      

                       Computation: 52359 steps/s (collection: 1.785s, learning 0.092s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.0249
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 56.1211
                       Mean reward: 1.79
               Mean episode length: 215.03
    Episode_Reward/reaching_object: 0.3926
     Episode_Reward/lifting_object: -0.0083
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.7083
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 1.88s
                      Time elapsed: 00:05:00
                               ETA: 01:08:03

################################################################################
                     [1m Learning iteration 137/2000 [0m                      

                       Computation: 51743 steps/s (collection: 1.801s, learning 0.099s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.0560
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 56.2211
                       Mean reward: 1.77
               Mean episode length: 214.23
    Episode_Reward/reaching_object: 0.3789
     Episode_Reward/lifting_object: -0.0128
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.5000
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 1.90s
                      Time elapsed: 00:05:02
                               ETA: 01:07:57

################################################################################
                     [1m Learning iteration 138/2000 [0m                      

                       Computation: 51350 steps/s (collection: 1.821s, learning 0.093s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.5801
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 56.3532
                       Mean reward: 1.34
               Mean episode length: 211.75
    Episode_Reward/reaching_object: 0.3743
     Episode_Reward/lifting_object: -0.1034
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.6667
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 1.91s
                      Time elapsed: 00:05:03
                               ETA: 01:07:51

################################################################################
                     [1m Learning iteration 139/2000 [0m                      

                       Computation: 52019 steps/s (collection: 1.801s, learning 0.089s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.4387
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 56.3873
                       Mean reward: 1.65
               Mean episode length: 203.24
    Episode_Reward/reaching_object: 0.3664
     Episode_Reward/lifting_object: -0.0660
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.7917
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 1.89s
                      Time elapsed: 00:05:05
                               ETA: 01:07:45

################################################################################
                     [1m Learning iteration 140/2000 [0m                      

                       Computation: 50893 steps/s (collection: 1.838s, learning 0.094s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.5978
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.4381
                       Mean reward: 1.40
               Mean episode length: 199.67
    Episode_Reward/reaching_object: 0.3521
     Episode_Reward/lifting_object: -0.0579
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 5.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.7917
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 1.93s
                      Time elapsed: 00:05:07
                               ETA: 01:07:40

################################################################################
                     [1m Learning iteration 141/2000 [0m                      

                       Computation: 50513 steps/s (collection: 1.774s, learning 0.172s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.1624
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.4821
                       Mean reward: 1.46
               Mean episode length: 192.06
    Episode_Reward/reaching_object: 0.3576
     Episode_Reward/lifting_object: -0.0778
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.6250
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 1.95s
                      Time elapsed: 00:05:09
                               ETA: 01:07:35

################################################################################
                     [1m Learning iteration 142/2000 [0m                      

                       Computation: 50843 steps/s (collection: 1.815s, learning 0.119s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.0957
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 56.5771
                       Mean reward: 1.73
               Mean episode length: 201.57
    Episode_Reward/reaching_object: 0.3649
     Episode_Reward/lifting_object: -0.0246
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 4.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.2083
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 1.93s
                      Time elapsed: 00:05:11
                               ETA: 01:07:29

################################################################################
                     [1m Learning iteration 143/2000 [0m                      

                       Computation: 52608 steps/s (collection: 1.777s, learning 0.092s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.1022
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.6577
                       Mean reward: 1.59
               Mean episode length: 197.95
    Episode_Reward/reaching_object: 0.3818
     Episode_Reward/lifting_object: -0.0190
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 3.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 18.0417
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 1.87s
                      Time elapsed: 00:05:13
                               ETA: 01:07:23

################################################################################
                     [1m Learning iteration 144/2000 [0m                      

                       Computation: 52085 steps/s (collection: 1.797s, learning 0.090s)
             Mean action noise std: 1.51
          Mean value_function loss: 1.4752
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.7245
                       Mean reward: 0.92
               Mean episode length: 201.46
    Episode_Reward/reaching_object: 0.3922
     Episode_Reward/lifting_object: -0.1551
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.7083
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 1.89s
                      Time elapsed: 00:05:15
                               ETA: 01:07:17

################################################################################
                     [1m Learning iteration 145/2000 [0m                      

                       Computation: 50890 steps/s (collection: 1.811s, learning 0.121s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.0118
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 56.7656
                       Mean reward: 1.85
               Mean episode length: 200.08
    Episode_Reward/reaching_object: 0.3871
     Episode_Reward/lifting_object: -0.0492
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 1.93s
                      Time elapsed: 00:05:17
                               ETA: 01:07:12

################################################################################
                     [1m Learning iteration 146/2000 [0m                      

                       Computation: 51825 steps/s (collection: 1.784s, learning 0.113s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.1267
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 56.8627
                       Mean reward: 2.01
               Mean episode length: 199.84
    Episode_Reward/reaching_object: 0.4257
     Episode_Reward/lifting_object: -0.0243
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.9583
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 1.90s
                      Time elapsed: 00:05:19
                               ETA: 01:07:06

################################################################################
                     [1m Learning iteration 147/2000 [0m                      

                       Computation: 49701 steps/s (collection: 1.884s, learning 0.094s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.1634
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 56.9011
                       Mean reward: 1.93
               Mean episode length: 203.51
    Episode_Reward/reaching_object: 0.4018
     Episode_Reward/lifting_object: -0.0509
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 3.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.1250
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 1.98s
                      Time elapsed: 00:05:21
                               ETA: 01:07:02

################################################################################
                     [1m Learning iteration 148/2000 [0m                      

                       Computation: 51743 steps/s (collection: 1.792s, learning 0.108s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.1312
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 56.9957
                       Mean reward: 1.78
               Mean episode length: 205.17
    Episode_Reward/reaching_object: 0.4097
     Episode_Reward/lifting_object: -0.0161
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.3333
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 1.90s
                      Time elapsed: 00:05:23
                               ETA: 01:06:56

################################################################################
                     [1m Learning iteration 149/2000 [0m                      

                       Computation: 51481 steps/s (collection: 1.802s, learning 0.107s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.9599
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.0563
                       Mean reward: 0.49
               Mean episode length: 208.55
    Episode_Reward/reaching_object: 0.4127
     Episode_Reward/lifting_object: -0.1054
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.8333
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 1.91s
                      Time elapsed: 00:05:25
                               ETA: 01:06:51

################################################################################
                     [1m Learning iteration 150/2000 [0m                      

                       Computation: 52148 steps/s (collection: 1.797s, learning 0.088s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.0082
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 57.0921
                       Mean reward: 2.02
               Mean episode length: 207.82
    Episode_Reward/reaching_object: 0.4209
     Episode_Reward/lifting_object: -0.0263
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 1.89s
                      Time elapsed: 00:05:26
                               ETA: 01:06:45

################################################################################
                     [1m Learning iteration 151/2000 [0m                      

                       Computation: 51877 steps/s (collection: 1.793s, learning 0.102s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.7554
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 57.1679
                       Mean reward: 1.15
               Mean episode length: 211.84
    Episode_Reward/reaching_object: 0.4088
     Episode_Reward/lifting_object: -0.0399
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.8333
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 1.89s
                      Time elapsed: 00:05:28
                               ETA: 01:06:40

################################################################################
                     [1m Learning iteration 152/2000 [0m                      

                       Computation: 51501 steps/s (collection: 1.789s, learning 0.120s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.1326
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.2016
                       Mean reward: 1.72
               Mean episode length: 212.43
    Episode_Reward/reaching_object: 0.4133
     Episode_Reward/lifting_object: -0.0831
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.8333
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 1.91s
                      Time elapsed: 00:05:30
                               ETA: 01:06:34

################################################################################
                     [1m Learning iteration 153/2000 [0m                      

                       Computation: 52269 steps/s (collection: 1.790s, learning 0.091s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.0482
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.3158
                       Mean reward: 1.38
               Mean episode length: 204.61
    Episode_Reward/reaching_object: 0.4129
     Episode_Reward/lifting_object: -0.0317
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 1.88s
                      Time elapsed: 00:05:32
                               ETA: 01:06:29

################################################################################
                     [1m Learning iteration 154/2000 [0m                      

                       Computation: 51763 steps/s (collection: 1.792s, learning 0.108s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.6513
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 57.3885
                       Mean reward: 1.91
               Mean episode length: 198.39
    Episode_Reward/reaching_object: 0.4190
     Episode_Reward/lifting_object: -0.0863
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.1667
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 1.90s
                      Time elapsed: 00:05:34
                               ETA: 01:06:23

################################################################################
                     [1m Learning iteration 155/2000 [0m                      

                       Computation: 52681 steps/s (collection: 1.764s, learning 0.102s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.5219
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 57.4202
                       Mean reward: 1.86
               Mean episode length: 200.00
    Episode_Reward/reaching_object: 0.4152
     Episode_Reward/lifting_object: -0.0651
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.4583
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 1.87s
                      Time elapsed: 00:05:36
                               ETA: 01:06:18

################################################################################
                     [1m Learning iteration 156/2000 [0m                      

                       Computation: 51349 steps/s (collection: 1.814s, learning 0.101s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.0318
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 57.4655
                       Mean reward: 1.84
               Mean episode length: 202.20
    Episode_Reward/reaching_object: 0.4046
     Episode_Reward/lifting_object: -0.0069
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.7917
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 1.91s
                      Time elapsed: 00:05:38
                               ETA: 01:06:13

################################################################################
                     [1m Learning iteration 157/2000 [0m                      

                       Computation: 52085 steps/s (collection: 1.788s, learning 0.099s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.8491
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 57.5437
                       Mean reward: 0.04
               Mean episode length: 199.29
    Episode_Reward/reaching_object: 0.4019
     Episode_Reward/lifting_object: -0.0709
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 18.1667
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 1.89s
                      Time elapsed: 00:05:40
                               ETA: 01:06:08

################################################################################
                     [1m Learning iteration 158/2000 [0m                      

                       Computation: 50763 steps/s (collection: 1.839s, learning 0.097s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.2208
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 57.5815
                       Mean reward: 1.55
               Mean episode length: 199.57
    Episode_Reward/reaching_object: 0.3975
     Episode_Reward/lifting_object: -0.0250
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.4167
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 1.94s
                      Time elapsed: 00:05:42
                               ETA: 01:06:03

################################################################################
                     [1m Learning iteration 159/2000 [0m                      

                       Computation: 51228 steps/s (collection: 1.828s, learning 0.091s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.1414
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 57.6991
                       Mean reward: 1.89
               Mean episode length: 204.76
    Episode_Reward/reaching_object: 0.4233
     Episode_Reward/lifting_object: -0.0594
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 4.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.6667
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 1.92s
                      Time elapsed: 00:05:44
                               ETA: 01:05:58

################################################################################
                     [1m Learning iteration 160/2000 [0m                      

                       Computation: 51578 steps/s (collection: 1.791s, learning 0.115s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.2871
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 57.7805
                       Mean reward: 1.77
               Mean episode length: 200.36
    Episode_Reward/reaching_object: 0.4091
     Episode_Reward/lifting_object: -0.0437
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.1250
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 1.91s
                      Time elapsed: 00:05:45
                               ETA: 01:05:53

################################################################################
                     [1m Learning iteration 161/2000 [0m                      

                       Computation: 51562 steps/s (collection: 1.799s, learning 0.107s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.0709
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 57.8193
                       Mean reward: 2.13
               Mean episode length: 206.25
    Episode_Reward/reaching_object: 0.4435
     Episode_Reward/lifting_object: -0.0238
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.9167
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 1.91s
                      Time elapsed: 00:05:47
                               ETA: 01:05:48

################################################################################
                     [1m Learning iteration 162/2000 [0m                      

                       Computation: 49850 steps/s (collection: 1.844s, learning 0.128s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.1872
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 57.9186
                       Mean reward: 1.67
               Mean episode length: 198.12
    Episode_Reward/reaching_object: 0.4329
     Episode_Reward/lifting_object: -0.0639
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.0833
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 1.97s
                      Time elapsed: 00:05:49
                               ETA: 01:05:44

################################################################################
                     [1m Learning iteration 163/2000 [0m                      

                       Computation: 49371 steps/s (collection: 1.854s, learning 0.138s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.5244
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.9511
                       Mean reward: 0.82
               Mean episode length: 207.61
    Episode_Reward/reaching_object: 0.4315
     Episode_Reward/lifting_object: -0.0696
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 4.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.9167
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 1.99s
                      Time elapsed: 00:05:51
                               ETA: 01:05:40

################################################################################
                     [1m Learning iteration 164/2000 [0m                      

                       Computation: 52801 steps/s (collection: 1.757s, learning 0.105s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.0504
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 57.9883
                       Mean reward: 2.13
               Mean episode length: 206.81
    Episode_Reward/reaching_object: 0.4474
     Episode_Reward/lifting_object: -0.0287
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 1.86s
                      Time elapsed: 00:05:53
                               ETA: 01:05:35

################################################################################
                     [1m Learning iteration 165/2000 [0m                      

                       Computation: 47757 steps/s (collection: 1.965s, learning 0.094s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.1392
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 58.0787
                       Mean reward: 1.66
               Mean episode length: 204.81
    Episode_Reward/reaching_object: 0.4238
     Episode_Reward/lifting_object: -0.0255
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 5.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 2.06s
                      Time elapsed: 00:05:55
                               ETA: 01:05:32

################################################################################
                     [1m Learning iteration 166/2000 [0m                      

                       Computation: 52641 steps/s (collection: 1.778s, learning 0.089s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.0718
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.1176
                       Mean reward: 1.96
               Mean episode length: 211.17
    Episode_Reward/reaching_object: 0.4630
     Episode_Reward/lifting_object: -0.0017
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 1.87s
                      Time elapsed: 00:05:57
                               ETA: 01:05:27

################################################################################
                     [1m Learning iteration 167/2000 [0m                      

                       Computation: 52210 steps/s (collection: 1.778s, learning 0.105s)
             Mean action noise std: 1.58
          Mean value_function loss: 1.0953
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 58.2195
                       Mean reward: 2.01
               Mean episode length: 209.47
    Episode_Reward/reaching_object: 0.4622
     Episode_Reward/lifting_object: -0.0900
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 6.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 1.88s
                      Time elapsed: 00:05:59
                               ETA: 01:05:22

################################################################################
                     [1m Learning iteration 168/2000 [0m                      

                       Computation: 52197 steps/s (collection: 1.793s, learning 0.090s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.0774
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.2576
                       Mean reward: 1.37
               Mean episode length: 211.31
    Episode_Reward/reaching_object: 0.4875
     Episode_Reward/lifting_object: -0.0445
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 1.88s
                      Time elapsed: 00:06:01
                               ETA: 01:05:17

################################################################################
                     [1m Learning iteration 169/2000 [0m                      

                       Computation: 52441 steps/s (collection: 1.781s, learning 0.094s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.6588
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 58.3489
                       Mean reward: 2.39
               Mean episode length: 213.93
    Episode_Reward/reaching_object: 0.5028
     Episode_Reward/lifting_object: -0.0928
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 1.87s
                      Time elapsed: 00:06:03
                               ETA: 01:05:12

################################################################################
                     [1m Learning iteration 170/2000 [0m                      

                       Computation: 49365 steps/s (collection: 1.858s, learning 0.133s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.0536
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.3803
                       Mean reward: 2.07
               Mean episode length: 212.10
    Episode_Reward/reaching_object: 0.4911
     Episode_Reward/lifting_object: -0.0093
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 1.99s
                      Time elapsed: 00:06:05
                               ETA: 01:05:08

################################################################################
                     [1m Learning iteration 171/2000 [0m                      

                       Computation: 48554 steps/s (collection: 1.900s, learning 0.125s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.0269
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 58.4770
                       Mean reward: 2.22
               Mean episode length: 211.64
    Episode_Reward/reaching_object: 0.5105
     Episode_Reward/lifting_object: -0.0141
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 2.02s
                      Time elapsed: 00:06:07
                               ETA: 01:05:05

################################################################################
                     [1m Learning iteration 172/2000 [0m                      

                       Computation: 48967 steps/s (collection: 1.912s, learning 0.096s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.1244
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 58.5708
                       Mean reward: 2.43
               Mean episode length: 221.82
    Episode_Reward/reaching_object: 0.5275
     Episode_Reward/lifting_object: -0.0511
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 2.01s
                      Time elapsed: 00:06:09
                               ETA: 01:05:01

################################################################################
                     [1m Learning iteration 173/2000 [0m                      

                       Computation: 51158 steps/s (collection: 1.830s, learning 0.092s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.0390
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 58.5921
                       Mean reward: 2.13
               Mean episode length: 225.40
    Episode_Reward/reaching_object: 0.5132
     Episode_Reward/lifting_object: -0.0201
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 1.92s
                      Time elapsed: 00:06:11
                               ETA: 01:04:57

################################################################################
                     [1m Learning iteration 174/2000 [0m                      

                       Computation: 50606 steps/s (collection: 1.838s, learning 0.105s)
             Mean action noise std: 1.61
          Mean value_function loss: 1.0576
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 58.6414
                       Mean reward: 2.34
               Mean episode length: 224.25
    Episode_Reward/reaching_object: 0.5263
     Episode_Reward/lifting_object: -0.0294
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 1.94s
                      Time elapsed: 00:06:13
                               ETA: 01:04:53

################################################################################
                     [1m Learning iteration 175/2000 [0m                      

                       Computation: 50958 steps/s (collection: 1.829s, learning 0.100s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.3131
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.6636
                       Mean reward: 1.30
               Mean episode length: 221.47
    Episode_Reward/reaching_object: 0.5096
     Episode_Reward/lifting_object: -0.1026
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 1.93s
                      Time elapsed: 00:06:15
                               ETA: 01:04:49

################################################################################
                     [1m Learning iteration 176/2000 [0m                      

                       Computation: 52061 steps/s (collection: 1.784s, learning 0.104s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.3361
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.7354
                       Mean reward: 1.83
               Mean episode length: 219.69
    Episode_Reward/reaching_object: 0.5402
     Episode_Reward/lifting_object: -0.0793
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 1.89s
                      Time elapsed: 00:06:16
                               ETA: 01:04:44

################################################################################
                     [1m Learning iteration 177/2000 [0m                      

                       Computation: 49513 steps/s (collection: 1.828s, learning 0.158s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.1515
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.7658
                       Mean reward: 2.21
               Mean episode length: 210.13
    Episode_Reward/reaching_object: 0.5414
     Episode_Reward/lifting_object: -0.0359
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 1.99s
                      Time elapsed: 00:06:18
                               ETA: 01:04:40

################################################################################
                     [1m Learning iteration 178/2000 [0m                      

                       Computation: 51287 steps/s (collection: 1.803s, learning 0.114s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.2006
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.8309
                       Mean reward: 2.53
               Mean episode length: 209.75
    Episode_Reward/reaching_object: 0.5515
     Episode_Reward/lifting_object: -0.0078
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 1.92s
                      Time elapsed: 00:06:20
                               ETA: 01:04:36

################################################################################
                     [1m Learning iteration 179/2000 [0m                      

                       Computation: 51514 steps/s (collection: 1.805s, learning 0.104s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.2588
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 58.8681
                       Mean reward: 2.29
               Mean episode length: 212.26
    Episode_Reward/reaching_object: 0.5460
     Episode_Reward/lifting_object: -0.0350
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 1.91s
                      Time elapsed: 00:06:22
                               ETA: 01:04:32

################################################################################
                     [1m Learning iteration 180/2000 [0m                      

                       Computation: 51927 steps/s (collection: 1.797s, learning 0.096s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.4043
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 58.9337
                       Mean reward: 1.93
               Mean episode length: 213.77
    Episode_Reward/reaching_object: 0.5661
     Episode_Reward/lifting_object: -0.1152
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 1.89s
                      Time elapsed: 00:06:24
                               ETA: 01:04:27

################################################################################
                     [1m Learning iteration 181/2000 [0m                      

                       Computation: 51152 steps/s (collection: 1.813s, learning 0.109s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.0613
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.9608
                       Mean reward: 3.09
               Mean episode length: 219.66
    Episode_Reward/reaching_object: 0.6369
     Episode_Reward/lifting_object: -0.0379
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 1.92s
                      Time elapsed: 00:06:26
                               ETA: 01:04:23

################################################################################
                     [1m Learning iteration 182/2000 [0m                      

                       Computation: 52337 steps/s (collection: 1.779s, learning 0.100s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.4447
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 59.0238
                       Mean reward: 3.68
               Mean episode length: 231.15
    Episode_Reward/reaching_object: 0.7054
     Episode_Reward/lifting_object: 0.0032
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 1.88s
                      Time elapsed: 00:06:28
                               ETA: 01:04:19

################################################################################
                     [1m Learning iteration 183/2000 [0m                      

                       Computation: 51829 steps/s (collection: 1.809s, learning 0.088s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.1732
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 59.0488
                       Mean reward: 3.41
               Mean episode length: 230.47
    Episode_Reward/reaching_object: 0.7013
     Episode_Reward/lifting_object: -0.0630
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0253
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 1.90s
                      Time elapsed: 00:06:30
                               ETA: 01:04:14

################################################################################
                     [1m Learning iteration 184/2000 [0m                      

                       Computation: 52016 steps/s (collection: 1.800s, learning 0.090s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.2450
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 59.1155
                       Mean reward: 3.21
               Mean episode length: 232.88
    Episode_Reward/reaching_object: 0.6927
     Episode_Reward/lifting_object: -0.1592
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 1.89s
                      Time elapsed: 00:06:32
                               ETA: 01:04:10

################################################################################
                     [1m Learning iteration 185/2000 [0m                      

                       Computation: 51006 steps/s (collection: 1.834s, learning 0.094s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.3089
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 59.1893
                       Mean reward: 3.26
               Mean episode length: 232.51
    Episode_Reward/reaching_object: 0.7018
     Episode_Reward/lifting_object: -0.0311
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 1.93s
                      Time elapsed: 00:06:34
                               ETA: 01:04:06

################################################################################
                     [1m Learning iteration 186/2000 [0m                      

                       Computation: 51511 steps/s (collection: 1.819s, learning 0.089s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.4071
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 59.2251
                       Mean reward: 3.47
               Mean episode length: 226.55
    Episode_Reward/reaching_object: 0.7060
     Episode_Reward/lifting_object: -0.0274
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 1.91s
                      Time elapsed: 00:06:36
                               ETA: 01:04:02

################################################################################
                     [1m Learning iteration 187/2000 [0m                      

                       Computation: 51206 steps/s (collection: 1.825s, learning 0.095s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.3368
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 59.2521
                       Mean reward: 3.25
               Mean episode length: 229.43
    Episode_Reward/reaching_object: 0.7083
     Episode_Reward/lifting_object: -0.0411
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 1.92s
                      Time elapsed: 00:06:37
                               ETA: 01:03:58

################################################################################
                     [1m Learning iteration 188/2000 [0m                      

                       Computation: 51830 steps/s (collection: 1.810s, learning 0.087s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.3501
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 59.3559
                       Mean reward: 3.45
               Mean episode length: 236.60
    Episode_Reward/reaching_object: 0.7230
     Episode_Reward/lifting_object: -0.0707
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 1.90s
                      Time elapsed: 00:06:39
                               ETA: 01:03:53

################################################################################
                     [1m Learning iteration 189/2000 [0m                      

                       Computation: 51566 steps/s (collection: 1.788s, learning 0.118s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.3553
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 59.4633
                       Mean reward: 2.85
               Mean episode length: 231.87
    Episode_Reward/reaching_object: 0.7288
     Episode_Reward/lifting_object: -0.1778
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 1.91s
                      Time elapsed: 00:06:41
                               ETA: 01:03:49

################################################################################
                     [1m Learning iteration 190/2000 [0m                      

                       Computation: 50695 steps/s (collection: 1.840s, learning 0.099s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.9198
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 59.4866
                       Mean reward: 2.89
               Mean episode length: 229.98
    Episode_Reward/reaching_object: 0.7498
     Episode_Reward/lifting_object: -0.1078
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 1.94s
                      Time elapsed: 00:06:43
                               ETA: 01:03:45

################################################################################
                     [1m Learning iteration 191/2000 [0m                      

                       Computation: 51450 steps/s (collection: 1.810s, learning 0.101s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.9032
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 59.5110
                       Mean reward: 2.87
               Mean episode length: 233.70
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: -0.0776
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 1.91s
                      Time elapsed: 00:06:45
                               ETA: 01:03:41

################################################################################
                     [1m Learning iteration 192/2000 [0m                      

                       Computation: 50973 steps/s (collection: 1.835s, learning 0.094s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.0246
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 59.5285
                       Mean reward: 3.76
               Mean episode length: 230.03
    Episode_Reward/reaching_object: 0.8039
     Episode_Reward/lifting_object: -0.1064
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 1.93s
                      Time elapsed: 00:06:47
                               ETA: 01:03:38

################################################################################
                     [1m Learning iteration 193/2000 [0m                      

                       Computation: 52483 steps/s (collection: 1.786s, learning 0.087s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.1649
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 59.5684
                       Mean reward: 3.74
               Mean episode length: 230.67
    Episode_Reward/reaching_object: 0.7806
     Episode_Reward/lifting_object: -0.0867
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 1.87s
                      Time elapsed: 00:06:49
                               ETA: 01:03:33

################################################################################
                     [1m Learning iteration 194/2000 [0m                      

                       Computation: 51503 steps/s (collection: 1.811s, learning 0.098s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.8022
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 59.6397
                       Mean reward: 4.05
               Mean episode length: 235.43
    Episode_Reward/reaching_object: 0.8453
     Episode_Reward/lifting_object: -0.0435
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 1.91s
                      Time elapsed: 00:06:51
                               ETA: 01:03:29

################################################################################
                     [1m Learning iteration 195/2000 [0m                      

                       Computation: 50527 steps/s (collection: 1.845s, learning 0.101s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.7710
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 59.6635
                       Mean reward: 3.62
               Mean episode length: 239.95
    Episode_Reward/reaching_object: 0.8457
     Episode_Reward/lifting_object: -0.0351
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 1.95s
                      Time elapsed: 00:06:53
                               ETA: 01:03:26

################################################################################
                     [1m Learning iteration 196/2000 [0m                      

                       Computation: 50349 steps/s (collection: 1.857s, learning 0.095s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.7051
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 59.7073
                       Mean reward: 3.37
               Mean episode length: 230.12
    Episode_Reward/reaching_object: 0.8251
     Episode_Reward/lifting_object: -0.0853
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 1.95s
                      Time elapsed: 00:06:55
                               ETA: 01:03:22

################################################################################
                     [1m Learning iteration 197/2000 [0m                      

                       Computation: 50244 steps/s (collection: 1.844s, learning 0.112s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.2877
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 59.7533
                       Mean reward: 3.51
               Mean episode length: 236.02
    Episode_Reward/reaching_object: 0.8402
     Episode_Reward/lifting_object: -0.1216
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 1.96s
                      Time elapsed: 00:06:57
                               ETA: 01:03:19

################################################################################
                     [1m Learning iteration 198/2000 [0m                      

                       Computation: 50920 steps/s (collection: 1.825s, learning 0.105s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.3992
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 59.8395
                       Mean reward: 3.92
               Mean episode length: 240.91
    Episode_Reward/reaching_object: 0.8389
     Episode_Reward/lifting_object: -0.0523
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 1.93s
                      Time elapsed: 00:06:59
                               ETA: 01:03:15

################################################################################
                     [1m Learning iteration 199/2000 [0m                      

                       Computation: 51474 steps/s (collection: 1.822s, learning 0.088s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.3936
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.8636
                       Mean reward: 4.02
               Mean episode length: 235.32
    Episode_Reward/reaching_object: 0.8651
     Episode_Reward/lifting_object: -0.1076
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 1.91s
                      Time elapsed: 00:07:01
                               ETA: 01:03:11

################################################################################
                     [1m Learning iteration 200/2000 [0m                      

                       Computation: 51518 steps/s (collection: 1.822s, learning 0.086s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.7176
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 59.9192
                       Mean reward: 2.01
               Mean episode length: 231.19
    Episode_Reward/reaching_object: 0.8505
     Episode_Reward/lifting_object: -0.1527
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 1.91s
                      Time elapsed: 00:07:02
                               ETA: 01:03:07

################################################################################
                     [1m Learning iteration 201/2000 [0m                      

                       Computation: 50874 steps/s (collection: 1.844s, learning 0.089s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.1641
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 59.9374
                       Mean reward: 3.68
               Mean episode length: 227.55
    Episode_Reward/reaching_object: 0.8523
     Episode_Reward/lifting_object: -0.0416
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 1.93s
                      Time elapsed: 00:07:04
                               ETA: 01:03:04

################################################################################
                     [1m Learning iteration 202/2000 [0m                      

                       Computation: 51149 steps/s (collection: 1.827s, learning 0.095s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.5582
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 59.9782
                       Mean reward: 3.51
               Mean episode length: 227.23
    Episode_Reward/reaching_object: 0.8826
     Episode_Reward/lifting_object: -0.0813
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 1.92s
                      Time elapsed: 00:07:06
                               ETA: 01:03:00

################################################################################
                     [1m Learning iteration 203/2000 [0m                      

                       Computation: 51717 steps/s (collection: 1.808s, learning 0.093s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.2488
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 59.9994
                       Mean reward: 4.16
               Mean episode length: 225.03
    Episode_Reward/reaching_object: 0.8744
     Episode_Reward/lifting_object: -0.0926
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 1.90s
                      Time elapsed: 00:07:08
                               ETA: 01:02:56

################################################################################
                     [1m Learning iteration 204/2000 [0m                      

                       Computation: 50549 steps/s (collection: 1.843s, learning 0.102s)
             Mean action noise std: 1.68
          Mean value_function loss: 2.1817
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 60.0426
                       Mean reward: 2.13
               Mean episode length: 225.45
    Episode_Reward/reaching_object: 0.8949
     Episode_Reward/lifting_object: -0.0841
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0268
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 1.94s
                      Time elapsed: 00:07:10
                               ETA: 01:02:53

################################################################################
                     [1m Learning iteration 205/2000 [0m                      

                       Computation: 52233 steps/s (collection: 1.778s, learning 0.104s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.0714
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 60.0586
                       Mean reward: 4.79
               Mean episode length: 243.09
    Episode_Reward/reaching_object: 0.9149
     Episode_Reward/lifting_object: -0.0984
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 1.88s
                      Time elapsed: 00:07:12
                               ETA: 01:02:48

################################################################################
                     [1m Learning iteration 206/2000 [0m                      

                       Computation: 46938 steps/s (collection: 1.969s, learning 0.125s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.0422
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 60.0866
                       Mean reward: 3.99
               Mean episode length: 239.67
    Episode_Reward/reaching_object: 0.9769
     Episode_Reward/lifting_object: -0.1034
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 2.09s
                      Time elapsed: 00:07:14
                               ETA: 01:02:46

################################################################################
                     [1m Learning iteration 207/2000 [0m                      

                       Computation: 49779 steps/s (collection: 1.878s, learning 0.097s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.1167
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 60.1275
                       Mean reward: 3.93
               Mean episode length: 242.49
    Episode_Reward/reaching_object: 0.9237
     Episode_Reward/lifting_object: -0.0408
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 1.97s
                      Time elapsed: 00:07:16
                               ETA: 01:02:43

################################################################################
                     [1m Learning iteration 208/2000 [0m                      

                       Computation: 47336 steps/s (collection: 1.910s, learning 0.167s)
             Mean action noise std: 1.69
          Mean value_function loss: 2.3789
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 60.1726
                       Mean reward: 3.59
               Mean episode length: 227.49
    Episode_Reward/reaching_object: 0.9273
     Episode_Reward/lifting_object: -0.0600
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 2.08s
                      Time elapsed: 00:07:18
                               ETA: 01:02:41

################################################################################
                     [1m Learning iteration 209/2000 [0m                      

                       Computation: 48385 steps/s (collection: 1.904s, learning 0.128s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.1859
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 60.1894
                       Mean reward: 3.81
               Mean episode length: 235.85
    Episode_Reward/reaching_object: 0.9343
     Episode_Reward/lifting_object: -0.1454
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 2.03s
                      Time elapsed: 00:07:20
                               ETA: 01:02:38

################################################################################
                     [1m Learning iteration 210/2000 [0m                      

                       Computation: 49238 steps/s (collection: 1.878s, learning 0.118s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.1347
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 60.2614
                       Mean reward: 4.70
               Mean episode length: 230.00
    Episode_Reward/reaching_object: 0.9513
     Episode_Reward/lifting_object: -0.0210
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 2.00s
                      Time elapsed: 00:07:22
                               ETA: 01:02:35

################################################################################
                     [1m Learning iteration 211/2000 [0m                      

                       Computation: 49323 steps/s (collection: 1.844s, learning 0.149s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.2299
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 60.3593
                       Mean reward: 4.21
               Mean episode length: 232.03
    Episode_Reward/reaching_object: 0.9259
     Episode_Reward/lifting_object: -0.0546
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 1.99s
                      Time elapsed: 00:07:24
                               ETA: 01:02:32

################################################################################
                     [1m Learning iteration 212/2000 [0m                      

                       Computation: 49047 steps/s (collection: 1.892s, learning 0.112s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.5746
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 60.4130
                       Mean reward: 4.34
               Mean episode length: 234.93
    Episode_Reward/reaching_object: 0.9330
     Episode_Reward/lifting_object: -0.0708
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 2.00s
                      Time elapsed: 00:07:26
                               ETA: 01:02:29

################################################################################
                     [1m Learning iteration 213/2000 [0m                      

                       Computation: 50567 steps/s (collection: 1.845s, learning 0.099s)
             Mean action noise std: 1.70
          Mean value_function loss: 1.7127
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 60.4405
                       Mean reward: 4.66
               Mean episode length: 216.88
    Episode_Reward/reaching_object: 0.9219
     Episode_Reward/lifting_object: -0.1235
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 1.94s
                      Time elapsed: 00:07:28
                               ETA: 01:02:26

################################################################################
                     [1m Learning iteration 214/2000 [0m                      

                       Computation: 50137 steps/s (collection: 1.860s, learning 0.101s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.4360
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 60.4580
                       Mean reward: 4.84
               Mean episode length: 240.44
    Episode_Reward/reaching_object: 0.9420
     Episode_Reward/lifting_object: -0.0661
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 1.96s
                      Time elapsed: 00:07:30
                               ETA: 01:02:23

################################################################################
                     [1m Learning iteration 215/2000 [0m                      

                       Computation: 50485 steps/s (collection: 1.847s, learning 0.100s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.0569
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 60.5125
                       Mean reward: 4.82
               Mean episode length: 241.55
    Episode_Reward/reaching_object: 0.9673
     Episode_Reward/lifting_object: -0.1044
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 1.95s
                      Time elapsed: 00:07:32
                               ETA: 01:02:19

################################################################################
                     [1m Learning iteration 216/2000 [0m                      

                       Computation: 50607 steps/s (collection: 1.839s, learning 0.104s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.2084
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 60.5776
                       Mean reward: 4.70
               Mean episode length: 245.22
    Episode_Reward/reaching_object: 1.0375
     Episode_Reward/lifting_object: -0.0646
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 1.94s
                      Time elapsed: 00:07:34
                               ETA: 01:02:16

################################################################################
                     [1m Learning iteration 217/2000 [0m                      

                       Computation: 50985 steps/s (collection: 1.811s, learning 0.117s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.5478
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 60.6368
                       Mean reward: 4.19
               Mean episode length: 233.85
    Episode_Reward/reaching_object: 1.0457
     Episode_Reward/lifting_object: -0.0884
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 1.93s
                      Time elapsed: 00:07:36
                               ETA: 01:02:13

################################################################################
                     [1m Learning iteration 218/2000 [0m                      

                       Computation: 50359 steps/s (collection: 1.840s, learning 0.112s)
             Mean action noise std: 1.72
          Mean value_function loss: 1.0277
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 60.6590
                       Mean reward: 3.77
               Mean episode length: 230.30
    Episode_Reward/reaching_object: 1.0216
     Episode_Reward/lifting_object: -0.1194
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 1.95s
                      Time elapsed: 00:07:38
                               ETA: 01:02:09

################################################################################
                     [1m Learning iteration 219/2000 [0m                      

                       Computation: 50376 steps/s (collection: 1.830s, learning 0.121s)
             Mean action noise std: 1.72
          Mean value_function loss: 1.4605
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 60.7012
                       Mean reward: 2.08
               Mean episode length: 226.42
    Episode_Reward/reaching_object: 1.0024
     Episode_Reward/lifting_object: -0.2706
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 1.95s
                      Time elapsed: 00:07:40
                               ETA: 01:02:06

################################################################################
                     [1m Learning iteration 220/2000 [0m                      

                       Computation: 49469 steps/s (collection: 1.843s, learning 0.144s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.9319
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 60.7200
                       Mean reward: 4.35
               Mean episode length: 238.19
    Episode_Reward/reaching_object: 1.0542
     Episode_Reward/lifting_object: -0.0995
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 1.99s
                      Time elapsed: 00:07:42
                               ETA: 01:02:03

################################################################################
                     [1m Learning iteration 221/2000 [0m                      

                       Computation: 50477 steps/s (collection: 1.849s, learning 0.098s)
             Mean action noise std: 1.72
          Mean value_function loss: 1.6582
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 60.7672
                       Mean reward: 2.30
               Mean episode length: 242.81
    Episode_Reward/reaching_object: 1.0279
     Episode_Reward/lifting_object: -0.1242
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 1.95s
                      Time elapsed: 00:07:44
                               ETA: 01:02:00

################################################################################
                     [1m Learning iteration 222/2000 [0m                      

                       Computation: 50565 steps/s (collection: 1.827s, learning 0.117s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.1132
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 60.7833
                       Mean reward: 3.99
               Mean episode length: 239.23
    Episode_Reward/reaching_object: 1.0889
     Episode_Reward/lifting_object: -0.0867
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 1.94s
                      Time elapsed: 00:07:46
                               ETA: 01:01:57

################################################################################
                     [1m Learning iteration 223/2000 [0m                      

                       Computation: 52470 steps/s (collection: 1.784s, learning 0.089s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.0871
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 60.8129
                       Mean reward: 4.93
               Mean episode length: 244.90
    Episode_Reward/reaching_object: 1.0760
     Episode_Reward/lifting_object: -0.0140
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 1.87s
                      Time elapsed: 00:07:48
                               ETA: 01:01:53

################################################################################
                     [1m Learning iteration 224/2000 [0m                      

                       Computation: 50498 steps/s (collection: 1.824s, learning 0.123s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.0985
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 60.8469
                       Mean reward: 4.92
               Mean episode length: 243.88
    Episode_Reward/reaching_object: 1.0587
     Episode_Reward/lifting_object: -0.0137
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 1.95s
                      Time elapsed: 00:07:50
                               ETA: 01:01:50

################################################################################
                     [1m Learning iteration 225/2000 [0m                      

                       Computation: 50932 steps/s (collection: 1.826s, learning 0.105s)
             Mean action noise std: 1.73
          Mean value_function loss: 1.0235
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 60.8954
                       Mean reward: 4.08
               Mean episode length: 235.54
    Episode_Reward/reaching_object: 1.0106
     Episode_Reward/lifting_object: -0.0431
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 1.93s
                      Time elapsed: 00:07:51
                               ETA: 01:01:46

################################################################################
                     [1m Learning iteration 226/2000 [0m                      

                       Computation: 50604 steps/s (collection: 1.849s, learning 0.094s)
             Mean action noise std: 1.73
          Mean value_function loss: 1.4542
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 60.9419
                       Mean reward: 4.70
               Mean episode length: 225.49
    Episode_Reward/reaching_object: 0.9934
     Episode_Reward/lifting_object: -0.0892
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 1.94s
                      Time elapsed: 00:07:53
                               ETA: 01:01:43

################################################################################
                     [1m Learning iteration 227/2000 [0m                      

                       Computation: 48877 steps/s (collection: 1.904s, learning 0.108s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.5114
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 60.9582
                       Mean reward: 4.40
               Mean episode length: 230.68
    Episode_Reward/reaching_object: 1.0526
     Episode_Reward/lifting_object: -0.0437
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 2.01s
                      Time elapsed: 00:07:55
                               ETA: 01:01:40

################################################################################
                     [1m Learning iteration 228/2000 [0m                      

                       Computation: 49956 steps/s (collection: 1.863s, learning 0.105s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.2230
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 61.0158
                       Mean reward: 4.74
               Mean episode length: 238.49
    Episode_Reward/reaching_object: 1.0768
     Episode_Reward/lifting_object: -0.0460
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 1.97s
                      Time elapsed: 00:07:57
                               ETA: 01:01:37

################################################################################
                     [1m Learning iteration 229/2000 [0m                      

                       Computation: 50647 steps/s (collection: 1.835s, learning 0.106s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.1119
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 61.1172
                       Mean reward: 5.13
               Mean episode length: 244.23
    Episode_Reward/reaching_object: 1.0622
     Episode_Reward/lifting_object: 0.0159
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 1.94s
                      Time elapsed: 00:07:59
                               ETA: 01:01:34

################################################################################
                     [1m Learning iteration 230/2000 [0m                      

                       Computation: 50989 steps/s (collection: 1.819s, learning 0.109s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.4712
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 61.1886
                       Mean reward: 4.77
               Mean episode length: 235.70
    Episode_Reward/reaching_object: 1.0748
     Episode_Reward/lifting_object: -0.0356
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 1.93s
                      Time elapsed: 00:08:01
                               ETA: 01:01:31

################################################################################
                     [1m Learning iteration 231/2000 [0m                      

                       Computation: 50495 steps/s (collection: 1.826s, learning 0.121s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.0688
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 61.2181
                       Mean reward: 5.36
               Mean episode length: 240.85
    Episode_Reward/reaching_object: 1.0803
     Episode_Reward/lifting_object: -0.0193
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 1.95s
                      Time elapsed: 00:08:03
                               ETA: 01:01:28

################################################################################
                     [1m Learning iteration 232/2000 [0m                      

                       Computation: 47568 steps/s (collection: 1.974s, learning 0.093s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.5405
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 61.2663
                       Mean reward: 4.86
               Mean episode length: 237.05
    Episode_Reward/reaching_object: 1.1205
     Episode_Reward/lifting_object: -0.0201
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 2.07s
                      Time elapsed: 00:08:05
                               ETA: 01:01:26

################################################################################
                     [1m Learning iteration 233/2000 [0m                      

                       Computation: 49627 steps/s (collection: 1.874s, learning 0.106s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.6089
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 61.3002
                       Mean reward: 4.75
               Mean episode length: 226.69
    Episode_Reward/reaching_object: 1.0858
     Episode_Reward/lifting_object: -0.1111
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 1.98s
                      Time elapsed: 00:08:07
                               ETA: 01:01:23

################################################################################
                     [1m Learning iteration 234/2000 [0m                      

                       Computation: 47005 steps/s (collection: 2.002s, learning 0.090s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.8339
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 61.3524
                       Mean reward: 3.29
               Mean episode length: 222.50
    Episode_Reward/reaching_object: 1.0422
     Episode_Reward/lifting_object: -0.2077
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 2.09s
                      Time elapsed: 00:08:09
                               ETA: 01:01:21

################################################################################
                     [1m Learning iteration 235/2000 [0m                      

                       Computation: 41764 steps/s (collection: 2.183s, learning 0.171s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.2364
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 61.3720
                       Mean reward: 5.18
               Mean episode length: 231.75
    Episode_Reward/reaching_object: 1.0838
     Episode_Reward/lifting_object: -0.0794
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 2.35s
                      Time elapsed: 00:08:12
                               ETA: 01:01:21

################################################################################
                     [1m Learning iteration 236/2000 [0m                      

                       Computation: 38860 steps/s (collection: 2.389s, learning 0.141s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.3148
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 61.4257
                       Mean reward: 5.71
               Mean episode length: 241.81
    Episode_Reward/reaching_object: 1.1295
     Episode_Reward/lifting_object: -0.0698
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 2.53s
                      Time elapsed: 00:08:14
                               ETA: 01:01:22

################################################################################
                     [1m Learning iteration 237/2000 [0m                      

                       Computation: 36243 steps/s (collection: 2.438s, learning 0.274s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.5512
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 61.4667
                       Mean reward: 4.72
               Mean episode length: 235.76
    Episode_Reward/reaching_object: 1.1572
     Episode_Reward/lifting_object: -0.1389
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 2.71s
                      Time elapsed: 00:08:17
                               ETA: 01:01:24

################################################################################
                     [1m Learning iteration 238/2000 [0m                      

                       Computation: 37211 steps/s (collection: 2.453s, learning 0.189s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.2419
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 61.5036
                       Mean reward: 5.40
               Mean episode length: 234.95
    Episode_Reward/reaching_object: 1.1672
     Episode_Reward/lifting_object: -0.0162
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 2.64s
                      Time elapsed: 00:08:20
                               ETA: 01:01:26

################################################################################
                     [1m Learning iteration 239/2000 [0m                      

                       Computation: 34482 steps/s (collection: 2.610s, learning 0.241s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.7884
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 61.5541
                       Mean reward: 5.08
               Mean episode length: 230.63
    Episode_Reward/reaching_object: 1.1275
     Episode_Reward/lifting_object: -0.1397
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 2.85s
                      Time elapsed: 00:08:22
                               ETA: 01:01:30

################################################################################
                     [1m Learning iteration 240/2000 [0m                      

                       Computation: 37288 steps/s (collection: 2.457s, learning 0.179s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.3077
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 61.5758
                       Mean reward: 5.33
               Mean episode length: 231.27
    Episode_Reward/reaching_object: 1.1632
     Episode_Reward/lifting_object: -0.0749
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 2.64s
                      Time elapsed: 00:08:25
                               ETA: 01:01:32

################################################################################
                     [1m Learning iteration 241/2000 [0m                      

                       Computation: 41921 steps/s (collection: 2.183s, learning 0.162s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.3372
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 61.6172
                       Mean reward: 5.10
               Mean episode length: 233.81
    Episode_Reward/reaching_object: 1.1947
     Episode_Reward/lifting_object: -0.0315
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 2.34s
                      Time elapsed: 00:08:27
                               ETA: 01:01:31

################################################################################
                     [1m Learning iteration 242/2000 [0m                      

                       Computation: 39704 steps/s (collection: 2.298s, learning 0.177s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.3889
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 61.6721
                       Mean reward: 5.78
               Mean episode length: 234.48
    Episode_Reward/reaching_object: 1.1647
     Episode_Reward/lifting_object: -0.1108
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 2.48s
                      Time elapsed: 00:08:30
                               ETA: 01:01:32

################################################################################
                     [1m Learning iteration 243/2000 [0m                      

                       Computation: 44211 steps/s (collection: 2.126s, learning 0.098s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.1472
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 61.7496
                       Mean reward: 5.68
               Mean episode length: 239.88
    Episode_Reward/reaching_object: 1.1972
     Episode_Reward/lifting_object: -0.0158
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 2.22s
                      Time elapsed: 00:08:32
                               ETA: 01:01:31

################################################################################
                     [1m Learning iteration 244/2000 [0m                      

                       Computation: 42552 steps/s (collection: 2.139s, learning 0.172s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.4025
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 61.7812
                       Mean reward: 5.51
               Mean episode length: 234.54
    Episode_Reward/reaching_object: 1.2215
     Episode_Reward/lifting_object: -0.0298
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 2.31s
                      Time elapsed: 00:08:34
                               ETA: 01:01:30

################################################################################
                     [1m Learning iteration 245/2000 [0m                      

                       Computation: 40318 steps/s (collection: 2.233s, learning 0.206s)
             Mean action noise std: 1.78
          Mean value_function loss: 1.6446
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 61.8126
                       Mean reward: 5.06
               Mean episode length: 232.85
    Episode_Reward/reaching_object: 1.1668
     Episode_Reward/lifting_object: -0.0914
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 2.44s
                      Time elapsed: 00:08:37
                               ETA: 01:01:30

################################################################################
                     [1m Learning iteration 246/2000 [0m                      

                       Computation: 40322 steps/s (collection: 2.250s, learning 0.188s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.0868
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 61.8263
                       Mean reward: 4.70
               Mean episode length: 247.22
    Episode_Reward/reaching_object: 1.2002
     Episode_Reward/lifting_object: -0.0966
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 2.44s
                      Time elapsed: 00:08:39
                               ETA: 01:01:31

################################################################################
                     [1m Learning iteration 247/2000 [0m                      

                       Computation: 41171 steps/s (collection: 2.222s, learning 0.166s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.1166
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 61.8579
                       Mean reward: 5.66
               Mean episode length: 240.62
    Episode_Reward/reaching_object: 1.2266
     Episode_Reward/lifting_object: -0.0211
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0330
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 2.39s
                      Time elapsed: 00:08:42
                               ETA: 01:01:31

################################################################################
                     [1m Learning iteration 248/2000 [0m                      

                       Computation: 44901 steps/s (collection: 2.017s, learning 0.173s)
             Mean action noise std: 1.79
          Mean value_function loss: 0.0865
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 61.9059
                       Mean reward: 5.59
               Mean episode length: 241.03
    Episode_Reward/reaching_object: 1.1754
     Episode_Reward/lifting_object: 0.0099
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 2.19s
                      Time elapsed: 00:08:44
                               ETA: 01:01:29

################################################################################
                     [1m Learning iteration 249/2000 [0m                      

                       Computation: 46535 steps/s (collection: 1.942s, learning 0.170s)
             Mean action noise std: 1.79
          Mean value_function loss: 0.7039
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 61.9448
                       Mean reward: 5.76
               Mean episode length: 239.21
    Episode_Reward/reaching_object: 1.2039
     Episode_Reward/lifting_object: -0.0141
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 2.11s
                      Time elapsed: 00:08:46
                               ETA: 01:01:27

################################################################################
                     [1m Learning iteration 250/2000 [0m                      

                       Computation: 48163 steps/s (collection: 1.931s, learning 0.110s)
             Mean action noise std: 1.79
          Mean value_function loss: 0.2587
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 61.9649
                       Mean reward: 5.54
               Mean episode length: 224.24
    Episode_Reward/reaching_object: 1.1668
     Episode_Reward/lifting_object: -0.0215
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 2.04s
                      Time elapsed: 00:08:48
                               ETA: 01:01:24

################################################################################
                     [1m Learning iteration 251/2000 [0m                      

                       Computation: 46793 steps/s (collection: 1.983s, learning 0.118s)
             Mean action noise std: 1.79
          Mean value_function loss: 0.4892
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 62.0108
                       Mean reward: 5.40
               Mean episode length: 234.74
    Episode_Reward/reaching_object: 1.2036
     Episode_Reward/lifting_object: -0.0404
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 2.10s
                      Time elapsed: 00:08:50
                               ETA: 01:01:22

################################################################################
                     [1m Learning iteration 252/2000 [0m                      

                       Computation: 48697 steps/s (collection: 1.909s, learning 0.110s)
             Mean action noise std: 1.80
          Mean value_function loss: 0.4133
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 62.0643
                       Mean reward: 5.01
               Mean episode length: 232.64
    Episode_Reward/reaching_object: 1.1762
     Episode_Reward/lifting_object: -0.0269
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 2.02s
                      Time elapsed: 00:08:52
                               ETA: 01:01:20

################################################################################
                     [1m Learning iteration 253/2000 [0m                      

                       Computation: 46778 steps/s (collection: 2.006s, learning 0.096s)
             Mean action noise std: 1.80
          Mean value_function loss: 0.8861
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 62.1195
                       Mean reward: 4.47
               Mean episode length: 240.01
    Episode_Reward/reaching_object: 1.1991
     Episode_Reward/lifting_object: -0.1370
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 2.10s
                      Time elapsed: 00:08:54
                               ETA: 01:01:17

################################################################################
                     [1m Learning iteration 254/2000 [0m                      

                       Computation: 47797 steps/s (collection: 1.927s, learning 0.130s)
             Mean action noise std: 1.80
          Mean value_function loss: 0.3827
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 62.1336
                       Mean reward: 4.53
               Mean episode length: 230.55
    Episode_Reward/reaching_object: 1.1979
     Episode_Reward/lifting_object: -0.0501
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 2.06s
                      Time elapsed: 00:08:56
                               ETA: 01:01:15

################################################################################
                     [1m Learning iteration 255/2000 [0m                      

                       Computation: 48223 steps/s (collection: 1.905s, learning 0.133s)
             Mean action noise std: 1.80
          Mean value_function loss: 0.1779
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 62.1779
                       Mean reward: 4.98
               Mean episode length: 234.48
    Episode_Reward/reaching_object: 1.1835
     Episode_Reward/lifting_object: -0.1164
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 2.04s
                      Time elapsed: 00:08:58
                               ETA: 01:01:12

################################################################################
                     [1m Learning iteration 256/2000 [0m                      

                       Computation: 46793 steps/s (collection: 1.983s, learning 0.118s)
             Mean action noise std: 1.81
          Mean value_function loss: 0.1405
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 62.2519
                       Mean reward: 5.84
               Mean episode length: 235.31
    Episode_Reward/reaching_object: 1.2110
     Episode_Reward/lifting_object: -0.0194
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 2.10s
                      Time elapsed: 00:09:00
                               ETA: 01:01:10

################################################################################
                     [1m Learning iteration 257/2000 [0m                      

                       Computation: 48455 steps/s (collection: 1.908s, learning 0.121s)
             Mean action noise std: 1.81
          Mean value_function loss: 0.0609
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 62.3214
                       Mean reward: 5.41
               Mean episode length: 240.59
    Episode_Reward/reaching_object: 1.2541
     Episode_Reward/lifting_object: -0.0735
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 2.03s
                      Time elapsed: 00:09:02
                               ETA: 01:01:08

################################################################################
                     [1m Learning iteration 258/2000 [0m                      

                       Computation: 49485 steps/s (collection: 1.893s, learning 0.094s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.3818
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 62.3946
                       Mean reward: 5.59
               Mean episode length: 237.67
    Episode_Reward/reaching_object: 1.2321
     Episode_Reward/lifting_object: 0.0007
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 1.99s
                      Time elapsed: 00:09:04
                               ETA: 01:01:05

################################################################################
                     [1m Learning iteration 259/2000 [0m                      

                       Computation: 48509 steps/s (collection: 1.932s, learning 0.094s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.1903
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 62.4524
                       Mean reward: 5.52
               Mean episode length: 238.14
    Episode_Reward/reaching_object: 1.2030
     Episode_Reward/lifting_object: -0.0158
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 2.03s
                      Time elapsed: 00:09:06
                               ETA: 01:01:02

################################################################################
                     [1m Learning iteration 260/2000 [0m                      

                       Computation: 48008 steps/s (collection: 1.934s, learning 0.114s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.3573
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 62.5183
                       Mean reward: 6.24
               Mean episode length: 238.22
    Episode_Reward/reaching_object: 1.2511
     Episode_Reward/lifting_object: -0.0081
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 2.05s
                      Time elapsed: 00:09:09
                               ETA: 01:01:00

################################################################################
                     [1m Learning iteration 261/2000 [0m                      

                       Computation: 46474 steps/s (collection: 1.997s, learning 0.118s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.4839
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 62.5436
                       Mean reward: 6.51
               Mean episode length: 236.34
    Episode_Reward/reaching_object: 1.2270
     Episode_Reward/lifting_object: -0.0244
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 2.12s
                      Time elapsed: 00:09:11
                               ETA: 01:00:58

################################################################################
                     [1m Learning iteration 262/2000 [0m                      

                       Computation: 48897 steps/s (collection: 1.913s, learning 0.097s)
             Mean action noise std: 1.83
          Mean value_function loss: 0.8498
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 62.5860
                       Mean reward: 4.07
               Mean episode length: 230.90
    Episode_Reward/reaching_object: 1.2344
     Episode_Reward/lifting_object: -0.0677
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 2.01s
                      Time elapsed: 00:09:13
                               ETA: 01:00:55

################################################################################
                     [1m Learning iteration 263/2000 [0m                      

                       Computation: 49732 steps/s (collection: 1.882s, learning 0.095s)
             Mean action noise std: 1.83
          Mean value_function loss: 0.4219
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 62.6257
                       Mean reward: 6.27
               Mean episode length: 239.04
    Episode_Reward/reaching_object: 1.2424
     Episode_Reward/lifting_object: -0.0324
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 1.98s
                      Time elapsed: 00:09:15
                               ETA: 01:00:52

################################################################################
                     [1m Learning iteration 264/2000 [0m                      

                       Computation: 49411 steps/s (collection: 1.900s, learning 0.089s)
             Mean action noise std: 1.83
          Mean value_function loss: 0.6278
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 62.6912
                       Mean reward: 5.47
               Mean episode length: 229.39
    Episode_Reward/reaching_object: 1.2533
     Episode_Reward/lifting_object: -0.0628
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 1.99s
                      Time elapsed: 00:09:17
                               ETA: 01:00:49

################################################################################
                     [1m Learning iteration 265/2000 [0m                      

                       Computation: 50142 steps/s (collection: 1.863s, learning 0.098s)
             Mean action noise std: 1.84
          Mean value_function loss: 0.2556
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 62.7500
                       Mean reward: 6.05
               Mean episode length: 237.39
    Episode_Reward/reaching_object: 1.2711
     Episode_Reward/lifting_object: -0.0089
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 1.96s
                      Time elapsed: 00:09:19
                               ETA: 01:00:46

################################################################################
                     [1m Learning iteration 266/2000 [0m                      

                       Computation: 48580 steps/s (collection: 1.924s, learning 0.099s)
             Mean action noise std: 1.84
          Mean value_function loss: 0.6212
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 62.8261
                       Mean reward: 4.66
               Mean episode length: 235.52
    Episode_Reward/reaching_object: 1.2272
     Episode_Reward/lifting_object: -0.1164
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0350
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 2.02s
                      Time elapsed: 00:09:21
                               ETA: 01:00:44

################################################################################
                     [1m Learning iteration 267/2000 [0m                      

                       Computation: 49162 steps/s (collection: 1.890s, learning 0.110s)
             Mean action noise std: 1.85
          Mean value_function loss: 0.4993
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 62.8835
                       Mean reward: 5.12
               Mean episode length: 234.33
    Episode_Reward/reaching_object: 1.2469
     Episode_Reward/lifting_object: -0.0586
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 2.00s
                      Time elapsed: 00:09:23
                               ETA: 01:00:41

################################################################################
                     [1m Learning iteration 268/2000 [0m                      

                       Computation: 47023 steps/s (collection: 1.973s, learning 0.118s)
             Mean action noise std: 1.85
          Mean value_function loss: 0.4961
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 62.9551
                       Mean reward: 5.66
               Mean episode length: 229.23
    Episode_Reward/reaching_object: 1.1993
     Episode_Reward/lifting_object: 0.0262
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 2.09s
                      Time elapsed: 00:09:25
                               ETA: 01:00:39

################################################################################
                     [1m Learning iteration 269/2000 [0m                      

                       Computation: 48997 steps/s (collection: 1.918s, learning 0.088s)
             Mean action noise std: 1.85
          Mean value_function loss: 0.4419
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 63.0130
                       Mean reward: 7.05
               Mean episode length: 241.95
    Episode_Reward/reaching_object: 1.2653
     Episode_Reward/lifting_object: 0.0895
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 2.01s
                      Time elapsed: 00:09:27
                               ETA: 01:00:36

################################################################################
                     [1m Learning iteration 270/2000 [0m                      

                       Computation: 48473 steps/s (collection: 1.915s, learning 0.113s)
             Mean action noise std: 1.86
          Mean value_function loss: 0.3210
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 63.0613
                       Mean reward: 6.30
               Mean episode length: 238.25
    Episode_Reward/reaching_object: 1.2456
     Episode_Reward/lifting_object: 0.0152
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 2.03s
                      Time elapsed: 00:09:29
                               ETA: 01:00:33

################################################################################
                     [1m Learning iteration 271/2000 [0m                      

                       Computation: 48837 steps/s (collection: 1.926s, learning 0.087s)
             Mean action noise std: 1.86
          Mean value_function loss: 0.8274
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 63.1139
                       Mean reward: 6.16
               Mean episode length: 233.65
    Episode_Reward/reaching_object: 1.2275
     Episode_Reward/lifting_object: 0.0216
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 2.01s
                      Time elapsed: 00:09:31
                               ETA: 01:00:31

################################################################################
                     [1m Learning iteration 272/2000 [0m                      

                       Computation: 48011 steps/s (collection: 1.956s, learning 0.092s)
             Mean action noise std: 1.86
          Mean value_function loss: 0.4285
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 63.1479
                       Mean reward: 6.49
               Mean episode length: 240.61
    Episode_Reward/reaching_object: 1.2464
     Episode_Reward/lifting_object: 0.0235
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 2.05s
                      Time elapsed: 00:09:33
                               ETA: 01:00:28

################################################################################
                     [1m Learning iteration 273/2000 [0m                      

                       Computation: 49172 steps/s (collection: 1.912s, learning 0.088s)
             Mean action noise std: 1.86
          Mean value_function loss: 0.5350
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 63.2051
                       Mean reward: 7.11
               Mean episode length: 242.48
    Episode_Reward/reaching_object: 1.2267
     Episode_Reward/lifting_object: 0.1122
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 2.00s
                      Time elapsed: 00:09:35
                               ETA: 01:00:26

################################################################################
                     [1m Learning iteration 274/2000 [0m                      

                       Computation: 49390 steps/s (collection: 1.889s, learning 0.102s)
             Mean action noise std: 1.87
          Mean value_function loss: 0.1774
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 63.2567
                       Mean reward: 6.49
               Mean episode length: 239.27
    Episode_Reward/reaching_object: 1.2357
     Episode_Reward/lifting_object: -0.0235
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 1.99s
                      Time elapsed: 00:09:37
                               ETA: 01:00:23

################################################################################
                     [1m Learning iteration 275/2000 [0m                      

                       Computation: 49341 steps/s (collection: 1.903s, learning 0.090s)
             Mean action noise std: 1.87
          Mean value_function loss: 0.6933
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 63.3224
                       Mean reward: 5.67
               Mean episode length: 232.04
    Episode_Reward/reaching_object: 1.2508
     Episode_Reward/lifting_object: 0.0236
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 1.99s
                      Time elapsed: 00:09:39
                               ETA: 01:00:20

################################################################################
                     [1m Learning iteration 276/2000 [0m                      

                       Computation: 49578 steps/s (collection: 1.874s, learning 0.109s)
             Mean action noise std: 1.87
          Mean value_function loss: 0.3943
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 63.3605
                       Mean reward: 5.99
               Mean episode length: 236.36
    Episode_Reward/reaching_object: 1.2212
     Episode_Reward/lifting_object: 0.0381
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 1.98s
                      Time elapsed: 00:09:41
                               ETA: 01:00:17

################################################################################
                     [1m Learning iteration 277/2000 [0m                      

                       Computation: 49712 steps/s (collection: 1.888s, learning 0.089s)
             Mean action noise std: 1.88
          Mean value_function loss: 0.3621
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 63.3915
                       Mean reward: 5.28
               Mean episode length: 232.21
    Episode_Reward/reaching_object: 1.1931
     Episode_Reward/lifting_object: 0.0292
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 1.98s
                      Time elapsed: 00:09:43
                               ETA: 01:00:14

################################################################################
                     [1m Learning iteration 278/2000 [0m                      

                       Computation: 48894 steps/s (collection: 1.923s, learning 0.088s)
             Mean action noise std: 1.88
          Mean value_function loss: 0.3880
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 63.4551
                       Mean reward: 5.85
               Mean episode length: 237.17
    Episode_Reward/reaching_object: 1.2931
     Episode_Reward/lifting_object: 0.0071
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 2.01s
                      Time elapsed: 00:09:45
                               ETA: 01:00:12

################################################################################
                     [1m Learning iteration 279/2000 [0m                      

                       Computation: 49210 steps/s (collection: 1.894s, learning 0.104s)
             Mean action noise std: 1.88
          Mean value_function loss: 0.2873
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 63.5294
                       Mean reward: 5.59
               Mean episode length: 236.72
    Episode_Reward/reaching_object: 1.1971
     Episode_Reward/lifting_object: 0.0734
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 2.00s
                      Time elapsed: 00:09:47
                               ETA: 01:00:09

################################################################################
                     [1m Learning iteration 280/2000 [0m                      

                       Computation: 45687 steps/s (collection: 2.009s, learning 0.143s)
             Mean action noise std: 1.89
          Mean value_function loss: 0.6012
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 63.5858
                       Mean reward: 6.35
               Mean episode length: 244.28
    Episode_Reward/reaching_object: 1.2944
     Episode_Reward/lifting_object: 0.0320
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 2.15s
                      Time elapsed: 00:09:49
                               ETA: 01:00:07

################################################################################
                     [1m Learning iteration 281/2000 [0m                      

                       Computation: 45229 steps/s (collection: 2.071s, learning 0.103s)
             Mean action noise std: 1.89
          Mean value_function loss: 1.0675
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 63.6219
                       Mean reward: 6.89
               Mean episode length: 246.85
    Episode_Reward/reaching_object: 1.2777
     Episode_Reward/lifting_object: 0.0719
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 2.17s
                      Time elapsed: 00:09:51
                               ETA: 01:00:06

################################################################################
                     [1m Learning iteration 282/2000 [0m                      

                       Computation: 47177 steps/s (collection: 1.951s, learning 0.133s)
             Mean action noise std: 1.89
          Mean value_function loss: 0.5793
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 63.6674
                       Mean reward: 5.16
               Mean episode length: 243.26
    Episode_Reward/reaching_object: 1.2668
     Episode_Reward/lifting_object: -0.1273
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 18.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 2.08s
                      Time elapsed: 00:09:53
                               ETA: 01:00:03

################################################################################
                     [1m Learning iteration 283/2000 [0m                      

                       Computation: 45320 steps/s (collection: 2.058s, learning 0.112s)
             Mean action noise std: 1.90
          Mean value_function loss: 0.4626
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 63.7214
                       Mean reward: 6.06
               Mean episode length: 239.31
    Episode_Reward/reaching_object: 1.2502
     Episode_Reward/lifting_object: 0.0728
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 2.17s
                      Time elapsed: 00:09:55
                               ETA: 01:00:02

################################################################################
                     [1m Learning iteration 284/2000 [0m                      

                       Computation: 47616 steps/s (collection: 1.945s, learning 0.119s)
             Mean action noise std: 1.90
          Mean value_function loss: 0.5134
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 63.7880
                       Mean reward: 6.43
               Mean episode length: 240.07
    Episode_Reward/reaching_object: 1.1993
     Episode_Reward/lifting_object: 0.0633
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 2.06s
                      Time elapsed: 00:09:57
                               ETA: 00:59:59

################################################################################
                     [1m Learning iteration 285/2000 [0m                      

                       Computation: 47969 steps/s (collection: 1.952s, learning 0.097s)
             Mean action noise std: 1.90
          Mean value_function loss: 0.3824
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 63.8393
                       Mean reward: 5.74
               Mean episode length: 227.30
    Episode_Reward/reaching_object: 1.2056
     Episode_Reward/lifting_object: 0.0732
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 2.05s
                      Time elapsed: 00:09:59
                               ETA: 00:59:57

################################################################################
                     [1m Learning iteration 286/2000 [0m                      

                       Computation: 46949 steps/s (collection: 2.000s, learning 0.094s)
             Mean action noise std: 1.91
          Mean value_function loss: 1.1955
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 63.8931
                       Mean reward: 7.12
               Mean episode length: 241.71
    Episode_Reward/reaching_object: 1.2704
     Episode_Reward/lifting_object: 0.0441
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 2.09s
                      Time elapsed: 00:10:02
                               ETA: 00:59:55

################################################################################
                     [1m Learning iteration 287/2000 [0m                      

                       Computation: 49523 steps/s (collection: 1.878s, learning 0.107s)
             Mean action noise std: 1.91
          Mean value_function loss: 0.2078
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 63.9716
                       Mean reward: 7.22
               Mean episode length: 240.31
    Episode_Reward/reaching_object: 1.2319
     Episode_Reward/lifting_object: 0.0545
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 1.98s
                      Time elapsed: 00:10:04
                               ETA: 00:59:52

################################################################################
                     [1m Learning iteration 288/2000 [0m                      

                       Computation: 50261 steps/s (collection: 1.862s, learning 0.094s)
             Mean action noise std: 1.91
          Mean value_function loss: 0.5161
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 64.0284
                       Mean reward: 7.16
               Mean episode length: 241.10
    Episode_Reward/reaching_object: 1.2310
     Episode_Reward/lifting_object: 0.2119
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 1.96s
                      Time elapsed: 00:10:05
                               ETA: 00:59:49

################################################################################
                     [1m Learning iteration 289/2000 [0m                      

                       Computation: 47951 steps/s (collection: 1.952s, learning 0.098s)
             Mean action noise std: 1.92
          Mean value_function loss: 0.6305
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 64.0662
                       Mean reward: 6.04
               Mean episode length: 236.41
    Episode_Reward/reaching_object: 1.1817
     Episode_Reward/lifting_object: 0.1400
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 2.05s
                      Time elapsed: 00:10:08
                               ETA: 00:59:47

################################################################################
                     [1m Learning iteration 290/2000 [0m                      

                       Computation: 50042 steps/s (collection: 1.860s, learning 0.104s)
             Mean action noise std: 1.92
          Mean value_function loss: 0.6953
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 64.1095
                       Mean reward: 6.29
               Mean episode length: 235.52
    Episode_Reward/reaching_object: 1.1521
     Episode_Reward/lifting_object: 0.1170
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 1.96s
                      Time elapsed: 00:10:09
                               ETA: 00:59:44

################################################################################
                     [1m Learning iteration 291/2000 [0m                      

                       Computation: 49161 steps/s (collection: 1.911s, learning 0.088s)
             Mean action noise std: 1.92
          Mean value_function loss: 0.6659
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 64.1376
                       Mean reward: 7.02
               Mean episode length: 234.33
    Episode_Reward/reaching_object: 1.1867
     Episode_Reward/lifting_object: 0.0933
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 2.00s
                      Time elapsed: 00:10:11
                               ETA: 00:59:41

################################################################################
                     [1m Learning iteration 292/2000 [0m                      

                       Computation: 49541 steps/s (collection: 1.892s, learning 0.093s)
             Mean action noise std: 1.92
          Mean value_function loss: 0.5593
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 64.1932
                       Mean reward: 6.35
               Mean episode length: 234.48
    Episode_Reward/reaching_object: 1.1965
     Episode_Reward/lifting_object: 0.0313
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 1.98s
                      Time elapsed: 00:10:13
                               ETA: 00:59:39

################################################################################
                     [1m Learning iteration 293/2000 [0m                      

                       Computation: 49660 steps/s (collection: 1.871s, learning 0.108s)
             Mean action noise std: 1.93
          Mean value_function loss: 0.7687
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 64.2553
                       Mean reward: 6.42
               Mean episode length: 234.18
    Episode_Reward/reaching_object: 1.2092
     Episode_Reward/lifting_object: 0.0334
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 1.98s
                      Time elapsed: 00:10:15
                               ETA: 00:59:36

################################################################################
                     [1m Learning iteration 294/2000 [0m                      

                       Computation: 49689 steps/s (collection: 1.877s, learning 0.102s)
             Mean action noise std: 1.93
          Mean value_function loss: 0.8328
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 64.3086
                       Mean reward: 5.82
               Mean episode length: 230.98
    Episode_Reward/reaching_object: 1.1903
     Episode_Reward/lifting_object: 0.0880
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 1.98s
                      Time elapsed: 00:10:17
                               ETA: 00:59:33

################################################################################
                     [1m Learning iteration 295/2000 [0m                      

                       Computation: 49129 steps/s (collection: 1.910s, learning 0.091s)
             Mean action noise std: 1.93
          Mean value_function loss: 0.4591
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 64.3491
                       Mean reward: 6.85
               Mean episode length: 236.60
    Episode_Reward/reaching_object: 1.1779
     Episode_Reward/lifting_object: 0.0707
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 2.00s
                      Time elapsed: 00:10:19
                               ETA: 00:59:30

################################################################################
                     [1m Learning iteration 296/2000 [0m                      

                       Computation: 49878 steps/s (collection: 1.883s, learning 0.088s)
             Mean action noise std: 1.94
          Mean value_function loss: 0.5039
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 64.4027
                       Mean reward: 6.09
               Mean episode length: 232.30
    Episode_Reward/reaching_object: 1.1319
     Episode_Reward/lifting_object: 0.1722
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 1.97s
                      Time elapsed: 00:10:21
                               ETA: 00:59:28

################################################################################
                     [1m Learning iteration 297/2000 [0m                      

                       Computation: 49040 steps/s (collection: 1.910s, learning 0.095s)
             Mean action noise std: 1.94
          Mean value_function loss: 0.6342
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 64.4829
                       Mean reward: 6.94
               Mean episode length: 234.61
    Episode_Reward/reaching_object: 1.1649
     Episode_Reward/lifting_object: 0.2106
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 2.00s
                      Time elapsed: 00:10:23
                               ETA: 00:59:25

################################################################################
                     [1m Learning iteration 298/2000 [0m                      

                       Computation: 49549 steps/s (collection: 1.895s, learning 0.089s)
             Mean action noise std: 1.95
          Mean value_function loss: 0.5649
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 64.5604
                       Mean reward: 6.34
               Mean episode length: 245.37
    Episode_Reward/reaching_object: 1.1344
     Episode_Reward/lifting_object: 0.2419
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 1.98s
                      Time elapsed: 00:10:25
                               ETA: 00:59:22

################################################################################
                     [1m Learning iteration 299/2000 [0m                      

                       Computation: 48813 steps/s (collection: 1.890s, learning 0.124s)
             Mean action noise std: 1.95
          Mean value_function loss: 0.7036
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 64.6147
                       Mean reward: 6.47
               Mean episode length: 238.90
    Episode_Reward/reaching_object: 1.1624
     Episode_Reward/lifting_object: 0.1228
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 2.01s
                      Time elapsed: 00:10:27
                               ETA: 00:59:20

################################################################################
                     [1m Learning iteration 300/2000 [0m                      

                       Computation: 49356 steps/s (collection: 1.901s, learning 0.091s)
             Mean action noise std: 1.95
          Mean value_function loss: 0.7806
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 64.6709
                       Mean reward: 5.75
               Mean episode length: 238.57
    Episode_Reward/reaching_object: 1.1297
     Episode_Reward/lifting_object: 0.1619
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 1.99s
                      Time elapsed: 00:10:29
                               ETA: 00:59:17

################################################################################
                     [1m Learning iteration 301/2000 [0m                      

                       Computation: 49238 steps/s (collection: 1.909s, learning 0.087s)
             Mean action noise std: 1.96
          Mean value_function loss: 0.6008
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 64.7136
                       Mean reward: 6.70
               Mean episode length: 239.63
    Episode_Reward/reaching_object: 1.1792
     Episode_Reward/lifting_object: 0.1976
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 2.00s
                      Time elapsed: 00:10:31
                               ETA: 00:59:14

################################################################################
                     [1m Learning iteration 302/2000 [0m                      

                       Computation: 49322 steps/s (collection: 1.906s, learning 0.088s)
             Mean action noise std: 1.96
          Mean value_function loss: 0.7334
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 64.7747
                       Mean reward: 7.98
               Mean episode length: 241.53
    Episode_Reward/reaching_object: 1.1700
     Episode_Reward/lifting_object: 0.3349
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0445
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 1.99s
                      Time elapsed: 00:10:33
                               ETA: 00:59:12

################################################################################
                     [1m Learning iteration 303/2000 [0m                      

                       Computation: 49288 steps/s (collection: 1.872s, learning 0.122s)
             Mean action noise std: 1.96
          Mean value_function loss: 1.5927
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 64.8220
                       Mean reward: 7.08
               Mean episode length: 234.67
    Episode_Reward/reaching_object: 1.1548
     Episode_Reward/lifting_object: 0.2494
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 1.99s
                      Time elapsed: 00:10:35
                               ETA: 00:59:09

################################################################################
                     [1m Learning iteration 304/2000 [0m                      

                       Computation: 47624 steps/s (collection: 1.949s, learning 0.115s)
             Mean action noise std: 1.97
          Mean value_function loss: 0.7142
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 64.8508
                       Mean reward: 7.21
               Mean episode length: 236.98
    Episode_Reward/reaching_object: 1.1339
     Episode_Reward/lifting_object: 0.3035
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 2.06s
                      Time elapsed: 00:10:37
                               ETA: 00:59:07

################################################################################
                     [1m Learning iteration 305/2000 [0m                      

                       Computation: 49535 steps/s (collection: 1.885s, learning 0.100s)
             Mean action noise std: 1.97
          Mean value_function loss: 0.7388
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 64.9161
                       Mean reward: 6.56
               Mean episode length: 235.00
    Episode_Reward/reaching_object: 1.1593
     Episode_Reward/lifting_object: 0.3166
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 1.98s
                      Time elapsed: 00:10:39
                               ETA: 00:59:04

################################################################################
                     [1m Learning iteration 306/2000 [0m                      

                       Computation: 48163 steps/s (collection: 1.945s, learning 0.097s)
             Mean action noise std: 1.97
          Mean value_function loss: 0.8478
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 64.9691
                       Mean reward: 7.24
               Mean episode length: 236.90
    Episode_Reward/reaching_object: 1.1337
     Episode_Reward/lifting_object: 0.4455
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 2.04s
                      Time elapsed: 00:10:41
                               ETA: 00:59:02

################################################################################
                     [1m Learning iteration 307/2000 [0m                      

                       Computation: 48815 steps/s (collection: 1.910s, learning 0.104s)
             Mean action noise std: 1.98
          Mean value_function loss: 0.8671
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 65.0185
                       Mean reward: 6.08
               Mean episode length: 226.32
    Episode_Reward/reaching_object: 1.1044
     Episode_Reward/lifting_object: 0.2728
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 2.01s
                      Time elapsed: 00:10:43
                               ETA: 00:58:59

################################################################################
                     [1m Learning iteration 308/2000 [0m                      

                       Computation: 48640 steps/s (collection: 1.934s, learning 0.087s)
             Mean action noise std: 1.98
          Mean value_function loss: 1.5618
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 65.0814
                       Mean reward: 7.46
               Mean episode length: 231.60
    Episode_Reward/reaching_object: 1.0724
     Episode_Reward/lifting_object: 0.3961
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 2.02s
                      Time elapsed: 00:10:46
                               ETA: 00:58:57

################################################################################
                     [1m Learning iteration 309/2000 [0m                      

                       Computation: 48698 steps/s (collection: 1.929s, learning 0.089s)
             Mean action noise std: 1.98
          Mean value_function loss: 1.5997
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 65.1449
                       Mean reward: 6.49
               Mean episode length: 228.22
    Episode_Reward/reaching_object: 1.1047
     Episode_Reward/lifting_object: 0.4213
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 2.02s
                      Time elapsed: 00:10:48
                               ETA: 00:58:54

################################################################################
                     [1m Learning iteration 310/2000 [0m                      

                       Computation: 49083 steps/s (collection: 1.908s, learning 0.095s)
             Mean action noise std: 1.99
          Mean value_function loss: 1.2404
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 65.1963
                       Mean reward: 7.08
               Mean episode length: 231.80
    Episode_Reward/reaching_object: 1.0854
     Episode_Reward/lifting_object: 0.3231
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 2.00s
                      Time elapsed: 00:10:50
                               ETA: 00:58:52

################################################################################
                     [1m Learning iteration 311/2000 [0m                      

                       Computation: 48370 steps/s (collection: 1.939s, learning 0.093s)
             Mean action noise std: 1.99
          Mean value_function loss: 1.0740
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 65.2463
                       Mean reward: 5.51
               Mean episode length: 226.88
    Episode_Reward/reaching_object: 1.0916
     Episode_Reward/lifting_object: 0.2121
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 2.03s
                      Time elapsed: 00:10:52
                               ETA: 00:58:49

################################################################################
                     [1m Learning iteration 312/2000 [0m                      

                       Computation: 47755 steps/s (collection: 1.967s, learning 0.092s)
             Mean action noise std: 1.99
          Mean value_function loss: 1.2247
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 65.3071
                       Mean reward: 8.26
               Mean episode length: 224.81
    Episode_Reward/reaching_object: 1.0334
     Episode_Reward/lifting_object: 0.3302
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 2.06s
                      Time elapsed: 00:10:54
                               ETA: 00:58:47

################################################################################
                     [1m Learning iteration 313/2000 [0m                      

                       Computation: 48341 steps/s (collection: 1.926s, learning 0.108s)
             Mean action noise std: 2.00
          Mean value_function loss: 1.0213
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 65.3366
                       Mean reward: 5.71
               Mean episode length: 229.86
    Episode_Reward/reaching_object: 1.0150
     Episode_Reward/lifting_object: 0.3240
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 2.03s
                      Time elapsed: 00:10:56
                               ETA: 00:58:45

################################################################################
                     [1m Learning iteration 314/2000 [0m                      

                       Computation: 48509 steps/s (collection: 1.933s, learning 0.093s)
             Mean action noise std: 2.00
          Mean value_function loss: 1.0682
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 65.3936
                       Mean reward: 6.86
               Mean episode length: 227.38
    Episode_Reward/reaching_object: 1.0286
     Episode_Reward/lifting_object: 0.3014
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 2.03s
                      Time elapsed: 00:10:58
                               ETA: 00:58:42

################################################################################
                     [1m Learning iteration 315/2000 [0m                      

                       Computation: 47940 steps/s (collection: 1.952s, learning 0.099s)
             Mean action noise std: 2.00
          Mean value_function loss: 6.4788
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 65.4334
                       Mean reward: 0.67
               Mean episode length: 225.22
    Episode_Reward/reaching_object: 1.0122
     Episode_Reward/lifting_object: 0.0023
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 2.05s
                      Time elapsed: 00:11:00
                               ETA: 00:58:40

################################################################################
                     [1m Learning iteration 316/2000 [0m                      

                       Computation: 48239 steps/s (collection: 1.937s, learning 0.101s)
             Mean action noise std: 2.01
          Mean value_function loss: 1.8693
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 65.4749
                       Mean reward: 5.60
               Mean episode length: 210.17
    Episode_Reward/reaching_object: 0.9694
     Episode_Reward/lifting_object: 0.2738
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 2.04s
                      Time elapsed: 00:11:02
                               ETA: 00:58:38

################################################################################
                     [1m Learning iteration 317/2000 [0m                      

                       Computation: 49352 steps/s (collection: 1.901s, learning 0.091s)
             Mean action noise std: 2.01
          Mean value_function loss: 1.0592
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 65.5027
                       Mean reward: 5.45
               Mean episode length: 219.50
    Episode_Reward/reaching_object: 1.0040
     Episode_Reward/lifting_object: 0.2448
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0455
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 1.99s
                      Time elapsed: 00:11:04
                               ETA: 00:58:35

################################################################################
                     [1m Learning iteration 318/2000 [0m                      

                       Computation: 48994 steps/s (collection: 1.916s, learning 0.091s)
             Mean action noise std: 2.01
          Mean value_function loss: 0.9843
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 65.5555
                       Mean reward: 7.44
               Mean episode length: 218.39
    Episode_Reward/reaching_object: 1.0178
     Episode_Reward/lifting_object: 0.2923
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 2.01s
                      Time elapsed: 00:11:06
                               ETA: 00:58:33

################################################################################
                     [1m Learning iteration 319/2000 [0m                      

                       Computation: 48711 steps/s (collection: 1.917s, learning 0.101s)
             Mean action noise std: 2.01
          Mean value_function loss: 4.1356
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 65.5996
                       Mean reward: 6.19
               Mean episode length: 223.12
    Episode_Reward/reaching_object: 0.9985
     Episode_Reward/lifting_object: 0.4007
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 2.02s
                      Time elapsed: 00:11:08
                               ETA: 00:58:30

################################################################################
                     [1m Learning iteration 320/2000 [0m                      

                       Computation: 48198 steps/s (collection: 1.945s, learning 0.094s)
             Mean action noise std: 2.02
          Mean value_function loss: 1.5007
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 65.6269
                       Mean reward: 8.49
               Mean episode length: 219.90
    Episode_Reward/reaching_object: 0.9762
     Episode_Reward/lifting_object: 0.3432
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 2.04s
                      Time elapsed: 00:11:10
                               ETA: 00:58:28

################################################################################
                     [1m Learning iteration 321/2000 [0m                      

                       Computation: 47521 steps/s (collection: 1.961s, learning 0.108s)
             Mean action noise std: 2.02
          Mean value_function loss: 3.6190
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 65.6765
                       Mean reward: 6.94
               Mean episode length: 223.27
    Episode_Reward/reaching_object: 0.9860
     Episode_Reward/lifting_object: 0.4096
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 2.07s
                      Time elapsed: 00:11:12
                               ETA: 00:58:26

################################################################################
                     [1m Learning iteration 322/2000 [0m                      

                       Computation: 47378 steps/s (collection: 1.956s, learning 0.119s)
             Mean action noise std: 2.02
          Mean value_function loss: 1.7839
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 65.7231
                       Mean reward: 6.36
               Mean episode length: 225.80
    Episode_Reward/reaching_object: 0.9660
     Episode_Reward/lifting_object: 0.2267
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 2.07s
                      Time elapsed: 00:11:14
                               ETA: 00:58:23

################################################################################
                     [1m Learning iteration 323/2000 [0m                      

                       Computation: 48664 steps/s (collection: 1.926s, learning 0.094s)
             Mean action noise std: 2.03
          Mean value_function loss: 3.6765
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 65.7835
                       Mean reward: 3.94
               Mean episode length: 228.18
    Episode_Reward/reaching_object: 0.9774
     Episode_Reward/lifting_object: 0.3680
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 2.02s
                      Time elapsed: 00:11:16
                               ETA: 00:58:21

################################################################################
                     [1m Learning iteration 324/2000 [0m                      

                       Computation: 48507 steps/s (collection: 1.935s, learning 0.092s)
             Mean action noise std: 2.03
          Mean value_function loss: 1.2303
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 65.8659
                       Mean reward: 6.32
               Mean episode length: 217.97
    Episode_Reward/reaching_object: 0.9498
     Episode_Reward/lifting_object: 0.4323
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 2.03s
                      Time elapsed: 00:11:18
                               ETA: 00:58:19

################################################################################
                     [1m Learning iteration 325/2000 [0m                      

                       Computation: 48688 steps/s (collection: 1.921s, learning 0.098s)
             Mean action noise std: 2.04
          Mean value_function loss: 1.4114
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 65.9488
                       Mean reward: 3.70
               Mean episode length: 215.18
    Episode_Reward/reaching_object: 0.9739
     Episode_Reward/lifting_object: 0.1690
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 2.02s
                      Time elapsed: 00:11:20
                               ETA: 00:58:16

################################################################################
                     [1m Learning iteration 326/2000 [0m                      

                       Computation: 48267 steps/s (collection: 1.922s, learning 0.115s)
             Mean action noise std: 2.04
          Mean value_function loss: 1.7604
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 66.0019
                       Mean reward: 7.03
               Mean episode length: 226.26
    Episode_Reward/reaching_object: 0.9668
     Episode_Reward/lifting_object: 0.5461
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 2.04s
                      Time elapsed: 00:11:22
                               ETA: 00:58:14

################################################################################
                     [1m Learning iteration 327/2000 [0m                      

                       Computation: 48459 steps/s (collection: 1.939s, learning 0.090s)
             Mean action noise std: 2.04
          Mean value_function loss: 2.3938
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 66.0454
                       Mean reward: 7.32
               Mean episode length: 228.24
    Episode_Reward/reaching_object: 0.9568
     Episode_Reward/lifting_object: 0.5522
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 2.03s
                      Time elapsed: 00:11:24
                               ETA: 00:58:11

################################################################################
                     [1m Learning iteration 328/2000 [0m                      

                       Computation: 48978 steps/s (collection: 1.895s, learning 0.112s)
             Mean action noise std: 2.05
          Mean value_function loss: 1.7344
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 66.0909
                       Mean reward: 7.39
               Mean episode length: 229.22
    Episode_Reward/reaching_object: 0.9758
     Episode_Reward/lifting_object: 0.4450
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 2.01s
                      Time elapsed: 00:11:26
                               ETA: 00:58:09

################################################################################
                     [1m Learning iteration 329/2000 [0m                      

                       Computation: 48800 steps/s (collection: 1.921s, learning 0.093s)
             Mean action noise std: 2.05
          Mean value_function loss: 1.4470
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 66.1320
                       Mean reward: 7.17
               Mean episode length: 229.68
    Episode_Reward/reaching_object: 0.9491
     Episode_Reward/lifting_object: 0.4426
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 2.01s
                      Time elapsed: 00:11:28
                               ETA: 00:58:06

################################################################################
                     [1m Learning iteration 330/2000 [0m                      

                       Computation: 48216 steps/s (collection: 1.944s, learning 0.095s)
             Mean action noise std: 2.05
          Mean value_function loss: 2.5454
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 66.1729
                       Mean reward: 6.51
               Mean episode length: 217.73
    Episode_Reward/reaching_object: 0.9161
     Episode_Reward/lifting_object: 0.4189
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 2.04s
                      Time elapsed: 00:11:30
                               ETA: 00:58:04

################################################################################
                     [1m Learning iteration 331/2000 [0m                      

                       Computation: 48938 steps/s (collection: 1.898s, learning 0.111s)
             Mean action noise std: 2.06
          Mean value_function loss: 1.4646
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 66.2292
                       Mean reward: 5.76
               Mean episode length: 227.61
    Episode_Reward/reaching_object: 0.9582
     Episode_Reward/lifting_object: 0.4415
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 2.01s
                      Time elapsed: 00:11:32
                               ETA: 00:58:02

################################################################################
                     [1m Learning iteration 332/2000 [0m                      

                       Computation: 48676 steps/s (collection: 1.930s, learning 0.089s)
             Mean action noise std: 2.06
          Mean value_function loss: 1.3815
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 66.2826
                       Mean reward: 8.31
               Mean episode length: 228.81
    Episode_Reward/reaching_object: 0.9544
     Episode_Reward/lifting_object: 0.4257
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 2.02s
                      Time elapsed: 00:11:34
                               ETA: 00:57:59

################################################################################
                     [1m Learning iteration 333/2000 [0m                      

                       Computation: 18808 steps/s (collection: 5.094s, learning 0.133s)
             Mean action noise std: 2.06
          Mean value_function loss: 4.7050
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 66.3249
                       Mean reward: 4.79
               Mean episode length: 225.01
    Episode_Reward/reaching_object: 0.9705
     Episode_Reward/lifting_object: 0.5087
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 5.23s
                      Time elapsed: 00:11:39
                               ETA: 00:58:13

################################################################################
                     [1m Learning iteration 334/2000 [0m                      

                       Computation: 14321 steps/s (collection: 6.755s, learning 0.110s)
             Mean action noise std: 2.06
          Mean value_function loss: 2.0760
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 66.3366
                       Mean reward: 6.52
               Mean episode length: 228.71
    Episode_Reward/reaching_object: 0.9988
     Episode_Reward/lifting_object: 0.5742
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 6.86s
                      Time elapsed: 00:11:46
                               ETA: 00:58:34

################################################################################
                     [1m Learning iteration 335/2000 [0m                      

                       Computation: 14130 steps/s (collection: 6.842s, learning 0.115s)
             Mean action noise std: 2.06
          Mean value_function loss: 2.1337
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 66.3687
                       Mean reward: 7.54
               Mean episode length: 222.64
    Episode_Reward/reaching_object: 1.0100
     Episode_Reward/lifting_object: 0.6103
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 6.96s
                      Time elapsed: 00:11:53
                               ETA: 00:58:56

################################################################################
                     [1m Learning iteration 336/2000 [0m                      

                       Computation: 14202 steps/s (collection: 6.795s, learning 0.127s)
             Mean action noise std: 2.07
          Mean value_function loss: 3.7516
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 66.4142
                       Mean reward: 5.44
               Mean episode length: 217.85
    Episode_Reward/reaching_object: 0.9872
     Episode_Reward/lifting_object: 0.5245
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 6.92s
                      Time elapsed: 00:12:00
                               ETA: 00:59:18

################################################################################
                     [1m Learning iteration 337/2000 [0m                      

                       Computation: 13778 steps/s (collection: 7.005s, learning 0.129s)
             Mean action noise std: 2.07
          Mean value_function loss: 1.9303
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 66.4719
                       Mean reward: 5.45
               Mean episode length: 214.03
    Episode_Reward/reaching_object: 0.9447
     Episode_Reward/lifting_object: 0.5653
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 7.13s
                      Time elapsed: 00:12:07
                               ETA: 00:59:40

################################################################################
                     [1m Learning iteration 338/2000 [0m                      

                       Computation: 14343 steps/s (collection: 6.735s, learning 0.118s)
             Mean action noise std: 2.07
          Mean value_function loss: 2.8893
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 66.5148
                       Mean reward: 7.34
               Mean episode length: 221.77
    Episode_Reward/reaching_object: 0.9539
     Episode_Reward/lifting_object: 0.4734
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 6.85s
                      Time elapsed: 00:12:14
                               ETA: 01:00:01

################################################################################
                     [1m Learning iteration 339/2000 [0m                      

                       Computation: 14035 steps/s (collection: 6.878s, learning 0.126s)
             Mean action noise std: 2.08
          Mean value_function loss: 1.7077
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 66.5442
                       Mean reward: 7.35
               Mean episode length: 204.40
    Episode_Reward/reaching_object: 0.9376
     Episode_Reward/lifting_object: 0.5721
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 7.00s
                      Time elapsed: 00:12:21
                               ETA: 01:00:23

################################################################################
                     [1m Learning iteration 340/2000 [0m                      

                       Computation: 14477 steps/s (collection: 6.668s, learning 0.122s)
             Mean action noise std: 2.08
          Mean value_function loss: 1.9772
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 66.6081
                       Mean reward: 7.45
               Mean episode length: 208.81
    Episode_Reward/reaching_object: 0.9433
     Episode_Reward/lifting_object: 0.6123
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 6.79s
                      Time elapsed: 00:12:28
                               ETA: 01:00:43

################################################################################
                     [1m Learning iteration 341/2000 [0m                      

                       Computation: 12759 steps/s (collection: 7.581s, learning 0.123s)
             Mean action noise std: 2.09
          Mean value_function loss: 2.5307
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 66.6678
                       Mean reward: 8.06
               Mean episode length: 220.66
    Episode_Reward/reaching_object: 0.9693
     Episode_Reward/lifting_object: 0.6347
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 7.70s
                      Time elapsed: 00:12:36
                               ETA: 01:01:07

################################################################################
                     [1m Learning iteration 342/2000 [0m                      

                       Computation: 48739 steps/s (collection: 1.925s, learning 0.092s)
             Mean action noise std: 2.09
          Mean value_function loss: 2.5525
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 66.7248
                       Mean reward: 8.66
               Mean episode length: 218.92
    Episode_Reward/reaching_object: 0.9568
     Episode_Reward/lifting_object: 0.6642
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 2.02s
                      Time elapsed: 00:12:38
                               ETA: 01:01:04

################################################################################
                     [1m Learning iteration 343/2000 [0m                      

                       Computation: 49710 steps/s (collection: 1.885s, learning 0.093s)
             Mean action noise std: 2.09
          Mean value_function loss: 1.2969
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 66.7839
                       Mean reward: 8.01
               Mean episode length: 218.51
    Episode_Reward/reaching_object: 0.9720
     Episode_Reward/lifting_object: 0.6867
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 1.98s
                      Time elapsed: 00:12:40
                               ETA: 01:01:01

################################################################################
                     [1m Learning iteration 344/2000 [0m                      

                       Computation: 49579 steps/s (collection: 1.884s, learning 0.099s)
             Mean action noise std: 2.10
          Mean value_function loss: 3.0429
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 66.8357
                       Mean reward: 7.01
               Mean episode length: 208.79
    Episode_Reward/reaching_object: 0.9495
     Episode_Reward/lifting_object: 0.6009
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 1.98s
                      Time elapsed: 00:12:42
                               ETA: 01:00:58

################################################################################
                     [1m Learning iteration 345/2000 [0m                      

                       Computation: 46944 steps/s (collection: 1.928s, learning 0.166s)
             Mean action noise std: 2.10
          Mean value_function loss: 2.7226
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 66.9084
                       Mean reward: 4.99
               Mean episode length: 218.23
    Episode_Reward/reaching_object: 0.9418
     Episode_Reward/lifting_object: 0.4585
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 2.09s
                      Time elapsed: 00:12:44
                               ETA: 01:00:55

################################################################################
                     [1m Learning iteration 346/2000 [0m                      

                       Computation: 48838 steps/s (collection: 1.901s, learning 0.112s)
             Mean action noise std: 2.11
          Mean value_function loss: 1.9359
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 66.9719
                       Mean reward: 6.73
               Mean episode length: 219.57
    Episode_Reward/reaching_object: 0.9715
     Episode_Reward/lifting_object: 0.7160
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 2.01s
                      Time elapsed: 00:12:46
                               ETA: 01:00:52

################################################################################
                     [1m Learning iteration 347/2000 [0m                      

                       Computation: 49456 steps/s (collection: 1.884s, learning 0.104s)
             Mean action noise std: 2.11
          Mean value_function loss: 2.7529
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 67.0299
                       Mean reward: 6.91
               Mean episode length: 217.52
    Episode_Reward/reaching_object: 0.9563
     Episode_Reward/lifting_object: 0.6302
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 1.99s
                      Time elapsed: 00:12:48
                               ETA: 01:00:49

################################################################################
                     [1m Learning iteration 348/2000 [0m                      

                       Computation: 48988 steps/s (collection: 1.904s, learning 0.103s)
             Mean action noise std: 2.11
          Mean value_function loss: 4.5458
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 67.0602
                       Mean reward: 8.27
               Mean episode length: 218.14
    Episode_Reward/reaching_object: 0.9474
     Episode_Reward/lifting_object: 0.6682
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 2.01s
                      Time elapsed: 00:12:50
                               ETA: 01:00:45

################################################################################
                     [1m Learning iteration 349/2000 [0m                      

                       Computation: 49979 steps/s (collection: 1.873s, learning 0.094s)
             Mean action noise std: 2.11
          Mean value_function loss: 2.1979
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 67.0712
                       Mean reward: 8.46
               Mean episode length: 217.35
    Episode_Reward/reaching_object: 0.9227
     Episode_Reward/lifting_object: 0.6559
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 1.97s
                      Time elapsed: 00:12:52
                               ETA: 01:00:42

################################################################################
                     [1m Learning iteration 350/2000 [0m                      

                       Computation: 48360 steps/s (collection: 1.912s, learning 0.121s)
             Mean action noise std: 2.12
          Mean value_function loss: 2.0018
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 67.1068
                       Mean reward: 7.42
               Mean episode length: 215.82
    Episode_Reward/reaching_object: 0.9198
     Episode_Reward/lifting_object: 0.6723
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 2.03s
                      Time elapsed: 00:12:54
                               ETA: 01:00:39

################################################################################
                     [1m Learning iteration 351/2000 [0m                      

                       Computation: 49647 steps/s (collection: 1.885s, learning 0.095s)
             Mean action noise std: 2.12
          Mean value_function loss: 2.6755
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 67.1648
                       Mean reward: 5.98
               Mean episode length: 214.41
    Episode_Reward/reaching_object: 0.9678
     Episode_Reward/lifting_object: 0.6734
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 1.98s
                      Time elapsed: 00:12:56
                               ETA: 01:00:36

################################################################################
                     [1m Learning iteration 352/2000 [0m                      

                       Computation: 49696 steps/s (collection: 1.885s, learning 0.093s)
             Mean action noise std: 2.12
          Mean value_function loss: 1.6941
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 67.2286
                       Mean reward: 8.86
               Mean episode length: 202.69
    Episode_Reward/reaching_object: 0.8774
     Episode_Reward/lifting_object: 0.8035
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 1.98s
                      Time elapsed: 00:12:58
                               ETA: 01:00:32

################################################################################
                     [1m Learning iteration 353/2000 [0m                      

                       Computation: 49256 steps/s (collection: 1.903s, learning 0.093s)
             Mean action noise std: 2.13
          Mean value_function loss: 2.6774
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 67.2911
                       Mean reward: 7.91
               Mean episode length: 212.21
    Episode_Reward/reaching_object: 0.9286
     Episode_Reward/lifting_object: 0.7746
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 2.00s
                      Time elapsed: 00:13:00
                               ETA: 01:00:29

################################################################################
                     [1m Learning iteration 354/2000 [0m                      

                       Computation: 49314 steps/s (collection: 1.894s, learning 0.100s)
             Mean action noise std: 2.13
          Mean value_function loss: 2.4467
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 67.3520
                       Mean reward: 6.59
               Mean episode length: 200.31
    Episode_Reward/reaching_object: 0.8591
     Episode_Reward/lifting_object: 0.6686
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 1.99s
                      Time elapsed: 00:13:02
                               ETA: 01:00:26

################################################################################
                     [1m Learning iteration 355/2000 [0m                      

                       Computation: 48200 steps/s (collection: 1.934s, learning 0.106s)
             Mean action noise std: 2.13
          Mean value_function loss: 5.0127
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 67.3871
                       Mean reward: 6.82
               Mean episode length: 203.87
    Episode_Reward/reaching_object: 0.9001
     Episode_Reward/lifting_object: 0.6747
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 2.04s
                      Time elapsed: 00:13:04
                               ETA: 01:00:23

################################################################################
                     [1m Learning iteration 356/2000 [0m                      

                       Computation: 49819 steps/s (collection: 1.878s, learning 0.096s)
             Mean action noise std: 2.14
          Mean value_function loss: 2.1346
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 67.4547
                       Mean reward: 8.43
               Mean episode length: 202.76
    Episode_Reward/reaching_object: 0.8702
     Episode_Reward/lifting_object: 0.6794
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 1.97s
                      Time elapsed: 00:13:06
                               ETA: 01:00:20

################################################################################
                     [1m Learning iteration 357/2000 [0m                      

                       Computation: 49589 steps/s (collection: 1.884s, learning 0.099s)
             Mean action noise std: 2.14
          Mean value_function loss: 2.2300
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 67.5056
                       Mean reward: 8.48
               Mean episode length: 209.44
    Episode_Reward/reaching_object: 0.8515
     Episode_Reward/lifting_object: 0.7307
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 1.98s
                      Time elapsed: 00:13:08
                               ETA: 01:00:17

################################################################################
                     [1m Learning iteration 358/2000 [0m                      

                       Computation: 49964 steps/s (collection: 1.875s, learning 0.093s)
             Mean action noise std: 2.15
          Mean value_function loss: 3.1233
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 67.5406
                       Mean reward: 8.54
               Mean episode length: 200.70
    Episode_Reward/reaching_object: 0.8742
     Episode_Reward/lifting_object: 0.6188
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 1.97s
                      Time elapsed: 00:13:10
                               ETA: 01:00:13

################################################################################
                     [1m Learning iteration 359/2000 [0m                      

                       Computation: 49353 steps/s (collection: 1.882s, learning 0.110s)
             Mean action noise std: 2.15
          Mean value_function loss: 2.0172
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 67.5923
                       Mean reward: 8.65
               Mean episode length: 215.00
    Episode_Reward/reaching_object: 0.9179
     Episode_Reward/lifting_object: 0.7634
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 1.99s
                      Time elapsed: 00:13:12
                               ETA: 01:00:10

################################################################################
                     [1m Learning iteration 360/2000 [0m                      

                       Computation: 49910 steps/s (collection: 1.884s, learning 0.086s)
             Mean action noise std: 2.15
          Mean value_function loss: 4.8078
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 67.6466
                       Mean reward: 6.30
               Mean episode length: 209.58
    Episode_Reward/reaching_object: 0.8879
     Episode_Reward/lifting_object: 0.8098
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 1.97s
                      Time elapsed: 00:13:14
                               ETA: 01:00:07

################################################################################
                     [1m Learning iteration 361/2000 [0m                      

                       Computation: 49007 steps/s (collection: 1.898s, learning 0.108s)
             Mean action noise std: 2.16
          Mean value_function loss: 2.7110
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 67.7008
                       Mean reward: 5.99
               Mean episode length: 221.44
    Episode_Reward/reaching_object: 0.8986
     Episode_Reward/lifting_object: 0.6160
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 2.01s
                      Time elapsed: 00:13:16
                               ETA: 01:00:04

################################################################################
                     [1m Learning iteration 362/2000 [0m                      

                       Computation: 49582 steps/s (collection: 1.890s, learning 0.093s)
             Mean action noise std: 2.16
          Mean value_function loss: 3.0530
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 67.7480
                       Mean reward: 8.39
               Mean episode length: 209.28
    Episode_Reward/reaching_object: 0.9287
     Episode_Reward/lifting_object: 0.6470
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 1.98s
                      Time elapsed: 00:13:18
                               ETA: 01:00:01

################################################################################
                     [1m Learning iteration 363/2000 [0m                      

                       Computation: 49260 steps/s (collection: 1.886s, learning 0.110s)
             Mean action noise std: 2.16
          Mean value_function loss: 2.8352
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 67.7834
                       Mean reward: 8.18
               Mean episode length: 214.85
    Episode_Reward/reaching_object: 0.9103
     Episode_Reward/lifting_object: 0.8739
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 2.00s
                      Time elapsed: 00:13:20
                               ETA: 00:59:58

################################################################################
                     [1m Learning iteration 364/2000 [0m                      

                       Computation: 48904 steps/s (collection: 1.904s, learning 0.107s)
             Mean action noise std: 2.16
          Mean value_function loss: 4.0431
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 67.8304
                       Mean reward: 7.86
               Mean episode length: 220.60
    Episode_Reward/reaching_object: 0.9325
     Episode_Reward/lifting_object: 0.6549
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 2.01s
                      Time elapsed: 00:13:22
                               ETA: 00:59:55

################################################################################
                     [1m Learning iteration 365/2000 [0m                      

                       Computation: 49912 steps/s (collection: 1.880s, learning 0.090s)
             Mean action noise std: 2.17
          Mean value_function loss: 2.5876
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 67.8545
                       Mean reward: 6.78
               Mean episode length: 212.57
    Episode_Reward/reaching_object: 0.8911
     Episode_Reward/lifting_object: 0.7379
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 1.97s
                      Time elapsed: 00:13:24
                               ETA: 00:59:51

################################################################################
                     [1m Learning iteration 366/2000 [0m                      

                       Computation: 50270 steps/s (collection: 1.855s, learning 0.100s)
             Mean action noise std: 2.17
          Mean value_function loss: 2.9087
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 67.8988
                       Mean reward: 7.20
               Mean episode length: 197.30
    Episode_Reward/reaching_object: 0.8880
     Episode_Reward/lifting_object: 0.7784
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 1.96s
                      Time elapsed: 00:13:26
                               ETA: 00:59:48

################################################################################
                     [1m Learning iteration 367/2000 [0m                      

                       Computation: 50461 steps/s (collection: 1.856s, learning 0.092s)
             Mean action noise std: 2.17
          Mean value_function loss: 3.1317
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 67.9447
                       Mean reward: 6.83
               Mean episode length: 199.90
    Episode_Reward/reaching_object: 0.8593
     Episode_Reward/lifting_object: 0.6112
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 1.95s
                      Time elapsed: 00:13:27
                               ETA: 00:59:45

################################################################################
                     [1m Learning iteration 368/2000 [0m                      

                       Computation: 50203 steps/s (collection: 1.867s, learning 0.091s)
             Mean action noise std: 2.17
          Mean value_function loss: 3.2739
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 67.9698
                       Mean reward: 11.41
               Mean episode length: 201.70
    Episode_Reward/reaching_object: 0.8804
     Episode_Reward/lifting_object: 0.9096
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 1.96s
                      Time elapsed: 00:13:29
                               ETA: 00:59:42

################################################################################
                     [1m Learning iteration 369/2000 [0m                      

                       Computation: 49811 steps/s (collection: 1.883s, learning 0.091s)
             Mean action noise std: 2.18
          Mean value_function loss: 4.3097
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 67.9984
                       Mean reward: 7.90
               Mean episode length: 215.84
    Episode_Reward/reaching_object: 0.8882
     Episode_Reward/lifting_object: 0.9020
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 1.97s
                      Time elapsed: 00:13:31
                               ETA: 00:59:38

################################################################################
                     [1m Learning iteration 370/2000 [0m                      

                       Computation: 49356 steps/s (collection: 1.889s, learning 0.103s)
             Mean action noise std: 2.18
          Mean value_function loss: 3.7632
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 68.0404
                       Mean reward: 9.15
               Mean episode length: 202.03
    Episode_Reward/reaching_object: 0.9019
     Episode_Reward/lifting_object: 0.7807
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 1.99s
                      Time elapsed: 00:13:33
                               ETA: 00:59:35

################################################################################
                     [1m Learning iteration 371/2000 [0m                      

                       Computation: 48954 steps/s (collection: 1.914s, learning 0.094s)
             Mean action noise std: 2.18
          Mean value_function loss: 4.1264
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 68.0740
                       Mean reward: 7.64
               Mean episode length: 182.86
    Episode_Reward/reaching_object: 0.8345
     Episode_Reward/lifting_object: 0.8724
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 2.01s
                      Time elapsed: 00:13:35
                               ETA: 00:59:32

################################################################################
                     [1m Learning iteration 372/2000 [0m                      

                       Computation: 49091 steps/s (collection: 1.903s, learning 0.099s)
             Mean action noise std: 2.19
          Mean value_function loss: 4.5698
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 68.1032
                       Mean reward: 7.72
               Mean episode length: 190.22
    Episode_Reward/reaching_object: 0.8141
     Episode_Reward/lifting_object: 1.0506
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 2.00s
                      Time elapsed: 00:13:37
                               ETA: 00:59:29

################################################################################
                     [1m Learning iteration 373/2000 [0m                      

                       Computation: 48893 steps/s (collection: 1.921s, learning 0.090s)
             Mean action noise std: 2.19
          Mean value_function loss: 3.7247
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 68.1543
                       Mean reward: 6.36
               Mean episode length: 183.14
    Episode_Reward/reaching_object: 0.7745
     Episode_Reward/lifting_object: 0.7155
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 2.01s
                      Time elapsed: 00:13:39
                               ETA: 00:59:26

################################################################################
                     [1m Learning iteration 374/2000 [0m                      

                       Computation: 49382 steps/s (collection: 1.900s, learning 0.091s)
             Mean action noise std: 2.19
          Mean value_function loss: 5.9622
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 68.1933
                       Mean reward: 8.29
               Mean episode length: 163.09
    Episode_Reward/reaching_object: 0.7501
     Episode_Reward/lifting_object: 0.9753
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.7917
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 1.99s
                      Time elapsed: 00:13:41
                               ETA: 00:59:23

################################################################################
                     [1m Learning iteration 375/2000 [0m                      

                       Computation: 47785 steps/s (collection: 1.940s, learning 0.118s)
             Mean action noise std: 2.19
          Mean value_function loss: 4.7045
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 68.2200
                       Mean reward: 6.53
               Mean episode length: 172.50
    Episode_Reward/reaching_object: 0.7308
     Episode_Reward/lifting_object: 0.7989
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 17.7500
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 2.06s
                      Time elapsed: 00:13:43
                               ETA: 00:59:20

################################################################################
                     [1m Learning iteration 376/2000 [0m                      

                       Computation: 49029 steps/s (collection: 1.895s, learning 0.111s)
             Mean action noise std: 2.20
          Mean value_function loss: 5.5255
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 68.2576
                       Mean reward: 7.28
               Mean episode length: 182.97
    Episode_Reward/reaching_object: 0.7501
     Episode_Reward/lifting_object: 0.7177
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 2.01s
                      Time elapsed: 00:13:45
                               ETA: 00:59:17

################################################################################
                     [1m Learning iteration 377/2000 [0m                      

                       Computation: 49297 steps/s (collection: 1.900s, learning 0.095s)
             Mean action noise std: 2.20
          Mean value_function loss: 4.2154
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 68.2904
                       Mean reward: 7.63
               Mean episode length: 167.55
    Episode_Reward/reaching_object: 0.7228
     Episode_Reward/lifting_object: 0.7792
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 1.99s
                      Time elapsed: 00:13:47
                               ETA: 00:59:14

################################################################################
                     [1m Learning iteration 378/2000 [0m                      

                       Computation: 49036 steps/s (collection: 1.916s, learning 0.089s)
             Mean action noise std: 2.20
          Mean value_function loss: 3.8187
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 68.3379
                       Mean reward: 9.74
               Mean episode length: 186.42
    Episode_Reward/reaching_object: 0.7273
     Episode_Reward/lifting_object: 0.7932
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 2.00s
                      Time elapsed: 00:13:49
                               ETA: 00:59:11

################################################################################
                     [1m Learning iteration 379/2000 [0m                      

                       Computation: 47627 steps/s (collection: 1.966s, learning 0.098s)
             Mean action noise std: 2.20
          Mean value_function loss: 7.5244
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 68.3610
                       Mean reward: 9.51
               Mean episode length: 184.57
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 0.7681
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 2.06s
                      Time elapsed: 00:13:52
                               ETA: 00:59:09

################################################################################
                     [1m Learning iteration 380/2000 [0m                      

                       Computation: 48397 steps/s (collection: 1.938s, learning 0.093s)
             Mean action noise std: 2.21
          Mean value_function loss: 5.2387
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 68.4032
                       Mean reward: 6.96
               Mean episode length: 170.27
    Episode_Reward/reaching_object: 0.7176
     Episode_Reward/lifting_object: 0.8260
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 2.03s
                      Time elapsed: 00:13:54
                               ETA: 00:59:06

################################################################################
                     [1m Learning iteration 381/2000 [0m                      

                       Computation: 48672 steps/s (collection: 1.928s, learning 0.091s)
             Mean action noise std: 2.21
          Mean value_function loss: 8.2754
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 68.4489
                       Mean reward: 10.50
               Mean episode length: 187.13
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 0.8374
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 2.02s
                      Time elapsed: 00:13:56
                               ETA: 00:59:03

################################################################################
                     [1m Learning iteration 382/2000 [0m                      

                       Computation: 49754 steps/s (collection: 1.889s, learning 0.087s)
             Mean action noise std: 2.21
          Mean value_function loss: 4.2512
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 68.4669
                       Mean reward: 8.54
               Mean episode length: 178.63
    Episode_Reward/reaching_object: 0.7534
     Episode_Reward/lifting_object: 1.0699
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.9583
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 1.98s
                      Time elapsed: 00:13:58
                               ETA: 00:59:00

################################################################################
                     [1m Learning iteration 383/2000 [0m                      

                       Computation: 47472 steps/s (collection: 1.968s, learning 0.103s)
             Mean action noise std: 2.21
          Mean value_function loss: 7.5740
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 68.4909
                       Mean reward: 6.77
               Mean episode length: 189.11
    Episode_Reward/reaching_object: 0.7614
     Episode_Reward/lifting_object: 0.8769
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.3750
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 2.07s
                      Time elapsed: 00:14:00
                               ETA: 00:58:57

################################################################################
                     [1m Learning iteration 384/2000 [0m                      

                       Computation: 49163 steps/s (collection: 1.899s, learning 0.100s)
             Mean action noise std: 2.21
          Mean value_function loss: 10.2789
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 68.5257
                       Mean reward: 10.70
               Mean episode length: 193.98
    Episode_Reward/reaching_object: 0.7952
     Episode_Reward/lifting_object: 0.8695
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 2.00s
                      Time elapsed: 00:14:02
                               ETA: 00:58:54

################################################################################
                     [1m Learning iteration 385/2000 [0m                      

                       Computation: 49049 steps/s (collection: 1.886s, learning 0.119s)
             Mean action noise std: 2.22
          Mean value_function loss: 8.2476
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 68.5536
                       Mean reward: 5.64
               Mean episode length: 194.33
    Episode_Reward/reaching_object: 0.7872
     Episode_Reward/lifting_object: 0.9527
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 2.00s
                      Time elapsed: 00:14:04
                               ETA: 00:58:51

################################################################################
                     [1m Learning iteration 386/2000 [0m                      

                       Computation: 48440 steps/s (collection: 1.935s, learning 0.095s)
             Mean action noise std: 2.22
          Mean value_function loss: 4.5087
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 68.6002
                       Mean reward: 9.14
               Mean episode length: 186.06
    Episode_Reward/reaching_object: 0.7899
     Episode_Reward/lifting_object: 1.1166
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 2.03s
                      Time elapsed: 00:14:06
                               ETA: 00:58:48

################################################################################
                     [1m Learning iteration 387/2000 [0m                      

                       Computation: 48591 steps/s (collection: 1.906s, learning 0.117s)
             Mean action noise std: 2.22
          Mean value_function loss: 3.8050
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 68.6423
                       Mean reward: 9.98
               Mean episode length: 183.47
    Episode_Reward/reaching_object: 0.7296
     Episode_Reward/lifting_object: 1.0698
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 2.02s
                      Time elapsed: 00:14:08
                               ETA: 00:58:46

################################################################################
                     [1m Learning iteration 388/2000 [0m                      

                       Computation: 49540 steps/s (collection: 1.885s, learning 0.099s)
             Mean action noise std: 2.22
          Mean value_function loss: 4.9748
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 68.6692
                       Mean reward: 9.40
               Mean episode length: 194.91
    Episode_Reward/reaching_object: 0.7914
     Episode_Reward/lifting_object: 0.8302
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 1.98s
                      Time elapsed: 00:14:10
                               ETA: 00:58:43

################################################################################
                     [1m Learning iteration 389/2000 [0m                      

                       Computation: 48963 steps/s (collection: 1.919s, learning 0.089s)
             Mean action noise std: 2.23
          Mean value_function loss: 5.1496
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 68.6972
                       Mean reward: 9.50
               Mean episode length: 198.52
    Episode_Reward/reaching_object: 0.8116
     Episode_Reward/lifting_object: 1.0354
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 2.01s
                      Time elapsed: 00:14:12
                               ETA: 00:58:40

################################################################################
                     [1m Learning iteration 390/2000 [0m                      

                       Computation: 48937 steps/s (collection: 1.899s, learning 0.110s)
             Mean action noise std: 2.23
          Mean value_function loss: 4.0148
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 68.7455
                       Mean reward: 8.77
               Mean episode length: 194.62
    Episode_Reward/reaching_object: 0.7743
     Episode_Reward/lifting_object: 1.1401
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 2.01s
                      Time elapsed: 00:14:14
                               ETA: 00:58:37

################################################################################
                     [1m Learning iteration 391/2000 [0m                      

                       Computation: 48512 steps/s (collection: 1.908s, learning 0.118s)
             Mean action noise std: 2.23
          Mean value_function loss: 6.2661
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 68.7757
                       Mean reward: 11.03
               Mean episode length: 203.15
    Episode_Reward/reaching_object: 0.7979
     Episode_Reward/lifting_object: 1.0610
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 2.03s
                      Time elapsed: 00:14:16
                               ETA: 00:58:34

################################################################################
                     [1m Learning iteration 392/2000 [0m                      

                       Computation: 48148 steps/s (collection: 1.931s, learning 0.111s)
             Mean action noise std: 2.23
          Mean value_function loss: 3.9063
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 68.7954
                       Mean reward: 12.38
               Mean episode length: 198.47
    Episode_Reward/reaching_object: 0.8495
     Episode_Reward/lifting_object: 1.0521
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 2.04s
                      Time elapsed: 00:14:18
                               ETA: 00:58:31

################################################################################
                     [1m Learning iteration 393/2000 [0m                      

                       Computation: 47780 steps/s (collection: 1.959s, learning 0.099s)
             Mean action noise std: 2.24
          Mean value_function loss: 4.5271
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 68.8320
                       Mean reward: 8.65
               Mean episode length: 199.30
    Episode_Reward/reaching_object: 0.8327
     Episode_Reward/lifting_object: 1.4373
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 2.06s
                      Time elapsed: 00:14:20
                               ETA: 00:58:28

################################################################################
                     [1m Learning iteration 394/2000 [0m                      

                       Computation: 47353 steps/s (collection: 1.933s, learning 0.143s)
             Mean action noise std: 2.24
          Mean value_function loss: 5.9498
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 68.8700
                       Mean reward: 9.56
               Mean episode length: 198.93
    Episode_Reward/reaching_object: 0.8309
     Episode_Reward/lifting_object: 1.1865
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 2.08s
                      Time elapsed: 00:14:22
                               ETA: 00:58:26

################################################################################
                     [1m Learning iteration 395/2000 [0m                      

                       Computation: 46329 steps/s (collection: 2.009s, learning 0.113s)
             Mean action noise std: 2.24
          Mean value_function loss: 7.4759
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 68.9007
                       Mean reward: 14.09
               Mean episode length: 197.09
    Episode_Reward/reaching_object: 0.8511
     Episode_Reward/lifting_object: 1.5115
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 11.1667
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 2.12s
                      Time elapsed: 00:14:24
                               ETA: 00:58:23

################################################################################
                     [1m Learning iteration 396/2000 [0m                      

                       Computation: 47554 steps/s (collection: 1.970s, learning 0.097s)
             Mean action noise std: 2.25
          Mean value_function loss: 5.6923
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 68.9395
                       Mean reward: 8.73
               Mean episode length: 195.35
    Episode_Reward/reaching_object: 0.8339
     Episode_Reward/lifting_object: 1.2950
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 2.07s
                      Time elapsed: 00:14:26
                               ETA: 00:58:21

################################################################################
                     [1m Learning iteration 397/2000 [0m                      

                       Computation: 47960 steps/s (collection: 1.946s, learning 0.104s)
             Mean action noise std: 2.25
          Mean value_function loss: 6.6041
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 68.9839
                       Mean reward: 12.44
               Mean episode length: 200.82
    Episode_Reward/reaching_object: 0.8125
     Episode_Reward/lifting_object: 1.1616
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 2.05s
                      Time elapsed: 00:14:28
                               ETA: 00:58:18

################################################################################
                     [1m Learning iteration 398/2000 [0m                      

                       Computation: 46850 steps/s (collection: 1.980s, learning 0.119s)
             Mean action noise std: 2.25
          Mean value_function loss: 10.2782
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 69.0177
                       Mean reward: 9.62
               Mean episode length: 188.48
    Episode_Reward/reaching_object: 0.7776
     Episode_Reward/lifting_object: 1.0799
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 2.10s
                      Time elapsed: 00:14:30
                               ETA: 00:58:15

################################################################################
                     [1m Learning iteration 399/2000 [0m                      

                       Computation: 47433 steps/s (collection: 1.965s, learning 0.107s)
             Mean action noise std: 2.25
          Mean value_function loss: 7.7843
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 69.0523
                       Mean reward: 10.47
               Mean episode length: 202.87
    Episode_Reward/reaching_object: 0.8116
     Episode_Reward/lifting_object: 1.3436
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 2.07s
                      Time elapsed: 00:14:32
                               ETA: 00:58:13

################################################################################
                     [1m Learning iteration 400/2000 [0m                      

                       Computation: 47551 steps/s (collection: 1.937s, learning 0.131s)
             Mean action noise std: 2.26
          Mean value_function loss: 5.6485
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 69.0865
                       Mean reward: 12.97
               Mean episode length: 201.89
    Episode_Reward/reaching_object: 0.8286
     Episode_Reward/lifting_object: 1.4137
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 2.07s
                      Time elapsed: 00:14:34
                               ETA: 00:58:10

################################################################################
                     [1m Learning iteration 401/2000 [0m                      

                       Computation: 45060 steps/s (collection: 2.021s, learning 0.161s)
             Mean action noise std: 2.26
          Mean value_function loss: 7.1359
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 69.1200
                       Mean reward: 10.91
               Mean episode length: 208.60
    Episode_Reward/reaching_object: 0.8383
     Episode_Reward/lifting_object: 1.2894
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 2.18s
                      Time elapsed: 00:14:37
                               ETA: 00:58:08

################################################################################
                     [1m Learning iteration 402/2000 [0m                      

                       Computation: 48022 steps/s (collection: 1.947s, learning 0.101s)
             Mean action noise std: 2.26
          Mean value_function loss: 5.5480
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 69.1653
                       Mean reward: 14.75
               Mean episode length: 198.95
    Episode_Reward/reaching_object: 0.8589
     Episode_Reward/lifting_object: 1.3035
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 2.05s
                      Time elapsed: 00:14:39
                               ETA: 00:58:05

################################################################################
                     [1m Learning iteration 403/2000 [0m                      

                       Computation: 47657 steps/s (collection: 1.955s, learning 0.108s)
             Mean action noise std: 2.26
          Mean value_function loss: 11.1103
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 69.2034
                       Mean reward: 8.57
               Mean episode length: 198.88
    Episode_Reward/reaching_object: 0.8643
     Episode_Reward/lifting_object: 1.2443
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 2.06s
                      Time elapsed: 00:14:41
                               ETA: 00:58:03

################################################################################
                     [1m Learning iteration 404/2000 [0m                      

                       Computation: 47100 steps/s (collection: 1.985s, learning 0.103s)
             Mean action noise std: 2.27
          Mean value_function loss: 6.7233
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 69.2388
                       Mean reward: 12.48
               Mean episode length: 211.23
    Episode_Reward/reaching_object: 0.8385
     Episode_Reward/lifting_object: 1.2718
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 2.09s
                      Time elapsed: 00:14:43
                               ETA: 00:58:00

################################################################################
                     [1m Learning iteration 405/2000 [0m                      

                       Computation: 47206 steps/s (collection: 1.968s, learning 0.115s)
             Mean action noise std: 2.27
          Mean value_function loss: 7.3833
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 69.2671
                       Mean reward: 12.00
               Mean episode length: 221.43
    Episode_Reward/reaching_object: 0.9099
     Episode_Reward/lifting_object: 1.6685
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 2.08s
                      Time elapsed: 00:14:45
                               ETA: 00:57:58

################################################################################
                     [1m Learning iteration 406/2000 [0m                      

                       Computation: 48279 steps/s (collection: 1.921s, learning 0.115s)
             Mean action noise std: 2.27
          Mean value_function loss: 6.1909
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 69.2984
                       Mean reward: 11.51
               Mean episode length: 191.49
    Episode_Reward/reaching_object: 0.8685
     Episode_Reward/lifting_object: 1.3646
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 2.04s
                      Time elapsed: 00:14:47
                               ETA: 00:57:55

################################################################################
                     [1m Learning iteration 407/2000 [0m                      

                       Computation: 48039 steps/s (collection: 1.930s, learning 0.116s)
             Mean action noise std: 2.28
          Mean value_function loss: 4.9805
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 69.3481
                       Mean reward: 10.24
               Mean episode length: 214.95
    Episode_Reward/reaching_object: 0.8644
     Episode_Reward/lifting_object: 1.3860
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 2.05s
                      Time elapsed: 00:14:49
                               ETA: 00:57:52

################################################################################
                     [1m Learning iteration 408/2000 [0m                      

                       Computation: 47364 steps/s (collection: 1.962s, learning 0.114s)
             Mean action noise std: 2.28
          Mean value_function loss: 5.1895
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 69.3792
                       Mean reward: 9.71
               Mean episode length: 207.33
    Episode_Reward/reaching_object: 0.8584
     Episode_Reward/lifting_object: 1.5481
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 2.08s
                      Time elapsed: 00:14:51
                               ETA: 00:57:49

################################################################################
                     [1m Learning iteration 409/2000 [0m                      

                       Computation: 48844 steps/s (collection: 1.905s, learning 0.108s)
             Mean action noise std: 2.28
          Mean value_function loss: 10.2848
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 69.4226
                       Mean reward: 11.35
               Mean episode length: 203.53
    Episode_Reward/reaching_object: 0.8357
     Episode_Reward/lifting_object: 1.6178
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 2.01s
                      Time elapsed: 00:14:53
                               ETA: 00:57:47

################################################################################
                     [1m Learning iteration 410/2000 [0m                      

                       Computation: 48382 steps/s (collection: 1.924s, learning 0.108s)
             Mean action noise std: 2.29
          Mean value_function loss: 7.7695
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 69.4745
                       Mean reward: 9.37
               Mean episode length: 195.58
    Episode_Reward/reaching_object: 0.7900
     Episode_Reward/lifting_object: 1.4460
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 2.03s
                      Time elapsed: 00:14:55
                               ETA: 00:57:44

################################################################################
                     [1m Learning iteration 411/2000 [0m                      

                       Computation: 48223 steps/s (collection: 1.924s, learning 0.115s)
             Mean action noise std: 2.29
          Mean value_function loss: 12.0587
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 69.5203
                       Mean reward: 9.41
               Mean episode length: 195.83
    Episode_Reward/reaching_object: 0.7760
     Episode_Reward/lifting_object: 1.4991
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 2.04s
                      Time elapsed: 00:14:57
                               ETA: 00:57:41

################################################################################
                     [1m Learning iteration 412/2000 [0m                      

                       Computation: 48970 steps/s (collection: 1.911s, learning 0.097s)
             Mean action noise std: 2.29
          Mean value_function loss: 6.9221
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 69.5700
                       Mean reward: 10.78
               Mean episode length: 196.17
    Episode_Reward/reaching_object: 0.8115
     Episode_Reward/lifting_object: 1.4523
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 2.01s
                      Time elapsed: 00:14:59
                               ETA: 00:57:38

################################################################################
                     [1m Learning iteration 413/2000 [0m                      

                       Computation: 49040 steps/s (collection: 1.913s, learning 0.092s)
             Mean action noise std: 2.30
          Mean value_function loss: 8.0696
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 69.6150
                       Mean reward: 7.16
               Mean episode length: 199.14
    Episode_Reward/reaching_object: 0.8213
     Episode_Reward/lifting_object: 1.2020
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 2.00s
                      Time elapsed: 00:15:01
                               ETA: 00:57:35

################################################################################
                     [1m Learning iteration 414/2000 [0m                      

                       Computation: 48491 steps/s (collection: 1.939s, learning 0.089s)
             Mean action noise std: 2.30
          Mean value_function loss: 5.6541
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 69.6574
                       Mean reward: 12.64
               Mean episode length: 197.01
    Episode_Reward/reaching_object: 0.7761
     Episode_Reward/lifting_object: 1.7222
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.6667
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 2.03s
                      Time elapsed: 00:15:03
                               ETA: 00:57:33

################################################################################
                     [1m Learning iteration 415/2000 [0m                      

                       Computation: 48086 steps/s (collection: 1.945s, learning 0.100s)
             Mean action noise std: 2.30
          Mean value_function loss: 6.3836
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 69.6853
                       Mean reward: 8.62
               Mean episode length: 190.83
    Episode_Reward/reaching_object: 0.7787
     Episode_Reward/lifting_object: 1.1785
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 2.04s
                      Time elapsed: 00:15:05
                               ETA: 00:57:30

################################################################################
                     [1m Learning iteration 416/2000 [0m                      

                       Computation: 47480 steps/s (collection: 1.958s, learning 0.113s)
             Mean action noise std: 2.30
          Mean value_function loss: 6.8277
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 69.7273
                       Mean reward: 13.55
               Mean episode length: 177.97
    Episode_Reward/reaching_object: 0.8064
     Episode_Reward/lifting_object: 1.7467
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 2.07s
                      Time elapsed: 00:15:07
                               ETA: 00:57:27

################################################################################
                     [1m Learning iteration 417/2000 [0m                      

                       Computation: 49011 steps/s (collection: 1.917s, learning 0.089s)
             Mean action noise std: 2.31
          Mean value_function loss: 5.8353
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 69.7707
                       Mean reward: 15.14
               Mean episode length: 183.94
    Episode_Reward/reaching_object: 0.7609
     Episode_Reward/lifting_object: 1.7125
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 2.01s
                      Time elapsed: 00:15:09
                               ETA: 00:57:25

################################################################################
                     [1m Learning iteration 418/2000 [0m                      

                       Computation: 48784 steps/s (collection: 1.903s, learning 0.113s)
             Mean action noise std: 2.31
          Mean value_function loss: 5.9191
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 69.8001
                       Mean reward: 11.28
               Mean episode length: 190.90
    Episode_Reward/reaching_object: 0.7708
     Episode_Reward/lifting_object: 1.5242
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.2500
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 2.02s
                      Time elapsed: 00:15:11
                               ETA: 00:57:22

################################################################################
                     [1m Learning iteration 419/2000 [0m                      

                       Computation: 48373 steps/s (collection: 1.943s, learning 0.090s)
             Mean action noise std: 2.31
          Mean value_function loss: 18.4754
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 69.8247
                       Mean reward: 5.65
               Mean episode length: 186.31
    Episode_Reward/reaching_object: 0.7515
     Episode_Reward/lifting_object: 1.4033
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 2.03s
                      Time elapsed: 00:15:13
                               ETA: 00:57:19

################################################################################
                     [1m Learning iteration 420/2000 [0m                      

                       Computation: 48837 steps/s (collection: 1.919s, learning 0.094s)
             Mean action noise std: 2.31
          Mean value_function loss: 7.7796
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 69.8434
                       Mean reward: 8.35
               Mean episode length: 190.13
    Episode_Reward/reaching_object: 0.7456
     Episode_Reward/lifting_object: 1.1858
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 2.01s
                      Time elapsed: 00:15:15
                               ETA: 00:57:16

################################################################################
                     [1m Learning iteration 421/2000 [0m                      

                       Computation: 48154 steps/s (collection: 1.938s, learning 0.103s)
             Mean action noise std: 2.31
          Mean value_function loss: 8.6335
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 69.8702
                       Mean reward: 10.94
               Mean episode length: 188.05
    Episode_Reward/reaching_object: 0.7548
     Episode_Reward/lifting_object: 1.4341
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 2.04s
                      Time elapsed: 00:15:17
                               ETA: 00:57:14

################################################################################
                     [1m Learning iteration 422/2000 [0m                      

                       Computation: 47765 steps/s (collection: 1.967s, learning 0.091s)
             Mean action noise std: 2.32
          Mean value_function loss: 9.1882
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 69.9059
                       Mean reward: 12.72
               Mean episode length: 185.52
    Episode_Reward/reaching_object: 0.7492
     Episode_Reward/lifting_object: 1.4459
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 2.06s
                      Time elapsed: 00:15:19
                               ETA: 00:57:11

################################################################################
                     [1m Learning iteration 423/2000 [0m                      

                       Computation: 48298 steps/s (collection: 1.937s, learning 0.099s)
             Mean action noise std: 2.32
          Mean value_function loss: 13.6646
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 69.9302
                       Mean reward: 9.74
               Mean episode length: 188.80
    Episode_Reward/reaching_object: 0.7765
     Episode_Reward/lifting_object: 1.4855
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 11.6667
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 2.04s
                      Time elapsed: 00:15:21
                               ETA: 00:57:08

################################################################################
                     [1m Learning iteration 424/2000 [0m                      

                       Computation: 48425 steps/s (collection: 1.933s, learning 0.097s)
             Mean action noise std: 2.32
          Mean value_function loss: 7.5860
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 69.9565
                       Mean reward: 11.99
               Mean episode length: 182.50
    Episode_Reward/reaching_object: 0.7749
     Episode_Reward/lifting_object: 1.4354
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 2.03s
                      Time elapsed: 00:15:23
                               ETA: 00:57:06

################################################################################
                     [1m Learning iteration 425/2000 [0m                      

                       Computation: 49154 steps/s (collection: 1.903s, learning 0.097s)
             Mean action noise std: 2.32
          Mean value_function loss: 8.0414
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 70.0020
                       Mean reward: 16.14
               Mean episode length: 198.25
    Episode_Reward/reaching_object: 0.7698
     Episode_Reward/lifting_object: 2.0353
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 2.00s
                      Time elapsed: 00:15:25
                               ETA: 00:57:03

################################################################################
                     [1m Learning iteration 426/2000 [0m                      

                       Computation: 49029 steps/s (collection: 1.914s, learning 0.092s)
             Mean action noise std: 2.33
          Mean value_function loss: 8.9173
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 70.0393
                       Mean reward: 13.15
               Mean episode length: 186.25
    Episode_Reward/reaching_object: 0.7886
     Episode_Reward/lifting_object: 1.8761
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 2.01s
                      Time elapsed: 00:15:27
                               ETA: 00:57:00

################################################################################
                     [1m Learning iteration 427/2000 [0m                      

                       Computation: 48444 steps/s (collection: 1.928s, learning 0.101s)
             Mean action noise std: 2.33
          Mean value_function loss: 9.7638
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 70.0798
                       Mean reward: 10.19
               Mean episode length: 192.95
    Episode_Reward/reaching_object: 0.7876
     Episode_Reward/lifting_object: 1.4430
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 2.03s
                      Time elapsed: 00:15:29
                               ETA: 00:56:57

################################################################################
                     [1m Learning iteration 428/2000 [0m                      

                       Computation: 48967 steps/s (collection: 1.918s, learning 0.090s)
             Mean action noise std: 2.33
          Mean value_function loss: 14.1560
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 70.1268
                       Mean reward: 14.77
               Mean episode length: 194.89
    Episode_Reward/reaching_object: 0.7783
     Episode_Reward/lifting_object: 1.8319
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 2.01s
                      Time elapsed: 00:15:31
                               ETA: 00:56:55

################################################################################
                     [1m Learning iteration 429/2000 [0m                      

                       Computation: 48055 steps/s (collection: 1.948s, learning 0.098s)
             Mean action noise std: 2.34
          Mean value_function loss: 9.2122
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 70.1734
                       Mean reward: 11.92
               Mean episode length: 199.29
    Episode_Reward/reaching_object: 0.8028
     Episode_Reward/lifting_object: 1.8346
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 2.05s
                      Time elapsed: 00:15:34
                               ETA: 00:56:52

################################################################################
                     [1m Learning iteration 430/2000 [0m                      

                       Computation: 47941 steps/s (collection: 1.940s, learning 0.111s)
             Mean action noise std: 2.34
          Mean value_function loss: 7.4536
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 70.2081
                       Mean reward: 12.50
               Mean episode length: 193.92
    Episode_Reward/reaching_object: 0.7652
     Episode_Reward/lifting_object: 1.7852
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 2.05s
                      Time elapsed: 00:15:36
                               ETA: 00:56:49

################################################################################
                     [1m Learning iteration 431/2000 [0m                      

                       Computation: 49121 steps/s (collection: 1.906s, learning 0.096s)
             Mean action noise std: 2.34
          Mean value_function loss: 7.6717
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 70.2346
                       Mean reward: 12.19
               Mean episode length: 195.37
    Episode_Reward/reaching_object: 0.7807
     Episode_Reward/lifting_object: 1.9736
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 2.00s
                      Time elapsed: 00:15:38
                               ETA: 00:56:47

################################################################################
                     [1m Learning iteration 432/2000 [0m                      

                       Computation: 48724 steps/s (collection: 1.924s, learning 0.093s)
             Mean action noise std: 2.34
          Mean value_function loss: 14.7661
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 70.2482
                       Mean reward: 11.68
               Mean episode length: 198.30
    Episode_Reward/reaching_object: 0.7885
     Episode_Reward/lifting_object: 1.9477
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 2.02s
                      Time elapsed: 00:15:40
                               ETA: 00:56:44

################################################################################
                     [1m Learning iteration 433/2000 [0m                      

                       Computation: 48542 steps/s (collection: 1.920s, learning 0.105s)
             Mean action noise std: 2.34
          Mean value_function loss: 10.5036
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 70.2560
                       Mean reward: 11.97
               Mean episode length: 204.19
    Episode_Reward/reaching_object: 0.7759
     Episode_Reward/lifting_object: 1.2412
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 2.03s
                      Time elapsed: 00:15:42
                               ETA: 00:56:41

################################################################################
                     [1m Learning iteration 434/2000 [0m                      

                       Computation: 47010 steps/s (collection: 1.965s, learning 0.126s)
             Mean action noise std: 2.35
          Mean value_function loss: 8.2257
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 70.2799
                       Mean reward: 15.39
               Mean episode length: 198.06
    Episode_Reward/reaching_object: 0.8096
     Episode_Reward/lifting_object: 1.9092
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 2.09s
                      Time elapsed: 00:15:44
                               ETA: 00:56:39

################################################################################
                     [1m Learning iteration 435/2000 [0m                      

                       Computation: 47047 steps/s (collection: 1.955s, learning 0.134s)
             Mean action noise std: 2.35
          Mean value_function loss: 11.1010
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 70.3190
                       Mean reward: 14.19
               Mean episode length: 206.13
    Episode_Reward/reaching_object: 0.7772
     Episode_Reward/lifting_object: 1.7777
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 2.09s
                      Time elapsed: 00:15:46
                               ETA: 00:56:36

################################################################################
                     [1m Learning iteration 436/2000 [0m                      

                       Computation: 47841 steps/s (collection: 1.939s, learning 0.116s)
             Mean action noise std: 2.35
          Mean value_function loss: 9.4266
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 70.3584
                       Mean reward: 11.56
               Mean episode length: 206.18
    Episode_Reward/reaching_object: 0.7933
     Episode_Reward/lifting_object: 1.8106
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 2.05s
                      Time elapsed: 00:15:48
                               ETA: 00:56:34

################################################################################
                     [1m Learning iteration 437/2000 [0m                      

                       Computation: 48307 steps/s (collection: 1.936s, learning 0.099s)
             Mean action noise std: 2.35
          Mean value_function loss: 7.2341
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 70.3828
                       Mean reward: 18.09
               Mean episode length: 208.94
    Episode_Reward/reaching_object: 0.8184
     Episode_Reward/lifting_object: 2.4882
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 2.03s
                      Time elapsed: 00:15:50
                               ETA: 00:56:31

################################################################################
                     [1m Learning iteration 438/2000 [0m                      

                       Computation: 48669 steps/s (collection: 1.897s, learning 0.123s)
             Mean action noise std: 2.36
          Mean value_function loss: 9.8105
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 70.4150
                       Mean reward: 12.32
               Mean episode length: 215.60
    Episode_Reward/reaching_object: 0.8353
     Episode_Reward/lifting_object: 1.9595
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0692
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 2.02s
                      Time elapsed: 00:15:52
                               ETA: 00:56:28

################################################################################
                     [1m Learning iteration 439/2000 [0m                      

                       Computation: 48172 steps/s (collection: 1.948s, learning 0.092s)
             Mean action noise std: 2.36
          Mean value_function loss: 7.1332
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 70.4409
                       Mean reward: 17.17
               Mean episode length: 211.25
    Episode_Reward/reaching_object: 0.8305
     Episode_Reward/lifting_object: 2.0881
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 2.04s
                      Time elapsed: 00:15:54
                               ETA: 00:56:26

################################################################################
                     [1m Learning iteration 440/2000 [0m                      

                       Computation: 48772 steps/s (collection: 1.918s, learning 0.098s)
             Mean action noise std: 2.36
          Mean value_function loss: 6.1604
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 70.4639
                       Mean reward: 15.79
               Mean episode length: 214.90
    Episode_Reward/reaching_object: 0.8048
     Episode_Reward/lifting_object: 2.1050
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 2.02s
                      Time elapsed: 00:15:56
                               ETA: 00:56:23

################################################################################
                     [1m Learning iteration 441/2000 [0m                      

                       Computation: 47044 steps/s (collection: 1.972s, learning 0.118s)
             Mean action noise std: 2.36
          Mean value_function loss: 6.2248
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 70.4911
                       Mean reward: 12.32
               Mean episode length: 205.84
    Episode_Reward/reaching_object: 0.7904
     Episode_Reward/lifting_object: 1.9525
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0686
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 2.09s
                      Time elapsed: 00:15:58
                               ETA: 00:56:20

################################################################################
                     [1m Learning iteration 442/2000 [0m                      

                       Computation: 49043 steps/s (collection: 1.905s, learning 0.100s)
             Mean action noise std: 2.36
          Mean value_function loss: 8.5228
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 70.5271
                       Mean reward: 14.84
               Mean episode length: 218.24
    Episode_Reward/reaching_object: 0.7933
     Episode_Reward/lifting_object: 2.2093
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 2.00s
                      Time elapsed: 00:16:00
                               ETA: 00:56:18

################################################################################
                     [1m Learning iteration 443/2000 [0m                      

                       Computation: 47475 steps/s (collection: 1.975s, learning 0.096s)
             Mean action noise std: 2.37
          Mean value_function loss: 10.3269
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 70.5626
                       Mean reward: 12.04
               Mean episode length: 209.32
    Episode_Reward/reaching_object: 0.7774
     Episode_Reward/lifting_object: 1.8921
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 2.07s
                      Time elapsed: 00:16:02
                               ETA: 00:56:15

################################################################################
                     [1m Learning iteration 444/2000 [0m                      

                       Computation: 49171 steps/s (collection: 1.904s, learning 0.096s)
             Mean action noise std: 2.37
          Mean value_function loss: 10.5250
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 70.6077
                       Mean reward: 13.96
               Mean episode length: 203.04
    Episode_Reward/reaching_object: 0.7705
     Episode_Reward/lifting_object: 1.6383
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 2.00s
                      Time elapsed: 00:16:04
                               ETA: 00:56:12

################################################################################
                     [1m Learning iteration 445/2000 [0m                      

                       Computation: 48132 steps/s (collection: 1.925s, learning 0.118s)
             Mean action noise std: 2.37
          Mean value_function loss: 8.1541
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 70.6441
                       Mean reward: 10.23
               Mean episode length: 206.66
    Episode_Reward/reaching_object: 0.7725
     Episode_Reward/lifting_object: 2.0894
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 2.04s
                      Time elapsed: 00:16:06
                               ETA: 00:56:10

################################################################################
                     [1m Learning iteration 446/2000 [0m                      

                       Computation: 48890 steps/s (collection: 1.907s, learning 0.104s)
             Mean action noise std: 2.37
          Mean value_function loss: 8.2060
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 70.6646
                       Mean reward: 16.53
               Mean episode length: 188.92
    Episode_Reward/reaching_object: 0.7735
     Episode_Reward/lifting_object: 1.9724
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 2.01s
                      Time elapsed: 00:16:08
                               ETA: 00:56:07

################################################################################
                     [1m Learning iteration 447/2000 [0m                      

                       Computation: 47595 steps/s (collection: 1.955s, learning 0.110s)
             Mean action noise std: 2.38
          Mean value_function loss: 9.2535
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 70.6869
                       Mean reward: 15.00
               Mean episode length: 204.80
    Episode_Reward/reaching_object: 0.7678
     Episode_Reward/lifting_object: 2.1545
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 2.07s
                      Time elapsed: 00:16:10
                               ETA: 00:56:05

################################################################################
                     [1m Learning iteration 448/2000 [0m                      

                       Computation: 48147 steps/s (collection: 1.930s, learning 0.112s)
             Mean action noise std: 2.38
          Mean value_function loss: 15.7072
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 70.7233
                       Mean reward: 16.82
               Mean episode length: 188.96
    Episode_Reward/reaching_object: 0.7373
     Episode_Reward/lifting_object: 1.9622
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 2.04s
                      Time elapsed: 00:16:12
                               ETA: 00:56:02

################################################################################
                     [1m Learning iteration 449/2000 [0m                      

                       Computation: 48989 steps/s (collection: 1.917s, learning 0.090s)
             Mean action noise std: 2.38
          Mean value_function loss: 13.6640
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 70.7666
                       Mean reward: 15.44
               Mean episode length: 201.10
    Episode_Reward/reaching_object: 0.7158
     Episode_Reward/lifting_object: 2.0293
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 2.01s
                      Time elapsed: 00:16:14
                               ETA: 00:55:59

################################################################################
                     [1m Learning iteration 450/2000 [0m                      

                       Computation: 48911 steps/s (collection: 1.922s, learning 0.088s)
             Mean action noise std: 2.39
          Mean value_function loss: 11.5125
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 70.8138
                       Mean reward: 16.37
               Mean episode length: 192.85
    Episode_Reward/reaching_object: 0.7161
     Episode_Reward/lifting_object: 1.4768
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 2.01s
                      Time elapsed: 00:16:16
                               ETA: 00:55:57

################################################################################
                     [1m Learning iteration 451/2000 [0m                      

                       Computation: 47802 steps/s (collection: 1.955s, learning 0.102s)
             Mean action noise std: 2.39
          Mean value_function loss: 22.3501
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 70.8506
                       Mean reward: 17.29
               Mean episode length: 200.34
    Episode_Reward/reaching_object: 0.7264
     Episode_Reward/lifting_object: 2.3362
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 2.06s
                      Time elapsed: 00:16:18
                               ETA: 00:55:54

################################################################################
                     [1m Learning iteration 452/2000 [0m                      

                       Computation: 47048 steps/s (collection: 1.993s, learning 0.096s)
             Mean action noise std: 2.39
          Mean value_function loss: 11.8301
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 70.8925
                       Mean reward: 13.27
               Mean episode length: 177.95
    Episode_Reward/reaching_object: 0.6979
     Episode_Reward/lifting_object: 1.7732
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 2.09s
                      Time elapsed: 00:16:20
                               ETA: 00:55:52

################################################################################
                     [1m Learning iteration 453/2000 [0m                      

                       Computation: 47604 steps/s (collection: 1.972s, learning 0.093s)
             Mean action noise std: 2.40
          Mean value_function loss: 18.3268
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 70.9323
                       Mean reward: 13.92
               Mean episode length: 175.45
    Episode_Reward/reaching_object: 0.6963
     Episode_Reward/lifting_object: 2.1203
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.0417
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 2.07s
                      Time elapsed: 00:16:23
                               ETA: 00:55:49

################################################################################
                     [1m Learning iteration 454/2000 [0m                      

                       Computation: 47665 steps/s (collection: 1.969s, learning 0.094s)
             Mean action noise std: 2.40
          Mean value_function loss: 9.3617
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 70.9725
                       Mean reward: 11.21
               Mean episode length: 172.41
    Episode_Reward/reaching_object: 0.6835
     Episode_Reward/lifting_object: 1.8079
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 2.06s
                      Time elapsed: 00:16:25
                               ETA: 00:55:47

################################################################################
                     [1m Learning iteration 455/2000 [0m                      

                       Computation: 47672 steps/s (collection: 1.953s, learning 0.110s)
             Mean action noise std: 2.40
          Mean value_function loss: 19.5036
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 70.9946
                       Mean reward: 10.30
               Mean episode length: 175.66
    Episode_Reward/reaching_object: 0.6727
     Episode_Reward/lifting_object: 1.8835
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 13.3750
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 2.06s
                      Time elapsed: 00:16:27
                               ETA: 00:55:44

################################################################################
                     [1m Learning iteration 456/2000 [0m                      

                       Computation: 48274 steps/s (collection: 1.947s, learning 0.090s)
             Mean action noise std: 2.40
          Mean value_function loss: 14.5476
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 71.0105
                       Mean reward: 13.53
               Mean episode length: 169.63
    Episode_Reward/reaching_object: 0.6676
     Episode_Reward/lifting_object: 1.6169
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 2.04s
                      Time elapsed: 00:16:29
                               ETA: 00:55:41

################################################################################
                     [1m Learning iteration 457/2000 [0m                      

                       Computation: 48383 steps/s (collection: 1.942s, learning 0.090s)
             Mean action noise std: 2.40
          Mean value_function loss: 14.6108
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 71.0437
                       Mean reward: 8.98
               Mean episode length: 162.53
    Episode_Reward/reaching_object: 0.6359
     Episode_Reward/lifting_object: 1.9694
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 2.03s
                      Time elapsed: 00:16:31
                               ETA: 00:55:39

################################################################################
                     [1m Learning iteration 458/2000 [0m                      

                       Computation: 46954 steps/s (collection: 1.986s, learning 0.108s)
             Mean action noise std: 2.41
          Mean value_function loss: 14.8425
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 71.0778
                       Mean reward: 8.35
               Mean episode length: 171.64
    Episode_Reward/reaching_object: 0.6659
     Episode_Reward/lifting_object: 1.7987
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 2.09s
                      Time elapsed: 00:16:33
                               ETA: 00:55:36

################################################################################
                     [1m Learning iteration 459/2000 [0m                      

                       Computation: 47681 steps/s (collection: 1.969s, learning 0.093s)
             Mean action noise std: 2.41
          Mean value_function loss: 17.0518
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 71.1202
                       Mean reward: 11.89
               Mean episode length: 174.31
    Episode_Reward/reaching_object: 0.6569
     Episode_Reward/lifting_object: 1.7504
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 2.06s
                      Time elapsed: 00:16:35
                               ETA: 00:55:34

################################################################################
                     [1m Learning iteration 460/2000 [0m                      

                       Computation: 47446 steps/s (collection: 1.984s, learning 0.088s)
             Mean action noise std: 2.41
          Mean value_function loss: 24.0583
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 71.1554
                       Mean reward: 15.97
               Mean episode length: 169.73
    Episode_Reward/reaching_object: 0.6697
     Episode_Reward/lifting_object: 2.1719
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 2.07s
                      Time elapsed: 00:16:37
                               ETA: 00:55:31

################################################################################
                     [1m Learning iteration 461/2000 [0m                      

                       Computation: 47867 steps/s (collection: 1.949s, learning 0.105s)
             Mean action noise std: 2.41
          Mean value_function loss: 18.1063
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 71.1748
                       Mean reward: 10.76
               Mean episode length: 181.75
    Episode_Reward/reaching_object: 0.6671
     Episode_Reward/lifting_object: 1.7200
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 2.05s
                      Time elapsed: 00:16:39
                               ETA: 00:55:29

################################################################################
                     [1m Learning iteration 462/2000 [0m                      

                       Computation: 47389 steps/s (collection: 1.983s, learning 0.092s)
             Mean action noise std: 2.42
          Mean value_function loss: 12.9816
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 71.2118
                       Mean reward: 11.21
               Mean episode length: 182.66
    Episode_Reward/reaching_object: 0.6585
     Episode_Reward/lifting_object: 2.1972
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 14.3333
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 2.07s
                      Time elapsed: 00:16:41
                               ETA: 00:55:27

################################################################################
                     [1m Learning iteration 463/2000 [0m                      

                       Computation: 48538 steps/s (collection: 1.928s, learning 0.098s)
             Mean action noise std: 2.42
          Mean value_function loss: 16.7715
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 71.2455
                       Mean reward: 12.23
               Mean episode length: 184.49
    Episode_Reward/reaching_object: 0.6725
     Episode_Reward/lifting_object: 1.5194
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 13.4583
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 2.03s
                      Time elapsed: 00:16:43
                               ETA: 00:55:24

################################################################################
                     [1m Learning iteration 464/2000 [0m                      

                       Computation: 46748 steps/s (collection: 1.973s, learning 0.130s)
             Mean action noise std: 2.42
          Mean value_function loss: 14.9958
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 71.2701
                       Mean reward: 16.17
               Mean episode length: 167.41
    Episode_Reward/reaching_object: 0.6829
     Episode_Reward/lifting_object: 2.1884
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 2.10s
                      Time elapsed: 00:16:45
                               ETA: 00:55:22

################################################################################
                     [1m Learning iteration 465/2000 [0m                      

                       Computation: 46617 steps/s (collection: 1.980s, learning 0.129s)
             Mean action noise std: 2.43
          Mean value_function loss: 23.3816
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 71.3012
                       Mean reward: 12.04
               Mean episode length: 162.27
    Episode_Reward/reaching_object: 0.6475
     Episode_Reward/lifting_object: 2.0819
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 15.2500
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 2.11s
                      Time elapsed: 00:16:47
                               ETA: 00:55:19

################################################################################
                     [1m Learning iteration 466/2000 [0m                      

                       Computation: 46877 steps/s (collection: 1.985s, learning 0.112s)
             Mean action noise std: 2.43
          Mean value_function loss: 29.4617
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 71.3318
                       Mean reward: 11.03
               Mean episode length: 178.74
    Episode_Reward/reaching_object: 0.6164
     Episode_Reward/lifting_object: 1.6580
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 17.6250
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 2.10s
                      Time elapsed: 00:16:49
                               ETA: 00:55:17

################################################################################
                     [1m Learning iteration 467/2000 [0m                      

                       Computation: 47863 steps/s (collection: 1.963s, learning 0.091s)
             Mean action noise std: 2.43
          Mean value_function loss: 21.3876
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 71.3593
                       Mean reward: 15.91
               Mean episode length: 157.99
    Episode_Reward/reaching_object: 0.6541
     Episode_Reward/lifting_object: 2.1438
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 16.4583
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 2.05s
                      Time elapsed: 00:16:51
                               ETA: 00:55:14

################################################################################
                     [1m Learning iteration 468/2000 [0m                      

                       Computation: 48078 steps/s (collection: 1.948s, learning 0.097s)
             Mean action noise std: 2.43
          Mean value_function loss: 17.2805
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 71.3981
                       Mean reward: 13.34
               Mean episode length: 179.54
    Episode_Reward/reaching_object: 0.6527
     Episode_Reward/lifting_object: 1.6011
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 2.04s
                      Time elapsed: 00:16:53
                               ETA: 00:55:12

################################################################################
                     [1m Learning iteration 469/2000 [0m                      

                       Computation: 48116 steps/s (collection: 1.947s, learning 0.096s)
             Mean action noise std: 2.44
          Mean value_function loss: 22.2193
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 71.4393
                       Mean reward: 13.39
               Mean episode length: 166.77
    Episode_Reward/reaching_object: 0.6566
     Episode_Reward/lifting_object: 2.0547
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 14.1250
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 2.04s
                      Time elapsed: 00:16:56
                               ETA: 00:55:09

################################################################################
                     [1m Learning iteration 470/2000 [0m                      

                       Computation: 48133 steps/s (collection: 1.954s, learning 0.089s)
             Mean action noise std: 2.44
          Mean value_function loss: 15.7180
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 71.4826
                       Mean reward: 17.27
               Mean episode length: 178.11
    Episode_Reward/reaching_object: 0.6839
     Episode_Reward/lifting_object: 2.0236
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 2.04s
                      Time elapsed: 00:16:58
                               ETA: 00:55:07

################################################################################
                     [1m Learning iteration 471/2000 [0m                      

                       Computation: 48267 steps/s (collection: 1.949s, learning 0.088s)
             Mean action noise std: 2.44
          Mean value_function loss: 18.0491
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 71.5025
                       Mean reward: 16.36
               Mean episode length: 188.66
    Episode_Reward/reaching_object: 0.7043
     Episode_Reward/lifting_object: 2.0884
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 2.04s
                      Time elapsed: 00:17:00
                               ETA: 00:55:04

################################################################################
                     [1m Learning iteration 472/2000 [0m                      

                       Computation: 47378 steps/s (collection: 1.977s, learning 0.098s)
             Mean action noise std: 2.44
          Mean value_function loss: 12.4091
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 71.5215
                       Mean reward: 15.01
               Mean episode length: 180.63
    Episode_Reward/reaching_object: 0.7110
     Episode_Reward/lifting_object: 2.4334
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 2.07s
                      Time elapsed: 00:17:02
                               ETA: 00:55:02

################################################################################
                     [1m Learning iteration 473/2000 [0m                      

                       Computation: 46636 steps/s (collection: 2.013s, learning 0.095s)
             Mean action noise std: 2.44
          Mean value_function loss: 23.9918
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 71.5375
                       Mean reward: 14.99
               Mean episode length: 173.60
    Episode_Reward/reaching_object: 0.7113
     Episode_Reward/lifting_object: 2.5365
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 2.11s
                      Time elapsed: 00:17:04
                               ETA: 00:54:59

################################################################################
                     [1m Learning iteration 474/2000 [0m                      

                       Computation: 47537 steps/s (collection: 1.972s, learning 0.095s)
             Mean action noise std: 2.45
          Mean value_function loss: 15.4785
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 71.5677
                       Mean reward: 20.71
               Mean episode length: 182.83
    Episode_Reward/reaching_object: 0.7096
     Episode_Reward/lifting_object: 2.4341
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 2.07s
                      Time elapsed: 00:17:06
                               ETA: 00:54:57

################################################################################
                     [1m Learning iteration 475/2000 [0m                      

                       Computation: 47204 steps/s (collection: 1.970s, learning 0.113s)
             Mean action noise std: 2.45
          Mean value_function loss: 14.4686
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 71.5925
                       Mean reward: 15.45
               Mean episode length: 187.90
    Episode_Reward/reaching_object: 0.7263
     Episode_Reward/lifting_object: 2.7688
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 2.08s
                      Time elapsed: 00:17:08
                               ETA: 00:54:54

################################################################################
                     [1m Learning iteration 476/2000 [0m                      

                       Computation: 47665 steps/s (collection: 1.972s, learning 0.091s)
             Mean action noise std: 2.45
          Mean value_function loss: 24.7200
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 71.6048
                       Mean reward: 11.88
               Mean episode length: 177.26
    Episode_Reward/reaching_object: 0.7053
     Episode_Reward/lifting_object: 2.4699
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 13.0417
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 2.06s
                      Time elapsed: 00:17:10
                               ETA: 00:54:52

################################################################################
                     [1m Learning iteration 477/2000 [0m                      

                       Computation: 47197 steps/s (collection: 1.975s, learning 0.108s)
             Mean action noise std: 2.45
          Mean value_function loss: 13.2110
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 71.6162
                       Mean reward: 15.00
               Mean episode length: 175.48
    Episode_Reward/reaching_object: 0.7139
     Episode_Reward/lifting_object: 2.3757
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 2.08s
                      Time elapsed: 00:17:12
                               ETA: 00:54:50

################################################################################
                     [1m Learning iteration 478/2000 [0m                      

                       Computation: 46770 steps/s (collection: 2.001s, learning 0.101s)
             Mean action noise std: 2.45
          Mean value_function loss: 12.1621
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 71.6397
                       Mean reward: 16.11
               Mean episode length: 182.55
    Episode_Reward/reaching_object: 0.7366
     Episode_Reward/lifting_object: 2.4741
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 2.10s
                      Time elapsed: 00:17:14
                               ETA: 00:54:47

################################################################################
                     [1m Learning iteration 479/2000 [0m                      

                       Computation: 48266 steps/s (collection: 1.944s, learning 0.093s)
             Mean action noise std: 2.45
          Mean value_function loss: 14.5559
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 71.6646
                       Mean reward: 14.83
               Mean episode length: 190.44
    Episode_Reward/reaching_object: 0.6769
     Episode_Reward/lifting_object: 2.2682
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 2.04s
                      Time elapsed: 00:17:16
                               ETA: 00:54:45

################################################################################
                     [1m Learning iteration 480/2000 [0m                      

                       Computation: 46703 steps/s (collection: 1.969s, learning 0.136s)
             Mean action noise std: 2.45
          Mean value_function loss: 14.7835
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 71.6777
                       Mean reward: 20.01
               Mean episode length: 187.37
    Episode_Reward/reaching_object: 0.6895
     Episode_Reward/lifting_object: 2.7358
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 12.7500
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 2.10s
                      Time elapsed: 00:17:18
                               ETA: 00:54:42

################################################################################
                     [1m Learning iteration 481/2000 [0m                      

                       Computation: 47131 steps/s (collection: 1.963s, learning 0.123s)
             Mean action noise std: 2.46
          Mean value_function loss: 14.1706
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 71.7061
                       Mean reward: 13.47
               Mean episode length: 173.16
    Episode_Reward/reaching_object: 0.6947
     Episode_Reward/lifting_object: 2.3903
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 2.09s
                      Time elapsed: 00:17:20
                               ETA: 00:54:40

################################################################################
                     [1m Learning iteration 482/2000 [0m                      

                       Computation: 46694 steps/s (collection: 2.005s, learning 0.100s)
             Mean action noise std: 2.46
          Mean value_function loss: 28.2876
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 71.7399
                       Mean reward: 20.53
               Mean episode length: 188.71
    Episode_Reward/reaching_object: 0.7014
     Episode_Reward/lifting_object: 2.8618
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 2.11s
                      Time elapsed: 00:17:23
                               ETA: 00:54:38

################################################################################
                     [1m Learning iteration 483/2000 [0m                      

                       Computation: 46685 steps/s (collection: 1.994s, learning 0.112s)
             Mean action noise std: 2.46
          Mean value_function loss: 19.5064
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 71.7683
                       Mean reward: 8.67
               Mean episode length: 197.56
    Episode_Reward/reaching_object: 0.6990
     Episode_Reward/lifting_object: 1.8851
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 10.2083
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 2.11s
                      Time elapsed: 00:17:25
                               ETA: 00:54:35

################################################################################
                     [1m Learning iteration 484/2000 [0m                      

                       Computation: 46841 steps/s (collection: 2.003s, learning 0.096s)
             Mean action noise std: 2.46
          Mean value_function loss: 12.8706
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 71.7920
                       Mean reward: 17.31
               Mean episode length: 195.07
    Episode_Reward/reaching_object: 0.7332
     Episode_Reward/lifting_object: 2.5613
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0700
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 2.10s
                      Time elapsed: 00:17:27
                               ETA: 00:54:33

################################################################################
                     [1m Learning iteration 485/2000 [0m                      

                       Computation: 46956 steps/s (collection: 1.994s, learning 0.099s)
             Mean action noise std: 2.47
          Mean value_function loss: 15.3021
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 71.8179
                       Mean reward: 19.75
               Mean episode length: 188.45
    Episode_Reward/reaching_object: 0.7508
     Episode_Reward/lifting_object: 2.9824
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0718
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 2.09s
                      Time elapsed: 00:17:29
                               ETA: 00:54:31

################################################################################
                     [1m Learning iteration 486/2000 [0m                      

                       Computation: 46587 steps/s (collection: 1.980s, learning 0.130s)
             Mean action noise std: 2.47
          Mean value_function loss: 17.5466
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 71.8482
                       Mean reward: 12.23
               Mean episode length: 195.34
    Episode_Reward/reaching_object: 0.7073
     Episode_Reward/lifting_object: 2.1083
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0702
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 2.11s
                      Time elapsed: 00:17:31
                               ETA: 00:54:28

################################################################################
                     [1m Learning iteration 487/2000 [0m                      

                       Computation: 46947 steps/s (collection: 1.994s, learning 0.100s)
             Mean action noise std: 2.47
          Mean value_function loss: 31.0070
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 71.8854
                       Mean reward: 12.06
               Mean episode length: 174.05
    Episode_Reward/reaching_object: 0.7084
     Episode_Reward/lifting_object: 2.9601
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 2.09s
                      Time elapsed: 00:17:33
                               ETA: 00:54:26

################################################################################
                     [1m Learning iteration 488/2000 [0m                      

                       Computation: 47271 steps/s (collection: 1.988s, learning 0.092s)
             Mean action noise std: 2.47
          Mean value_function loss: 33.5391
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 71.9094
                       Mean reward: 15.11
               Mean episode length: 176.12
    Episode_Reward/reaching_object: 0.6861
     Episode_Reward/lifting_object: 2.6680
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 2.08s
                      Time elapsed: 00:17:35
                               ETA: 00:54:23

################################################################################
                     [1m Learning iteration 489/2000 [0m                      

                       Computation: 47040 steps/s (collection: 1.977s, learning 0.113s)
             Mean action noise std: 2.47
          Mean value_function loss: 38.3853
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 71.9218
                       Mean reward: 13.49
               Mean episode length: 176.26
    Episode_Reward/reaching_object: 0.6821
     Episode_Reward/lifting_object: 2.2521
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 2.09s
                      Time elapsed: 00:17:37
                               ETA: 00:54:21

################################################################################
                     [1m Learning iteration 490/2000 [0m                      

                       Computation: 46048 steps/s (collection: 2.024s, learning 0.111s)
             Mean action noise std: 2.48
          Mean value_function loss: 25.5368
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 71.9408
                       Mean reward: 20.32
               Mean episode length: 170.14
    Episode_Reward/reaching_object: 0.7030
     Episode_Reward/lifting_object: 2.6499
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 14.1250
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 2.13s
                      Time elapsed: 00:17:39
                               ETA: 00:54:19

################################################################################
                     [1m Learning iteration 491/2000 [0m                      

                       Computation: 45644 steps/s (collection: 2.008s, learning 0.146s)
             Mean action noise std: 2.48
          Mean value_function loss: 29.3885
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 71.9705
                       Mean reward: 10.83
               Mean episode length: 166.85
    Episode_Reward/reaching_object: 0.7005
     Episode_Reward/lifting_object: 2.4677
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 2.15s
                      Time elapsed: 00:17:41
                               ETA: 00:54:17

################################################################################
                     [1m Learning iteration 492/2000 [0m                      

                       Computation: 46787 steps/s (collection: 2.002s, learning 0.099s)
             Mean action noise std: 2.48
          Mean value_function loss: 20.3671
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.0065
                       Mean reward: 15.12
               Mean episode length: 165.03
    Episode_Reward/reaching_object: 0.6663
     Episode_Reward/lifting_object: 2.4110
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 14.6667
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 2.10s
                      Time elapsed: 00:17:44
                               ETA: 00:54:14

################################################################################
                     [1m Learning iteration 493/2000 [0m                      

                       Computation: 45631 steps/s (collection: 2.033s, learning 0.121s)
             Mean action noise std: 2.48
          Mean value_function loss: 17.0921
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 72.0330
                       Mean reward: 13.67
               Mean episode length: 167.33
    Episode_Reward/reaching_object: 0.6409
     Episode_Reward/lifting_object: 2.7746
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 16.6667
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 2.15s
                      Time elapsed: 00:17:46
                               ETA: 00:54:12

################################################################################
                     [1m Learning iteration 494/2000 [0m                      

                       Computation: 46805 steps/s (collection: 2.007s, learning 0.093s)
             Mean action noise std: 2.49
          Mean value_function loss: 40.8359
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 72.0633
                       Mean reward: 14.33
               Mean episode length: 158.05
    Episode_Reward/reaching_object: 0.6417
     Episode_Reward/lifting_object: 2.6436
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 17.7500
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 2.10s
                      Time elapsed: 00:17:48
                               ETA: 00:54:10

################################################################################
                     [1m Learning iteration 495/2000 [0m                      

                       Computation: 46125 steps/s (collection: 2.028s, learning 0.103s)
             Mean action noise std: 2.49
          Mean value_function loss: 36.7733
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.0977
                       Mean reward: 14.62
               Mean episode length: 166.18
    Episode_Reward/reaching_object: 0.6113
     Episode_Reward/lifting_object: 1.5972
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 17.5833
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 2.13s
                      Time elapsed: 00:17:50
                               ETA: 00:54:08

################################################################################
                     [1m Learning iteration 496/2000 [0m                      

                       Computation: 46953 steps/s (collection: 2.001s, learning 0.092s)
             Mean action noise std: 2.49
          Mean value_function loss: 17.4513
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 72.1311
                       Mean reward: 16.01
               Mean episode length: 159.95
    Episode_Reward/reaching_object: 0.6542
     Episode_Reward/lifting_object: 2.3888
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 15.3333
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 2.09s
                      Time elapsed: 00:17:52
                               ETA: 00:54:05

################################################################################
                     [1m Learning iteration 497/2000 [0m                      

                       Computation: 46850 steps/s (collection: 1.994s, learning 0.105s)
             Mean action noise std: 2.49
          Mean value_function loss: 17.7193
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 72.1549
                       Mean reward: 17.11
               Mean episode length: 175.32
    Episode_Reward/reaching_object: 0.6385
     Episode_Reward/lifting_object: 2.4906
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 15.8333
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 2.10s
                      Time elapsed: 00:17:54
                               ETA: 00:54:03

################################################################################
                     [1m Learning iteration 498/2000 [0m                      

                       Computation: 46094 steps/s (collection: 2.011s, learning 0.121s)
             Mean action noise std: 2.49
          Mean value_function loss: 30.0085
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 72.1752
                       Mean reward: 10.60
               Mean episode length: 170.81
    Episode_Reward/reaching_object: 0.6481
     Episode_Reward/lifting_object: 3.0221
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.9167
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 2.13s
                      Time elapsed: 00:17:56
                               ETA: 00:54:01

################################################################################
                     [1m Learning iteration 499/2000 [0m                      

                       Computation: 46436 steps/s (collection: 1.999s, learning 0.118s)
             Mean action noise std: 2.50
          Mean value_function loss: 19.0384
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 72.2086
                       Mean reward: 19.39
               Mean episode length: 168.82
    Episode_Reward/reaching_object: 0.6266
     Episode_Reward/lifting_object: 3.0815
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 18.0833
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 2.12s
                      Time elapsed: 00:17:58
                               ETA: 00:53:58

################################################################################
                     [1m Learning iteration 500/2000 [0m                      

                       Computation: 45745 steps/s (collection: 2.017s, learning 0.132s)
             Mean action noise std: 2.50
          Mean value_function loss: 22.1265
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 72.2238
                       Mean reward: 21.74
               Mean episode length: 168.12
    Episode_Reward/reaching_object: 0.6220
     Episode_Reward/lifting_object: 3.1419
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 18.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 2.15s
                      Time elapsed: 00:18:01
                               ETA: 00:53:56

################################################################################
                     [1m Learning iteration 501/2000 [0m                      

                       Computation: 45751 steps/s (collection: 2.044s, learning 0.105s)
             Mean action noise std: 2.50
          Mean value_function loss: 22.6967
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 72.2518
                       Mean reward: 12.81
               Mean episode length: 167.59
    Episode_Reward/reaching_object: 0.6309
     Episode_Reward/lifting_object: 3.0296
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 17.8333
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 2.15s
                      Time elapsed: 00:18:03
                               ETA: 00:53:54

################################################################################
                     [1m Learning iteration 502/2000 [0m                      

                       Computation: 47280 steps/s (collection: 1.989s, learning 0.090s)
             Mean action noise std: 2.50
          Mean value_function loss: 20.1871
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 72.2750
                       Mean reward: 20.06
               Mean episode length: 170.44
    Episode_Reward/reaching_object: 0.6307
     Episode_Reward/lifting_object: 3.3963
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 16.6250
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 2.08s
                      Time elapsed: 00:18:05
                               ETA: 00:53:52

################################################################################
                     [1m Learning iteration 503/2000 [0m                      

                       Computation: 46406 steps/s (collection: 2.006s, learning 0.112s)
             Mean action noise std: 2.50
          Mean value_function loss: 32.1583
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 72.3024
                       Mean reward: 18.23
               Mean episode length: 167.50
    Episode_Reward/reaching_object: 0.6171
     Episode_Reward/lifting_object: 2.7201
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 17.2917
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 2.12s
                      Time elapsed: 00:18:07
                               ETA: 00:53:49

################################################################################
                     [1m Learning iteration 504/2000 [0m                      

                       Computation: 46586 steps/s (collection: 2.007s, learning 0.104s)
             Mean action noise std: 2.51
          Mean value_function loss: 29.0611
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 72.3223
                       Mean reward: 14.84
               Mean episode length: 173.19
    Episode_Reward/reaching_object: 0.6213
     Episode_Reward/lifting_object: 3.1602
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 18.0833
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 2.11s
                      Time elapsed: 00:18:09
                               ETA: 00:53:47

################################################################################
                     [1m Learning iteration 505/2000 [0m                      

                       Computation: 47387 steps/s (collection: 1.982s, learning 0.093s)
             Mean action noise std: 2.51
          Mean value_function loss: 70.0415
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 72.3447
                       Mean reward: 12.21
               Mean episode length: 162.13
    Episode_Reward/reaching_object: 0.6053
     Episode_Reward/lifting_object: 2.4085
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 17.3333
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 2.07s
                      Time elapsed: 00:18:11
                               ETA: 00:53:45

################################################################################
                     [1m Learning iteration 506/2000 [0m                      

                       Computation: 46652 steps/s (collection: 2.015s, learning 0.092s)
             Mean action noise std: 2.51
          Mean value_function loss: 54.6887
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 72.3671
                       Mean reward: 19.51
               Mean episode length: 167.33
    Episode_Reward/reaching_object: 0.6078
     Episode_Reward/lifting_object: 2.9244
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 18.9167
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 2.11s
                      Time elapsed: 00:18:13
                               ETA: 00:53:42

################################################################################
                     [1m Learning iteration 507/2000 [0m                      

                       Computation: 46149 steps/s (collection: 2.017s, learning 0.113s)
             Mean action noise std: 2.51
          Mean value_function loss: 31.1433
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 72.3896
                       Mean reward: 10.57
               Mean episode length: 162.24
    Episode_Reward/reaching_object: 0.6051
     Episode_Reward/lifting_object: 3.0018
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 17.7083
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 2.13s
                      Time elapsed: 00:18:15
                               ETA: 00:53:40

################################################################################
                     [1m Learning iteration 508/2000 [0m                      

                       Computation: 46102 steps/s (collection: 2.015s, learning 0.118s)
             Mean action noise std: 2.51
          Mean value_function loss: 44.5167
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 72.4059
                       Mean reward: 11.06
               Mean episode length: 156.19
    Episode_Reward/reaching_object: 0.6200
     Episode_Reward/lifting_object: 2.8834
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 18.5833
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 2.13s
                      Time elapsed: 00:18:17
                               ETA: 00:53:38

################################################################################
                     [1m Learning iteration 509/2000 [0m                      

                       Computation: 45458 steps/s (collection: 2.034s, learning 0.129s)
             Mean action noise std: 2.52
          Mean value_function loss: 26.2317
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 72.4309
                       Mean reward: 20.61
               Mean episode length: 174.99
    Episode_Reward/reaching_object: 0.5792
     Episode_Reward/lifting_object: 2.5551
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 18.5417
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 2.16s
                      Time elapsed: 00:18:20
                               ETA: 00:53:36

################################################################################
                     [1m Learning iteration 510/2000 [0m                      

                       Computation: 46263 steps/s (collection: 1.979s, learning 0.146s)
             Mean action noise std: 2.52
          Mean value_function loss: 21.3059
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 72.4580
                       Mean reward: 20.97
               Mean episode length: 170.43
    Episode_Reward/reaching_object: 0.5757
     Episode_Reward/lifting_object: 2.8381
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 16.4167
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 2.12s
                      Time elapsed: 00:18:22
                               ETA: 00:53:34

################################################################################
                     [1m Learning iteration 511/2000 [0m                      

                       Computation: 46200 steps/s (collection: 2.021s, learning 0.107s)
             Mean action noise std: 2.52
          Mean value_function loss: 21.4962
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 72.4702
                       Mean reward: 19.49
               Mean episode length: 179.55
    Episode_Reward/reaching_object: 0.6180
     Episode_Reward/lifting_object: 3.3114
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 16.4167
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 2.13s
                      Time elapsed: 00:18:24
                               ETA: 00:53:31

################################################################################
                     [1m Learning iteration 512/2000 [0m                      

                       Computation: 46968 steps/s (collection: 2.001s, learning 0.092s)
             Mean action noise std: 2.52
          Mean value_function loss: 23.1056
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 72.4871
                       Mean reward: 18.26
               Mean episode length: 161.05
    Episode_Reward/reaching_object: 0.6176
     Episode_Reward/lifting_object: 3.4458
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 16.6667
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 2.09s
                      Time elapsed: 00:18:26
                               ETA: 00:53:29

################################################################################
                     [1m Learning iteration 513/2000 [0m                      

                       Computation: 46146 steps/s (collection: 2.023s, learning 0.107s)
             Mean action noise std: 2.52
          Mean value_function loss: 23.2900
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 72.5066
                       Mean reward: 18.75
               Mean episode length: 175.37
    Episode_Reward/reaching_object: 0.6146
     Episode_Reward/lifting_object: 3.6009
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0705
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 2.13s
                      Time elapsed: 00:18:28
                               ETA: 00:53:27

################################################################################
                     [1m Learning iteration 514/2000 [0m                      

                       Computation: 46283 steps/s (collection: 2.029s, learning 0.095s)
             Mean action noise std: 2.52
          Mean value_function loss: 35.8262
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 72.5263
                       Mean reward: 20.57
               Mean episode length: 171.45
    Episode_Reward/reaching_object: 0.6354
     Episode_Reward/lifting_object: 3.3878
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0718
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 14.7083
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 2.12s
                      Time elapsed: 00:18:30
                               ETA: 00:53:24

################################################################################
                     [1m Learning iteration 515/2000 [0m                      

                       Computation: 46449 steps/s (collection: 2.008s, learning 0.109s)
             Mean action noise std: 2.52
          Mean value_function loss: 25.8354
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.5406
                       Mean reward: 21.10
               Mean episode length: 180.96
    Episode_Reward/reaching_object: 0.6379
     Episode_Reward/lifting_object: 3.6861
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0720
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 2.12s
                      Time elapsed: 00:18:32
                               ETA: 00:53:22

################################################################################
                     [1m Learning iteration 516/2000 [0m                      

                       Computation: 45516 steps/s (collection: 2.037s, learning 0.123s)
             Mean action noise std: 2.53
          Mean value_function loss: 21.4725
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 72.5568
                       Mean reward: 24.48
               Mean episode length: 178.75
    Episode_Reward/reaching_object: 0.6422
     Episode_Reward/lifting_object: 3.2697
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0719
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 2.16s
                      Time elapsed: 00:18:35
                               ETA: 00:53:20

################################################################################
                     [1m Learning iteration 517/2000 [0m                      

                       Computation: 44177 steps/s (collection: 2.058s, learning 0.167s)
             Mean action noise std: 2.53
          Mean value_function loss: 25.6460
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 72.5832
                       Mean reward: 21.07
               Mean episode length: 170.53
    Episode_Reward/reaching_object: 0.6623
     Episode_Reward/lifting_object: 3.9042
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0735
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 2.23s
                      Time elapsed: 00:18:37
                               ETA: 00:53:18

################################################################################
                     [1m Learning iteration 518/2000 [0m                      

                       Computation: 44593 steps/s (collection: 2.074s, learning 0.130s)
             Mean action noise std: 2.53
          Mean value_function loss: 24.4742
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 72.6141
                       Mean reward: 23.72
               Mean episode length: 168.67
    Episode_Reward/reaching_object: 0.6401
     Episode_Reward/lifting_object: 3.6624
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0713
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 15.6667
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 2.20s
                      Time elapsed: 00:18:39
                               ETA: 00:53:16

################################################################################
                     [1m Learning iteration 519/2000 [0m                      

                       Computation: 46570 steps/s (collection: 2.010s, learning 0.101s)
             Mean action noise std: 2.53
          Mean value_function loss: 28.8700
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 72.6327
                       Mean reward: 21.60
               Mean episode length: 181.33
    Episode_Reward/reaching_object: 0.6637
     Episode_Reward/lifting_object: 3.5819
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0727
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 16.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 2.11s
                      Time elapsed: 00:18:41
                               ETA: 00:53:14

################################################################################
                     [1m Learning iteration 520/2000 [0m                      

                       Computation: 45999 steps/s (collection: 2.046s, learning 0.091s)
             Mean action noise std: 2.53
          Mean value_function loss: 27.2800
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 72.6541
                       Mean reward: 19.98
               Mean episode length: 177.97
    Episode_Reward/reaching_object: 0.6763
     Episode_Reward/lifting_object: 3.6001
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0744
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 16.1667
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 2.14s
                      Time elapsed: 00:18:43
                               ETA: 00:53:12

################################################################################
                     [1m Learning iteration 521/2000 [0m                      

                       Computation: 46396 steps/s (collection: 2.029s, learning 0.090s)
             Mean action noise std: 2.53
          Mean value_function loss: 29.1404
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 72.6692
                       Mean reward: 21.15
               Mean episode length: 172.00
    Episode_Reward/reaching_object: 0.6471
     Episode_Reward/lifting_object: 3.9053
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0724
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 16.3750
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 2.12s
                      Time elapsed: 00:18:45
                               ETA: 00:53:09

################################################################################
                     [1m Learning iteration 522/2000 [0m                      

                       Computation: 46014 steps/s (collection: 2.046s, learning 0.091s)
             Mean action noise std: 2.54
          Mean value_function loss: 22.7112
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 72.6842
                       Mean reward: 25.06
               Mean episode length: 163.88
    Episode_Reward/reaching_object: 0.6204
     Episode_Reward/lifting_object: 3.5758
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0715
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 16.7083
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 2.14s
                      Time elapsed: 00:18:47
                               ETA: 00:53:07

################################################################################
                     [1m Learning iteration 523/2000 [0m                      

                       Computation: 45364 steps/s (collection: 2.057s, learning 0.110s)
             Mean action noise std: 2.54
          Mean value_function loss: 41.9418
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 72.7064
                       Mean reward: 21.43
               Mean episode length: 157.41
    Episode_Reward/reaching_object: 0.6272
     Episode_Reward/lifting_object: 3.6726
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 16.2083
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 2.17s
                      Time elapsed: 00:18:50
                               ETA: 00:53:05

################################################################################
                     [1m Learning iteration 524/2000 [0m                      

                       Computation: 45099 steps/s (collection: 2.043s, learning 0.137s)
             Mean action noise std: 2.54
          Mean value_function loss: 30.1930
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 72.7187
                       Mean reward: 20.36
               Mean episode length: 165.98
    Episode_Reward/reaching_object: 0.5836
     Episode_Reward/lifting_object: 3.2028
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0701
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 16.9167
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 2.18s
                      Time elapsed: 00:18:52
                               ETA: 00:53:03

################################################################################
                     [1m Learning iteration 525/2000 [0m                      

                       Computation: 43990 steps/s (collection: 2.065s, learning 0.170s)
             Mean action noise std: 2.54
          Mean value_function loss: 48.9885
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 72.7321
                       Mean reward: 13.59
               Mean episode length: 151.94
    Episode_Reward/reaching_object: 0.6147
     Episode_Reward/lifting_object: 3.5271
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 18.0417
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 2.23s
                      Time elapsed: 00:18:54
                               ETA: 00:53:01

################################################################################
                     [1m Learning iteration 526/2000 [0m                      

                       Computation: 45460 steps/s (collection: 2.059s, learning 0.103s)
             Mean action noise std: 2.54
          Mean value_function loss: 36.2484
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 72.7561
                       Mean reward: 18.49
               Mean episode length: 178.44
    Episode_Reward/reaching_object: 0.6232
     Episode_Reward/lifting_object: 3.7766
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 17.7917
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 2.16s
                      Time elapsed: 00:18:56
                               ETA: 00:52:59

################################################################################
                     [1m Learning iteration 527/2000 [0m                      

                       Computation: 45852 steps/s (collection: 2.055s, learning 0.089s)
             Mean action noise std: 2.55
          Mean value_function loss: 27.9219
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 72.7847
                       Mean reward: 26.67
               Mean episode length: 166.44
    Episode_Reward/reaching_object: 0.6162
     Episode_Reward/lifting_object: 3.6939
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 17.9167
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 2.14s
                      Time elapsed: 00:18:58
                               ETA: 00:52:57

################################################################################
                     [1m Learning iteration 528/2000 [0m                      

                       Computation: 44745 steps/s (collection: 2.076s, learning 0.121s)
             Mean action noise std: 2.55
          Mean value_function loss: 26.9333
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 72.8104
                       Mean reward: 20.61
               Mean episode length: 163.28
    Episode_Reward/reaching_object: 0.6460
     Episode_Reward/lifting_object: 4.3553
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0744
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 15.9583
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 2.20s
                      Time elapsed: 00:19:01
                               ETA: 00:52:55

################################################################################
                     [1m Learning iteration 529/2000 [0m                      

                       Computation: 43467 steps/s (collection: 2.083s, learning 0.178s)
             Mean action noise std: 2.55
          Mean value_function loss: 33.0993
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 72.8261
                       Mean reward: 20.45
               Mean episode length: 170.32
    Episode_Reward/reaching_object: 0.6762
     Episode_Reward/lifting_object: 4.4526
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0766
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 16.0833
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 2.26s
                      Time elapsed: 00:19:03
                               ETA: 00:52:53

################################################################################
                     [1m Learning iteration 530/2000 [0m                      

                       Computation: 45639 steps/s (collection: 2.021s, learning 0.133s)
             Mean action noise std: 2.55
          Mean value_function loss: 36.2835
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 72.8340
                       Mean reward: 24.69
               Mean episode length: 171.62
    Episode_Reward/reaching_object: 0.6473
     Episode_Reward/lifting_object: 3.6626
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0762
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 15.7500
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 2.15s
                      Time elapsed: 00:19:05
                               ETA: 00:52:50

################################################################################
                     [1m Learning iteration 531/2000 [0m                      

                       Computation: 44644 steps/s (collection: 2.084s, learning 0.118s)
             Mean action noise std: 2.55
          Mean value_function loss: 39.0374
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 72.8404
                       Mean reward: 19.63
               Mean episode length: 174.15
    Episode_Reward/reaching_object: 0.6319
     Episode_Reward/lifting_object: 3.5716
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0732
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 16.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 2.20s
                      Time elapsed: 00:19:07
                               ETA: 00:52:48

################################################################################
                     [1m Learning iteration 532/2000 [0m                      

                       Computation: 46140 steps/s (collection: 2.032s, learning 0.099s)
             Mean action noise std: 2.55
          Mean value_function loss: 25.8467
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 72.8446
                       Mean reward: 25.09
               Mean episode length: 172.41
    Episode_Reward/reaching_object: 0.6651
     Episode_Reward/lifting_object: 4.1962
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0765
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 16.5833
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 2.13s
                      Time elapsed: 00:19:09
                               ETA: 00:52:46

################################################################################
                     [1m Learning iteration 533/2000 [0m                      

                       Computation: 45853 steps/s (collection: 2.033s, learning 0.111s)
             Mean action noise std: 2.55
          Mean value_function loss: 27.6160
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 72.8505
                       Mean reward: 24.75
               Mean episode length: 174.38
    Episode_Reward/reaching_object: 0.6470
     Episode_Reward/lifting_object: 4.3740
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0735
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 16.6250
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 2.14s
                      Time elapsed: 00:19:11
                               ETA: 00:52:44

################################################################################
                     [1m Learning iteration 534/2000 [0m                      

                       Computation: 45867 steps/s (collection: 2.033s, learning 0.111s)
             Mean action noise std: 2.55
          Mean value_function loss: 31.9973
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 72.8624
                       Mean reward: 27.69
               Mean episode length: 175.29
    Episode_Reward/reaching_object: 0.6470
     Episode_Reward/lifting_object: 4.6172
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0755
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 16.3750
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 2.14s
                      Time elapsed: 00:19:14
                               ETA: 00:52:42

################################################################################
                     [1m Learning iteration 535/2000 [0m                      

                       Computation: 45859 steps/s (collection: 2.045s, learning 0.098s)
             Mean action noise std: 2.55
          Mean value_function loss: 22.4933
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 72.8864
                       Mean reward: 28.66
               Mean episode length: 168.75
    Episode_Reward/reaching_object: 0.6655
     Episode_Reward/lifting_object: 4.7437
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0776
      Episode_Termination/time_out: 6.3750
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 16.7917
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 2.14s
                      Time elapsed: 00:19:16
                               ETA: 00:52:40

################################################################################
                     [1m Learning iteration 536/2000 [0m                      

                       Computation: 45007 steps/s (collection: 2.073s, learning 0.111s)
             Mean action noise std: 2.56
          Mean value_function loss: 24.4782
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 72.9117
                       Mean reward: 15.68
               Mean episode length: 168.80
    Episode_Reward/reaching_object: 0.6532
     Episode_Reward/lifting_object: 4.2662
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0745
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.8333
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 2.18s
                      Time elapsed: 00:19:18
                               ETA: 00:52:38

################################################################################
                     [1m Learning iteration 537/2000 [0m                      

                       Computation: 45454 steps/s (collection: 2.058s, learning 0.105s)
             Mean action noise std: 2.56
          Mean value_function loss: 30.3706
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 72.9323
                       Mean reward: 25.72
               Mean episode length: 179.54
    Episode_Reward/reaching_object: 0.6576
     Episode_Reward/lifting_object: 4.9244
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0788
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 17.3333
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 2.16s
                      Time elapsed: 00:19:20
                               ETA: 00:52:35

################################################################################
                     [1m Learning iteration 538/2000 [0m                      

                       Computation: 46025 steps/s (collection: 2.033s, learning 0.103s)
             Mean action noise std: 2.56
          Mean value_function loss: 22.0806
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 72.9428
                       Mean reward: 19.40
               Mean episode length: 162.89
    Episode_Reward/reaching_object: 0.6391
     Episode_Reward/lifting_object: 4.5739
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0742
      Episode_Termination/time_out: 5.3750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 18.0833
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 2.14s
                      Time elapsed: 00:19:22
                               ETA: 00:52:33

################################################################################
                     [1m Learning iteration 539/2000 [0m                      

                       Computation: 44591 steps/s (collection: 2.090s, learning 0.115s)
             Mean action noise std: 2.56
          Mean value_function loss: 51.0212
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 72.9486
                       Mean reward: 21.63
               Mean episode length: 153.57
    Episode_Reward/reaching_object: 0.6412
     Episode_Reward/lifting_object: 4.1647
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 17.2917
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 2.20s
                      Time elapsed: 00:19:24
                               ETA: 00:52:31

################################################################################
                     [1m Learning iteration 540/2000 [0m                      

                       Computation: 45446 steps/s (collection: 2.047s, learning 0.116s)
             Mean action noise std: 2.56
          Mean value_function loss: 33.5982
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 72.9559
                       Mean reward: 26.90
               Mean episode length: 179.26
    Episode_Reward/reaching_object: 0.6384
     Episode_Reward/lifting_object: 4.4954
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0751
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 18.1667
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 2.16s
                      Time elapsed: 00:19:27
                               ETA: 00:52:29

################################################################################
                     [1m Learning iteration 541/2000 [0m                      

                       Computation: 45541 steps/s (collection: 2.066s, learning 0.093s)
             Mean action noise std: 2.56
          Mean value_function loss: 26.2065
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 72.9683
                       Mean reward: 28.39
               Mean episode length: 180.42
    Episode_Reward/reaching_object: 0.6408
     Episode_Reward/lifting_object: 4.8236
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0754
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 17.6667
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 2.16s
                      Time elapsed: 00:19:29
                               ETA: 00:52:27

################################################################################
                     [1m Learning iteration 542/2000 [0m                      

                       Computation: 45000 steps/s (collection: 2.069s, learning 0.116s)
             Mean action noise std: 2.56
          Mean value_function loss: 30.7842
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 72.9839
                       Mean reward: 29.21
               Mean episode length: 169.28
    Episode_Reward/reaching_object: 0.6284
     Episode_Reward/lifting_object: 4.4766
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0745
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 19.3333
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 2.18s
                      Time elapsed: 00:19:31
                               ETA: 00:52:25

################################################################################
                     [1m Learning iteration 543/2000 [0m                      

                       Computation: 43567 steps/s (collection: 2.076s, learning 0.181s)
             Mean action noise std: 2.56
          Mean value_function loss: 27.4460
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 73.0028
                       Mean reward: 33.22
               Mean episode length: 176.74
    Episode_Reward/reaching_object: 0.6439
     Episode_Reward/lifting_object: 4.2137
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0748
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 17.0417
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 2.26s
                      Time elapsed: 00:19:33
                               ETA: 00:52:23

################################################################################
                     [1m Learning iteration 544/2000 [0m                      

                       Computation: 46023 steps/s (collection: 2.034s, learning 0.102s)
             Mean action noise std: 2.56
          Mean value_function loss: 29.6013
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.0150
                       Mean reward: 24.01
               Mean episode length: 167.36
    Episode_Reward/reaching_object: 0.6336
     Episode_Reward/lifting_object: 4.3474
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0742
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.0417
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 2.14s
                      Time elapsed: 00:19:35
                               ETA: 00:52:21

################################################################################
                     [1m Learning iteration 545/2000 [0m                      

                       Computation: 46412 steps/s (collection: 2.026s, learning 0.092s)
             Mean action noise std: 2.56
          Mean value_function loss: 30.9048
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 73.0312
                       Mean reward: 24.99
               Mean episode length: 173.75
    Episode_Reward/reaching_object: 0.6421
     Episode_Reward/lifting_object: 4.4433
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0742
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 18.7083
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 2.12s
                      Time elapsed: 00:19:37
                               ETA: 00:52:18

################################################################################
                     [1m Learning iteration 546/2000 [0m                      

                       Computation: 45341 steps/s (collection: 2.072s, learning 0.097s)
             Mean action noise std: 2.57
          Mean value_function loss: 37.5910
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.0416
                       Mean reward: 26.39
               Mean episode length: 172.42
    Episode_Reward/reaching_object: 0.6479
     Episode_Reward/lifting_object: 4.7632
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0767
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 16.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 2.17s
                      Time elapsed: 00:19:40
                               ETA: 00:52:16

################################################################################
                     [1m Learning iteration 547/2000 [0m                      

                       Computation: 44699 steps/s (collection: 2.096s, learning 0.103s)
             Mean action noise std: 2.57
          Mean value_function loss: 27.0042
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 73.0580
                       Mean reward: 29.92
               Mean episode length: 167.28
    Episode_Reward/reaching_object: 0.6700
     Episode_Reward/lifting_object: 4.9794
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0762
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 17.2083
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 2.20s
                      Time elapsed: 00:19:42
                               ETA: 00:52:14

################################################################################
                     [1m Learning iteration 548/2000 [0m                      

                       Computation: 41326 steps/s (collection: 2.171s, learning 0.208s)
             Mean action noise std: 2.57
          Mean value_function loss: 37.0170
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 73.0793
                       Mean reward: 28.99
               Mean episode length: 169.06
    Episode_Reward/reaching_object: 0.6439
     Episode_Reward/lifting_object: 4.9978
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0750
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 18.6667
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 2.38s
                      Time elapsed: 00:19:44
                               ETA: 00:52:13

################################################################################
                     [1m Learning iteration 549/2000 [0m                      

                       Computation: 44712 steps/s (collection: 2.085s, learning 0.114s)
             Mean action noise std: 2.57
          Mean value_function loss: 30.6351
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 73.0936
                       Mean reward: 31.81
               Mean episode length: 180.06
    Episode_Reward/reaching_object: 0.6763
     Episode_Reward/lifting_object: 4.9615
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0783
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 16.2917
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 2.20s
                      Time elapsed: 00:19:46
                               ETA: 00:52:11

################################################################################
                     [1m Learning iteration 550/2000 [0m                      

                       Computation: 44630 steps/s (collection: 2.093s, learning 0.110s)
             Mean action noise std: 2.57
          Mean value_function loss: 33.1858
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 73.1072
                       Mean reward: 25.56
               Mean episode length: 181.79
    Episode_Reward/reaching_object: 0.6740
     Episode_Reward/lifting_object: 4.7797
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0770
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.3333
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 2.20s
                      Time elapsed: 00:19:49
                               ETA: 00:52:09

################################################################################
                     [1m Learning iteration 551/2000 [0m                      

                       Computation: 44664 steps/s (collection: 2.099s, learning 0.102s)
             Mean action noise std: 2.57
          Mean value_function loss: 31.8569
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 73.1291
                       Mean reward: 30.50
               Mean episode length: 177.97
    Episode_Reward/reaching_object: 0.6970
     Episode_Reward/lifting_object: 5.5532
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0805
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.1667
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 2.20s
                      Time elapsed: 00:19:51
                               ETA: 00:52:07

################################################################################
                     [1m Learning iteration 552/2000 [0m                      

                       Computation: 45417 steps/s (collection: 2.050s, learning 0.114s)
             Mean action noise std: 2.57
          Mean value_function loss: 33.5032
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 73.1445
                       Mean reward: 28.98
               Mean episode length: 190.68
    Episode_Reward/reaching_object: 0.6748
     Episode_Reward/lifting_object: 4.9337
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0793
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 15.0833
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 2.16s
                      Time elapsed: 00:19:53
                               ETA: 00:52:04

################################################################################
                     [1m Learning iteration 553/2000 [0m                      

                       Computation: 45040 steps/s (collection: 2.045s, learning 0.137s)
             Mean action noise std: 2.58
          Mean value_function loss: 29.5556
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 73.1611
                       Mean reward: 25.80
               Mean episode length: 185.35
    Episode_Reward/reaching_object: 0.6883
     Episode_Reward/lifting_object: 4.5151
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0790
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 15.3750
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 2.18s
                      Time elapsed: 00:19:55
                               ETA: 00:52:02

################################################################################
                     [1m Learning iteration 554/2000 [0m                      

                       Computation: 43933 steps/s (collection: 2.086s, learning 0.151s)
             Mean action noise std: 2.58
          Mean value_function loss: 45.9877
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 73.1734
                       Mean reward: 19.83
               Mean episode length: 167.52
    Episode_Reward/reaching_object: 0.6800
     Episode_Reward/lifting_object: 4.9807
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0792
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 2.24s
                      Time elapsed: 00:19:57
                               ETA: 00:52:00

################################################################################
                     [1m Learning iteration 555/2000 [0m                      

                       Computation: 46069 steps/s (collection: 2.033s, learning 0.101s)
             Mean action noise std: 2.58
          Mean value_function loss: 42.4872
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 73.1940
                       Mean reward: 27.78
               Mean episode length: 180.66
    Episode_Reward/reaching_object: 0.7069
     Episode_Reward/lifting_object: 5.3875
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0799
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 2.13s
                      Time elapsed: 00:19:59
                               ETA: 00:51:58

################################################################################
                     [1m Learning iteration 556/2000 [0m                      

                       Computation: 45964 steps/s (collection: 2.050s, learning 0.089s)
             Mean action noise std: 2.58
          Mean value_function loss: 34.9467
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 73.2207
                       Mean reward: 35.74
               Mean episode length: 192.64
    Episode_Reward/reaching_object: 0.6998
     Episode_Reward/lifting_object: 5.3284
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0800
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 15.5000
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 2.14s
                      Time elapsed: 00:20:02
                               ETA: 00:51:56

################################################################################
                     [1m Learning iteration 557/2000 [0m                      

                       Computation: 46119 steps/s (collection: 2.039s, learning 0.093s)
             Mean action noise std: 2.58
          Mean value_function loss: 41.3974
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.2411
                       Mean reward: 27.16
               Mean episode length: 185.67
    Episode_Reward/reaching_object: 0.6860
     Episode_Reward/lifting_object: 5.0337
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0814
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.6667
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 2.13s
                      Time elapsed: 00:20:04
                               ETA: 00:51:54

################################################################################
                     [1m Learning iteration 558/2000 [0m                      

                       Computation: 45335 steps/s (collection: 2.062s, learning 0.107s)
             Mean action noise std: 2.58
          Mean value_function loss: 27.3881
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 73.2641
                       Mean reward: 26.14
               Mean episode length: 186.65
    Episode_Reward/reaching_object: 0.6873
     Episode_Reward/lifting_object: 5.1972
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0808
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.2917
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 2.17s
                      Time elapsed: 00:20:06
                               ETA: 00:51:52

################################################################################
                     [1m Learning iteration 559/2000 [0m                      

                       Computation: 44485 steps/s (collection: 2.071s, learning 0.139s)
             Mean action noise std: 2.59
          Mean value_function loss: 38.0313
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 73.2760
                       Mean reward: 29.38
               Mean episode length: 184.82
    Episode_Reward/reaching_object: 0.7013
     Episode_Reward/lifting_object: 5.0516
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0800
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 14.1250
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 2.21s
                      Time elapsed: 00:20:08
                               ETA: 00:51:50

################################################################################
                     [1m Learning iteration 560/2000 [0m                      

                       Computation: 45514 steps/s (collection: 2.045s, learning 0.115s)
             Mean action noise std: 2.59
          Mean value_function loss: 27.5760
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 73.2838
                       Mean reward: 33.68
               Mean episode length: 178.92
    Episode_Reward/reaching_object: 0.6901
     Episode_Reward/lifting_object: 5.2675
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0815
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.7917
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 2.16s
                      Time elapsed: 00:20:10
                               ETA: 00:51:47

################################################################################
                     [1m Learning iteration 561/2000 [0m                      

                       Computation: 45639 steps/s (collection: 2.031s, learning 0.123s)
             Mean action noise std: 2.59
          Mean value_function loss: 20.6658
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 73.2936
                       Mean reward: 29.33
               Mean episode length: 183.92
    Episode_Reward/reaching_object: 0.6820
     Episode_Reward/lifting_object: 5.4893
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 6.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 2.15s
                      Time elapsed: 00:20:12
                               ETA: 00:51:45

################################################################################
                     [1m Learning iteration 562/2000 [0m                      

                       Computation: 46177 steps/s (collection: 2.032s, learning 0.097s)
             Mean action noise std: 2.59
          Mean value_function loss: 22.2170
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.3045
                       Mean reward: 24.00
               Mean episode length: 180.95
    Episode_Reward/reaching_object: 0.6886
     Episode_Reward/lifting_object: 5.2786
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0816
      Episode_Termination/time_out: 6.1250
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 14.2500
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 2.13s
                      Time elapsed: 00:20:15
                               ETA: 00:51:43

################################################################################
                     [1m Learning iteration 563/2000 [0m                      

                       Computation: 45048 steps/s (collection: 2.087s, learning 0.096s)
             Mean action noise std: 2.59
          Mean value_function loss: 20.4276
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 73.3215
                       Mean reward: 31.84
               Mean episode length: 180.30
    Episode_Reward/reaching_object: 0.7031
     Episode_Reward/lifting_object: 5.2529
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0832
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 2.18s
                      Time elapsed: 00:20:17
                               ETA: 00:51:41

################################################################################
                     [1m Learning iteration 564/2000 [0m                      

                       Computation: 45601 steps/s (collection: 2.058s, learning 0.098s)
             Mean action noise std: 2.59
          Mean value_function loss: 22.4476
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 73.3401
                       Mean reward: 32.22
               Mean episode length: 180.84
    Episode_Reward/reaching_object: 0.7023
     Episode_Reward/lifting_object: 5.4750
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0837
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 2.16s
                      Time elapsed: 00:20:19
                               ETA: 00:51:39

################################################################################
                     [1m Learning iteration 565/2000 [0m                      

                       Computation: 46129 steps/s (collection: 2.038s, learning 0.093s)
             Mean action noise std: 2.59
          Mean value_function loss: 35.2053
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.3511
                       Mean reward: 35.36
               Mean episode length: 188.18
    Episode_Reward/reaching_object: 0.6919
     Episode_Reward/lifting_object: 5.8213
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0821
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 2.13s
                      Time elapsed: 00:20:21
                               ETA: 00:51:36

################################################################################
                     [1m Learning iteration 566/2000 [0m                      

                       Computation: 45807 steps/s (collection: 2.019s, learning 0.128s)
             Mean action noise std: 2.59
          Mean value_function loss: 36.3337
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 73.3666
                       Mean reward: 19.84
               Mean episode length: 171.26
    Episode_Reward/reaching_object: 0.6753
     Episode_Reward/lifting_object: 4.7506
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0809
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 15.2917
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 2.15s
                      Time elapsed: 00:20:23
                               ETA: 00:51:34

################################################################################
                     [1m Learning iteration 567/2000 [0m                      

                       Computation: 44870 steps/s (collection: 2.084s, learning 0.107s)
             Mean action noise std: 2.60
          Mean value_function loss: 30.2279
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.3907
                       Mean reward: 25.40
               Mean episode length: 183.63
    Episode_Reward/reaching_object: 0.7058
     Episode_Reward/lifting_object: 5.5372
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0824
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 15.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 2.19s
                      Time elapsed: 00:20:25
                               ETA: 00:51:32

################################################################################
                     [1m Learning iteration 568/2000 [0m                      

                       Computation: 44353 steps/s (collection: 2.098s, learning 0.119s)
             Mean action noise std: 2.60
          Mean value_function loss: 25.2314
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 73.4032
                       Mean reward: 26.97
               Mean episode length: 181.77
    Episode_Reward/reaching_object: 0.6765
     Episode_Reward/lifting_object: 5.2872
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0819
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 2.22s
                      Time elapsed: 00:20:28
                               ETA: 00:51:30

################################################################################
                     [1m Learning iteration 569/2000 [0m                      

                       Computation: 44695 steps/s (collection: 2.077s, learning 0.123s)
             Mean action noise std: 2.60
          Mean value_function loss: 26.0316
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.4099
                       Mean reward: 30.64
               Mean episode length: 182.89
    Episode_Reward/reaching_object: 0.6876
     Episode_Reward/lifting_object: 5.3913
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 2.20s
                      Time elapsed: 00:20:30
                               ETA: 00:51:28

################################################################################
                     [1m Learning iteration 570/2000 [0m                      

                       Computation: 44083 steps/s (collection: 2.108s, learning 0.122s)
             Mean action noise std: 2.60
          Mean value_function loss: 32.2092
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 73.4181
                       Mean reward: 32.07
               Mean episode length: 176.17
    Episode_Reward/reaching_object: 0.6880
     Episode_Reward/lifting_object: 5.6426
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0802
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.3333
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 2.23s
                      Time elapsed: 00:20:32
                               ETA: 00:51:26

################################################################################
                     [1m Learning iteration 571/2000 [0m                      

                       Computation: 44673 steps/s (collection: 2.078s, learning 0.123s)
             Mean action noise std: 2.60
          Mean value_function loss: 33.0999
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 73.4286
                       Mean reward: 33.87
               Mean episode length: 181.25
    Episode_Reward/reaching_object: 0.6969
     Episode_Reward/lifting_object: 5.7511
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.4583
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 2.20s
                      Time elapsed: 00:20:34
                               ETA: 00:51:24

################################################################################
                     [1m Learning iteration 572/2000 [0m                      

                       Computation: 44605 steps/s (collection: 2.080s, learning 0.124s)
             Mean action noise std: 2.60
          Mean value_function loss: 33.6293
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 73.4368
                       Mean reward: 42.05
               Mean episode length: 180.92
    Episode_Reward/reaching_object: 0.7325
     Episode_Reward/lifting_object: 6.6298
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0806
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.7083
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 2.20s
                      Time elapsed: 00:20:36
                               ETA: 00:51:22

################################################################################
                     [1m Learning iteration 573/2000 [0m                      

                       Computation: 45452 steps/s (collection: 2.063s, learning 0.100s)
             Mean action noise std: 2.60
          Mean value_function loss: 35.6388
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 73.4500
                       Mean reward: 36.22
               Mean episode length: 176.49
    Episode_Reward/reaching_object: 0.6824
     Episode_Reward/lifting_object: 6.2279
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0788
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 17.2917
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 2.16s
                      Time elapsed: 00:20:39
                               ETA: 00:51:20

################################################################################
                     [1m Learning iteration 574/2000 [0m                      

                       Computation: 42886 steps/s (collection: 2.158s, learning 0.134s)
             Mean action noise std: 2.60
          Mean value_function loss: 31.0385
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 73.4641
                       Mean reward: 38.90
               Mean episode length: 179.95
    Episode_Reward/reaching_object: 0.6986
     Episode_Reward/lifting_object: 6.2409
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0803
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 17.3750
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 2.29s
                      Time elapsed: 00:20:41
                               ETA: 00:51:18

################################################################################
                     [1m Learning iteration 575/2000 [0m                      

                       Computation: 43569 steps/s (collection: 2.129s, learning 0.128s)
             Mean action noise std: 2.60
          Mean value_function loss: 30.4817
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.4737
                       Mean reward: 32.20
               Mean episode length: 169.25
    Episode_Reward/reaching_object: 0.6875
     Episode_Reward/lifting_object: 6.0553
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0784
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 17.2917
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 2.26s
                      Time elapsed: 00:20:43
                               ETA: 00:51:16

################################################################################
                     [1m Learning iteration 576/2000 [0m                      

                       Computation: 43227 steps/s (collection: 2.134s, learning 0.140s)
             Mean action noise std: 2.60
          Mean value_function loss: 50.9607
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 73.4841
                       Mean reward: 41.13
               Mean episode length: 176.78
    Episode_Reward/reaching_object: 0.7031
     Episode_Reward/lifting_object: 6.5023
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0780
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 20.2083
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 2.27s
                      Time elapsed: 00:20:45
                               ETA: 00:51:14

################################################################################
                     [1m Learning iteration 577/2000 [0m                      

                       Computation: 45126 steps/s (collection: 2.082s, learning 0.096s)
             Mean action noise std: 2.60
          Mean value_function loss: 35.9238
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 73.5017
                       Mean reward: 30.74
               Mean episode length: 162.17
    Episode_Reward/reaching_object: 0.6666
     Episode_Reward/lifting_object: 6.0049
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0771
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 20.3333
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 2.18s
                      Time elapsed: 00:20:48
                               ETA: 00:51:12

################################################################################
                     [1m Learning iteration 578/2000 [0m                      

                       Computation: 45141 steps/s (collection: 2.083s, learning 0.095s)
             Mean action noise std: 2.61
          Mean value_function loss: 42.7992
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 73.5146
                       Mean reward: 33.93
               Mean episode length: 159.22
    Episode_Reward/reaching_object: 0.6599
     Episode_Reward/lifting_object: 6.4303
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0766
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 20.7083
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 2.18s
                      Time elapsed: 00:20:50
                               ETA: 00:51:10

################################################################################
                     [1m Learning iteration 579/2000 [0m                      

                       Computation: 44680 steps/s (collection: 2.104s, learning 0.097s)
             Mean action noise std: 2.61
          Mean value_function loss: 41.2750
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 73.5237
                       Mean reward: 31.22
               Mean episode length: 157.33
    Episode_Reward/reaching_object: 0.6470
     Episode_Reward/lifting_object: 5.7501
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0759
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 20.2500
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 2.20s
                      Time elapsed: 00:20:52
                               ETA: 00:51:08

################################################################################
                     [1m Learning iteration 580/2000 [0m                      

                       Computation: 44828 steps/s (collection: 2.101s, learning 0.092s)
             Mean action noise std: 2.61
          Mean value_function loss: 42.5439
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 73.5329
                       Mean reward: 28.31
               Mean episode length: 152.79
    Episode_Reward/reaching_object: 0.6494
     Episode_Reward/lifting_object: 6.1107
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0756
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 22.6667
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 2.19s
                      Time elapsed: 00:20:54
                               ETA: 00:51:06

################################################################################
                     [1m Learning iteration 581/2000 [0m                      

                       Computation: 44112 steps/s (collection: 2.063s, learning 0.165s)
             Mean action noise std: 2.61
          Mean value_function loss: 43.5399
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 73.5459
                       Mean reward: 32.79
               Mean episode length: 158.21
    Episode_Reward/reaching_object: 0.6193
     Episode_Reward/lifting_object: 6.2295
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0727
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 21.7917
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 2.23s
                      Time elapsed: 00:20:56
                               ETA: 00:51:04

################################################################################
                     [1m Learning iteration 582/2000 [0m                      

                       Computation: 46004 steps/s (collection: 2.027s, learning 0.110s)
             Mean action noise std: 2.61
          Mean value_function loss: 48.0234
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 73.5595
                       Mean reward: 39.57
               Mean episode length: 153.48
    Episode_Reward/reaching_object: 0.6300
     Episode_Reward/lifting_object: 6.2974
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0741
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 23.7083
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 2.14s
                      Time elapsed: 00:20:59
                               ETA: 00:51:02

################################################################################
                     [1m Learning iteration 583/2000 [0m                      

                       Computation: 45322 steps/s (collection: 2.076s, learning 0.093s)
             Mean action noise std: 2.61
          Mean value_function loss: 42.7716
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 73.5721
                       Mean reward: 27.54
               Mean episode length: 155.73
    Episode_Reward/reaching_object: 0.5967
     Episode_Reward/lifting_object: 5.9754
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0712
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 23.1667
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 2.17s
                      Time elapsed: 00:21:01
                               ETA: 00:51:00

################################################################################
                     [1m Learning iteration 584/2000 [0m                      

                       Computation: 45338 steps/s (collection: 2.076s, learning 0.093s)
             Mean action noise std: 2.61
          Mean value_function loss: 47.0829
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 73.5760
                       Mean reward: 29.07
               Mean episode length: 143.91
    Episode_Reward/reaching_object: 0.6159
     Episode_Reward/lifting_object: 6.2735
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0721
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 25.8333
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 2.17s
                      Time elapsed: 00:21:03
                               ETA: 00:50:57

################################################################################
                     [1m Learning iteration 585/2000 [0m                      

                       Computation: 45594 steps/s (collection: 2.057s, learning 0.099s)
             Mean action noise std: 2.61
          Mean value_function loss: 33.1696
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 73.5831
                       Mean reward: 33.09
               Mean episode length: 148.95
    Episode_Reward/reaching_object: 0.5859
     Episode_Reward/lifting_object: 6.4166
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0705
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 25.1250
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 2.16s
                      Time elapsed: 00:21:05
                               ETA: 00:50:55

################################################################################
                     [1m Learning iteration 586/2000 [0m                      

                       Computation: 44681 steps/s (collection: 2.093s, learning 0.107s)
             Mean action noise std: 2.61
          Mean value_function loss: 45.2801
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 73.5894
                       Mean reward: 42.63
               Mean episode length: 156.45
    Episode_Reward/reaching_object: 0.6239
     Episode_Reward/lifting_object: 7.8534
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0721
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 24.1667
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 2.20s
                      Time elapsed: 00:21:07
                               ETA: 00:50:53

################################################################################
                     [1m Learning iteration 587/2000 [0m                      

                       Computation: 44553 steps/s (collection: 2.091s, learning 0.115s)
             Mean action noise std: 2.61
          Mean value_function loss: 45.6073
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 73.5939
                       Mean reward: 36.00
               Mean episode length: 151.51
    Episode_Reward/reaching_object: 0.5939
     Episode_Reward/lifting_object: 6.4093
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0704
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 25.0833
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 2.21s
                      Time elapsed: 00:21:09
                               ETA: 00:50:51

################################################################################
                     [1m Learning iteration 588/2000 [0m                      

                       Computation: 45112 steps/s (collection: 2.053s, learning 0.126s)
             Mean action noise std: 2.61
          Mean value_function loss: 41.8257
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 73.6022
                       Mean reward: 34.56
               Mean episode length: 142.54
    Episode_Reward/reaching_object: 0.5657
     Episode_Reward/lifting_object: 6.2290
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 23.7083
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 2.18s
                      Time elapsed: 00:21:12
                               ETA: 00:50:49

################################################################################
                     [1m Learning iteration 589/2000 [0m                      

                       Computation: 44936 steps/s (collection: 2.092s, learning 0.096s)
             Mean action noise std: 2.62
          Mean value_function loss: 54.9353
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 73.6183
                       Mean reward: 36.22
               Mean episode length: 148.39
    Episode_Reward/reaching_object: 0.5801
     Episode_Reward/lifting_object: 6.4012
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0717
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 24.9167
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 2.19s
                      Time elapsed: 00:21:14
                               ETA: 00:50:47

################################################################################
                     [1m Learning iteration 590/2000 [0m                      

                       Computation: 44594 steps/s (collection: 2.087s, learning 0.118s)
             Mean action noise std: 2.62
          Mean value_function loss: 57.2564
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 73.6351
                       Mean reward: 33.02
               Mean episode length: 145.82
    Episode_Reward/reaching_object: 0.5990
     Episode_Reward/lifting_object: 6.5421
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0715
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 25.2500
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 2.20s
                      Time elapsed: 00:21:16
                               ETA: 00:50:45

################################################################################
                     [1m Learning iteration 591/2000 [0m                      

                       Computation: 45190 steps/s (collection: 2.068s, learning 0.107s)
             Mean action noise std: 2.62
          Mean value_function loss: 61.0065
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 73.6403
                       Mean reward: 35.65
               Mean episode length: 162.10
    Episode_Reward/reaching_object: 0.5930
     Episode_Reward/lifting_object: 6.8045
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0739
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 24.7083
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 2.18s
                      Time elapsed: 00:21:18
                               ETA: 00:50:43

################################################################################
                     [1m Learning iteration 592/2000 [0m                      

                       Computation: 45553 steps/s (collection: 2.054s, learning 0.104s)
             Mean action noise std: 2.62
          Mean value_function loss: 39.3036
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 73.6421
                       Mean reward: 29.75
               Mean episode length: 134.40
    Episode_Reward/reaching_object: 0.5718
     Episode_Reward/lifting_object: 6.5391
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 25.9583
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 2.16s
                      Time elapsed: 00:21:20
                               ETA: 00:50:41

################################################################################
                     [1m Learning iteration 593/2000 [0m                      

                       Computation: 42419 steps/s (collection: 2.125s, learning 0.192s)
             Mean action noise std: 2.62
          Mean value_function loss: 50.5455
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 73.6428
                       Mean reward: 40.00
               Mean episode length: 158.03
    Episode_Reward/reaching_object: 0.5726
     Episode_Reward/lifting_object: 6.1121
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0709
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 26.7083
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 2.32s
                      Time elapsed: 00:21:23
                               ETA: 00:50:39

################################################################################
                     [1m Learning iteration 594/2000 [0m                      

                       Computation: 45664 steps/s (collection: 2.064s, learning 0.089s)
             Mean action noise std: 2.62
          Mean value_function loss: 38.6723
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 73.6430
                       Mean reward: 25.50
               Mean episode length: 146.17
    Episode_Reward/reaching_object: 0.5651
     Episode_Reward/lifting_object: 6.1054
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0690
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 26.3333
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 2.15s
                      Time elapsed: 00:21:25
                               ETA: 00:50:37

################################################################################
                     [1m Learning iteration 595/2000 [0m                      

                       Computation: 45391 steps/s (collection: 2.062s, learning 0.104s)
             Mean action noise std: 2.62
          Mean value_function loss: 43.8853
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 73.6431
                       Mean reward: 31.36
               Mean episode length: 133.81
    Episode_Reward/reaching_object: 0.5707
     Episode_Reward/lifting_object: 6.6005
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 26.3750
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 2.17s
                      Time elapsed: 00:21:27
                               ETA: 00:50:35

################################################################################
                     [1m Learning iteration 596/2000 [0m                      

                       Computation: 45629 steps/s (collection: 2.064s, learning 0.091s)
             Mean action noise std: 2.62
          Mean value_function loss: 59.0710
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 73.6433
                       Mean reward: 36.64
               Mean episode length: 144.57
    Episode_Reward/reaching_object: 0.5289
     Episode_Reward/lifting_object: 6.2711
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 27.8750
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 2.15s
                      Time elapsed: 00:21:29
                               ETA: 00:50:32

################################################################################
                     [1m Learning iteration 597/2000 [0m                      

                       Computation: 44920 steps/s (collection: 2.084s, learning 0.104s)
             Mean action noise std: 2.62
          Mean value_function loss: 41.0514
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 73.6436
                       Mean reward: 33.72
               Mean episode length: 158.07
    Episode_Reward/reaching_object: 0.5458
     Episode_Reward/lifting_object: 5.9277
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0702
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 25.6250
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 2.19s
                      Time elapsed: 00:21:31
                               ETA: 00:50:30

################################################################################
                     [1m Learning iteration 598/2000 [0m                      

                       Computation: 45193 steps/s (collection: 2.067s, learning 0.109s)
             Mean action noise std: 2.62
          Mean value_function loss: 52.7871
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 73.6454
                       Mean reward: 37.11
               Mean episode length: 143.04
    Episode_Reward/reaching_object: 0.5723
     Episode_Reward/lifting_object: 6.7486
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0710
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 24.6667
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 2.18s
                      Time elapsed: 00:21:33
                               ETA: 00:50:28

################################################################################
                     [1m Learning iteration 599/2000 [0m                      

                       Computation: 45537 steps/s (collection: 2.006s, learning 0.153s)
             Mean action noise std: 2.62
          Mean value_function loss: 43.4472
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 73.6544
                       Mean reward: 29.18
               Mean episode length: 142.20
    Episode_Reward/reaching_object: 0.5514
     Episode_Reward/lifting_object: 6.2004
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0686
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 26.4167
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 2.16s
                      Time elapsed: 00:21:36
                               ETA: 00:50:26

################################################################################
                     [1m Learning iteration 600/2000 [0m                      

                       Computation: 47026 steps/s (collection: 1.998s, learning 0.092s)
             Mean action noise std: 2.62
          Mean value_function loss: 39.4344
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 73.6667
                       Mean reward: 36.08
               Mean episode length: 137.41
    Episode_Reward/reaching_object: 0.5396
     Episode_Reward/lifting_object: 6.2846
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0695
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 27.6250
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 2.09s
                      Time elapsed: 00:21:38
                               ETA: 00:50:24

################################################################################
                     [1m Learning iteration 601/2000 [0m                      

                       Computation: 45684 steps/s (collection: 2.063s, learning 0.089s)
             Mean action noise std: 2.62
          Mean value_function loss: 38.4169
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 73.6701
                       Mean reward: 29.89
               Mean episode length: 153.42
    Episode_Reward/reaching_object: 0.5501
     Episode_Reward/lifting_object: 6.3947
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0691
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 25.4583
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 2.15s
                      Time elapsed: 00:21:40
                               ETA: 00:50:21

################################################################################
                     [1m Learning iteration 602/2000 [0m                      

                       Computation: 45819 steps/s (collection: 2.039s, learning 0.106s)
             Mean action noise std: 2.62
          Mean value_function loss: 76.7596
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 73.6723
                       Mean reward: 30.30
               Mean episode length: 149.27
    Episode_Reward/reaching_object: 0.5366
     Episode_Reward/lifting_object: 5.8754
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 27.1250
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 2.15s
                      Time elapsed: 00:21:42
                               ETA: 00:50:19

################################################################################
                     [1m Learning iteration 603/2000 [0m                      

                       Computation: 45158 steps/s (collection: 2.082s, learning 0.094s)
             Mean action noise std: 2.62
          Mean value_function loss: 56.6677
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.6814
                       Mean reward: 40.03
               Mean episode length: 143.96
    Episode_Reward/reaching_object: 0.5619
     Episode_Reward/lifting_object: 6.6122
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0693
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 26.1667
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 2.18s
                      Time elapsed: 00:21:44
                               ETA: 00:50:17

################################################################################
                     [1m Learning iteration 604/2000 [0m                      

                       Computation: 46306 steps/s (collection: 2.031s, learning 0.092s)
             Mean action noise std: 2.62
          Mean value_function loss: 53.7433
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 73.6964
                       Mean reward: 35.01
               Mean episode length: 149.64
    Episode_Reward/reaching_object: 0.5644
     Episode_Reward/lifting_object: 6.6046
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0699
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 26.5833
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 2.12s
                      Time elapsed: 00:21:46
                               ETA: 00:50:15

################################################################################
                     [1m Learning iteration 605/2000 [0m                      

                       Computation: 45033 steps/s (collection: 2.021s, learning 0.162s)
             Mean action noise std: 2.62
          Mean value_function loss: 63.0541
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 73.7093
                       Mean reward: 34.45
               Mean episode length: 141.70
    Episode_Reward/reaching_object: 0.5688
     Episode_Reward/lifting_object: 6.7332
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 25.2500
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 2.18s
                      Time elapsed: 00:21:49
                               ETA: 00:50:13

################################################################################
                     [1m Learning iteration 606/2000 [0m                      

                       Computation: 45589 steps/s (collection: 2.052s, learning 0.104s)
             Mean action noise std: 2.62
          Mean value_function loss: 48.5354
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 73.7168
                       Mean reward: 29.29
               Mean episode length: 145.31
    Episode_Reward/reaching_object: 0.5625
     Episode_Reward/lifting_object: 6.5168
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0709
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 25.7500
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 2.16s
                      Time elapsed: 00:21:51
                               ETA: 00:50:11

################################################################################
                     [1m Learning iteration 607/2000 [0m                      

                       Computation: 46664 steps/s (collection: 2.018s, learning 0.089s)
             Mean action noise std: 2.62
          Mean value_function loss: 59.0420
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 73.7180
                       Mean reward: 38.38
               Mean episode length: 142.77
    Episode_Reward/reaching_object: 0.5750
     Episode_Reward/lifting_object: 6.7960
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0709
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 26.1250
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 2.11s
                      Time elapsed: 00:21:53
                               ETA: 00:50:08

################################################################################
                     [1m Learning iteration 608/2000 [0m                      

                       Computation: 46878 steps/s (collection: 2.008s, learning 0.089s)
             Mean action noise std: 2.62
          Mean value_function loss: 45.7707
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 73.7212
                       Mean reward: 33.80
               Mean episode length: 137.95
    Episode_Reward/reaching_object: 0.5950
     Episode_Reward/lifting_object: 7.0167
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0717
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 27.5000
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 2.10s
                      Time elapsed: 00:21:55
                               ETA: 00:50:06

################################################################################
                     [1m Learning iteration 609/2000 [0m                      

                       Computation: 47200 steps/s (collection: 1.996s, learning 0.087s)
             Mean action noise std: 2.62
          Mean value_function loss: 56.2383
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.7309
                       Mean reward: 40.83
               Mean episode length: 145.16
    Episode_Reward/reaching_object: 0.6048
     Episode_Reward/lifting_object: 7.9163
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0686
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 25.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 2.08s
                      Time elapsed: 00:21:57
                               ETA: 00:50:04

################################################################################
                     [1m Learning iteration 610/2000 [0m                      

                       Computation: 46559 steps/s (collection: 2.023s, learning 0.089s)
             Mean action noise std: 2.63
          Mean value_function loss: 49.8828
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 73.7470
                       Mean reward: 45.46
               Mean episode length: 146.90
    Episode_Reward/reaching_object: 0.5831
     Episode_Reward/lifting_object: 6.7353
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0704
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 24.0833
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 2.11s
                      Time elapsed: 00:21:59
                               ETA: 00:50:01

################################################################################
                     [1m Learning iteration 611/2000 [0m                      

                       Computation: 46401 steps/s (collection: 2.025s, learning 0.094s)
             Mean action noise std: 2.63
          Mean value_function loss: 41.7554
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 73.7600
                       Mean reward: 37.20
               Mean episode length: 148.27
    Episode_Reward/reaching_object: 0.5795
     Episode_Reward/lifting_object: 7.0294
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0691
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 25.4583
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 2.12s
                      Time elapsed: 00:22:01
                               ETA: 00:49:59

################################################################################
                     [1m Learning iteration 612/2000 [0m                      

                       Computation: 46193 steps/s (collection: 2.011s, learning 0.117s)
             Mean action noise std: 2.63
          Mean value_function loss: 53.1578
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 73.7763
                       Mean reward: 37.00
               Mean episode length: 139.13
    Episode_Reward/reaching_object: 0.5960
     Episode_Reward/lifting_object: 7.5505
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0700
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 26.3333
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 2.13s
                      Time elapsed: 00:22:03
                               ETA: 00:49:57

################################################################################
                     [1m Learning iteration 613/2000 [0m                      

                       Computation: 46365 steps/s (collection: 2.026s, learning 0.095s)
             Mean action noise std: 2.63
          Mean value_function loss: 52.2469
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 73.7858
                       Mean reward: 33.54
               Mean episode length: 154.99
    Episode_Reward/reaching_object: 0.5795
     Episode_Reward/lifting_object: 6.7609
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0707
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 26.4167
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 2.12s
                      Time elapsed: 00:22:05
                               ETA: 00:49:55

################################################################################
                     [1m Learning iteration 614/2000 [0m                      

                       Computation: 46352 steps/s (collection: 2.016s, learning 0.105s)
             Mean action noise std: 2.63
          Mean value_function loss: 46.6437
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.7913
                       Mean reward: 34.23
               Mean episode length: 146.25
    Episode_Reward/reaching_object: 0.5972
     Episode_Reward/lifting_object: 6.7725
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0714
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 24.6667
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 2.12s
                      Time elapsed: 00:22:08
                               ETA: 00:49:52

################################################################################
                     [1m Learning iteration 615/2000 [0m                      

                       Computation: 45666 steps/s (collection: 2.029s, learning 0.124s)
             Mean action noise std: 2.63
          Mean value_function loss: 61.3254
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 73.8002
                       Mean reward: 38.24
               Mean episode length: 148.27
    Episode_Reward/reaching_object: 0.5821
     Episode_Reward/lifting_object: 7.0767
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 27.9167
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 2.15s
                      Time elapsed: 00:22:10
                               ETA: 00:49:50

################################################################################
                     [1m Learning iteration 616/2000 [0m                      

                       Computation: 46075 steps/s (collection: 2.045s, learning 0.089s)
             Mean action noise std: 2.63
          Mean value_function loss: 55.0018
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 73.8078
                       Mean reward: 38.71
               Mean episode length: 150.47
    Episode_Reward/reaching_object: 0.6044
     Episode_Reward/lifting_object: 7.9170
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 28.2500
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 2.13s
                      Time elapsed: 00:22:12
                               ETA: 00:49:48

################################################################################
                     [1m Learning iteration 617/2000 [0m                      

                       Computation: 45411 steps/s (collection: 2.037s, learning 0.128s)
             Mean action noise std: 2.63
          Mean value_function loss: 61.5491
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 73.8137
                       Mean reward: 41.19
               Mean episode length: 140.13
    Episode_Reward/reaching_object: 0.6156
     Episode_Reward/lifting_object: 7.8754
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0703
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 27.4167
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 2.16s
                      Time elapsed: 00:22:14
                               ETA: 00:49:46

################################################################################
                     [1m Learning iteration 618/2000 [0m                      

                       Computation: 45416 steps/s (collection: 2.032s, learning 0.133s)
             Mean action noise std: 2.63
          Mean value_function loss: 61.5794
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 73.8225
                       Mean reward: 39.33
               Mean episode length: 142.41
    Episode_Reward/reaching_object: 0.6073
     Episode_Reward/lifting_object: 7.8716
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 25.5833
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 2.16s
                      Time elapsed: 00:22:16
                               ETA: 00:49:44

################################################################################
                     [1m Learning iteration 619/2000 [0m                      

                       Computation: 45825 steps/s (collection: 2.057s, learning 0.088s)
             Mean action noise std: 2.63
          Mean value_function loss: 63.8694
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 73.8340
                       Mean reward: 44.65
               Mean episode length: 149.82
    Episode_Reward/reaching_object: 0.6159
     Episode_Reward/lifting_object: 8.5139
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0691
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 27.2500
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 2.15s
                      Time elapsed: 00:22:18
                               ETA: 00:49:42

################################################################################
                     [1m Learning iteration 620/2000 [0m                      

                       Computation: 45675 steps/s (collection: 2.049s, learning 0.103s)
             Mean action noise std: 2.63
          Mean value_function loss: 67.7697
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 73.8429
                       Mean reward: 46.03
               Mean episode length: 137.72
    Episode_Reward/reaching_object: 0.6085
     Episode_Reward/lifting_object: 8.5004
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 27.9583
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 2.15s
                      Time elapsed: 00:22:20
                               ETA: 00:49:39

################################################################################
                     [1m Learning iteration 621/2000 [0m                      

                       Computation: 44925 steps/s (collection: 2.045s, learning 0.143s)
             Mean action noise std: 2.64
          Mean value_function loss: 57.3213
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 73.8517
                       Mean reward: 38.25
               Mean episode length: 134.53
    Episode_Reward/reaching_object: 0.6000
     Episode_Reward/lifting_object: 8.1744
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 28.0833
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 2.19s
                      Time elapsed: 00:22:23
                               ETA: 00:49:37

################################################################################
                     [1m Learning iteration 622/2000 [0m                      

                       Computation: 45557 steps/s (collection: 2.019s, learning 0.139s)
             Mean action noise std: 2.64
          Mean value_function loss: 60.9799
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 73.8578
                       Mean reward: 40.21
               Mean episode length: 140.76
    Episode_Reward/reaching_object: 0.6035
     Episode_Reward/lifting_object: 8.1078
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 28.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 2.16s
                      Time elapsed: 00:22:25
                               ETA: 00:49:35

################################################################################
                     [1m Learning iteration 623/2000 [0m                      

                       Computation: 44461 steps/s (collection: 2.084s, learning 0.127s)
             Mean action noise std: 2.64
          Mean value_function loss: 67.6459
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 73.8619
                       Mean reward: 39.77
               Mean episode length: 134.91
    Episode_Reward/reaching_object: 0.5991
     Episode_Reward/lifting_object: 8.2744
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 31.4583
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 2.21s
                      Time elapsed: 00:22:27
                               ETA: 00:49:33

################################################################################
                     [1m Learning iteration 624/2000 [0m                      

                       Computation: 46561 steps/s (collection: 2.019s, learning 0.092s)
             Mean action noise std: 2.64
          Mean value_function loss: 69.5460
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 73.8635
                       Mean reward: 21.96
               Mean episode length: 143.33
    Episode_Reward/reaching_object: 0.5963
     Episode_Reward/lifting_object: 7.8785
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 28.7083
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 2.11s
                      Time elapsed: 00:22:29
                               ETA: 00:49:31

################################################################################
                     [1m Learning iteration 625/2000 [0m                      

                       Computation: 45684 steps/s (collection: 2.041s, learning 0.111s)
             Mean action noise std: 2.64
          Mean value_function loss: 59.1930
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.8665
                       Mean reward: 52.95
               Mean episode length: 144.71
    Episode_Reward/reaching_object: 0.5926
     Episode_Reward/lifting_object: 8.6320
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 30.8333
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 2.15s
                      Time elapsed: 00:22:31
                               ETA: 00:49:29

################################################################################
                     [1m Learning iteration 626/2000 [0m                      

                       Computation: 45816 steps/s (collection: 2.032s, learning 0.113s)
             Mean action noise std: 2.64
          Mean value_function loss: 60.8478
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 73.8711
                       Mean reward: 45.32
               Mean episode length: 135.01
    Episode_Reward/reaching_object: 0.5997
     Episode_Reward/lifting_object: 8.6437
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 28.9583
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 2.15s
                      Time elapsed: 00:22:33
                               ETA: 00:49:26

################################################################################
                     [1m Learning iteration 627/2000 [0m                      

                       Computation: 45387 steps/s (collection: 2.072s, learning 0.094s)
             Mean action noise std: 2.64
          Mean value_function loss: 76.5983
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 73.8783
                       Mean reward: 42.30
               Mean episode length: 128.88
    Episode_Reward/reaching_object: 0.5769
     Episode_Reward/lifting_object: 8.2099
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 28.6250
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 2.17s
                      Time elapsed: 00:22:36
                               ETA: 00:49:24

################################################################################
                     [1m Learning iteration 628/2000 [0m                      

                       Computation: 45121 steps/s (collection: 2.028s, learning 0.151s)
             Mean action noise std: 2.64
          Mean value_function loss: 64.6297
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 73.8845
                       Mean reward: 36.89
               Mean episode length: 137.79
    Episode_Reward/reaching_object: 0.5789
     Episode_Reward/lifting_object: 8.7491
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 29.5833
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 2.18s
                      Time elapsed: 00:22:38
                               ETA: 00:49:22

################################################################################
                     [1m Learning iteration 629/2000 [0m                      

                       Computation: 46129 steps/s (collection: 2.035s, learning 0.096s)
             Mean action noise std: 2.64
          Mean value_function loss: 77.7179
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 73.8877
                       Mean reward: 41.34
               Mean episode length: 131.37
    Episode_Reward/reaching_object: 0.6057
     Episode_Reward/lifting_object: 8.9762
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 29.1667
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 2.13s
                      Time elapsed: 00:22:40
                               ETA: 00:49:20

################################################################################
                     [1m Learning iteration 630/2000 [0m                      

                       Computation: 45585 steps/s (collection: 2.062s, learning 0.094s)
             Mean action noise std: 2.64
          Mean value_function loss: 73.2065
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 73.8965
                       Mean reward: 46.90
               Mean episode length: 129.15
    Episode_Reward/reaching_object: 0.5942
     Episode_Reward/lifting_object: 8.5819
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 29.7500
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 2.16s
                      Time elapsed: 00:22:42
                               ETA: 00:49:18

################################################################################
                     [1m Learning iteration 631/2000 [0m                      

                       Computation: 45524 steps/s (collection: 2.059s, learning 0.101s)
             Mean action noise std: 2.64
          Mean value_function loss: 71.1526
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 73.9070
                       Mean reward: 52.35
               Mean episode length: 129.15
    Episode_Reward/reaching_object: 0.6100
     Episode_Reward/lifting_object: 9.3107
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 31.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 2.16s
                      Time elapsed: 00:22:44
                               ETA: 00:49:16

################################################################################
                     [1m Learning iteration 632/2000 [0m                      

                       Computation: 45184 steps/s (collection: 2.055s, learning 0.120s)
             Mean action noise std: 2.64
          Mean value_function loss: 88.0036
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 73.9169
                       Mean reward: 49.60
               Mean episode length: 136.15
    Episode_Reward/reaching_object: 0.6203
     Episode_Reward/lifting_object: 9.2968
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 30.2083
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 2.18s
                      Time elapsed: 00:22:46
                               ETA: 00:49:14

################################################################################
                     [1m Learning iteration 633/2000 [0m                      

                       Computation: 45446 steps/s (collection: 2.068s, learning 0.095s)
             Mean action noise std: 2.64
          Mean value_function loss: 94.4929
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.9252
                       Mean reward: 53.82
               Mean episode length: 135.69
    Episode_Reward/reaching_object: 0.5971
     Episode_Reward/lifting_object: 9.1162
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 28.9583
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 2.16s
                      Time elapsed: 00:22:49
                               ETA: 00:49:11

################################################################################
                     [1m Learning iteration 634/2000 [0m                      

                       Computation: 45917 steps/s (collection: 2.042s, learning 0.099s)
             Mean action noise std: 2.64
          Mean value_function loss: 90.4629
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 73.9380
                       Mean reward: 43.94
               Mean episode length: 137.95
    Episode_Reward/reaching_object: 0.5956
     Episode_Reward/lifting_object: 9.2250
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 31.4583
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 2.14s
                      Time elapsed: 00:22:51
                               ETA: 00:49:09

################################################################################
                     [1m Learning iteration 635/2000 [0m                      

                       Computation: 44779 steps/s (collection: 2.055s, learning 0.140s)
             Mean action noise std: 2.64
          Mean value_function loss: 89.8538
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 73.9447
                       Mean reward: 51.15
               Mean episode length: 126.18
    Episode_Reward/reaching_object: 0.5994
     Episode_Reward/lifting_object: 9.6958
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 31.5000
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 2.20s
                      Time elapsed: 00:22:53
                               ETA: 00:49:07

################################################################################
                     [1m Learning iteration 636/2000 [0m                      

                       Computation: 45162 steps/s (collection: 2.071s, learning 0.105s)
             Mean action noise std: 2.64
          Mean value_function loss: 163.4225
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 73.9490
                       Mean reward: 39.38
               Mean episode length: 123.56
    Episode_Reward/reaching_object: 0.5745
     Episode_Reward/lifting_object: 8.9339
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 29.4167
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 2.18s
                      Time elapsed: 00:22:55
                               ETA: 00:49:05

################################################################################
                     [1m Learning iteration 637/2000 [0m                      

                       Computation: 44375 steps/s (collection: 2.116s, learning 0.099s)
             Mean action noise std: 2.64
          Mean value_function loss: 96.2502
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 73.9559
                       Mean reward: 40.89
               Mean episode length: 130.27
    Episode_Reward/reaching_object: 0.5824
     Episode_Reward/lifting_object: 9.0530
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 31.5833
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 2.22s
                      Time elapsed: 00:22:57
                               ETA: 00:49:03

################################################################################
                     [1m Learning iteration 638/2000 [0m                      

                       Computation: 45614 steps/s (collection: 2.061s, learning 0.094s)
             Mean action noise std: 2.65
          Mean value_function loss: 91.3856
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 73.9633
                       Mean reward: 50.90
               Mean episode length: 123.76
    Episode_Reward/reaching_object: 0.5870
     Episode_Reward/lifting_object: 9.2872
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 33.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 2.16s
                      Time elapsed: 00:22:59
                               ETA: 00:49:01

################################################################################
                     [1m Learning iteration 639/2000 [0m                      

                       Computation: 44723 steps/s (collection: 2.088s, learning 0.110s)
             Mean action noise std: 2.65
          Mean value_function loss: 88.1354
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 73.9696
                       Mean reward: 57.48
               Mean episode length: 128.28
    Episode_Reward/reaching_object: 0.5822
     Episode_Reward/lifting_object: 10.2362
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 31.7083
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 2.20s
                      Time elapsed: 00:23:02
                               ETA: 00:48:59

################################################################################
                     [1m Learning iteration 640/2000 [0m                      

                       Computation: 45557 steps/s (collection: 2.052s, learning 0.106s)
             Mean action noise std: 2.65
          Mean value_function loss: 93.8483
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 73.9740
                       Mean reward: 51.17
               Mean episode length: 116.11
    Episode_Reward/reaching_object: 0.5777
     Episode_Reward/lifting_object: 10.4539
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 32.6667
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 2.16s
                      Time elapsed: 00:23:04
                               ETA: 00:48:57

################################################################################
                     [1m Learning iteration 641/2000 [0m                      

                       Computation: 44099 steps/s (collection: 2.080s, learning 0.149s)
             Mean action noise std: 2.65
          Mean value_function loss: 108.2927
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 73.9813
                       Mean reward: 56.35
               Mean episode length: 135.69
    Episode_Reward/reaching_object: 0.5648
     Episode_Reward/lifting_object: 9.9644
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 32.2917
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 2.23s
                      Time elapsed: 00:23:06
                               ETA: 00:48:55

################################################################################
                     [1m Learning iteration 642/2000 [0m                      

                       Computation: 44134 steps/s (collection: 2.110s, learning 0.118s)
             Mean action noise std: 2.65
          Mean value_function loss: 112.0966
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 73.9894
                       Mean reward: 61.72
               Mean episode length: 123.06
    Episode_Reward/reaching_object: 0.5887
     Episode_Reward/lifting_object: 10.6046
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 30.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 2.23s
                      Time elapsed: 00:23:08
                               ETA: 00:48:53

################################################################################
                     [1m Learning iteration 643/2000 [0m                      

                       Computation: 45004 steps/s (collection: 2.069s, learning 0.116s)
             Mean action noise std: 2.65
          Mean value_function loss: 84.8079
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 73.9933
                       Mean reward: 51.41
               Mean episode length: 126.25
    Episode_Reward/reaching_object: 0.5947
     Episode_Reward/lifting_object: 10.9360
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 31.4167
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 2.18s
                      Time elapsed: 00:23:10
                               ETA: 00:48:50

################################################################################
                     [1m Learning iteration 644/2000 [0m                      

                       Computation: 44504 steps/s (collection: 2.099s, learning 0.110s)
             Mean action noise std: 2.65
          Mean value_function loss: 76.1514
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 73.9966
                       Mean reward: 50.39
               Mean episode length: 125.76
    Episode_Reward/reaching_object: 0.5825
     Episode_Reward/lifting_object: 10.3742
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 30.2917
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 2.21s
                      Time elapsed: 00:23:13
                               ETA: 00:48:48

################################################################################
                     [1m Learning iteration 645/2000 [0m                      

                       Computation: 44505 steps/s (collection: 2.118s, learning 0.091s)
             Mean action noise std: 2.65
          Mean value_function loss: 95.0666
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 74.0030
                       Mean reward: 59.59
               Mean episode length: 125.91
    Episode_Reward/reaching_object: 0.5999
     Episode_Reward/lifting_object: 11.1230
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 30.8750
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 2.21s
                      Time elapsed: 00:23:15
                               ETA: 00:48:46

################################################################################
                     [1m Learning iteration 646/2000 [0m                      

                       Computation: 44432 steps/s (collection: 2.095s, learning 0.118s)
             Mean action noise std: 2.65
          Mean value_function loss: 81.5876
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 74.0106
                       Mean reward: 65.22
               Mean episode length: 137.90
    Episode_Reward/reaching_object: 0.6095
     Episode_Reward/lifting_object: 11.2199
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 31.2500
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 2.21s
                      Time elapsed: 00:23:17
                               ETA: 00:48:44

################################################################################
                     [1m Learning iteration 647/2000 [0m                      

                       Computation: 45311 steps/s (collection: 2.077s, learning 0.093s)
             Mean action noise std: 2.65
          Mean value_function loss: 93.6009
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 74.0198
                       Mean reward: 54.23
               Mean episode length: 127.08
    Episode_Reward/reaching_object: 0.5971
     Episode_Reward/lifting_object: 10.8264
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 33.6250
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 2.17s
                      Time elapsed: 00:23:19
                               ETA: 00:48:42

################################################################################
                     [1m Learning iteration 648/2000 [0m                      

                       Computation: 45210 steps/s (collection: 2.072s, learning 0.103s)
             Mean action noise std: 2.65
          Mean value_function loss: 98.5689
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 74.0283
                       Mean reward: 72.88
               Mean episode length: 124.52
    Episode_Reward/reaching_object: 0.5950
     Episode_Reward/lifting_object: 11.7609
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 33.5000
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 2.17s
                      Time elapsed: 00:23:21
                               ETA: 00:48:40

################################################################################
                     [1m Learning iteration 649/2000 [0m                      

                       Computation: 44988 steps/s (collection: 2.079s, learning 0.106s)
             Mean action noise std: 2.65
          Mean value_function loss: 87.0421
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.0340
                       Mean reward: 46.32
               Mean episode length: 127.39
    Episode_Reward/reaching_object: 0.5814
     Episode_Reward/lifting_object: 10.1911
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 32.9167
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 2.19s
                      Time elapsed: 00:23:24
                               ETA: 00:48:38

################################################################################
                     [1m Learning iteration 650/2000 [0m                      

                       Computation: 44119 steps/s (collection: 2.123s, learning 0.105s)
             Mean action noise std: 2.65
          Mean value_function loss: 101.3035
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 74.0385
                       Mean reward: 63.24
               Mean episode length: 123.35
    Episode_Reward/reaching_object: 0.6032
     Episode_Reward/lifting_object: 11.6515
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 34.5000
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 2.23s
                      Time elapsed: 00:23:26
                               ETA: 00:48:36

################################################################################
                     [1m Learning iteration 651/2000 [0m                      

                       Computation: 44883 steps/s (collection: 2.084s, learning 0.107s)
             Mean action noise std: 2.65
          Mean value_function loss: 121.3771
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 74.0432
                       Mean reward: 53.52
               Mean episode length: 120.65
    Episode_Reward/reaching_object: 0.5794
     Episode_Reward/lifting_object: 11.0048
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 33.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 2.19s
                      Time elapsed: 00:23:28
                               ETA: 00:48:34

################################################################################
                     [1m Learning iteration 652/2000 [0m                      

                       Computation: 43644 steps/s (collection: 2.149s, learning 0.103s)
             Mean action noise std: 2.65
          Mean value_function loss: 116.1523
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 74.0497
                       Mean reward: 58.88
               Mean episode length: 119.05
    Episode_Reward/reaching_object: 0.5814
     Episode_Reward/lifting_object: 10.8998
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 35.5417
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 2.25s
                      Time elapsed: 00:23:30
                               ETA: 00:48:32

################################################################################
                     [1m Learning iteration 653/2000 [0m                      

                       Computation: 44295 steps/s (collection: 2.110s, learning 0.110s)
             Mean action noise std: 2.65
          Mean value_function loss: 104.8255
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 74.0547
                       Mean reward: 54.90
               Mean episode length: 121.67
    Episode_Reward/reaching_object: 0.5860
     Episode_Reward/lifting_object: 11.9820
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 31.9583
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 2.22s
                      Time elapsed: 00:23:32
                               ETA: 00:48:30

################################################################################
                     [1m Learning iteration 654/2000 [0m                      

                       Computation: 45160 steps/s (collection: 2.087s, learning 0.090s)
             Mean action noise std: 2.65
          Mean value_function loss: 102.2015
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 74.0583
                       Mean reward: 56.05
               Mean episode length: 116.95
    Episode_Reward/reaching_object: 0.5891
     Episode_Reward/lifting_object: 12.2458
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 33.2917
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 2.18s
                      Time elapsed: 00:23:35
                               ETA: 00:48:28

################################################################################
                     [1m Learning iteration 655/2000 [0m                      

                       Computation: 45421 steps/s (collection: 2.067s, learning 0.097s)
             Mean action noise std: 2.65
          Mean value_function loss: 106.8838
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 74.0631
                       Mean reward: 63.17
               Mean episode length: 115.14
    Episode_Reward/reaching_object: 0.5852
     Episode_Reward/lifting_object: 11.9959
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 32.7917
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 2.16s
                      Time elapsed: 00:23:37
                               ETA: 00:48:25

################################################################################
                     [1m Learning iteration 656/2000 [0m                      

                       Computation: 44823 steps/s (collection: 2.081s, learning 0.112s)
             Mean action noise std: 2.65
          Mean value_function loss: 107.0553
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 74.0671
                       Mean reward: 68.78
               Mean episode length: 125.17
    Episode_Reward/reaching_object: 0.6014
     Episode_Reward/lifting_object: 12.4126
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 32.7500
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 2.19s
                      Time elapsed: 00:23:39
                               ETA: 00:48:23

################################################################################
                     [1m Learning iteration 657/2000 [0m                      

                       Computation: 45029 steps/s (collection: 2.070s, learning 0.113s)
             Mean action noise std: 2.66
          Mean value_function loss: 107.3862
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 74.0702
                       Mean reward: 60.34
               Mean episode length: 121.22
    Episode_Reward/reaching_object: 0.5664
     Episode_Reward/lifting_object: 11.5145
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 34.4167
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 2.18s
                      Time elapsed: 00:23:41
                               ETA: 00:48:21

################################################################################
                     [1m Learning iteration 658/2000 [0m                      

                       Computation: 45070 steps/s (collection: 2.072s, learning 0.109s)
             Mean action noise std: 2.66
          Mean value_function loss: 127.2515
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 74.0757
                       Mean reward: 52.21
               Mean episode length: 120.06
    Episode_Reward/reaching_object: 0.5885
     Episode_Reward/lifting_object: 12.0664
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 36.5417
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 2.18s
                      Time elapsed: 00:23:43
                               ETA: 00:48:19

################################################################################
                     [1m Learning iteration 659/2000 [0m                      

                       Computation: 45081 steps/s (collection: 2.085s, learning 0.095s)
             Mean action noise std: 2.66
          Mean value_function loss: 101.6701
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 74.0791
                       Mean reward: 71.69
               Mean episode length: 115.91
    Episode_Reward/reaching_object: 0.5959
     Episode_Reward/lifting_object: 12.2306
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 32.8750
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 2.18s
                      Time elapsed: 00:23:46
                               ETA: 00:48:17

################################################################################
                     [1m Learning iteration 660/2000 [0m                      

                       Computation: 45143 steps/s (collection: 2.066s, learning 0.112s)
             Mean action noise std: 2.66
          Mean value_function loss: 110.3858
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 74.0821
                       Mean reward: 62.96
               Mean episode length: 122.77
    Episode_Reward/reaching_object: 0.5761
     Episode_Reward/lifting_object: 12.2889
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 33.6667
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 2.18s
                      Time elapsed: 00:23:48
                               ETA: 00:48:15

################################################################################
                     [1m Learning iteration 661/2000 [0m                      

                       Computation: 45429 steps/s (collection: 2.074s, learning 0.090s)
             Mean action noise std: 2.66
          Mean value_function loss: 129.1020
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 74.0840
                       Mean reward: 78.80
               Mean episode length: 116.91
    Episode_Reward/reaching_object: 0.5811
     Episode_Reward/lifting_object: 12.1838
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 32.6667
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 2.16s
                      Time elapsed: 00:23:50
                               ETA: 00:48:13

################################################################################
                     [1m Learning iteration 662/2000 [0m                      

                       Computation: 44114 steps/s (collection: 2.094s, learning 0.134s)
             Mean action noise std: 2.66
          Mean value_function loss: 109.9279
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 74.0869
                       Mean reward: 62.32
               Mean episode length: 126.57
    Episode_Reward/reaching_object: 0.5745
     Episode_Reward/lifting_object: 12.5778
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 33.0417
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 2.23s
                      Time elapsed: 00:23:52
                               ETA: 00:48:11

################################################################################
                     [1m Learning iteration 663/2000 [0m                      

                       Computation: 45295 steps/s (collection: 2.070s, learning 0.100s)
             Mean action noise std: 2.66
          Mean value_function loss: 123.9174
               Mean surrogate loss: 0.0105
                 Mean entropy loss: 74.0891
                       Mean reward: 64.75
               Mean episode length: 122.13
    Episode_Reward/reaching_object: 0.5673
     Episode_Reward/lifting_object: 11.8035
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 32.7083
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 2.17s
                      Time elapsed: 00:23:54
                               ETA: 00:48:09

################################################################################
                     [1m Learning iteration 664/2000 [0m                      

                       Computation: 44520 steps/s (collection: 2.108s, learning 0.100s)
             Mean action noise std: 2.66
          Mean value_function loss: 128.7024
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 74.0902
                       Mean reward: 67.20
               Mean episode length: 120.15
    Episode_Reward/reaching_object: 0.5788
     Episode_Reward/lifting_object: 12.8911
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 34.8333
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 2.21s
                      Time elapsed: 00:23:57
                               ETA: 00:48:06

################################################################################
                     [1m Learning iteration 665/2000 [0m                      

                       Computation: 44955 steps/s (collection: 2.087s, learning 0.100s)
             Mean action noise std: 2.66
          Mean value_function loss: 140.0563
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 74.0937
                       Mean reward: 71.95
               Mean episode length: 123.65
    Episode_Reward/reaching_object: 0.5984
     Episode_Reward/lifting_object: 13.8804
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 34.7083
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 2.19s
                      Time elapsed: 00:23:59
                               ETA: 00:48:04

################################################################################
                     [1m Learning iteration 666/2000 [0m                      

                       Computation: 26728 steps/s (collection: 3.550s, learning 0.128s)
             Mean action noise std: 2.66
          Mean value_function loss: 186.1828
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 74.0984
                       Mean reward: 81.04
               Mean episode length: 115.56
    Episode_Reward/reaching_object: 0.5969
     Episode_Reward/lifting_object: 14.0363
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 35.0417
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 3.68s
                      Time elapsed: 00:24:02
                               ETA: 00:48:05

################################################################################
                     [1m Learning iteration 667/2000 [0m                      

                       Computation: 13714 steps/s (collection: 7.043s, learning 0.124s)
             Mean action noise std: 2.66
          Mean value_function loss: 188.6593
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 74.1034
                       Mean reward: 76.80
               Mean episode length: 117.75
    Episode_Reward/reaching_object: 0.5765
     Episode_Reward/lifting_object: 12.9509
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 36.5417
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 7.17s
                      Time elapsed: 00:24:10
                               ETA: 00:48:13

################################################################################
                     [1m Learning iteration 668/2000 [0m                      

                       Computation: 13939 steps/s (collection: 6.937s, learning 0.115s)
             Mean action noise std: 2.66
          Mean value_function loss: 150.9493
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.1070
                       Mean reward: 77.19
               Mean episode length: 114.48
    Episode_Reward/reaching_object: 0.5873
     Episode_Reward/lifting_object: 13.4843
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 36.0833
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 7.05s
                      Time elapsed: 00:24:17
                               ETA: 00:48:21

################################################################################
                     [1m Learning iteration 669/2000 [0m                      

                       Computation: 13886 steps/s (collection: 6.966s, learning 0.113s)
             Mean action noise std: 2.66
          Mean value_function loss: 140.5162
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 74.1100
                       Mean reward: 72.42
               Mean episode length: 111.63
    Episode_Reward/reaching_object: 0.6011
     Episode_Reward/lifting_object: 14.9505
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 32.9583
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 7.08s
                      Time elapsed: 00:24:24
                               ETA: 00:48:28

################################################################################
                     [1m Learning iteration 670/2000 [0m                      

                       Computation: 13780 steps/s (collection: 7.006s, learning 0.127s)
             Mean action noise std: 2.66
          Mean value_function loss: 152.4247
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 74.1111
                       Mean reward: 69.24
               Mean episode length: 112.99
    Episode_Reward/reaching_object: 0.5720
     Episode_Reward/lifting_object: 13.1305
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 36.4583
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 7.13s
                      Time elapsed: 00:24:31
                               ETA: 00:48:36

################################################################################
                     [1m Learning iteration 671/2000 [0m                      

                       Computation: 13821 steps/s (collection: 6.992s, learning 0.120s)
             Mean action noise std: 2.66
          Mean value_function loss: 152.5907
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 74.1135
                       Mean reward: 74.35
               Mean episode length: 109.21
    Episode_Reward/reaching_object: 0.5690
     Episode_Reward/lifting_object: 13.7516
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 36.5833
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 7.11s
                      Time elapsed: 00:24:38
                               ETA: 00:48:43

################################################################################
                     [1m Learning iteration 672/2000 [0m                      

                       Computation: 13728 steps/s (collection: 7.005s, learning 0.156s)
             Mean action noise std: 2.66
          Mean value_function loss: 143.0241
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 74.1183
                       Mean reward: 74.47
               Mean episode length: 115.13
    Episode_Reward/reaching_object: 0.5815
     Episode_Reward/lifting_object: 14.2839
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 35.2917
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 7.16s
                      Time elapsed: 00:24:45
                               ETA: 00:48:51

################################################################################
                     [1m Learning iteration 673/2000 [0m                      

                       Computation: 13735 steps/s (collection: 7.047s, learning 0.110s)
             Mean action noise std: 2.66
          Mean value_function loss: 170.3899
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 74.1240
                       Mean reward: 56.62
               Mean episode length: 110.76
    Episode_Reward/reaching_object: 0.5672
     Episode_Reward/lifting_object: 13.6861
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 33.5417
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 7.16s
                      Time elapsed: 00:24:52
                               ETA: 00:48:58

################################################################################
                     [1m Learning iteration 674/2000 [0m                      

                       Computation: 14055 steps/s (collection: 6.856s, learning 0.138s)
             Mean action noise std: 2.66
          Mean value_function loss: 156.2522
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 74.1297
                       Mean reward: 72.47
               Mean episode length: 110.07
    Episode_Reward/reaching_object: 0.5592
     Episode_Reward/lifting_object: 13.0261
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 32.9167
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 6.99s
                      Time elapsed: 00:24:59
                               ETA: 00:49:06

################################################################################
                     [1m Learning iteration 675/2000 [0m                      

                       Computation: 21566 steps/s (collection: 4.409s, learning 0.149s)
             Mean action noise std: 2.66
          Mean value_function loss: 146.4080
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 74.1341
                       Mean reward: 62.59
               Mean episode length: 122.66
    Episode_Reward/reaching_object: 0.5712
     Episode_Reward/lifting_object: 13.9745
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 36.0417
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 4.56s
                      Time elapsed: 00:25:04
                               ETA: 00:49:08

################################################################################
                     [1m Learning iteration 676/2000 [0m                      

                       Computation: 44537 steps/s (collection: 2.113s, learning 0.094s)
             Mean action noise std: 2.66
          Mean value_function loss: 148.8688
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 74.1416
                       Mean reward: 80.84
               Mean episode length: 112.12
    Episode_Reward/reaching_object: 0.5886
     Episode_Reward/lifting_object: 15.5442
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 36.5000
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 2.21s
                      Time elapsed: 00:25:06
                               ETA: 00:49:06

################################################################################
                     [1m Learning iteration 677/2000 [0m                      

                       Computation: 40802 steps/s (collection: 2.306s, learning 0.103s)
             Mean action noise std: 2.66
          Mean value_function loss: 143.7177
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 74.1472
                       Mean reward: 83.69
               Mean episode length: 111.97
    Episode_Reward/reaching_object: 0.5980
     Episode_Reward/lifting_object: 16.5756
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 36.7917
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 2.41s
                      Time elapsed: 00:25:08
                               ETA: 00:49:04

################################################################################
                     [1m Learning iteration 678/2000 [0m                      

                       Computation: 44148 steps/s (collection: 2.099s, learning 0.128s)
             Mean action noise std: 2.66
          Mean value_function loss: 150.6291
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 74.1495
                       Mean reward: 85.03
               Mean episode length: 110.60
    Episode_Reward/reaching_object: 0.5690
     Episode_Reward/lifting_object: 14.6415
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 35.6250
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 2.23s
                      Time elapsed: 00:25:11
                               ETA: 00:49:02

################################################################################
                     [1m Learning iteration 679/2000 [0m                      

                       Computation: 44808 steps/s (collection: 2.092s, learning 0.102s)
             Mean action noise std: 2.66
          Mean value_function loss: 142.1127
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 74.1499
                       Mean reward: 81.07
               Mean episode length: 107.50
    Episode_Reward/reaching_object: 0.5752
     Episode_Reward/lifting_object: 14.7364
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 35.7500
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 2.19s
                      Time elapsed: 00:25:13
                               ETA: 00:48:59

################################################################################
                     [1m Learning iteration 680/2000 [0m                      

                       Computation: 45074 steps/s (collection: 2.078s, learning 0.103s)
             Mean action noise std: 2.66
          Mean value_function loss: 141.1528
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 74.1508
                       Mean reward: 85.87
               Mean episode length: 113.34
    Episode_Reward/reaching_object: 0.5832
     Episode_Reward/lifting_object: 15.2495
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 37.1250
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 2.18s
                      Time elapsed: 00:25:15
                               ETA: 00:48:57

################################################################################
                     [1m Learning iteration 681/2000 [0m                      

                       Computation: 44647 steps/s (collection: 2.089s, learning 0.113s)
             Mean action noise std: 2.66
          Mean value_function loss: 149.7600
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 74.1515
                       Mean reward: 77.17
               Mean episode length: 110.91
    Episode_Reward/reaching_object: 0.5811
     Episode_Reward/lifting_object: 15.8709
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 38.5417
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 2.20s
                      Time elapsed: 00:25:17
                               ETA: 00:48:55

################################################################################
                     [1m Learning iteration 682/2000 [0m                      

                       Computation: 45122 steps/s (collection: 2.077s, learning 0.102s)
             Mean action noise std: 2.66
          Mean value_function loss: 173.2814
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.1530
                       Mean reward: 87.74
               Mean episode length: 108.51
    Episode_Reward/reaching_object: 0.5758
     Episode_Reward/lifting_object: 15.5807
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 37.5417
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 2.18s
                      Time elapsed: 00:25:19
                               ETA: 00:48:52

################################################################################
                     [1m Learning iteration 683/2000 [0m                      

                       Computation: 44791 steps/s (collection: 2.085s, learning 0.110s)
             Mean action noise std: 2.66
          Mean value_function loss: 183.3814
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 74.1542
                       Mean reward: 90.55
               Mean episode length: 100.99
    Episode_Reward/reaching_object: 0.5742
     Episode_Reward/lifting_object: 15.6536
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 34.2917
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 2.19s
                      Time elapsed: 00:25:22
                               ETA: 00:48:50

################################################################################
                     [1m Learning iteration 684/2000 [0m                      

                       Computation: 43908 steps/s (collection: 2.095s, learning 0.144s)
             Mean action noise std: 2.66
          Mean value_function loss: 166.7243
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 74.1564
                       Mean reward: 85.70
               Mean episode length: 105.31
    Episode_Reward/reaching_object: 0.5812
     Episode_Reward/lifting_object: 16.4468
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 38.2917
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 2.24s
                      Time elapsed: 00:25:24
                               ETA: 00:48:48

################################################################################
                     [1m Learning iteration 685/2000 [0m                      

                       Computation: 45340 steps/s (collection: 2.078s, learning 0.090s)
             Mean action noise std: 2.66
          Mean value_function loss: 180.0664
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 74.1589
                       Mean reward: 87.87
               Mean episode length: 113.76
    Episode_Reward/reaching_object: 0.5650
     Episode_Reward/lifting_object: 14.8011
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 36.8750
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 2.17s
                      Time elapsed: 00:25:26
                               ETA: 00:48:46

################################################################################
                     [1m Learning iteration 686/2000 [0m                      

                       Computation: 45071 steps/s (collection: 2.091s, learning 0.091s)
             Mean action noise std: 2.66
          Mean value_function loss: 189.2606
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 74.1611
                       Mean reward: 85.16
               Mean episode length: 109.59
    Episode_Reward/reaching_object: 0.5642
     Episode_Reward/lifting_object: 15.5524
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 37.0833
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 2.18s
                      Time elapsed: 00:25:28
                               ETA: 00:48:43

################################################################################
                     [1m Learning iteration 687/2000 [0m                      

                       Computation: 44731 steps/s (collection: 2.101s, learning 0.097s)
             Mean action noise std: 2.66
          Mean value_function loss: 162.5903
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.1643
                       Mean reward: 76.59
               Mean episode length: 105.07
    Episode_Reward/reaching_object: 0.5570
     Episode_Reward/lifting_object: 15.2518
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 37.7500
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 2.20s
                      Time elapsed: 00:25:30
                               ETA: 00:48:41

################################################################################
                     [1m Learning iteration 688/2000 [0m                      

                       Computation: 44869 steps/s (collection: 2.096s, learning 0.095s)
             Mean action noise std: 2.66
          Mean value_function loss: 207.7703
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 74.1687
                       Mean reward: 68.89
               Mean episode length: 103.31
    Episode_Reward/reaching_object: 0.5587
     Episode_Reward/lifting_object: 15.4806
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 37.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 2.19s
                      Time elapsed: 00:25:33
                               ETA: 00:48:39

################################################################################
                     [1m Learning iteration 689/2000 [0m                      

                       Computation: 44703 steps/s (collection: 2.066s, learning 0.133s)
             Mean action noise std: 2.67
          Mean value_function loss: 174.9843
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 74.1721
                       Mean reward: 78.40
               Mean episode length: 108.46
    Episode_Reward/reaching_object: 0.5790
     Episode_Reward/lifting_object: 16.4942
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 37.8750
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 2.20s
                      Time elapsed: 00:25:35
                               ETA: 00:48:36

################################################################################
                     [1m Learning iteration 690/2000 [0m                      

                       Computation: 44066 steps/s (collection: 2.113s, learning 0.118s)
             Mean action noise std: 2.67
          Mean value_function loss: 192.1257
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 74.1736
                       Mean reward: 77.71
               Mean episode length: 108.73
    Episode_Reward/reaching_object: 0.5608
     Episode_Reward/lifting_object: 16.0866
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 38.5833
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 2.23s
                      Time elapsed: 00:25:37
                               ETA: 00:48:34

################################################################################
                     [1m Learning iteration 691/2000 [0m                      

                       Computation: 44831 steps/s (collection: 2.084s, learning 0.109s)
             Mean action noise std: 2.67
          Mean value_function loss: 184.8959
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 74.1752
                       Mean reward: 96.68
               Mean episode length: 112.84
    Episode_Reward/reaching_object: 0.5722
     Episode_Reward/lifting_object: 16.4483
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 38.0833
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 2.19s
                      Time elapsed: 00:25:39
                               ETA: 00:48:32

################################################################################
                     [1m Learning iteration 692/2000 [0m                      

                       Computation: 45816 steps/s (collection: 2.047s, learning 0.099s)
             Mean action noise std: 2.67
          Mean value_function loss: 198.5928
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 74.1799
                       Mean reward: 91.25
               Mean episode length: 108.69
    Episode_Reward/reaching_object: 0.5643
     Episode_Reward/lifting_object: 16.8220
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 35.5000
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 2.15s
                      Time elapsed: 00:25:41
                               ETA: 00:48:30

################################################################################
                     [1m Learning iteration 693/2000 [0m                      

                       Computation: 44295 steps/s (collection: 2.129s, learning 0.090s)
             Mean action noise std: 2.67
          Mean value_function loss: 198.2391
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 74.1832
                       Mean reward: 85.64
               Mean episode length: 106.57
    Episode_Reward/reaching_object: 0.5958
     Episode_Reward/lifting_object: 17.3317
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 36.2500
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 2.22s
                      Time elapsed: 00:25:44
                               ETA: 00:48:27

################################################################################
                     [1m Learning iteration 694/2000 [0m                      

                       Computation: 42988 steps/s (collection: 2.155s, learning 0.132s)
             Mean action noise std: 2.67
          Mean value_function loss: 219.8174
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 74.1865
                       Mean reward: 89.17
               Mean episode length: 111.32
    Episode_Reward/reaching_object: 0.5889
     Episode_Reward/lifting_object: 17.9234
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 37.3333
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 2.29s
                      Time elapsed: 00:25:46
                               ETA: 00:48:25

################################################################################
                     [1m Learning iteration 695/2000 [0m                      

                       Computation: 44797 steps/s (collection: 2.083s, learning 0.111s)
             Mean action noise std: 2.67
          Mean value_function loss: 197.8473
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 74.1876
                       Mean reward: 96.69
               Mean episode length: 108.90
    Episode_Reward/reaching_object: 0.5904
     Episode_Reward/lifting_object: 18.8254
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 39.7083
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 2.19s
                      Time elapsed: 00:25:48
                               ETA: 00:48:23

################################################################################
                     [1m Learning iteration 696/2000 [0m                      

                       Computation: 44432 steps/s (collection: 2.115s, learning 0.097s)
             Mean action noise std: 2.67
          Mean value_function loss: 207.0565
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 74.1883
                       Mean reward: 89.07
               Mean episode length: 106.13
    Episode_Reward/reaching_object: 0.5693
     Episode_Reward/lifting_object: 17.6091
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 39.2500
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 2.21s
                      Time elapsed: 00:25:50
                               ETA: 00:48:21

################################################################################
                     [1m Learning iteration 697/2000 [0m                      

                       Computation: 44230 steps/s (collection: 2.107s, learning 0.115s)
             Mean action noise std: 2.67
          Mean value_function loss: 214.2193
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 74.1899
                       Mean reward: 100.03
               Mean episode length: 100.35
    Episode_Reward/reaching_object: 0.5789
     Episode_Reward/lifting_object: 18.4353
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 37.5833
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 2.22s
                      Time elapsed: 00:25:52
                               ETA: 00:48:19

################################################################################
                     [1m Learning iteration 698/2000 [0m                      

                       Computation: 44011 steps/s (collection: 2.129s, learning 0.105s)
             Mean action noise std: 2.67
          Mean value_function loss: 181.9529
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 74.1910
                       Mean reward: 88.39
               Mean episode length: 106.93
    Episode_Reward/reaching_object: 0.5782
     Episode_Reward/lifting_object: 18.2211
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 37.2083
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 2.23s
                      Time elapsed: 00:25:55
                               ETA: 00:48:16

################################################################################
                     [1m Learning iteration 699/2000 [0m                      

                       Computation: 44320 steps/s (collection: 2.094s, learning 0.124s)
             Mean action noise std: 2.67
          Mean value_function loss: 235.9927
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.1932
                       Mean reward: 104.86
               Mean episode length: 105.51
    Episode_Reward/reaching_object: 0.5831
     Episode_Reward/lifting_object: 19.2736
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 36.5000
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 2.22s
                      Time elapsed: 00:25:57
                               ETA: 00:48:14

################################################################################
                     [1m Learning iteration 700/2000 [0m                      

                       Computation: 43654 steps/s (collection: 2.155s, learning 0.097s)
             Mean action noise std: 2.67
          Mean value_function loss: 226.5964
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 74.1973
                       Mean reward: 98.48
               Mean episode length: 104.32
    Episode_Reward/reaching_object: 0.5889
     Episode_Reward/lifting_object: 18.6948
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 38.5417
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 2.25s
                      Time elapsed: 00:25:59
                               ETA: 00:48:12

################################################################################
                     [1m Learning iteration 701/2000 [0m                      

                       Computation: 43433 steps/s (collection: 2.154s, learning 0.109s)
             Mean action noise std: 2.67
          Mean value_function loss: 217.2319
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 74.2001
                       Mean reward: 112.66
               Mean episode length: 107.84
    Episode_Reward/reaching_object: 0.5962
     Episode_Reward/lifting_object: 18.7397
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 39.5417
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 2.26s
                      Time elapsed: 00:26:01
                               ETA: 00:48:10

################################################################################
                     [1m Learning iteration 702/2000 [0m                      

                       Computation: 44855 steps/s (collection: 2.097s, learning 0.094s)
             Mean action noise std: 2.67
          Mean value_function loss: 223.7896
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 74.2028
                       Mean reward: 104.42
               Mean episode length: 107.82
    Episode_Reward/reaching_object: 0.5731
     Episode_Reward/lifting_object: 18.4071
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 38.6250
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 2.19s
                      Time elapsed: 00:26:04
                               ETA: 00:48:07

################################################################################
                     [1m Learning iteration 703/2000 [0m                      

                       Computation: 43807 steps/s (collection: 2.107s, learning 0.137s)
             Mean action noise std: 2.67
          Mean value_function loss: 241.6710
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 74.2063
                       Mean reward: 84.91
               Mean episode length: 103.21
    Episode_Reward/reaching_object: 0.5940
     Episode_Reward/lifting_object: 19.6483
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 36.9167
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 2.24s
                      Time elapsed: 00:26:06
                               ETA: 00:48:05

################################################################################
                     [1m Learning iteration 704/2000 [0m                      

                       Computation: 43993 steps/s (collection: 2.117s, learning 0.118s)
             Mean action noise std: 2.67
          Mean value_function loss: 226.1585
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 74.2096
                       Mean reward: 86.96
               Mean episode length: 107.43
    Episode_Reward/reaching_object: 0.5807
     Episode_Reward/lifting_object: 18.4916
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 37.5417
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 2.23s
                      Time elapsed: 00:26:08
                               ETA: 00:48:03

################################################################################
                     [1m Learning iteration 705/2000 [0m                      

                       Computation: 43834 steps/s (collection: 2.127s, learning 0.116s)
             Mean action noise std: 2.67
          Mean value_function loss: 229.7554
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 74.2113
                       Mean reward: 115.83
               Mean episode length: 102.34
    Episode_Reward/reaching_object: 0.5943
     Episode_Reward/lifting_object: 20.0684
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 39.5833
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 2.24s
                      Time elapsed: 00:26:10
                               ETA: 00:48:01

################################################################################
                     [1m Learning iteration 706/2000 [0m                      

                       Computation: 44119 steps/s (collection: 2.113s, learning 0.115s)
             Mean action noise std: 2.67
          Mean value_function loss: 217.9842
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 74.2124
                       Mean reward: 94.85
               Mean episode length: 110.75
    Episode_Reward/reaching_object: 0.6088
     Episode_Reward/lifting_object: 20.9775
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 39.3750
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 2.23s
                      Time elapsed: 00:26:13
                               ETA: 00:47:59

################################################################################
                     [1m Learning iteration 707/2000 [0m                      

                       Computation: 43321 steps/s (collection: 2.129s, learning 0.141s)
             Mean action noise std: 2.67
          Mean value_function loss: 208.0075
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 74.2131
                       Mean reward: 103.52
               Mean episode length: 104.39
    Episode_Reward/reaching_object: 0.5923
     Episode_Reward/lifting_object: 20.3491
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 38.7500
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 2.27s
                      Time elapsed: 00:26:15
                               ETA: 00:47:56

################################################################################
                     [1m Learning iteration 708/2000 [0m                      

                       Computation: 44540 steps/s (collection: 2.095s, learning 0.113s)
             Mean action noise std: 2.67
          Mean value_function loss: 213.3332
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 74.2135
                       Mean reward: 95.57
               Mean episode length: 111.28
    Episode_Reward/reaching_object: 0.5951
     Episode_Reward/lifting_object: 20.4261
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 36.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 2.21s
                      Time elapsed: 00:26:17
                               ETA: 00:47:54

################################################################################
                     [1m Learning iteration 709/2000 [0m                      

                       Computation: 44108 steps/s (collection: 2.116s, learning 0.113s)
             Mean action noise std: 2.67
          Mean value_function loss: 198.0849
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 74.2139
                       Mean reward: 86.52
               Mean episode length: 101.66
    Episode_Reward/reaching_object: 0.5948
     Episode_Reward/lifting_object: 18.9551
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 36.6250
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 2.23s
                      Time elapsed: 00:26:19
                               ETA: 00:47:52

################################################################################
                     [1m Learning iteration 710/2000 [0m                      

                       Computation: 42742 steps/s (collection: 2.181s, learning 0.119s)
             Mean action noise std: 2.67
          Mean value_function loss: 196.7536
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 74.2143
                       Mean reward: 105.86
               Mean episode length: 106.64
    Episode_Reward/reaching_object: 0.6091
     Episode_Reward/lifting_object: 20.2216
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 37.2917
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 2.30s
                      Time elapsed: 00:26:22
                               ETA: 00:47:50

################################################################################
                     [1m Learning iteration 711/2000 [0m                      

                       Computation: 42122 steps/s (collection: 2.162s, learning 0.172s)
             Mean action noise std: 2.67
          Mean value_function loss: 215.4876
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 74.2150
                       Mean reward: 102.14
               Mean episode length: 111.73
    Episode_Reward/reaching_object: 0.5845
     Episode_Reward/lifting_object: 18.4933
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 38.4583
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 2.33s
                      Time elapsed: 00:26:24
                               ETA: 00:47:48

################################################################################
                     [1m Learning iteration 712/2000 [0m                      

                       Computation: 43051 steps/s (collection: 2.190s, learning 0.094s)
             Mean action noise std: 2.67
          Mean value_function loss: 225.5439
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 74.2161
                       Mean reward: 109.65
               Mean episode length: 111.28
    Episode_Reward/reaching_object: 0.5812
     Episode_Reward/lifting_object: 17.3008
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 37.5000
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 2.28s
                      Time elapsed: 00:26:26
                               ETA: 00:47:46

################################################################################
                     [1m Learning iteration 713/2000 [0m                      

                       Computation: 43186 steps/s (collection: 2.167s, learning 0.109s)
             Mean action noise std: 2.67
          Mean value_function loss: 198.8127
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 74.2170
                       Mean reward: 81.79
               Mean episode length: 109.45
    Episode_Reward/reaching_object: 0.5790
     Episode_Reward/lifting_object: 17.1933
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 31.8333
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 2.28s
                      Time elapsed: 00:26:28
                               ETA: 00:47:44

################################################################################
                     [1m Learning iteration 714/2000 [0m                      

                       Computation: 43067 steps/s (collection: 2.170s, learning 0.112s)
             Mean action noise std: 2.67
          Mean value_function loss: 208.4143
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 74.2174
                       Mean reward: 92.49
               Mean episode length: 109.10
    Episode_Reward/reaching_object: 0.5965
     Episode_Reward/lifting_object: 18.0996
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 35.6250
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 2.28s
                      Time elapsed: 00:26:31
                               ETA: 00:47:42

################################################################################
                     [1m Learning iteration 715/2000 [0m                      

                       Computation: 42814 steps/s (collection: 2.183s, learning 0.113s)
             Mean action noise std: 2.67
          Mean value_function loss: 208.3794
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 74.2178
                       Mean reward: 118.86
               Mean episode length: 120.28
    Episode_Reward/reaching_object: 0.6310
     Episode_Reward/lifting_object: 20.7693
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 35.3333
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 2.30s
                      Time elapsed: 00:26:33
                               ETA: 00:47:39

################################################################################
                     [1m Learning iteration 716/2000 [0m                      

                       Computation: 43572 steps/s (collection: 2.136s, learning 0.121s)
             Mean action noise std: 2.67
          Mean value_function loss: 181.7938
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 74.2185
                       Mean reward: 117.41
               Mean episode length: 111.93
    Episode_Reward/reaching_object: 0.6174
     Episode_Reward/lifting_object: 19.9846
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 37.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 2.26s
                      Time elapsed: 00:26:35
                               ETA: 00:47:37

################################################################################
                     [1m Learning iteration 717/2000 [0m                      

                       Computation: 44320 steps/s (collection: 2.103s, learning 0.115s)
             Mean action noise std: 2.67
          Mean value_function loss: 215.8646
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 74.2211
                       Mean reward: 101.10
               Mean episode length: 110.59
    Episode_Reward/reaching_object: 0.5948
     Episode_Reward/lifting_object: 17.7778
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 33.9583
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 2.22s
                      Time elapsed: 00:26:38
                               ETA: 00:47:35

################################################################################
                     [1m Learning iteration 718/2000 [0m                      

                       Computation: 43713 steps/s (collection: 2.152s, learning 0.097s)
             Mean action noise std: 2.67
          Mean value_function loss: 232.2102
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 74.2261
                       Mean reward: 94.02
               Mean episode length: 110.50
    Episode_Reward/reaching_object: 0.6099
     Episode_Reward/lifting_object: 19.6330
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 35.1667
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 2.25s
                      Time elapsed: 00:26:40
                               ETA: 00:47:33

################################################################################
                     [1m Learning iteration 719/2000 [0m                      

                       Computation: 43350 steps/s (collection: 2.132s, learning 0.136s)
             Mean action noise std: 2.67
          Mean value_function loss: 237.3650
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 74.2286
                       Mean reward: 107.32
               Mean episode length: 109.51
    Episode_Reward/reaching_object: 0.6066
     Episode_Reward/lifting_object: 19.2088
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 36.5417
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 2.27s
                      Time elapsed: 00:26:42
                               ETA: 00:47:31

################################################################################
                     [1m Learning iteration 720/2000 [0m                      

                       Computation: 42365 steps/s (collection: 2.212s, learning 0.109s)
             Mean action noise std: 2.67
          Mean value_function loss: 229.0035
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 74.2314
                       Mean reward: 104.26
               Mean episode length: 115.46
    Episode_Reward/reaching_object: 0.6064
     Episode_Reward/lifting_object: 19.3688
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 32.8333
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 2.32s
                      Time elapsed: 00:26:44
                               ETA: 00:47:29

################################################################################
                     [1m Learning iteration 721/2000 [0m                      

                       Computation: 43754 steps/s (collection: 2.113s, learning 0.134s)
             Mean action noise std: 2.67
          Mean value_function loss: 221.6327
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 74.2336
                       Mean reward: 111.90
               Mean episode length: 111.98
    Episode_Reward/reaching_object: 0.6321
     Episode_Reward/lifting_object: 20.8018
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 33.2083
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 2.25s
                      Time elapsed: 00:26:47
                               ETA: 00:47:26

################################################################################
                     [1m Learning iteration 722/2000 [0m                      

                       Computation: 43075 steps/s (collection: 2.166s, learning 0.117s)
             Mean action noise std: 2.67
          Mean value_function loss: 214.6261
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 74.2341
                       Mean reward: 98.66
               Mean episode length: 122.01
    Episode_Reward/reaching_object: 0.6261
     Episode_Reward/lifting_object: 20.4592
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 33.4167
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 2.28s
                      Time elapsed: 00:26:49
                               ETA: 00:47:24

################################################################################
                     [1m Learning iteration 723/2000 [0m                      

                       Computation: 43653 steps/s (collection: 2.116s, learning 0.136s)
             Mean action noise std: 2.67
          Mean value_function loss: 237.1158
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 74.2345
                       Mean reward: 98.94
               Mean episode length: 124.40
    Episode_Reward/reaching_object: 0.6359
     Episode_Reward/lifting_object: 21.6714
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 35.1667
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 2.25s
                      Time elapsed: 00:26:51
                               ETA: 00:47:22

################################################################################
                     [1m Learning iteration 724/2000 [0m                      

                       Computation: 44395 steps/s (collection: 2.125s, learning 0.089s)
             Mean action noise std: 2.67
          Mean value_function loss: 218.8649
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 74.2348
                       Mean reward: 111.47
               Mean episode length: 112.82
    Episode_Reward/reaching_object: 0.6354
     Episode_Reward/lifting_object: 21.0294
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 32.7917
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 2.21s
                      Time elapsed: 00:26:53
                               ETA: 00:47:20

################################################################################
                     [1m Learning iteration 725/2000 [0m                      

                       Computation: 44616 steps/s (collection: 2.105s, learning 0.099s)
             Mean action noise std: 2.67
          Mean value_function loss: 237.2371
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 74.2351
                       Mean reward: 110.78
               Mean episode length: 121.17
    Episode_Reward/reaching_object: 0.6496
     Episode_Reward/lifting_object: 23.1218
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 33.3333
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 2.20s
                      Time elapsed: 00:26:56
                               ETA: 00:47:18

################################################################################
                     [1m Learning iteration 726/2000 [0m                      

                       Computation: 44231 steps/s (collection: 2.128s, learning 0.095s)
             Mean action noise std: 2.67
          Mean value_function loss: 250.3273
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 74.2360
                       Mean reward: 116.13
               Mean episode length: 118.21
    Episode_Reward/reaching_object: 0.6380
     Episode_Reward/lifting_object: 22.0495
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 33.2500
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 2.22s
                      Time elapsed: 00:26:58
                               ETA: 00:47:15

################################################################################
                     [1m Learning iteration 727/2000 [0m                      

                       Computation: 43266 steps/s (collection: 2.143s, learning 0.129s)
             Mean action noise std: 2.67
          Mean value_function loss: 259.3550
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 74.2376
                       Mean reward: 124.55
               Mean episode length: 117.44
    Episode_Reward/reaching_object: 0.6602
     Episode_Reward/lifting_object: 24.2178
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 32.6250
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 2.27s
                      Time elapsed: 00:27:00
                               ETA: 00:47:13

################################################################################
                     [1m Learning iteration 728/2000 [0m                      

                       Computation: 44250 steps/s (collection: 2.134s, learning 0.088s)
             Mean action noise std: 2.67
          Mean value_function loss: 261.2072
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 74.2395
                       Mean reward: 106.79
               Mean episode length: 122.52
    Episode_Reward/reaching_object: 0.6510
     Episode_Reward/lifting_object: 22.9426
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 35.3333
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 2.22s
                      Time elapsed: 00:27:02
                               ETA: 00:47:11

################################################################################
                     [1m Learning iteration 729/2000 [0m                      

                       Computation: 44462 steps/s (collection: 2.115s, learning 0.096s)
             Mean action noise std: 2.67
          Mean value_function loss: 299.4881
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 74.2426
                       Mean reward: 113.08
               Mean episode length: 120.57
    Episode_Reward/reaching_object: 0.6547
     Episode_Reward/lifting_object: 24.3020
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 33.2083
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 2.21s
                      Time elapsed: 00:27:04
                               ETA: 00:47:09

################################################################################
                     [1m Learning iteration 730/2000 [0m                      

                       Computation: 44633 steps/s (collection: 2.111s, learning 0.091s)
             Mean action noise std: 2.67
          Mean value_function loss: 283.6594
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 74.2457
                       Mean reward: 105.15
               Mean episode length: 117.99
    Episode_Reward/reaching_object: 0.6436
     Episode_Reward/lifting_object: 23.9393
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 35.3750
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 2.20s
                      Time elapsed: 00:27:07
                               ETA: 00:47:06

################################################################################
                     [1m Learning iteration 731/2000 [0m                      

                       Computation: 44808 steps/s (collection: 2.100s, learning 0.094s)
             Mean action noise std: 2.67
          Mean value_function loss: 280.2209
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 74.2473
                       Mean reward: 130.93
               Mean episode length: 114.08
    Episode_Reward/reaching_object: 0.6690
     Episode_Reward/lifting_object: 26.8339
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 35.6250
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 2.19s
                      Time elapsed: 00:27:09
                               ETA: 00:47:04

################################################################################
                     [1m Learning iteration 732/2000 [0m                      

                       Computation: 43340 steps/s (collection: 2.142s, learning 0.126s)
             Mean action noise std: 2.67
          Mean value_function loss: 311.1448
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 74.2484
                       Mean reward: 152.01
               Mean episode length: 118.47
    Episode_Reward/reaching_object: 0.6632
     Episode_Reward/lifting_object: 26.3303
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 33.8333
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 2.27s
                      Time elapsed: 00:27:11
                               ETA: 00:47:02

################################################################################
                     [1m Learning iteration 733/2000 [0m                      

                       Computation: 43617 steps/s (collection: 2.145s, learning 0.109s)
             Mean action noise std: 2.67
          Mean value_function loss: 308.6310
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.2494
                       Mean reward: 113.54
               Mean episode length: 111.60
    Episode_Reward/reaching_object: 0.6636
     Episode_Reward/lifting_object: 26.4897
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 33.2917
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 2.25s
                      Time elapsed: 00:27:13
                               ETA: 00:47:00

################################################################################
                     [1m Learning iteration 734/2000 [0m                      

                       Computation: 44370 steps/s (collection: 2.112s, learning 0.104s)
             Mean action noise std: 2.67
          Mean value_function loss: 296.8376
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 74.2526
                       Mean reward: 116.87
               Mean episode length: 110.35
    Episode_Reward/reaching_object: 0.6550
     Episode_Reward/lifting_object: 26.3156
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 32.0417
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 2.22s
                      Time elapsed: 00:27:16
                               ETA: 00:46:58

################################################################################
                     [1m Learning iteration 735/2000 [0m                      

                       Computation: 45384 steps/s (collection: 2.079s, learning 0.087s)
             Mean action noise std: 2.67
          Mean value_function loss: 293.3375
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 74.2545
                       Mean reward: 115.55
               Mean episode length: 129.88
    Episode_Reward/reaching_object: 0.6584
     Episode_Reward/lifting_object: 25.9775
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 35.1250
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 2.17s
                      Time elapsed: 00:27:18
                               ETA: 00:46:55

################################################################################
                     [1m Learning iteration 736/2000 [0m                      

                       Computation: 43667 steps/s (collection: 2.133s, learning 0.119s)
             Mean action noise std: 2.67
          Mean value_function loss: 314.5014
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 74.2558
                       Mean reward: 142.30
               Mean episode length: 120.01
    Episode_Reward/reaching_object: 0.6501
     Episode_Reward/lifting_object: 26.6334
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 33.2083
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 2.25s
                      Time elapsed: 00:27:20
                               ETA: 00:46:53

################################################################################
                     [1m Learning iteration 737/2000 [0m                      

                       Computation: 43238 steps/s (collection: 2.112s, learning 0.162s)
             Mean action noise std: 2.67
          Mean value_function loss: 290.3228
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 74.2568
                       Mean reward: 152.97
               Mean episode length: 122.91
    Episode_Reward/reaching_object: 0.6691
     Episode_Reward/lifting_object: 28.9447
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 33.2500
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 2.27s
                      Time elapsed: 00:27:22
                               ETA: 00:46:51

################################################################################
                     [1m Learning iteration 738/2000 [0m                      

                       Computation: 44102 steps/s (collection: 2.126s, learning 0.103s)
             Mean action noise std: 2.67
          Mean value_function loss: 285.9140
               Mean surrogate loss: 0.0162
                 Mean entropy loss: 74.2571
                       Mean reward: 172.85
               Mean episode length: 115.97
    Episode_Reward/reaching_object: 0.6754
     Episode_Reward/lifting_object: 28.4042
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 33.6667
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 2.23s
                      Time elapsed: 00:27:25
                               ETA: 00:46:49

################################################################################
                     [1m Learning iteration 739/2000 [0m                      

                       Computation: 43736 steps/s (collection: 2.145s, learning 0.103s)
             Mean action noise std: 2.67
          Mean value_function loss: 267.3553
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 74.2571
                       Mean reward: 107.98
               Mean episode length: 121.39
    Episode_Reward/reaching_object: 0.6759
     Episode_Reward/lifting_object: 28.5759
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 33.2500
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 2.25s
                      Time elapsed: 00:27:27
                               ETA: 00:46:47

################################################################################
                     [1m Learning iteration 740/2000 [0m                      

                       Computation: 45180 steps/s (collection: 2.090s, learning 0.086s)
             Mean action noise std: 2.67
          Mean value_function loss: 272.4144
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 74.2581
                       Mean reward: 146.33
               Mean episode length: 129.08
    Episode_Reward/reaching_object: 0.6717
     Episode_Reward/lifting_object: 28.8584
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 32.6667
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 2.18s
                      Time elapsed: 00:27:29
                               ETA: 00:46:44

################################################################################
                     [1m Learning iteration 741/2000 [0m                      

                       Computation: 44638 steps/s (collection: 2.079s, learning 0.124s)
             Mean action noise std: 2.67
          Mean value_function loss: 261.9586
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 74.2607
                       Mean reward: 122.78
               Mean episode length: 120.59
    Episode_Reward/reaching_object: 0.6517
     Episode_Reward/lifting_object: 26.5941
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 31.7500
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 2.20s
                      Time elapsed: 00:27:31
                               ETA: 00:46:42

################################################################################
                     [1m Learning iteration 742/2000 [0m                      

                       Computation: 44915 steps/s (collection: 2.081s, learning 0.108s)
             Mean action noise std: 2.67
          Mean value_function loss: 282.9831
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 74.2620
                       Mean reward: 147.21
               Mean episode length: 117.47
    Episode_Reward/reaching_object: 0.6893
     Episode_Reward/lifting_object: 29.6074
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 32.8333
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 2.19s
                      Time elapsed: 00:27:33
                               ETA: 00:46:40

################################################################################
                     [1m Learning iteration 743/2000 [0m                      

                       Computation: 44873 steps/s (collection: 2.096s, learning 0.094s)
             Mean action noise std: 2.68
          Mean value_function loss: 275.3148
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 74.2647
                       Mean reward: 150.28
               Mean episode length: 124.01
    Episode_Reward/reaching_object: 0.6915
     Episode_Reward/lifting_object: 28.7678
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 33.0833
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 2.19s
                      Time elapsed: 00:27:36
                               ETA: 00:46:37

################################################################################
                     [1m Learning iteration 744/2000 [0m                      

                       Computation: 44087 steps/s (collection: 2.120s, learning 0.110s)
             Mean action noise std: 2.68
          Mean value_function loss: 260.1480
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 74.2667
                       Mean reward: 178.44
               Mean episode length: 127.02
    Episode_Reward/reaching_object: 0.6839
     Episode_Reward/lifting_object: 28.5706
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 34.7917
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 2.23s
                      Time elapsed: 00:27:38
                               ETA: 00:46:35

################################################################################
                     [1m Learning iteration 745/2000 [0m                      

                       Computation: 45106 steps/s (collection: 2.079s, learning 0.100s)
             Mean action noise std: 2.68
          Mean value_function loss: 266.3574
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 74.2689
                       Mean reward: 90.37
               Mean episode length: 124.24
    Episode_Reward/reaching_object: 0.6594
     Episode_Reward/lifting_object: 26.6232
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 30.8750
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 2.18s
                      Time elapsed: 00:27:40
                               ETA: 00:46:33

################################################################################
                     [1m Learning iteration 746/2000 [0m                      

                       Computation: 44608 steps/s (collection: 2.095s, learning 0.109s)
             Mean action noise std: 2.68
          Mean value_function loss: 272.9169
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 74.2705
                       Mean reward: 139.06
               Mean episode length: 123.25
    Episode_Reward/reaching_object: 0.6700
     Episode_Reward/lifting_object: 27.1031
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 29.7083
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 2.20s
                      Time elapsed: 00:27:42
                               ETA: 00:46:31

################################################################################
                     [1m Learning iteration 747/2000 [0m                      

                       Computation: 44116 steps/s (collection: 2.100s, learning 0.129s)
             Mean action noise std: 2.68
          Mean value_function loss: 262.6813
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 74.2709
                       Mean reward: 128.05
               Mean episode length: 123.74
    Episode_Reward/reaching_object: 0.6623
     Episode_Reward/lifting_object: 26.4384
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 30.8750
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 2.23s
                      Time elapsed: 00:27:44
                               ETA: 00:46:28

################################################################################
                     [1m Learning iteration 748/2000 [0m                      

                       Computation: 43754 steps/s (collection: 2.138s, learning 0.109s)
             Mean action noise std: 2.68
          Mean value_function loss: 289.1228
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 74.2720
                       Mean reward: 155.85
               Mean episode length: 119.53
    Episode_Reward/reaching_object: 0.6926
     Episode_Reward/lifting_object: 28.7167
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 31.0833
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 2.25s
                      Time elapsed: 00:27:47
                               ETA: 00:46:26

################################################################################
                     [1m Learning iteration 749/2000 [0m                      

                       Computation: 44540 steps/s (collection: 2.107s, learning 0.101s)
             Mean action noise std: 2.68
          Mean value_function loss: 301.4222
               Mean surrogate loss: 0.0101
                 Mean entropy loss: 74.2742
                       Mean reward: 172.43
               Mean episode length: 123.21
    Episode_Reward/reaching_object: 0.7151
     Episode_Reward/lifting_object: 29.9347
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 29.4167
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 2.21s
                      Time elapsed: 00:27:49
                               ETA: 00:46:24

################################################################################
                     [1m Learning iteration 750/2000 [0m                      

                       Computation: 44004 steps/s (collection: 2.119s, learning 0.115s)
             Mean action noise std: 2.68
          Mean value_function loss: 290.0930
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 74.2751
                       Mean reward: 140.78
               Mean episode length: 125.61
    Episode_Reward/reaching_object: 0.7208
     Episode_Reward/lifting_object: 30.5390
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 33.9583
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 2.23s
                      Time elapsed: 00:27:51
                               ETA: 00:46:22

################################################################################
                     [1m Learning iteration 751/2000 [0m                      

                       Computation: 45074 steps/s (collection: 2.093s, learning 0.088s)
             Mean action noise std: 2.68
          Mean value_function loss: 299.1279
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 74.2753
                       Mean reward: 136.20
               Mean episode length: 136.91
    Episode_Reward/reaching_object: 0.7174
     Episode_Reward/lifting_object: 30.7361
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 31.2917
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 2.18s
                      Time elapsed: 00:27:53
                               ETA: 00:46:19

################################################################################
                     [1m Learning iteration 752/2000 [0m                      

                       Computation: 43455 steps/s (collection: 2.156s, learning 0.106s)
             Mean action noise std: 2.68
          Mean value_function loss: 295.9797
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 74.2760
                       Mean reward: 153.10
               Mean episode length: 123.52
    Episode_Reward/reaching_object: 0.7629
     Episode_Reward/lifting_object: 34.8658
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 28.7083
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 2.26s
                      Time elapsed: 00:27:56
                               ETA: 00:46:17

################################################################################
                     [1m Learning iteration 753/2000 [0m                      

                       Computation: 44820 steps/s (collection: 2.103s, learning 0.091s)
             Mean action noise std: 2.68
          Mean value_function loss: 302.3099
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 74.2774
                       Mean reward: 164.32
               Mean episode length: 135.07
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 33.1915
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 28.5833
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 2.19s
                      Time elapsed: 00:27:58
                               ETA: 00:46:15

################################################################################
                     [1m Learning iteration 754/2000 [0m                      

                       Computation: 44533 steps/s (collection: 2.117s, learning 0.091s)
             Mean action noise std: 2.68
          Mean value_function loss: 309.8386
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 74.2783
                       Mean reward: 175.39
               Mean episode length: 135.01
    Episode_Reward/reaching_object: 0.7525
     Episode_Reward/lifting_object: 33.7947
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 29.9167
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 2.21s
                      Time elapsed: 00:28:00
                               ETA: 00:46:13

################################################################################
                     [1m Learning iteration 755/2000 [0m                      

                       Computation: 44675 steps/s (collection: 2.103s, learning 0.097s)
             Mean action noise std: 2.68
          Mean value_function loss: 321.9554
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 74.2799
                       Mean reward: 170.68
               Mean episode length: 125.25
    Episode_Reward/reaching_object: 0.7403
     Episode_Reward/lifting_object: 32.7791
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 27.6667
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 2.20s
                      Time elapsed: 00:28:02
                               ETA: 00:46:10

################################################################################
                     [1m Learning iteration 756/2000 [0m                      

                       Computation: 44061 steps/s (collection: 2.126s, learning 0.105s)
             Mean action noise std: 2.68
          Mean value_function loss: 323.3591
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 74.2824
                       Mean reward: 182.59
               Mean episode length: 130.93
    Episode_Reward/reaching_object: 0.7425
     Episode_Reward/lifting_object: 34.6592
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 29.3333
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 2.23s
                      Time elapsed: 00:28:04
                               ETA: 00:46:08

################################################################################
                     [1m Learning iteration 757/2000 [0m                      

                       Computation: 44417 steps/s (collection: 2.119s, learning 0.095s)
             Mean action noise std: 2.68
          Mean value_function loss: 334.8061
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 74.2851
                       Mean reward: 168.44
               Mean episode length: 126.77
    Episode_Reward/reaching_object: 0.7960
     Episode_Reward/lifting_object: 38.5833
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 29.0417
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 2.21s
                      Time elapsed: 00:28:07
                               ETA: 00:46:06

################################################################################
                     [1m Learning iteration 758/2000 [0m                      

                       Computation: 44946 steps/s (collection: 2.094s, learning 0.093s)
             Mean action noise std: 2.68
          Mean value_function loss: 334.0459
               Mean surrogate loss: 0.0307
                 Mean entropy loss: 74.2874
                       Mean reward: 179.02
               Mean episode length: 137.67
    Episode_Reward/reaching_object: 0.8244
     Episode_Reward/lifting_object: 41.0900
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 29.7500
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 2.19s
                      Time elapsed: 00:28:09
                               ETA: 00:46:04

################################################################################
                     [1m Learning iteration 759/2000 [0m                      

                       Computation: 44952 steps/s (collection: 2.081s, learning 0.106s)
             Mean action noise std: 2.68
          Mean value_function loss: 306.2419
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 74.2880
                       Mean reward: 188.66
               Mean episode length: 137.67
    Episode_Reward/reaching_object: 0.7739
     Episode_Reward/lifting_object: 35.5451
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 29.1250
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 2.19s
                      Time elapsed: 00:28:11
                               ETA: 00:46:01

################################################################################
                     [1m Learning iteration 760/2000 [0m                      

                       Computation: 43483 steps/s (collection: 2.169s, learning 0.092s)
             Mean action noise std: 2.68
          Mean value_function loss: 315.4340
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.2891
                       Mean reward: 158.96
               Mean episode length: 133.04
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 35.2660
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 28.3750
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 2.26s
                      Time elapsed: 00:28:13
                               ETA: 00:45:59

################################################################################
                     [1m Learning iteration 761/2000 [0m                      

                       Computation: 44732 steps/s (collection: 2.092s, learning 0.106s)
             Mean action noise std: 2.68
          Mean value_function loss: 346.2464
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 74.2922
                       Mean reward: 165.80
               Mean episode length: 126.34
    Episode_Reward/reaching_object: 0.7499
     Episode_Reward/lifting_object: 34.2974
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 27.6250
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 2.20s
                      Time elapsed: 00:28:15
                               ETA: 00:45:57

################################################################################
                     [1m Learning iteration 762/2000 [0m                      

                       Computation: 42217 steps/s (collection: 2.220s, learning 0.109s)
             Mean action noise std: 2.68
          Mean value_function loss: 334.7665
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 74.2946
                       Mean reward: 166.31
               Mean episode length: 135.18
    Episode_Reward/reaching_object: 0.7951
     Episode_Reward/lifting_object: 37.1054
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 24.8333
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 2.33s
                      Time elapsed: 00:28:18
                               ETA: 00:45:55

################################################################################
                     [1m Learning iteration 763/2000 [0m                      

                       Computation: 43101 steps/s (collection: 2.165s, learning 0.116s)
             Mean action noise std: 2.68
          Mean value_function loss: 331.0810
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 74.2959
                       Mean reward: 167.02
               Mean episode length: 138.37
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 36.3161
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 26.9583
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 2.28s
                      Time elapsed: 00:28:20
                               ETA: 00:45:53

################################################################################
                     [1m Learning iteration 764/2000 [0m                      

                       Computation: 44942 steps/s (collection: 2.093s, learning 0.094s)
             Mean action noise std: 2.68
          Mean value_function loss: 327.8584
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 74.2967
                       Mean reward: 198.98
               Mean episode length: 130.72
    Episode_Reward/reaching_object: 0.7943
     Episode_Reward/lifting_object: 37.4605
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 29.3750
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 2.19s
                      Time elapsed: 00:28:22
                               ETA: 00:45:51

################################################################################
                     [1m Learning iteration 765/2000 [0m                      

                       Computation: 43740 steps/s (collection: 2.122s, learning 0.125s)
             Mean action noise std: 2.68
          Mean value_function loss: 364.0184
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 74.2974
                       Mean reward: 176.91
               Mean episode length: 136.29
    Episode_Reward/reaching_object: 0.8084
     Episode_Reward/lifting_object: 39.9505
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 26.7500
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 2.25s
                      Time elapsed: 00:28:24
                               ETA: 00:45:48

################################################################################
                     [1m Learning iteration 766/2000 [0m                      

                       Computation: 43564 steps/s (collection: 2.143s, learning 0.114s)
             Mean action noise std: 2.68
          Mean value_function loss: 398.5691
               Mean surrogate loss: 0.0109
                 Mean entropy loss: 74.2981
                       Mean reward: 186.84
               Mean episode length: 130.47
    Episode_Reward/reaching_object: 0.8188
     Episode_Reward/lifting_object: 40.3133
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 29.3333
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 2.26s
                      Time elapsed: 00:28:27
                               ETA: 00:45:46

################################################################################
                     [1m Learning iteration 767/2000 [0m                      

                       Computation: 44485 steps/s (collection: 2.104s, learning 0.106s)
             Mean action noise std: 2.68
          Mean value_function loss: 352.2468
               Mean surrogate loss: 0.0115
                 Mean entropy loss: 74.2984
                       Mean reward: 199.21
               Mean episode length: 134.10
    Episode_Reward/reaching_object: 0.8350
     Episode_Reward/lifting_object: 43.5742
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 25.8333
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 2.21s
                      Time elapsed: 00:28:29
                               ETA: 00:45:44

################################################################################
                     [1m Learning iteration 768/2000 [0m                      

                       Computation: 44139 steps/s (collection: 2.131s, learning 0.096s)
             Mean action noise std: 2.68
          Mean value_function loss: 331.4924
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 74.2984
                       Mean reward: 250.75
               Mean episode length: 154.89
    Episode_Reward/reaching_object: 0.8186
     Episode_Reward/lifting_object: 40.5573
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 26.0417
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 2.23s
                      Time elapsed: 00:28:31
                               ETA: 00:45:42

################################################################################
                     [1m Learning iteration 769/2000 [0m                      

                       Computation: 44312 steps/s (collection: 2.119s, learning 0.100s)
             Mean action noise std: 2.68
          Mean value_function loss: 354.7252
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 74.2987
                       Mean reward: 217.40
               Mean episode length: 137.26
    Episode_Reward/reaching_object: 0.8301
     Episode_Reward/lifting_object: 41.4198
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 26.2083
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 2.22s
                      Time elapsed: 00:28:33
                               ETA: 00:45:39

################################################################################
                     [1m Learning iteration 770/2000 [0m                      

                       Computation: 44130 steps/s (collection: 2.120s, learning 0.108s)
             Mean action noise std: 2.68
          Mean value_function loss: 330.8455
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 74.2990
                       Mean reward: 215.27
               Mean episode length: 126.00
    Episode_Reward/reaching_object: 0.8423
     Episode_Reward/lifting_object: 42.4714
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 26.2083
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 2.23s
                      Time elapsed: 00:28:36
                               ETA: 00:45:37

################################################################################
                     [1m Learning iteration 771/2000 [0m                      

                       Computation: 44982 steps/s (collection: 2.092s, learning 0.094s)
             Mean action noise std: 2.68
          Mean value_function loss: 356.6705
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 74.2991
                       Mean reward: 172.32
               Mean episode length: 146.45
    Episode_Reward/reaching_object: 0.8051
     Episode_Reward/lifting_object: 39.2023
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 25.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 2.19s
                      Time elapsed: 00:28:38
                               ETA: 00:45:35

################################################################################
                     [1m Learning iteration 772/2000 [0m                      

                       Computation: 41655 steps/s (collection: 2.220s, learning 0.140s)
             Mean action noise std: 2.68
          Mean value_function loss: 331.7188
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 74.2994
                       Mean reward: 271.96
               Mean episode length: 156.82
    Episode_Reward/reaching_object: 0.8912
     Episode_Reward/lifting_object: 47.0051
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 25.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 2.36s
                      Time elapsed: 00:28:40
                               ETA: 00:45:33

################################################################################
                     [1m Learning iteration 773/2000 [0m                      

                       Computation: 40588 steps/s (collection: 2.296s, learning 0.126s)
             Mean action noise std: 2.68
          Mean value_function loss: 307.9535
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 74.2997
                       Mean reward: 254.79
               Mean episode length: 159.53
    Episode_Reward/reaching_object: 0.8948
     Episode_Reward/lifting_object: 46.5631
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.1250
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 2.42s
                      Time elapsed: 00:28:43
                               ETA: 00:45:31

################################################################################
                     [1m Learning iteration 774/2000 [0m                      

                       Computation: 44973 steps/s (collection: 2.090s, learning 0.096s)
             Mean action noise std: 2.68
          Mean value_function loss: 316.1428
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 74.3002
                       Mean reward: 197.55
               Mean episode length: 141.93
    Episode_Reward/reaching_object: 0.8472
     Episode_Reward/lifting_object: 42.0975
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 25.2917
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 2.19s
                      Time elapsed: 00:28:45
                               ETA: 00:45:29

################################################################################
                     [1m Learning iteration 775/2000 [0m                      

                       Computation: 42970 steps/s (collection: 2.187s, learning 0.101s)
             Mean action noise std: 2.68
          Mean value_function loss: 329.4770
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 74.3032
                       Mean reward: 223.17
               Mean episode length: 152.25
    Episode_Reward/reaching_object: 0.9096
     Episode_Reward/lifting_object: 47.5750
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 2.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 20.8333
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 2.29s
                      Time elapsed: 00:28:47
                               ETA: 00:45:27

################################################################################
                     [1m Learning iteration 776/2000 [0m                      

                       Computation: 43868 steps/s (collection: 2.144s, learning 0.097s)
             Mean action noise std: 2.68
          Mean value_function loss: 337.6059
               Mean surrogate loss: 0.0212
                 Mean entropy loss: 74.3057
                       Mean reward: 221.96
               Mean episode length: 140.99
    Episode_Reward/reaching_object: 0.8872
     Episode_Reward/lifting_object: 45.9394
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 24.0417
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 2.24s
                      Time elapsed: 00:28:49
                               ETA: 00:45:24

################################################################################
                     [1m Learning iteration 777/2000 [0m                      

                       Computation: 44073 steps/s (collection: 2.118s, learning 0.113s)
             Mean action noise std: 2.68
          Mean value_function loss: 329.6293
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 74.3061
                       Mean reward: 214.46
               Mean episode length: 154.46
    Episode_Reward/reaching_object: 0.8720
     Episode_Reward/lifting_object: 42.9334
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 24.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 2.23s
                      Time elapsed: 00:28:51
                               ETA: 00:45:22

################################################################################
                     [1m Learning iteration 778/2000 [0m                      

                       Computation: 45231 steps/s (collection: 2.085s, learning 0.088s)
             Mean action noise std: 2.68
          Mean value_function loss: 336.2002
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 74.3063
                       Mean reward: 278.35
               Mean episode length: 158.91
    Episode_Reward/reaching_object: 0.9073
     Episode_Reward/lifting_object: 47.8948
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 24.8750
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 2.17s
                      Time elapsed: 00:28:54
                               ETA: 00:45:20

################################################################################
                     [1m Learning iteration 779/2000 [0m                      

                       Computation: 44694 steps/s (collection: 2.108s, learning 0.091s)
             Mean action noise std: 2.68
          Mean value_function loss: 328.8488
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 74.3064
                       Mean reward: 278.70
               Mean episode length: 165.10
    Episode_Reward/reaching_object: 0.9164
     Episode_Reward/lifting_object: 48.4589
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 24.1250
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 2.20s
                      Time elapsed: 00:28:56
                               ETA: 00:45:18

################################################################################
                     [1m Learning iteration 780/2000 [0m                      

                       Computation: 45053 steps/s (collection: 2.089s, learning 0.093s)
             Mean action noise std: 2.68
          Mean value_function loss: 336.5298
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 74.3065
                       Mean reward: 233.47
               Mean episode length: 157.90
    Episode_Reward/reaching_object: 0.9010
     Episode_Reward/lifting_object: 46.5405
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 2.9167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 22.8750
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 2.18s
                      Time elapsed: 00:28:58
                               ETA: 00:45:15

################################################################################
                     [1m Learning iteration 781/2000 [0m                      

                       Computation: 44566 steps/s (collection: 2.111s, learning 0.095s)
             Mean action noise std: 2.68
          Mean value_function loss: 343.2736
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 74.3065
                       Mean reward: 275.75
               Mean episode length: 155.59
    Episode_Reward/reaching_object: 0.9077
     Episode_Reward/lifting_object: 49.0928
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 24.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 2.21s
                      Time elapsed: 00:29:00
                               ETA: 00:45:13

################################################################################
                     [1m Learning iteration 782/2000 [0m                      

                       Computation: 44724 steps/s (collection: 2.100s, learning 0.098s)
             Mean action noise std: 2.68
          Mean value_function loss: 348.6854
               Mean surrogate loss: 0.0112
                 Mean entropy loss: 74.3066
                       Mean reward: 230.86
               Mean episode length: 139.80
    Episode_Reward/reaching_object: 0.8798
     Episode_Reward/lifting_object: 46.8431
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 2.9583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 23.8333
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 2.20s
                      Time elapsed: 00:29:02
                               ETA: 00:45:11

################################################################################
                     [1m Learning iteration 783/2000 [0m                      

                       Computation: 43171 steps/s (collection: 2.157s, learning 0.120s)
             Mean action noise std: 2.68
          Mean value_function loss: 355.9070
               Mean surrogate loss: 0.0095
                 Mean entropy loss: 74.3067
                       Mean reward: 282.71
               Mean episode length: 143.75
    Episode_Reward/reaching_object: 0.9331
     Episode_Reward/lifting_object: 52.7387
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 2.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 23.3750
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 2.28s
                      Time elapsed: 00:29:05
                               ETA: 00:45:09

################################################################################
                     [1m Learning iteration 784/2000 [0m                      

                       Computation: 45037 steps/s (collection: 2.094s, learning 0.089s)
             Mean action noise std: 2.68
          Mean value_function loss: 347.5541
               Mean surrogate loss: 0.0106
                 Mean entropy loss: 74.3068
                       Mean reward: 246.55
               Mean episode length: 146.99
    Episode_Reward/reaching_object: 0.9262
     Episode_Reward/lifting_object: 51.3255
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 25.1667
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 2.18s
                      Time elapsed: 00:29:07
                               ETA: 00:45:06

################################################################################
                     [1m Learning iteration 785/2000 [0m                      

                       Computation: 43538 steps/s (collection: 2.091s, learning 0.167s)
             Mean action noise std: 2.68
          Mean value_function loss: 333.9225
               Mean surrogate loss: 0.0097
                 Mean entropy loss: 74.3069
                       Mean reward: 234.47
               Mean episode length: 151.19
    Episode_Reward/reaching_object: 0.9070
     Episode_Reward/lifting_object: 49.0811
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 22.5417
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 2.26s
                      Time elapsed: 00:29:09
                               ETA: 00:45:04

################################################################################
                     [1m Learning iteration 786/2000 [0m                      

                       Computation: 44841 steps/s (collection: 2.099s, learning 0.093s)
             Mean action noise std: 2.68
          Mean value_function loss: 354.0298
               Mean surrogate loss: 0.0091
                 Mean entropy loss: 74.3070
                       Mean reward: 315.73
               Mean episode length: 169.78
    Episode_Reward/reaching_object: 0.9287
     Episode_Reward/lifting_object: 52.2588
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 24.0417
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 2.19s
                      Time elapsed: 00:29:11
                               ETA: 00:45:02

################################################################################
                     [1m Learning iteration 787/2000 [0m                      

                       Computation: 45126 steps/s (collection: 2.088s, learning 0.090s)
             Mean action noise std: 2.68
          Mean value_function loss: 343.8300
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 74.3070
                       Mean reward: 272.37
               Mean episode length: 153.61
    Episode_Reward/reaching_object: 0.9505
     Episode_Reward/lifting_object: 52.9785
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 22.5833
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 2.18s
                      Time elapsed: 00:29:14
                               ETA: 00:45:00

################################################################################
                     [1m Learning iteration 788/2000 [0m                      

                       Computation: 44010 steps/s (collection: 2.125s, learning 0.109s)
             Mean action noise std: 2.68
          Mean value_function loss: 350.4977
               Mean surrogate loss: 0.0129
                 Mean entropy loss: 74.3072
                       Mean reward: 255.74
               Mean episode length: 147.10
    Episode_Reward/reaching_object: 0.9635
     Episode_Reward/lifting_object: 53.0130
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 22.1250
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 2.23s
                      Time elapsed: 00:29:16
                               ETA: 00:44:57

################################################################################
                     [1m Learning iteration 789/2000 [0m                      

                       Computation: 43958 steps/s (collection: 2.114s, learning 0.122s)
             Mean action noise std: 2.68
          Mean value_function loss: 347.8348
               Mean surrogate loss: 0.0098
                 Mean entropy loss: 74.3072
                       Mean reward: 269.45
               Mean episode length: 154.34
    Episode_Reward/reaching_object: 0.9501
     Episode_Reward/lifting_object: 52.5304
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 22.4583
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 2.24s
                      Time elapsed: 00:29:18
                               ETA: 00:44:55

################################################################################
                     [1m Learning iteration 790/2000 [0m                      

                       Computation: 43721 steps/s (collection: 2.138s, learning 0.111s)
             Mean action noise std: 2.68
          Mean value_function loss: 359.4023
               Mean surrogate loss: 0.0123
                 Mean entropy loss: 74.3073
                       Mean reward: 260.36
               Mean episode length: 147.85
    Episode_Reward/reaching_object: 0.9034
     Episode_Reward/lifting_object: 47.5097
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 24.5833
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 2.25s
                      Time elapsed: 00:29:20
                               ETA: 00:44:53

################################################################################
                     [1m Learning iteration 791/2000 [0m                      

                       Computation: 44658 steps/s (collection: 2.113s, learning 0.089s)
             Mean action noise std: 2.68
          Mean value_function loss: 353.2409
               Mean surrogate loss: 0.0105
                 Mean entropy loss: 74.3073
                       Mean reward: 234.24
               Mean episode length: 144.10
    Episode_Reward/reaching_object: 0.9137
     Episode_Reward/lifting_object: 48.1936
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 22.6250
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 2.20s
                      Time elapsed: 00:29:22
                               ETA: 00:44:51

################################################################################
                     [1m Learning iteration 792/2000 [0m                      

                       Computation: 44080 steps/s (collection: 2.126s, learning 0.104s)
             Mean action noise std: 2.68
          Mean value_function loss: 340.0916
               Mean surrogate loss: 0.0118
                 Mean entropy loss: 74.3074
                       Mean reward: 248.70
               Mean episode length: 153.48
    Episode_Reward/reaching_object: 0.9111
     Episode_Reward/lifting_object: 47.1127
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 4.1250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 23.4167
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 2.23s
                      Time elapsed: 00:29:25
                               ETA: 00:44:48

################################################################################
                     [1m Learning iteration 793/2000 [0m                      

                       Computation: 44962 steps/s (collection: 2.092s, learning 0.095s)
             Mean action noise std: 2.68
          Mean value_function loss: 376.1736
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 74.3075
                       Mean reward: 303.86
               Mean episode length: 160.53
    Episode_Reward/reaching_object: 0.9324
     Episode_Reward/lifting_object: 50.3490
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 22.1667
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 2.19s
                      Time elapsed: 00:29:27
                               ETA: 00:44:46

################################################################################
                     [1m Learning iteration 794/2000 [0m                      

                       Computation: 43130 steps/s (collection: 2.150s, learning 0.129s)
             Mean action noise std: 2.68
          Mean value_function loss: 358.8468
               Mean surrogate loss: 0.0082
                 Mean entropy loss: 74.3076
                       Mean reward: 226.47
               Mean episode length: 159.25
    Episode_Reward/reaching_object: 0.9474
     Episode_Reward/lifting_object: 52.0889
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 21.1667
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 2.28s
                      Time elapsed: 00:29:29
                               ETA: 00:44:44

################################################################################
                     [1m Learning iteration 795/2000 [0m                      

                       Computation: 44556 steps/s (collection: 2.100s, learning 0.106s)
             Mean action noise std: 2.68
          Mean value_function loss: 346.3233
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 74.3077
                       Mean reward: 292.29
               Mean episode length: 161.86
    Episode_Reward/reaching_object: 0.9729
     Episode_Reward/lifting_object: 54.5042
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 21.2500
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 2.21s
                      Time elapsed: 00:29:31
                               ETA: 00:44:42

################################################################################
                     [1m Learning iteration 796/2000 [0m                      

                       Computation: 44283 steps/s (collection: 2.125s, learning 0.095s)
             Mean action noise std: 2.68
          Mean value_function loss: 16715.4779
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 74.3078
                       Mean reward: 243.78
               Mean episode length: 152.89
    Episode_Reward/reaching_object: 0.9448
     Episode_Reward/lifting_object: 51.5475
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.1042
          Episode_Reward/joint_vel: -8.1937
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 21.4583
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 2.22s
                      Time elapsed: 00:29:34
                               ETA: 00:44:40

################################################################################
                     [1m Learning iteration 797/2000 [0m                      

                       Computation: 44711 steps/s (collection: 2.094s, learning 0.105s)
             Mean action noise std: 2.68
          Mean value_function loss: 359.1548
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 74.3081
                       Mean reward: 230.95
               Mean episode length: 148.93
    Episode_Reward/reaching_object: 0.9471
     Episode_Reward/lifting_object: 51.8564
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 25.2500
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 2.20s
                      Time elapsed: 00:29:36
                               ETA: 00:44:37

################################################################################
                     [1m Learning iteration 798/2000 [0m                      

                       Computation: 44591 steps/s (collection: 2.103s, learning 0.102s)
             Mean action noise std: 2.68
          Mean value_function loss: 350.0710
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 74.3112
                       Mean reward: 239.85
               Mean episode length: 148.06
    Episode_Reward/reaching_object: 0.9452
     Episode_Reward/lifting_object: 51.7097
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0690
      Episode_Termination/time_out: 3.8333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 20.6667
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 2.20s
                      Time elapsed: 00:29:38
                               ETA: 00:44:35

################################################################################
                     [1m Learning iteration 799/2000 [0m                      

                       Computation: 44316 steps/s (collection: 2.116s, learning 0.103s)
             Mean action noise std: 2.68
          Mean value_function loss: 336.2910
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 74.3165
                       Mean reward: 252.85
               Mean episode length: 161.76
    Episode_Reward/reaching_object: 0.9222
     Episode_Reward/lifting_object: 49.0261
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 21.5000
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 2.22s
                      Time elapsed: 00:29:40
                               ETA: 00:44:33

################################################################################
                     [1m Learning iteration 800/2000 [0m                      

                       Computation: 39995 steps/s (collection: 2.290s, learning 0.168s)
             Mean action noise std: 2.68
          Mean value_function loss: 386.3653
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 74.3204
                       Mean reward: 206.31
               Mean episode length: 146.82
    Episode_Reward/reaching_object: 0.9320
     Episode_Reward/lifting_object: 48.7314
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 21.3333
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 2.46s
                      Time elapsed: 00:29:43
                               ETA: 00:44:31

################################################################################
                     [1m Learning iteration 801/2000 [0m                      

                       Computation: 44480 steps/s (collection: 2.123s, learning 0.087s)
             Mean action noise std: 2.68
          Mean value_function loss: 356.5949
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 74.3236
                       Mean reward: 249.64
               Mean episode length: 152.84
    Episode_Reward/reaching_object: 0.9239
     Episode_Reward/lifting_object: 49.7910
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 19.5000
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 2.21s
                      Time elapsed: 00:29:45
                               ETA: 00:44:29

################################################################################
                     [1m Learning iteration 802/2000 [0m                      

                       Computation: 44689 steps/s (collection: 2.095s, learning 0.105s)
             Mean action noise std: 2.68
          Mean value_function loss: 337.6883
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.3274
                       Mean reward: 251.71
               Mean episode length: 164.21
    Episode_Reward/reaching_object: 0.9583
     Episode_Reward/lifting_object: 52.8131
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0702
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 19.0417
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 2.20s
                      Time elapsed: 00:29:47
                               ETA: 00:44:26

################################################################################
                     [1m Learning iteration 803/2000 [0m                      

                       Computation: 44079 steps/s (collection: 2.120s, learning 0.110s)
             Mean action noise std: 2.68
          Mean value_function loss: 428.2549
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 74.3316
                       Mean reward: 279.61
               Mean episode length: 163.97
    Episode_Reward/reaching_object: 0.9557
     Episode_Reward/lifting_object: 53.0865
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0692
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 21.3750
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 2.23s
                      Time elapsed: 00:29:49
                               ETA: 00:44:24

################################################################################
                     [1m Learning iteration 804/2000 [0m                      

                       Computation: 44086 steps/s (collection: 2.118s, learning 0.112s)
             Mean action noise std: 2.68
          Mean value_function loss: 390.5085
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 74.3341
                       Mean reward: 232.68
               Mean episode length: 150.44
    Episode_Reward/reaching_object: 0.9171
     Episode_Reward/lifting_object: 49.0844
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0693
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 19.7083
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 2.23s
                      Time elapsed: 00:29:52
                               ETA: 00:44:22

################################################################################
                     [1m Learning iteration 805/2000 [0m                      

                       Computation: 44641 steps/s (collection: 2.106s, learning 0.097s)
             Mean action noise std: 2.68
          Mean value_function loss: 379.4899
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 74.3373
                       Mean reward: 277.87
               Mean episode length: 158.51
    Episode_Reward/reaching_object: 0.9440
     Episode_Reward/lifting_object: 48.8748
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 4.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 19.9583
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 2.20s
                      Time elapsed: 00:29:54
                               ETA: 00:44:20

################################################################################
                     [1m Learning iteration 806/2000 [0m                      

                       Computation: 44571 steps/s (collection: 2.112s, learning 0.094s)
             Mean action noise std: 2.68
          Mean value_function loss: 380.6737
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 74.3411
                       Mean reward: 277.16
               Mean episode length: 173.62
    Episode_Reward/reaching_object: 0.9920
     Episode_Reward/lifting_object: 53.4171
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0725
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 17.7917
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 2.21s
                      Time elapsed: 00:29:56
                               ETA: 00:44:17

################################################################################
                     [1m Learning iteration 807/2000 [0m                      

                       Computation: 44291 steps/s (collection: 2.111s, learning 0.109s)
             Mean action noise std: 2.68
          Mean value_function loss: 344.5946
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 74.3427
                       Mean reward: 307.68
               Mean episode length: 177.50
    Episode_Reward/reaching_object: 0.9901
     Episode_Reward/lifting_object: 54.7474
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0719
      Episode_Termination/time_out: 5.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 19.6667
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 2.22s
                      Time elapsed: 00:29:58
                               ETA: 00:44:15

################################################################################
                     [1m Learning iteration 808/2000 [0m                      

                       Computation: 43335 steps/s (collection: 2.149s, learning 0.119s)
             Mean action noise std: 2.68
          Mean value_function loss: 367.5062
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 74.3441
                       Mean reward: 260.49
               Mean episode length: 161.61
    Episode_Reward/reaching_object: 0.9681
     Episode_Reward/lifting_object: 54.2743
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0703
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 19.9583
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 2.27s
                      Time elapsed: 00:30:00
                               ETA: 00:44:13

################################################################################
                     [1m Learning iteration 809/2000 [0m                      

                       Computation: 41855 steps/s (collection: 2.246s, learning 0.103s)
             Mean action noise std: 2.68
          Mean value_function loss: 372.1185
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.3459
                       Mean reward: 336.30
               Mean episode length: 176.13
    Episode_Reward/reaching_object: 0.9872
     Episode_Reward/lifting_object: 54.7311
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0705
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 20.7917
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 2.35s
                      Time elapsed: 00:30:03
                               ETA: 00:44:11

################################################################################
                     [1m Learning iteration 810/2000 [0m                      

                       Computation: 43348 steps/s (collection: 2.161s, learning 0.107s)
             Mean action noise std: 2.68
          Mean value_function loss: 403.8660
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 74.3496
                       Mean reward: 294.17
               Mean episode length: 172.73
    Episode_Reward/reaching_object: 0.9439
     Episode_Reward/lifting_object: 51.0659
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0702
      Episode_Termination/time_out: 5.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 19.7500
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 2.27s
                      Time elapsed: 00:30:05
                               ETA: 00:44:09

################################################################################
                     [1m Learning iteration 811/2000 [0m                      

                       Computation: 42502 steps/s (collection: 2.189s, learning 0.124s)
             Mean action noise std: 2.68
          Mean value_function loss: 379.1406
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 74.3520
                       Mean reward: 288.23
               Mean episode length: 172.98
    Episode_Reward/reaching_object: 0.9756
     Episode_Reward/lifting_object: 54.0029
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 18.9583
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 2.31s
                      Time elapsed: 00:30:07
                               ETA: 00:44:07

################################################################################
                     [1m Learning iteration 812/2000 [0m                      

                       Computation: 43193 steps/s (collection: 2.164s, learning 0.112s)
             Mean action noise std: 2.68
          Mean value_function loss: 379.7132
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 74.3528
                       Mean reward: 288.82
               Mean episode length: 161.48
    Episode_Reward/reaching_object: 1.0024
     Episode_Reward/lifting_object: 57.4451
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.7917
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 2.28s
                      Time elapsed: 00:30:10
                               ETA: 00:44:05

################################################################################
                     [1m Learning iteration 813/2000 [0m                      

                       Computation: 42309 steps/s (collection: 2.225s, learning 0.099s)
             Mean action noise std: 2.68
          Mean value_function loss: 369.4446
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 74.3536
                       Mean reward: 216.21
               Mean episode length: 150.70
    Episode_Reward/reaching_object: 1.0203
     Episode_Reward/lifting_object: 58.8347
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 20.2083
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 2.32s
                      Time elapsed: 00:30:12
                               ETA: 00:44:02

################################################################################
                     [1m Learning iteration 814/2000 [0m                      

                       Computation: 42868 steps/s (collection: 2.159s, learning 0.135s)
             Mean action noise std: 2.68
          Mean value_function loss: 404.8686
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 74.3555
                       Mean reward: 370.16
               Mean episode length: 174.43
    Episode_Reward/reaching_object: 1.0036
     Episode_Reward/lifting_object: 58.1661
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 5.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 21.4583
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 2.29s
                      Time elapsed: 00:30:14
                               ETA: 00:44:00

################################################################################
                     [1m Learning iteration 815/2000 [0m                      

                       Computation: 43001 steps/s (collection: 2.189s, learning 0.097s)
             Mean action noise std: 2.68
          Mean value_function loss: 474.0032
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 74.3587
                       Mean reward: 293.81
               Mean episode length: 145.86
    Episode_Reward/reaching_object: 1.0224
     Episode_Reward/lifting_object: 60.0912
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 21.6667
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 2.29s
                      Time elapsed: 00:30:17
                               ETA: 00:43:58

################################################################################
                     [1m Learning iteration 816/2000 [0m                      

                       Computation: 44296 steps/s (collection: 2.120s, learning 0.099s)
             Mean action noise std: 2.68
          Mean value_function loss: 391.5878
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 74.3602
                       Mean reward: 268.39
               Mean episode length: 158.76
    Episode_Reward/reaching_object: 0.9758
     Episode_Reward/lifting_object: 57.5557
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 19.3333
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 2.22s
                      Time elapsed: 00:30:19
                               ETA: 00:43:56

################################################################################
                     [1m Learning iteration 817/2000 [0m                      

                       Computation: 44304 steps/s (collection: 2.118s, learning 0.101s)
             Mean action noise std: 2.68
          Mean value_function loss: 421.9189
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 74.3620
                       Mean reward: 278.48
               Mean episode length: 154.76
    Episode_Reward/reaching_object: 0.9685
     Episode_Reward/lifting_object: 55.9612
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 21.2500
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 2.22s
                      Time elapsed: 00:30:21
                               ETA: 00:43:54

################################################################################
                     [1m Learning iteration 818/2000 [0m                      

                       Computation: 44511 steps/s (collection: 2.112s, learning 0.097s)
             Mean action noise std: 2.68
          Mean value_function loss: 412.4739
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 74.3624
                       Mean reward: 293.94
               Mean episode length: 161.23
    Episode_Reward/reaching_object: 0.9939
     Episode_Reward/lifting_object: 58.7548
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 5.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 19.1667
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 2.21s
                      Time elapsed: 00:30:23
                               ETA: 00:43:51

################################################################################
                     [1m Learning iteration 819/2000 [0m                      

                       Computation: 43254 steps/s (collection: 2.161s, learning 0.112s)
             Mean action noise std: 2.69
          Mean value_function loss: 432.5038
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 74.3627
                       Mean reward: 229.80
               Mean episode length: 152.84
    Episode_Reward/reaching_object: 0.9721
     Episode_Reward/lifting_object: 57.5547
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.1250
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 2.27s
                      Time elapsed: 00:30:25
                               ETA: 00:43:49

################################################################################
                     [1m Learning iteration 820/2000 [0m                      

                       Computation: 44071 steps/s (collection: 2.133s, learning 0.097s)
             Mean action noise std: 2.69
          Mean value_function loss: 394.1428
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 74.3636
                       Mean reward: 304.03
               Mean episode length: 152.81
    Episode_Reward/reaching_object: 0.9892
     Episode_Reward/lifting_object: 59.7737
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 5.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 19.6250
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 2.23s
                      Time elapsed: 00:30:28
                               ETA: 00:43:47

################################################################################
                     [1m Learning iteration 821/2000 [0m                      

                       Computation: 43643 steps/s (collection: 2.164s, learning 0.089s)
             Mean action noise std: 2.69
          Mean value_function loss: 426.9559
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 74.3652
                       Mean reward: 266.10
               Mean episode length: 154.84
    Episode_Reward/reaching_object: 0.9703
     Episode_Reward/lifting_object: 58.6814
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 21.5417
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 2.25s
                      Time elapsed: 00:30:30
                               ETA: 00:43:45

################################################################################
                     [1m Learning iteration 822/2000 [0m                      

                       Computation: 44459 steps/s (collection: 2.115s, learning 0.097s)
             Mean action noise std: 2.69
          Mean value_function loss: 425.8449
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 74.3667
                       Mean reward: 277.15
               Mean episode length: 147.37
    Episode_Reward/reaching_object: 0.9899
     Episode_Reward/lifting_object: 61.2105
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.2083
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 2.21s
                      Time elapsed: 00:30:32
                               ETA: 00:43:43

################################################################################
                     [1m Learning iteration 823/2000 [0m                      

                       Computation: 45123 steps/s (collection: 2.079s, learning 0.100s)
             Mean action noise std: 2.69
          Mean value_function loss: 425.4404
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 74.3675
                       Mean reward: 288.24
               Mean episode length: 167.48
    Episode_Reward/reaching_object: 1.0047
     Episode_Reward/lifting_object: 61.4562
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 5.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 20.9167
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 2.18s
                      Time elapsed: 00:30:34
                               ETA: 00:43:40

################################################################################
                     [1m Learning iteration 824/2000 [0m                      

                       Computation: 43907 steps/s (collection: 2.117s, learning 0.122s)
             Mean action noise std: 2.69
          Mean value_function loss: 447.9757
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 74.3680
                       Mean reward: 352.54
               Mean episode length: 171.14
    Episode_Reward/reaching_object: 1.0333
     Episode_Reward/lifting_object: 62.5129
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 5.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 22.4583
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 2.24s
                      Time elapsed: 00:30:37
                               ETA: 00:43:38

################################################################################
                     [1m Learning iteration 825/2000 [0m                      

                       Computation: 44296 steps/s (collection: 2.108s, learning 0.112s)
             Mean action noise std: 2.69
          Mean value_function loss: 470.2987
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 74.3690
                       Mean reward: 286.23
               Mean episode length: 141.64
    Episode_Reward/reaching_object: 1.0355
     Episode_Reward/lifting_object: 64.3635
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 20.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 2.22s
                      Time elapsed: 00:30:39
                               ETA: 00:43:36

################################################################################
                     [1m Learning iteration 826/2000 [0m                      

                       Computation: 44021 steps/s (collection: 2.117s, learning 0.117s)
             Mean action noise std: 2.69
          Mean value_function loss: 468.0490
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 74.3718
                       Mean reward: 324.19
               Mean episode length: 154.97
    Episode_Reward/reaching_object: 1.0182
     Episode_Reward/lifting_object: 63.9676
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 21.2083
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 2.23s
                      Time elapsed: 00:30:41
                               ETA: 00:43:34

################################################################################
                     [1m Learning iteration 827/2000 [0m                      

                       Computation: 44770 steps/s (collection: 2.103s, learning 0.093s)
             Mean action noise std: 2.69
          Mean value_function loss: 440.7900
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 74.3754
                       Mean reward: 342.47
               Mean episode length: 164.97
    Episode_Reward/reaching_object: 0.9855
     Episode_Reward/lifting_object: 58.2866
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.1667
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 2.20s
                      Time elapsed: 00:30:43
                               ETA: 00:43:31

################################################################################
                     [1m Learning iteration 828/2000 [0m                      

                       Computation: 43715 steps/s (collection: 2.136s, learning 0.113s)
             Mean action noise std: 2.69
          Mean value_function loss: 406.2759
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 74.3785
                       Mean reward: 295.15
               Mean episode length: 158.33
    Episode_Reward/reaching_object: 1.0057
     Episode_Reward/lifting_object: 61.4074
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 18.4167
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 2.25s
                      Time elapsed: 00:30:45
                               ETA: 00:43:29

################################################################################
                     [1m Learning iteration 829/2000 [0m                      

                       Computation: 44574 steps/s (collection: 2.111s, learning 0.095s)
             Mean action noise std: 2.69
          Mean value_function loss: 406.0158
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 74.3808
                       Mean reward: 321.41
               Mean episode length: 161.36
    Episode_Reward/reaching_object: 1.0626
     Episode_Reward/lifting_object: 64.3244
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 19.1667
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 2.21s
                      Time elapsed: 00:30:48
                               ETA: 00:43:27

################################################################################
                     [1m Learning iteration 830/2000 [0m                      

                       Computation: 44405 steps/s (collection: 2.118s, learning 0.096s)
             Mean action noise std: 2.69
          Mean value_function loss: 393.1408
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 74.3814
                       Mean reward: 327.21
               Mean episode length: 161.24
    Episode_Reward/reaching_object: 1.0666
     Episode_Reward/lifting_object: 65.4257
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0714
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 19.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 2.21s
                      Time elapsed: 00:30:50
                               ETA: 00:43:25

################################################################################
                     [1m Learning iteration 831/2000 [0m                      

                       Computation: 44091 steps/s (collection: 2.132s, learning 0.098s)
             Mean action noise std: 2.69
          Mean value_function loss: 428.6024
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 74.3817
                       Mean reward: 356.00
               Mean episode length: 164.14
    Episode_Reward/reaching_object: 1.0137
     Episode_Reward/lifting_object: 62.7107
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 21.3750
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 2.23s
                      Time elapsed: 00:30:52
                               ETA: 00:43:23

################################################################################
                     [1m Learning iteration 832/2000 [0m                      

                       Computation: 44815 steps/s (collection: 2.103s, learning 0.090s)
             Mean action noise std: 2.69
          Mean value_function loss: 394.7133
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 74.3819
                       Mean reward: 248.94
               Mean episode length: 148.32
    Episode_Reward/reaching_object: 0.9676
     Episode_Reward/lifting_object: 57.5532
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 19.5833
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 2.19s
                      Time elapsed: 00:30:54
                               ETA: 00:43:20

################################################################################
                     [1m Learning iteration 833/2000 [0m                      

                       Computation: 44836 steps/s (collection: 2.095s, learning 0.097s)
             Mean action noise std: 2.69
          Mean value_function loss: 411.9268
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 74.3820
                       Mean reward: 335.23
               Mean episode length: 160.92
    Episode_Reward/reaching_object: 1.0609
     Episode_Reward/lifting_object: 66.4311
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 5.3750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 20.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 2.19s
                      Time elapsed: 00:30:56
                               ETA: 00:43:18

################################################################################
                     [1m Learning iteration 834/2000 [0m                      

                       Computation: 44367 steps/s (collection: 2.123s, learning 0.093s)
             Mean action noise std: 2.69
          Mean value_function loss: 412.7592
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 74.3820
                       Mean reward: 337.99
               Mean episode length: 168.00
    Episode_Reward/reaching_object: 1.0642
     Episode_Reward/lifting_object: 66.2061
      Episode_Reward/object_height: 0.0075
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 19.6667
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 2.22s
                      Time elapsed: 00:30:59
                               ETA: 00:43:16

################################################################################
                     [1m Learning iteration 835/2000 [0m                      

                       Computation: 44646 steps/s (collection: 2.107s, learning 0.095s)
             Mean action noise std: 2.69
          Mean value_function loss: 415.8706
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 74.3821
                       Mean reward: 396.36
               Mean episode length: 170.11
    Episode_Reward/reaching_object: 1.0808
     Episode_Reward/lifting_object: 69.7449
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 19.2500
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 2.20s
                      Time elapsed: 00:31:01
                               ETA: 00:43:13

################################################################################
                     [1m Learning iteration 836/2000 [0m                      

                       Computation: 44244 steps/s (collection: 2.113s, learning 0.108s)
             Mean action noise std: 2.69
          Mean value_function loss: 454.7758
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 74.3822
                       Mean reward: 352.44
               Mean episode length: 155.77
    Episode_Reward/reaching_object: 1.0492
     Episode_Reward/lifting_object: 67.4466
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 22.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 2.22s
                      Time elapsed: 00:31:03
                               ETA: 00:43:11

################################################################################
                     [1m Learning iteration 837/2000 [0m                      

                       Computation: 43703 steps/s (collection: 2.136s, learning 0.113s)
             Mean action noise std: 2.69
          Mean value_function loss: 427.4832
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 74.3824
                       Mean reward: 354.60
               Mean episode length: 151.20
    Episode_Reward/reaching_object: 1.0625
     Episode_Reward/lifting_object: 68.0359
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 19.4583
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 2.25s
                      Time elapsed: 00:31:05
                               ETA: 00:43:09

################################################################################
                     [1m Learning iteration 838/2000 [0m                      

                       Computation: 43447 steps/s (collection: 2.148s, learning 0.115s)
             Mean action noise std: 2.69
          Mean value_function loss: 412.7461
               Mean surrogate loss: 0.0105
                 Mean entropy loss: 74.3826
                       Mean reward: 390.50
               Mean episode length: 159.42
    Episode_Reward/reaching_object: 1.0128
     Episode_Reward/lifting_object: 62.0192
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 20.6250
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 2.26s
                      Time elapsed: 00:31:08
                               ETA: 00:43:07

################################################################################
                     [1m Learning iteration 839/2000 [0m                      

                       Computation: 43877 steps/s (collection: 2.131s, learning 0.110s)
             Mean action noise std: 2.69
          Mean value_function loss: 432.9347
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 74.3827
                       Mean reward: 319.10
               Mean episode length: 164.03
    Episode_Reward/reaching_object: 1.0307
     Episode_Reward/lifting_object: 64.7227
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 21.5000
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 2.24s
                      Time elapsed: 00:31:10
                               ETA: 00:43:05

################################################################################
                     [1m Learning iteration 840/2000 [0m                      

                       Computation: 44413 steps/s (collection: 2.107s, learning 0.107s)
             Mean action noise std: 2.69
          Mean value_function loss: 433.0228
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 74.3828
                       Mean reward: 365.38
               Mean episode length: 156.80
    Episode_Reward/reaching_object: 1.0404
     Episode_Reward/lifting_object: 65.8284
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 5.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 20.7917
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 2.21s
                      Time elapsed: 00:31:12
                               ETA: 00:43:02

################################################################################
                     [1m Learning iteration 841/2000 [0m                      

                       Computation: 44521 steps/s (collection: 2.115s, learning 0.093s)
             Mean action noise std: 2.69
          Mean value_function loss: 416.1373
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 74.3836
                       Mean reward: 347.63
               Mean episode length: 168.11
    Episode_Reward/reaching_object: 1.0576
     Episode_Reward/lifting_object: 68.6723
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 20.0417
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 2.21s
                      Time elapsed: 00:31:14
                               ETA: 00:43:00

################################################################################
                     [1m Learning iteration 842/2000 [0m                      

                       Computation: 43694 steps/s (collection: 2.127s, learning 0.123s)
             Mean action noise std: 2.69
          Mean value_function loss: 437.2348
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 74.3860
                       Mean reward: 354.44
               Mean episode length: 163.61
    Episode_Reward/reaching_object: 1.0387
     Episode_Reward/lifting_object: 63.9151
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 19.3750
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 2.25s
                      Time elapsed: 00:31:17
                               ETA: 00:42:58

################################################################################
                     [1m Learning iteration 843/2000 [0m                      

                       Computation: 44635 steps/s (collection: 2.108s, learning 0.095s)
             Mean action noise std: 2.69
          Mean value_function loss: 443.3165
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 74.3879
                       Mean reward: 413.33
               Mean episode length: 164.93
    Episode_Reward/reaching_object: 1.0817
     Episode_Reward/lifting_object: 69.4741
      Episode_Reward/object_height: 0.0075
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 19.2917
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 2.20s
                      Time elapsed: 00:31:19
                               ETA: 00:42:56

################################################################################
                     [1m Learning iteration 844/2000 [0m                      

                       Computation: 44262 steps/s (collection: 2.124s, learning 0.097s)
             Mean action noise std: 2.69
          Mean value_function loss: 468.7683
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 74.3905
                       Mean reward: 370.33
               Mean episode length: 164.61
    Episode_Reward/reaching_object: 1.0644
     Episode_Reward/lifting_object: 68.3510
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 20.6667
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 2.22s
                      Time elapsed: 00:31:21
                               ETA: 00:42:53

################################################################################
                     [1m Learning iteration 845/2000 [0m                      

                       Computation: 43877 steps/s (collection: 2.138s, learning 0.102s)
             Mean action noise std: 2.69
          Mean value_function loss: 442.4157
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 74.3913
                       Mean reward: 357.23
               Mean episode length: 167.58
    Episode_Reward/reaching_object: 1.1079
     Episode_Reward/lifting_object: 72.0036
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 19.5417
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 2.24s
                      Time elapsed: 00:31:23
                               ETA: 00:42:51

################################################################################
                     [1m Learning iteration 846/2000 [0m                      

                       Computation: 43518 steps/s (collection: 2.149s, learning 0.110s)
             Mean action noise std: 2.69
          Mean value_function loss: 439.2656
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 74.3915
                       Mean reward: 329.62
               Mean episode length: 162.40
    Episode_Reward/reaching_object: 1.0782
     Episode_Reward/lifting_object: 69.1944
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.5000
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 2.26s
                      Time elapsed: 00:31:25
                               ETA: 00:42:49

################################################################################
                     [1m Learning iteration 847/2000 [0m                      

                       Computation: 43592 steps/s (collection: 2.143s, learning 0.112s)
             Mean action noise std: 2.69
          Mean value_function loss: 434.7730
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 74.3916
                       Mean reward: 347.45
               Mean episode length: 173.16
    Episode_Reward/reaching_object: 1.0911
     Episode_Reward/lifting_object: 71.3369
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 19.7083
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 2.26s
                      Time elapsed: 00:31:28
                               ETA: 00:42:47

################################################################################
                     [1m Learning iteration 848/2000 [0m                      

                       Computation: 43958 steps/s (collection: 2.121s, learning 0.116s)
             Mean action noise std: 2.69
          Mean value_function loss: 448.9362
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 74.3917
                       Mean reward: 379.29
               Mean episode length: 162.08
    Episode_Reward/reaching_object: 1.0391
     Episode_Reward/lifting_object: 66.8775
      Episode_Reward/object_height: 0.0075
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 20.3333
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 2.24s
                      Time elapsed: 00:31:30
                               ETA: 00:42:45

################################################################################
                     [1m Learning iteration 849/2000 [0m                      

                       Computation: 43850 steps/s (collection: 2.141s, learning 0.101s)
             Mean action noise std: 2.69
          Mean value_function loss: 417.6906
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 74.3918
                       Mean reward: 322.04
               Mean episode length: 161.62
    Episode_Reward/reaching_object: 1.0240
     Episode_Reward/lifting_object: 64.9644
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 19.3750
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 2.24s
                      Time elapsed: 00:31:32
                               ETA: 00:42:42

################################################################################
                     [1m Learning iteration 850/2000 [0m                      

                       Computation: 44014 steps/s (collection: 2.137s, learning 0.097s)
             Mean action noise std: 2.69
          Mean value_function loss: 436.3877
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 74.3919
                       Mean reward: 328.75
               Mean episode length: 168.52
    Episode_Reward/reaching_object: 1.0349
     Episode_Reward/lifting_object: 65.4811
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 19.4583
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 2.23s
                      Time elapsed: 00:31:34
                               ETA: 00:42:40

################################################################################
                     [1m Learning iteration 851/2000 [0m                      

                       Computation: 43839 steps/s (collection: 2.150s, learning 0.093s)
             Mean action noise std: 2.69
          Mean value_function loss: 434.2561
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 74.3921
                       Mean reward: 307.39
               Mean episode length: 153.21
    Episode_Reward/reaching_object: 1.0702
     Episode_Reward/lifting_object: 68.9489
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 19.8333
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 2.24s
                      Time elapsed: 00:31:37
                               ETA: 00:42:38

################################################################################
                     [1m Learning iteration 852/2000 [0m                      

                       Computation: 43805 steps/s (collection: 2.125s, learning 0.120s)
             Mean action noise std: 2.69
          Mean value_function loss: 423.7071
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 74.3922
                       Mean reward: 326.49
               Mean episode length: 162.64
    Episode_Reward/reaching_object: 1.1030
     Episode_Reward/lifting_object: 72.1517
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.2500
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 2.24s
                      Time elapsed: 00:31:39
                               ETA: 00:42:36

################################################################################
                     [1m Learning iteration 853/2000 [0m                      

                       Computation: 43943 steps/s (collection: 2.122s, learning 0.115s)
             Mean action noise std: 2.69
          Mean value_function loss: 429.0887
               Mean surrogate loss: 0.0102
                 Mean entropy loss: 74.3923
                       Mean reward: 324.03
               Mean episode length: 162.50
    Episode_Reward/reaching_object: 1.0328
     Episode_Reward/lifting_object: 64.8261
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 18.8333
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 2.24s
                      Time elapsed: 00:31:41
                               ETA: 00:42:34

################################################################################
                     [1m Learning iteration 854/2000 [0m                      

                       Computation: 43886 steps/s (collection: 2.137s, learning 0.103s)
             Mean action noise std: 2.69
          Mean value_function loss: 433.0371
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 74.3924
                       Mean reward: 373.94
               Mean episode length: 160.81
    Episode_Reward/reaching_object: 1.0788
     Episode_Reward/lifting_object: 70.5932
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 20.0417
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 2.24s
                      Time elapsed: 00:31:43
                               ETA: 00:42:31

################################################################################
                     [1m Learning iteration 855/2000 [0m                      

                       Computation: 44090 steps/s (collection: 2.129s, learning 0.100s)
             Mean action noise std: 2.69
          Mean value_function loss: 438.5823
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 74.3925
                       Mean reward: 317.26
               Mean episode length: 167.85
    Episode_Reward/reaching_object: 1.0600
     Episode_Reward/lifting_object: 69.0645
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 19.6250
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 2.23s
                      Time elapsed: 00:31:46
                               ETA: 00:42:29

################################################################################
                     [1m Learning iteration 856/2000 [0m                      

                       Computation: 44084 steps/s (collection: 2.129s, learning 0.101s)
             Mean action noise std: 2.69
          Mean value_function loss: 423.1697
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 74.3925
                       Mean reward: 358.07
               Mean episode length: 165.87
    Episode_Reward/reaching_object: 1.0951
     Episode_Reward/lifting_object: 73.0236
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.4167
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 2.23s
                      Time elapsed: 00:31:48
                               ETA: 00:42:27

################################################################################
                     [1m Learning iteration 857/2000 [0m                      

                       Computation: 43787 steps/s (collection: 2.142s, learning 0.103s)
             Mean action noise std: 2.69
          Mean value_function loss: 420.9123
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 74.3926
                       Mean reward: 337.20
               Mean episode length: 157.40
    Episode_Reward/reaching_object: 1.0816
     Episode_Reward/lifting_object: 70.3281
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 18.3333
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 2.25s
                      Time elapsed: 00:31:50
                               ETA: 00:42:25

################################################################################
                     [1m Learning iteration 858/2000 [0m                      

                       Computation: 43901 steps/s (collection: 2.131s, learning 0.108s)
             Mean action noise std: 2.69
          Mean value_function loss: 442.3146
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 74.3927
                       Mean reward: 367.36
               Mean episode length: 163.60
    Episode_Reward/reaching_object: 1.0584
     Episode_Reward/lifting_object: 67.8142
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 5.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.8333
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 2.24s
                      Time elapsed: 00:31:52
                               ETA: 00:42:23

################################################################################
                     [1m Learning iteration 859/2000 [0m                      

                       Computation: 43647 steps/s (collection: 2.149s, learning 0.103s)
             Mean action noise std: 2.69
          Mean value_function loss: 393.3713
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 74.3928
                       Mean reward: 334.91
               Mean episode length: 166.86
    Episode_Reward/reaching_object: 1.1214
     Episode_Reward/lifting_object: 74.0761
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 17.4167
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 2.25s
                      Time elapsed: 00:31:55
                               ETA: 00:42:20

################################################################################
                     [1m Learning iteration 860/2000 [0m                      

                       Computation: 43390 steps/s (collection: 2.138s, learning 0.127s)
             Mean action noise std: 2.69
          Mean value_function loss: 434.5259
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 74.3928
                       Mean reward: 302.68
               Mean episode length: 160.76
    Episode_Reward/reaching_object: 1.0632
     Episode_Reward/lifting_object: 68.9632
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 19.8750
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 2.27s
                      Time elapsed: 00:31:57
                               ETA: 00:42:18

################################################################################
                     [1m Learning iteration 861/2000 [0m                      

                       Computation: 44273 steps/s (collection: 2.127s, learning 0.093s)
             Mean action noise std: 2.69
          Mean value_function loss: 443.3448
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 74.3930
                       Mean reward: 360.11
               Mean episode length: 173.73
    Episode_Reward/reaching_object: 1.0736
     Episode_Reward/lifting_object: 68.7667
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 19.6667
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 2.22s
                      Time elapsed: 00:31:59
                               ETA: 00:42:16

################################################################################
                     [1m Learning iteration 862/2000 [0m                      

                       Computation: 44004 steps/s (collection: 2.137s, learning 0.097s)
             Mean action noise std: 2.69
          Mean value_function loss: 454.8484
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 74.3933
                       Mean reward: 409.14
               Mean episode length: 165.84
    Episode_Reward/reaching_object: 1.0669
     Episode_Reward/lifting_object: 68.7112
      Episode_Reward/object_height: 0.0075
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 19.8333
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 2.23s
                      Time elapsed: 00:32:01
                               ETA: 00:42:14

################################################################################
                     [1m Learning iteration 863/2000 [0m                      

                       Computation: 43596 steps/s (collection: 2.142s, learning 0.113s)
             Mean action noise std: 2.69
          Mean value_function loss: 420.4665
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 74.3938
                       Mean reward: 396.21
               Mean episode length: 167.37
    Episode_Reward/reaching_object: 1.0903
     Episode_Reward/lifting_object: 71.5701
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 5.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 18.2500
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 2.25s
                      Time elapsed: 00:32:04
                               ETA: 00:42:12

################################################################################
                     [1m Learning iteration 864/2000 [0m                      

                       Computation: 43605 steps/s (collection: 2.129s, learning 0.126s)
             Mean action noise std: 2.69
          Mean value_function loss: 418.7119
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 74.3941
                       Mean reward: 276.73
               Mean episode length: 153.42
    Episode_Reward/reaching_object: 1.1027
     Episode_Reward/lifting_object: 71.3864
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.3750
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 2.25s
                      Time elapsed: 00:32:06
                               ETA: 00:42:09

################################################################################
                     [1m Learning iteration 865/2000 [0m                      

                       Computation: 43491 steps/s (collection: 2.155s, learning 0.106s)
             Mean action noise std: 2.69
          Mean value_function loss: 387.2062
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 74.3945
                       Mean reward: 327.49
               Mean episode length: 167.14
    Episode_Reward/reaching_object: 1.0537
     Episode_Reward/lifting_object: 67.8153
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 16.7917
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 2.26s
                      Time elapsed: 00:32:08
                               ETA: 00:42:07

################################################################################
                     [1m Learning iteration 866/2000 [0m                      

                       Computation: 44222 steps/s (collection: 2.126s, learning 0.097s)
             Mean action noise std: 2.69
          Mean value_function loss: 460.1795
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 74.3965
                       Mean reward: 422.42
               Mean episode length: 164.96
    Episode_Reward/reaching_object: 1.0626
     Episode_Reward/lifting_object: 68.4014
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.8333
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 2.22s
                      Time elapsed: 00:32:10
                               ETA: 00:42:05

################################################################################
                     [1m Learning iteration 867/2000 [0m                      

                       Computation: 43956 steps/s (collection: 2.132s, learning 0.105s)
             Mean action noise std: 2.69
          Mean value_function loss: 512.6908
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 74.3987
                       Mean reward: 425.38
               Mean episode length: 178.20
    Episode_Reward/reaching_object: 1.1121
     Episode_Reward/lifting_object: 73.6107
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.9583
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 2.24s
                      Time elapsed: 00:32:13
                               ETA: 00:42:03

################################################################################
                     [1m Learning iteration 868/2000 [0m                      

                       Computation: 43265 steps/s (collection: 2.166s, learning 0.106s)
             Mean action noise std: 2.69
          Mean value_function loss: 495.3856
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 74.4039
                       Mean reward: 382.54
               Mean episode length: 169.30
    Episode_Reward/reaching_object: 1.0780
     Episode_Reward/lifting_object: 69.7119
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 17.7500
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 2.27s
                      Time elapsed: 00:32:15
                               ETA: 00:42:01

################################################################################
                     [1m Learning iteration 869/2000 [0m                      

                       Computation: 42999 steps/s (collection: 2.175s, learning 0.112s)
             Mean action noise std: 2.69
          Mean value_function loss: 442.5604
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 74.4063
                       Mean reward: 347.03
               Mean episode length: 164.43
    Episode_Reward/reaching_object: 1.0349
     Episode_Reward/lifting_object: 66.5374
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 18.0417
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 2.29s
                      Time elapsed: 00:32:17
                               ETA: 00:41:58

################################################################################
                     [1m Learning iteration 870/2000 [0m                      

                       Computation: 44013 steps/s (collection: 2.116s, learning 0.117s)
             Mean action noise std: 2.69
          Mean value_function loss: 438.7798
               Mean surrogate loss: 0.0131
                 Mean entropy loss: 74.4073
                       Mean reward: 329.35
               Mean episode length: 169.28
    Episode_Reward/reaching_object: 1.0637
     Episode_Reward/lifting_object: 67.8578
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 17.3333
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 2.23s
                      Time elapsed: 00:32:19
                               ETA: 00:41:56

################################################################################
                     [1m Learning iteration 871/2000 [0m                      

                       Computation: 42855 steps/s (collection: 2.177s, learning 0.117s)
             Mean action noise std: 2.69
          Mean value_function loss: 419.1017
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 74.4075
                       Mean reward: 438.22
               Mean episode length: 179.95
    Episode_Reward/reaching_object: 1.1311
     Episode_Reward/lifting_object: 75.1219
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0699
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 17.1667
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 2.29s
                      Time elapsed: 00:32:22
                               ETA: 00:41:54

################################################################################
                     [1m Learning iteration 872/2000 [0m                      

                       Computation: 43321 steps/s (collection: 2.147s, learning 0.123s)
             Mean action noise std: 2.69
          Mean value_function loss: 420.3276
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 74.4080
                       Mean reward: 387.60
               Mean episode length: 171.62
    Episode_Reward/reaching_object: 1.1663
     Episode_Reward/lifting_object: 78.4614
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0707
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 17.0833
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 2.27s
                      Time elapsed: 00:32:24
                               ETA: 00:41:52

################################################################################
                     [1m Learning iteration 873/2000 [0m                      

                       Computation: 42083 steps/s (collection: 2.227s, learning 0.109s)
             Mean action noise std: 2.69
          Mean value_function loss: 406.7023
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 74.4088
                       Mean reward: 336.55
               Mean episode length: 171.24
    Episode_Reward/reaching_object: 1.1380
     Episode_Reward/lifting_object: 75.4416
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0718
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 2.34s
                      Time elapsed: 00:32:26
                               ETA: 00:41:50

################################################################################
                     [1m Learning iteration 874/2000 [0m                      

                       Computation: 42980 steps/s (collection: 2.167s, learning 0.121s)
             Mean action noise std: 2.69
          Mean value_function loss: 406.3979
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 74.4095
                       Mean reward: 356.05
               Mean episode length: 173.10
    Episode_Reward/reaching_object: 1.1027
     Episode_Reward/lifting_object: 71.2503
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0705
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.2500
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 2.29s
                      Time elapsed: 00:32:29
                               ETA: 00:41:48

################################################################################
                     [1m Learning iteration 875/2000 [0m                      

                       Computation: 43228 steps/s (collection: 2.142s, learning 0.132s)
             Mean action noise std: 2.69
          Mean value_function loss: 424.8907
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 74.4102
                       Mean reward: 352.82
               Mean episode length: 167.16
    Episode_Reward/reaching_object: 1.1238
     Episode_Reward/lifting_object: 73.8375
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0714
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 16.2083
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 2.27s
                      Time elapsed: 00:32:31
                               ETA: 00:41:45

################################################################################
                     [1m Learning iteration 876/2000 [0m                      

                       Computation: 42700 steps/s (collection: 2.188s, learning 0.115s)
             Mean action noise std: 2.69
          Mean value_function loss: 436.5883
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 74.4105
                       Mean reward: 345.61
               Mean episode length: 172.34
    Episode_Reward/reaching_object: 1.0793
     Episode_Reward/lifting_object: 67.8692
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0715
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 17.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 2.30s
                      Time elapsed: 00:32:33
                               ETA: 00:41:43

################################################################################
                     [1m Learning iteration 877/2000 [0m                      

                       Computation: 43277 steps/s (collection: 2.172s, learning 0.100s)
             Mean action noise std: 2.69
          Mean value_function loss: 460.8835
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 74.4112
                       Mean reward: 364.05
               Mean episode length: 177.89
    Episode_Reward/reaching_object: 1.0347
     Episode_Reward/lifting_object: 66.2431
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0693
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.0833
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 2.27s
                      Time elapsed: 00:32:35
                               ETA: 00:41:41

################################################################################
                     [1m Learning iteration 878/2000 [0m                      

                       Computation: 43025 steps/s (collection: 2.170s, learning 0.115s)
             Mean action noise std: 2.69
          Mean value_function loss: 414.1658
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 74.4121
                       Mean reward: 344.54
               Mean episode length: 177.32
    Episode_Reward/reaching_object: 1.0380
     Episode_Reward/lifting_object: 63.5404
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.2500
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 2.28s
                      Time elapsed: 00:32:38
                               ETA: 00:41:39

################################################################################
                     [1m Learning iteration 879/2000 [0m                      

                       Computation: 42947 steps/s (collection: 2.189s, learning 0.100s)
             Mean action noise std: 2.69
          Mean value_function loss: 406.4046
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 74.4125
                       Mean reward: 271.95
               Mean episode length: 170.34
    Episode_Reward/reaching_object: 0.9978
     Episode_Reward/lifting_object: 59.4349
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0716
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.9167
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 2.29s
                      Time elapsed: 00:32:40
                               ETA: 00:41:37

################################################################################
                     [1m Learning iteration 880/2000 [0m                      

                       Computation: 42904 steps/s (collection: 2.172s, learning 0.119s)
             Mean action noise std: 2.69
          Mean value_function loss: 384.4077
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 74.4133
                       Mean reward: 268.31
               Mean episode length: 165.98
    Episode_Reward/reaching_object: 0.9947
     Episode_Reward/lifting_object: 59.7891
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0709
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 16.5417
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 2.29s
                      Time elapsed: 00:32:42
                               ETA: 00:41:35

################################################################################
                     [1m Learning iteration 881/2000 [0m                      

                       Computation: 43189 steps/s (collection: 2.146s, learning 0.130s)
             Mean action noise std: 2.69
          Mean value_function loss: 367.9849
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 74.4138
                       Mean reward: 278.04
               Mean episode length: 166.76
    Episode_Reward/reaching_object: 0.9640
     Episode_Reward/lifting_object: 54.7455
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0721
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.2917
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 2.28s
                      Time elapsed: 00:32:45
                               ETA: 00:41:33

################################################################################
                     [1m Learning iteration 882/2000 [0m                      

                       Computation: 42979 steps/s (collection: 2.173s, learning 0.114s)
             Mean action noise std: 2.69
          Mean value_function loss: 382.4993
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 74.4144
                       Mean reward: 264.13
               Mean episode length: 162.55
    Episode_Reward/reaching_object: 0.9886
     Episode_Reward/lifting_object: 57.1918
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0732
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.7083
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 2.29s
                      Time elapsed: 00:32:47
                               ETA: 00:41:30

################################################################################
                     [1m Learning iteration 883/2000 [0m                      

                       Computation: 43786 steps/s (collection: 2.134s, learning 0.111s)
             Mean action noise std: 2.69
          Mean value_function loss: 414.4875
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 74.4159
                       Mean reward: 273.03
               Mean episode length: 173.80
    Episode_Reward/reaching_object: 0.9404
     Episode_Reward/lifting_object: 52.4873
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 18.5417
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 2.25s
                      Time elapsed: 00:32:49
                               ETA: 00:41:28

################################################################################
                     [1m Learning iteration 884/2000 [0m                      

                       Computation: 44061 steps/s (collection: 2.139s, learning 0.092s)
             Mean action noise std: 2.69
          Mean value_function loss: 457.0156
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 74.4190
                       Mean reward: 289.84
               Mean episode length: 159.87
    Episode_Reward/reaching_object: 0.9129
     Episode_Reward/lifting_object: 51.5505
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0693
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 19.5000
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 2.23s
                      Time elapsed: 00:32:51
                               ETA: 00:41:26

################################################################################
                     [1m Learning iteration 885/2000 [0m                      

                       Computation: 43642 steps/s (collection: 2.153s, learning 0.100s)
             Mean action noise std: 2.69
          Mean value_function loss: 444.0144
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 74.4209
                       Mean reward: 278.74
               Mean episode length: 162.71
    Episode_Reward/reaching_object: 0.9873
     Episode_Reward/lifting_object: 59.1125
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0701
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 19.4583
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 2.25s
                      Time elapsed: 00:32:54
                               ETA: 00:41:24

################################################################################
                     [1m Learning iteration 886/2000 [0m                      

                       Computation: 42759 steps/s (collection: 2.178s, learning 0.121s)
             Mean action noise std: 2.69
          Mean value_function loss: 456.4290
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 74.4215
                       Mean reward: 359.73
               Mean episode length: 155.24
    Episode_Reward/reaching_object: 0.9624
     Episode_Reward/lifting_object: 57.6626
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0693
      Episode_Termination/time_out: 5.1250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 22.2917
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 2.30s
                      Time elapsed: 00:32:56
                               ETA: 00:41:22

################################################################################
                     [1m Learning iteration 887/2000 [0m                      

                       Computation: 43415 steps/s (collection: 2.132s, learning 0.133s)
             Mean action noise std: 2.69
          Mean value_function loss: 465.9423
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 74.4225
                       Mean reward: 372.03
               Mean episode length: 162.18
    Episode_Reward/reaching_object: 1.0705
     Episode_Reward/lifting_object: 68.8147
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 19.2083
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 2.26s
                      Time elapsed: 00:32:58
                               ETA: 00:41:19

################################################################################
                     [1m Learning iteration 888/2000 [0m                      

                       Computation: 44119 steps/s (collection: 2.133s, learning 0.095s)
             Mean action noise std: 2.69
          Mean value_function loss: 456.2257
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 74.4235
                       Mean reward: 356.67
               Mean episode length: 165.65
    Episode_Reward/reaching_object: 1.0819
     Episode_Reward/lifting_object: 70.6788
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 5.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.2917
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 2.23s
                      Time elapsed: 00:33:00
                               ETA: 00:41:17

################################################################################
                     [1m Learning iteration 889/2000 [0m                      

                       Computation: 44268 steps/s (collection: 2.129s, learning 0.092s)
             Mean action noise std: 2.69
          Mean value_function loss: 429.9173
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 74.4245
                       Mean reward: 366.98
               Mean episode length: 161.54
    Episode_Reward/reaching_object: 1.0446
     Episode_Reward/lifting_object: 66.8911
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 17.7083
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 2.22s
                      Time elapsed: 00:33:03
                               ETA: 00:41:15

################################################################################
                     [1m Learning iteration 890/2000 [0m                      

                       Computation: 43913 steps/s (collection: 2.139s, learning 0.100s)
             Mean action noise std: 2.69
          Mean value_function loss: 432.5690
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 74.4251
                       Mean reward: 366.61
               Mean episode length: 160.85
    Episode_Reward/reaching_object: 1.0707
     Episode_Reward/lifting_object: 71.3284
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 18.5833
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 2.24s
                      Time elapsed: 00:33:05
                               ETA: 00:41:13

################################################################################
                     [1m Learning iteration 891/2000 [0m                      

                       Computation: 44368 steps/s (collection: 2.106s, learning 0.110s)
             Mean action noise std: 2.69
          Mean value_function loss: 409.7040
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 74.4257
                       Mean reward: 388.13
               Mean episode length: 167.65
    Episode_Reward/reaching_object: 1.1389
     Episode_Reward/lifting_object: 76.9744
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 6.1250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 17.2083
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 2.22s
                      Time elapsed: 00:33:07
                               ETA: 00:41:11

################################################################################
                     [1m Learning iteration 892/2000 [0m                      

                       Computation: 43620 steps/s (collection: 2.137s, learning 0.117s)
             Mean action noise std: 2.69
          Mean value_function loss: 400.9459
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 74.4266
                       Mean reward: 403.12
               Mean episode length: 169.76
    Episode_Reward/reaching_object: 1.1155
     Episode_Reward/lifting_object: 75.5557
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0692
      Episode_Termination/time_out: 6.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.6667
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 2.25s
                      Time elapsed: 00:33:09
                               ETA: 00:41:08

################################################################################
                     [1m Learning iteration 893/2000 [0m                      

                       Computation: 43559 steps/s (collection: 2.141s, learning 0.116s)
             Mean action noise std: 2.69
          Mean value_function loss: 424.7937
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 74.4293
                       Mean reward: 302.49
               Mean episode length: 145.09
    Episode_Reward/reaching_object: 1.0991
     Episode_Reward/lifting_object: 74.1035
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 19.2083
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 2.26s
                      Time elapsed: 00:33:12
                               ETA: 00:41:06

################################################################################
                     [1m Learning iteration 894/2000 [0m                      

                       Computation: 43594 steps/s (collection: 2.138s, learning 0.117s)
             Mean action noise std: 2.69
          Mean value_function loss: 453.8372
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 74.4314
                       Mean reward: 389.90
               Mean episode length: 165.93
    Episode_Reward/reaching_object: 1.1152
     Episode_Reward/lifting_object: 75.5692
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 19.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 2.25s
                      Time elapsed: 00:33:14
                               ETA: 00:41:04

################################################################################
                     [1m Learning iteration 895/2000 [0m                      

                       Computation: 43996 steps/s (collection: 2.129s, learning 0.105s)
             Mean action noise std: 2.69
          Mean value_function loss: 500.1109
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 74.4335
                       Mean reward: 363.63
               Mean episode length: 157.45
    Episode_Reward/reaching_object: 1.1211
     Episode_Reward/lifting_object: 75.8559
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 19.7917
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 2.23s
                      Time elapsed: 00:33:16
                               ETA: 00:41:02

################################################################################
                     [1m Learning iteration 896/2000 [0m                      

                       Computation: 44159 steps/s (collection: 2.129s, learning 0.098s)
             Mean action noise std: 2.69
          Mean value_function loss: 500.7313
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 74.4356
                       Mean reward: 385.24
               Mean episode length: 157.04
    Episode_Reward/reaching_object: 1.1322
     Episode_Reward/lifting_object: 78.8723
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.2500
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 2.23s
                      Time elapsed: 00:33:18
                               ETA: 00:40:59

################################################################################
                     [1m Learning iteration 897/2000 [0m                      

                       Computation: 43924 steps/s (collection: 2.140s, learning 0.098s)
             Mean action noise std: 2.69
          Mean value_function loss: 444.4544
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 74.4362
                       Mean reward: 380.43
               Mean episode length: 163.42
    Episode_Reward/reaching_object: 1.1561
     Episode_Reward/lifting_object: 79.8435
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.7500
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 2.24s
                      Time elapsed: 00:33:20
                               ETA: 00:40:57

################################################################################
                     [1m Learning iteration 898/2000 [0m                      

                       Computation: 44051 steps/s (collection: 2.136s, learning 0.095s)
             Mean action noise std: 2.69
          Mean value_function loss: 478.1970
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 74.4379
                       Mean reward: 421.01
               Mean episode length: 164.46
    Episode_Reward/reaching_object: 1.1125
     Episode_Reward/lifting_object: 77.4212
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 20.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 2.23s
                      Time elapsed: 00:33:23
                               ETA: 00:40:55

################################################################################
                     [1m Learning iteration 899/2000 [0m                      

                       Computation: 44548 steps/s (collection: 2.112s, learning 0.095s)
             Mean action noise std: 2.69
          Mean value_function loss: 486.8540
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 74.4426
                       Mean reward: 393.65
               Mean episode length: 165.87
    Episode_Reward/reaching_object: 1.1037
     Episode_Reward/lifting_object: 75.2150
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 19.6667
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 2.21s
                      Time elapsed: 00:33:25
                               ETA: 00:40:53

################################################################################
                     [1m Learning iteration 900/2000 [0m                      

                       Computation: 43502 steps/s (collection: 2.155s, learning 0.105s)
             Mean action noise std: 2.69
          Mean value_function loss: 506.2270
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 74.4463
                       Mean reward: 372.85
               Mean episode length: 157.97
    Episode_Reward/reaching_object: 1.0805
     Episode_Reward/lifting_object: 73.3454
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 19.7083
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 2.26s
                      Time elapsed: 00:33:27
                               ETA: 00:40:51

################################################################################
                     [1m Learning iteration 901/2000 [0m                      

                       Computation: 43507 steps/s (collection: 2.161s, learning 0.099s)
             Mean action noise std: 2.69
          Mean value_function loss: 524.7316
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 74.4511
                       Mean reward: 401.01
               Mean episode length: 159.07
    Episode_Reward/reaching_object: 1.1307
     Episode_Reward/lifting_object: 78.5165
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 19.4583
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 2.26s
                      Time elapsed: 00:33:29
                               ETA: 00:40:48

################################################################################
                     [1m Learning iteration 902/2000 [0m                      

                       Computation: 43359 steps/s (collection: 2.156s, learning 0.112s)
             Mean action noise std: 2.69
          Mean value_function loss: 466.6548
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 74.4541
                       Mean reward: 357.16
               Mean episode length: 148.92
    Episode_Reward/reaching_object: 1.0953
     Episode_Reward/lifting_object: 75.1230
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 19.0417
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 2.27s
                      Time elapsed: 00:33:32
                               ETA: 00:40:46

################################################################################
                     [1m Learning iteration 903/2000 [0m                      

                       Computation: 43737 steps/s (collection: 2.131s, learning 0.117s)
             Mean action noise std: 2.69
          Mean value_function loss: 491.5535
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 74.4548
                       Mean reward: 365.31
               Mean episode length: 155.42
    Episode_Reward/reaching_object: 1.1089
     Episode_Reward/lifting_object: 76.3902
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.7917
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 2.25s
                      Time elapsed: 00:33:34
                               ETA: 00:40:44

################################################################################
                     [1m Learning iteration 904/2000 [0m                      

                       Computation: 44041 steps/s (collection: 2.130s, learning 0.102s)
             Mean action noise std: 2.69
          Mean value_function loss: 487.1892
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 74.4553
                       Mean reward: 403.27
               Mean episode length: 157.03
    Episode_Reward/reaching_object: 1.0903
     Episode_Reward/lifting_object: 74.8972
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 18.9583
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 2.23s
                      Time elapsed: 00:33:36
                               ETA: 00:40:42

################################################################################
                     [1m Learning iteration 905/2000 [0m                      

                       Computation: 44110 steps/s (collection: 2.134s, learning 0.095s)
             Mean action noise std: 2.69
          Mean value_function loss: 489.7823
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 74.4557
                       Mean reward: 357.41
               Mean episode length: 155.70
    Episode_Reward/reaching_object: 1.1435
     Episode_Reward/lifting_object: 79.4324
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 19.4583
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 2.23s
                      Time elapsed: 00:33:38
                               ETA: 00:40:40

################################################################################
                     [1m Learning iteration 906/2000 [0m                      

                       Computation: 43456 steps/s (collection: 2.152s, learning 0.110s)
             Mean action noise std: 2.69
          Mean value_function loss: 481.7098
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 74.4569
                       Mean reward: 379.94
               Mean episode length: 147.19
    Episode_Reward/reaching_object: 1.1207
     Episode_Reward/lifting_object: 77.5228
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 19.0417
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 2.26s
                      Time elapsed: 00:33:41
                               ETA: 00:40:37

################################################################################
                     [1m Learning iteration 907/2000 [0m                      

                       Computation: 43698 steps/s (collection: 2.133s, learning 0.117s)
             Mean action noise std: 2.70
          Mean value_function loss: 508.3989
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 74.4586
                       Mean reward: 387.83
               Mean episode length: 163.31
    Episode_Reward/reaching_object: 1.0809
     Episode_Reward/lifting_object: 73.7787
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 20.0417
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 2.25s
                      Time elapsed: 00:33:43
                               ETA: 00:40:35

################################################################################
                     [1m Learning iteration 908/2000 [0m                      

                       Computation: 43658 steps/s (collection: 2.136s, learning 0.116s)
             Mean action noise std: 2.70
          Mean value_function loss: 493.3861
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 74.4625
                       Mean reward: 404.49
               Mean episode length: 165.94
    Episode_Reward/reaching_object: 1.1439
     Episode_Reward/lifting_object: 79.6487
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 17.4583
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 2.25s
                      Time elapsed: 00:33:45
                               ETA: 00:40:33

################################################################################
                     [1m Learning iteration 909/2000 [0m                      

                       Computation: 43798 steps/s (collection: 2.142s, learning 0.102s)
             Mean action noise std: 2.70
          Mean value_function loss: 465.9143
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 74.4666
                       Mean reward: 460.94
               Mean episode length: 172.68
    Episode_Reward/reaching_object: 1.2329
     Episode_Reward/lifting_object: 88.2489
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 2.24s
                      Time elapsed: 00:33:47
                               ETA: 00:40:31

################################################################################
                     [1m Learning iteration 910/2000 [0m                      

                       Computation: 44251 steps/s (collection: 2.122s, learning 0.099s)
             Mean action noise std: 2.70
          Mean value_function loss: 480.8211
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 74.4727
                       Mean reward: 425.91
               Mean episode length: 166.92
    Episode_Reward/reaching_object: 1.1978
     Episode_Reward/lifting_object: 83.0772
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.2917
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 2.22s
                      Time elapsed: 00:33:50
                               ETA: 00:40:29

################################################################################
                     [1m Learning iteration 911/2000 [0m                      

                       Computation: 44139 steps/s (collection: 2.126s, learning 0.101s)
             Mean action noise std: 2.70
          Mean value_function loss: 468.6018
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 74.4747
                       Mean reward: 381.10
               Mean episode length: 167.41
    Episode_Reward/reaching_object: 1.1541
     Episode_Reward/lifting_object: 77.8091
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 18.4167
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 2.23s
                      Time elapsed: 00:33:52
                               ETA: 00:40:26

################################################################################
                     [1m Learning iteration 912/2000 [0m                      

                       Computation: 43463 steps/s (collection: 2.153s, learning 0.109s)
             Mean action noise std: 2.70
          Mean value_function loss: 468.9985
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 74.4753
                       Mean reward: 412.94
               Mean episode length: 161.55
    Episode_Reward/reaching_object: 1.1901
     Episode_Reward/lifting_object: 81.3442
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0693
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 17.8750
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 2.26s
                      Time elapsed: 00:33:54
                               ETA: 00:40:24

################################################################################
                     [1m Learning iteration 913/2000 [0m                      

                       Computation: 43296 steps/s (collection: 2.146s, learning 0.125s)
             Mean action noise std: 2.70
          Mean value_function loss: 486.6009
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 74.4763
                       Mean reward: 394.96
               Mean episode length: 161.57
    Episode_Reward/reaching_object: 1.2142
     Episode_Reward/lifting_object: 85.1082
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 2.27s
                      Time elapsed: 00:33:56
                               ETA: 00:40:22

################################################################################
                     [1m Learning iteration 914/2000 [0m                      

                       Computation: 43582 steps/s (collection: 2.155s, learning 0.101s)
             Mean action noise std: 2.70
          Mean value_function loss: 458.8029
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 74.4771
                       Mean reward: 425.36
               Mean episode length: 173.68
    Episode_Reward/reaching_object: 1.2336
     Episode_Reward/lifting_object: 87.0477
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.7917
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 2.26s
                      Time elapsed: 00:33:59
                               ETA: 00:40:20

################################################################################
                     [1m Learning iteration 915/2000 [0m                      

                       Computation: 44134 steps/s (collection: 2.129s, learning 0.098s)
             Mean action noise std: 2.70
          Mean value_function loss: 461.0699
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 74.4780
                       Mean reward: 431.49
               Mean episode length: 166.40
    Episode_Reward/reaching_object: 1.2086
     Episode_Reward/lifting_object: 85.8799
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.9167
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 2.23s
                      Time elapsed: 00:34:01
                               ETA: 00:40:17

################################################################################
                     [1m Learning iteration 916/2000 [0m                      

                       Computation: 44142 steps/s (collection: 2.131s, learning 0.096s)
             Mean action noise std: 2.70
          Mean value_function loss: 472.7428
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 74.4790
                       Mean reward: 375.94
               Mean episode length: 170.84
    Episode_Reward/reaching_object: 1.2540
     Episode_Reward/lifting_object: 88.9133
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0703
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 2.23s
                      Time elapsed: 00:34:03
                               ETA: 00:40:15

################################################################################
                     [1m Learning iteration 917/2000 [0m                      

                       Computation: 44187 steps/s (collection: 2.129s, learning 0.096s)
             Mean action noise std: 2.70
          Mean value_function loss: 444.6506
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 74.4814
                       Mean reward: 509.41
               Mean episode length: 187.26
    Episode_Reward/reaching_object: 1.2720
     Episode_Reward/lifting_object: 90.2239
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0709
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.7917
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 2.22s
                      Time elapsed: 00:34:05
                               ETA: 00:40:13

################################################################################
                     [1m Learning iteration 918/2000 [0m                      

                       Computation: 43395 steps/s (collection: 2.157s, learning 0.109s)
             Mean action noise std: 2.70
          Mean value_function loss: 491.8059
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 74.4865
                       Mean reward: 510.03
               Mean episode length: 186.29
    Episode_Reward/reaching_object: 1.2708
     Episode_Reward/lifting_object: 89.9514
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0708
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 2.27s
                      Time elapsed: 00:34:08
                               ETA: 00:40:11

################################################################################
                     [1m Learning iteration 919/2000 [0m                      

                       Computation: 43770 steps/s (collection: 2.131s, learning 0.115s)
             Mean action noise std: 2.70
          Mean value_function loss: 463.1738
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 74.4900
                       Mean reward: 368.45
               Mean episode length: 155.87
    Episode_Reward/reaching_object: 1.1968
     Episode_Reward/lifting_object: 82.5047
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0699
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.7083
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 2.25s
                      Time elapsed: 00:34:10
                               ETA: 00:40:09

################################################################################
                     [1m Learning iteration 920/2000 [0m                      

                       Computation: 44348 steps/s (collection: 2.112s, learning 0.105s)
             Mean action noise std: 2.70
          Mean value_function loss: 430.1677
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 74.4943
                       Mean reward: 377.59
               Mean episode length: 170.76
    Episode_Reward/reaching_object: 1.1673
     Episode_Reward/lifting_object: 78.9320
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.2083
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 2.22s
                      Time elapsed: 00:34:12
                               ETA: 00:40:06

################################################################################
                     [1m Learning iteration 921/2000 [0m                      

                       Computation: 43705 steps/s (collection: 2.139s, learning 0.110s)
             Mean action noise std: 2.70
          Mean value_function loss: 488.4021
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 74.4978
                       Mean reward: 390.36
               Mean episode length: 162.91
    Episode_Reward/reaching_object: 1.2260
     Episode_Reward/lifting_object: 86.1652
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0719
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.7917
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 2.25s
                      Time elapsed: 00:34:14
                               ETA: 00:40:04

################################################################################
                     [1m Learning iteration 922/2000 [0m                      

                       Computation: 43715 steps/s (collection: 2.146s, learning 0.103s)
             Mean action noise std: 2.70
          Mean value_function loss: 503.7522
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 74.4989
                       Mean reward: 438.51
               Mean episode length: 170.70
    Episode_Reward/reaching_object: 1.1972
     Episode_Reward/lifting_object: 83.8626
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 19.2917
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 2.25s
                      Time elapsed: 00:34:17
                               ETA: 00:40:02

################################################################################
                     [1m Learning iteration 923/2000 [0m                      

                       Computation: 43711 steps/s (collection: 2.142s, learning 0.107s)
             Mean action noise std: 2.70
          Mean value_function loss: 529.8203
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 74.5025
                       Mean reward: 464.23
               Mean episode length: 175.47
    Episode_Reward/reaching_object: 1.1773
     Episode_Reward/lifting_object: 82.9855
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.5417
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 2.25s
                      Time elapsed: 00:34:19
                               ETA: 00:40:00

################################################################################
                     [1m Learning iteration 924/2000 [0m                      

                       Computation: 43494 steps/s (collection: 2.158s, learning 0.102s)
             Mean action noise std: 2.70
          Mean value_function loss: 509.8262
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.5075
                       Mean reward: 417.15
               Mean episode length: 169.19
    Episode_Reward/reaching_object: 1.1184
     Episode_Reward/lifting_object: 76.6147
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.0833
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 2.26s
                      Time elapsed: 00:34:21
                               ETA: 00:39:58

################################################################################
                     [1m Learning iteration 925/2000 [0m                      

                       Computation: 43757 steps/s (collection: 2.129s, learning 0.118s)
             Mean action noise std: 2.70
          Mean value_function loss: 533.2742
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 74.5122
                       Mean reward: 401.10
               Mean episode length: 153.85
    Episode_Reward/reaching_object: 1.0536
     Episode_Reward/lifting_object: 72.7387
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 20.2500
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 2.25s
                      Time elapsed: 00:34:23
                               ETA: 00:39:55

################################################################################
                     [1m Learning iteration 926/2000 [0m                      

                       Computation: 43606 steps/s (collection: 2.159s, learning 0.096s)
             Mean action noise std: 2.70
          Mean value_function loss: 510.0500
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 74.5141
                       Mean reward: 445.77
               Mean episode length: 164.53
    Episode_Reward/reaching_object: 1.1236
     Episode_Reward/lifting_object: 80.1738
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 2.25s
                      Time elapsed: 00:34:26
                               ETA: 00:39:53

################################################################################
                     [1m Learning iteration 927/2000 [0m                      

                       Computation: 43362 steps/s (collection: 2.161s, learning 0.107s)
             Mean action noise std: 2.70
          Mean value_function loss: 524.8618
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 74.5157
                       Mean reward: 434.39
               Mean episode length: 165.38
    Episode_Reward/reaching_object: 1.1721
     Episode_Reward/lifting_object: 83.7956
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.6250
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 2.27s
                      Time elapsed: 00:34:28
                               ETA: 00:39:51

################################################################################
                     [1m Learning iteration 928/2000 [0m                      

                       Computation: 43745 steps/s (collection: 2.154s, learning 0.094s)
             Mean action noise std: 2.70
          Mean value_function loss: 560.6725
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 74.5181
                       Mean reward: 433.84
               Mean episode length: 160.25
    Episode_Reward/reaching_object: 1.0942
     Episode_Reward/lifting_object: 76.7214
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 18.5000
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 2.25s
                      Time elapsed: 00:34:30
                               ETA: 00:39:49

################################################################################
                     [1m Learning iteration 929/2000 [0m                      

                       Computation: 43198 steps/s (collection: 2.163s, learning 0.113s)
             Mean action noise std: 2.70
          Mean value_function loss: 508.8172
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 74.5206
                       Mean reward: 380.53
               Mean episode length: 155.16
    Episode_Reward/reaching_object: 1.1339
     Episode_Reward/lifting_object: 80.0917
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.6667
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 2.28s
                      Time elapsed: 00:34:32
                               ETA: 00:39:47

################################################################################
                     [1m Learning iteration 930/2000 [0m                      

                       Computation: 43452 steps/s (collection: 2.172s, learning 0.091s)
             Mean action noise std: 2.70
          Mean value_function loss: 505.6524
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 74.5234
                       Mean reward: 420.25
               Mean episode length: 160.95
    Episode_Reward/reaching_object: 1.1605
     Episode_Reward/lifting_object: 82.8459
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.2917
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 2.26s
                      Time elapsed: 00:34:35
                               ETA: 00:39:44

################################################################################
                     [1m Learning iteration 931/2000 [0m                      

                       Computation: 44359 steps/s (collection: 2.121s, learning 0.095s)
             Mean action noise std: 2.70
          Mean value_function loss: 466.5689
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 74.5242
                       Mean reward: 459.15
               Mean episode length: 171.96
    Episode_Reward/reaching_object: 1.1396
     Episode_Reward/lifting_object: 79.8943
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.0833
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 2.22s
                      Time elapsed: 00:34:37
                               ETA: 00:39:42

################################################################################
                     [1m Learning iteration 932/2000 [0m                      

                       Computation: 43886 steps/s (collection: 2.139s, learning 0.101s)
             Mean action noise std: 2.70
          Mean value_function loss: 439.1188
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 74.5254
                       Mean reward: 436.38
               Mean episode length: 177.69
    Episode_Reward/reaching_object: 1.2173
     Episode_Reward/lifting_object: 88.2495
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.8750
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 2.24s
                      Time elapsed: 00:34:39
                               ETA: 00:39:40

################################################################################
                     [1m Learning iteration 933/2000 [0m                      

                       Computation: 42837 steps/s (collection: 2.165s, learning 0.130s)
             Mean action noise std: 2.70
          Mean value_function loss: 420.7262
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 74.5258
                       Mean reward: 420.15
               Mean episode length: 169.69
    Episode_Reward/reaching_object: 1.2481
     Episode_Reward/lifting_object: 89.9793
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0709
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 2.29s
                      Time elapsed: 00:34:41
                               ETA: 00:39:38

################################################################################
                     [1m Learning iteration 934/2000 [0m                      

                       Computation: 44006 steps/s (collection: 2.139s, learning 0.095s)
             Mean action noise std: 2.70
          Mean value_function loss: 465.7794
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 74.5262
                       Mean reward: 475.03
               Mean episode length: 170.78
    Episode_Reward/reaching_object: 1.2560
     Episode_Reward/lifting_object: 91.0579
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.3750
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 2.23s
                      Time elapsed: 00:34:44
                               ETA: 00:39:36

################################################################################
                     [1m Learning iteration 935/2000 [0m                      

                       Computation: 43602 steps/s (collection: 2.132s, learning 0.122s)
             Mean action noise std: 2.70
          Mean value_function loss: 452.5576
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 74.5282
                       Mean reward: 487.14
               Mean episode length: 178.41
    Episode_Reward/reaching_object: 1.2252
     Episode_Reward/lifting_object: 89.7821
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.9167
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 2.25s
                      Time elapsed: 00:34:46
                               ETA: 00:39:33

################################################################################
                     [1m Learning iteration 936/2000 [0m                      

                       Computation: 43861 steps/s (collection: 2.142s, learning 0.099s)
             Mean action noise std: 2.70
          Mean value_function loss: 502.2554
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 74.5318
                       Mean reward: 443.62
               Mean episode length: 172.58
    Episode_Reward/reaching_object: 1.1917
     Episode_Reward/lifting_object: 86.5119
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0693
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 17.5000
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 2.24s
                      Time elapsed: 00:34:48
                               ETA: 00:39:31

################################################################################
                     [1m Learning iteration 937/2000 [0m                      

                       Computation: 43894 steps/s (collection: 2.131s, learning 0.108s)
             Mean action noise std: 2.70
          Mean value_function loss: 516.5057
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 74.5373
                       Mean reward: 421.01
               Mean episode length: 168.53
    Episode_Reward/reaching_object: 1.2100
     Episode_Reward/lifting_object: 87.1866
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.7083
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 2.24s
                      Time elapsed: 00:34:50
                               ETA: 00:39:29

################################################################################
                     [1m Learning iteration 938/2000 [0m                      

                       Computation: 43869 steps/s (collection: 2.136s, learning 0.105s)
             Mean action noise std: 2.70
          Mean value_function loss: 516.1004
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.5405
                       Mean reward: 500.86
               Mean episode length: 174.62
    Episode_Reward/reaching_object: 1.2841
     Episode_Reward/lifting_object: 95.3316
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.2500
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 2.24s
                      Time elapsed: 00:34:53
                               ETA: 00:39:27

################################################################################
                     [1m Learning iteration 939/2000 [0m                      

                       Computation: 43445 steps/s (collection: 2.143s, learning 0.120s)
             Mean action noise std: 2.70
          Mean value_function loss: 470.2215
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 74.5454
                       Mean reward: 511.27
               Mean episode length: 175.01
    Episode_Reward/reaching_object: 1.2430
     Episode_Reward/lifting_object: 91.1623
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0702
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 2.26s
                      Time elapsed: 00:34:55
                               ETA: 00:39:25

################################################################################
                     [1m Learning iteration 940/2000 [0m                      

                       Computation: 44185 steps/s (collection: 2.130s, learning 0.095s)
             Mean action noise std: 2.70
          Mean value_function loss: 501.5587
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 74.5512
                       Mean reward: 470.17
               Mean episode length: 172.90
    Episode_Reward/reaching_object: 1.2253
     Episode_Reward/lifting_object: 89.4383
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.3750
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 2.22s
                      Time elapsed: 00:34:57
                               ETA: 00:39:22

################################################################################
                     [1m Learning iteration 941/2000 [0m                      

                       Computation: 44179 steps/s (collection: 2.120s, learning 0.105s)
             Mean action noise std: 2.70
          Mean value_function loss: 491.8651
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 74.5532
                       Mean reward: 460.00
               Mean episode length: 166.78
    Episode_Reward/reaching_object: 1.2034
     Episode_Reward/lifting_object: 88.0375
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 2.23s
                      Time elapsed: 00:34:59
                               ETA: 00:39:20

################################################################################
                     [1m Learning iteration 942/2000 [0m                      

                       Computation: 42785 steps/s (collection: 2.173s, learning 0.125s)
             Mean action noise std: 2.71
          Mean value_function loss: 479.5895
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 74.5555
                       Mean reward: 526.35
               Mean episode length: 178.27
    Episode_Reward/reaching_object: 1.1942
     Episode_Reward/lifting_object: 86.9847
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0691
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.4167
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 2.30s
                      Time elapsed: 00:35:02
                               ETA: 00:39:18

################################################################################
                     [1m Learning iteration 943/2000 [0m                      

                       Computation: 43877 steps/s (collection: 2.123s, learning 0.118s)
             Mean action noise std: 2.71
          Mean value_function loss: 495.5338
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 74.5597
                       Mean reward: 479.15
               Mean episode length: 167.92
    Episode_Reward/reaching_object: 1.2825
     Episode_Reward/lifting_object: 96.5832
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0699
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.3750
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 2.24s
                      Time elapsed: 00:35:04
                               ETA: 00:39:16

################################################################################
                     [1m Learning iteration 944/2000 [0m                      

                       Computation: 43391 steps/s (collection: 2.156s, learning 0.110s)
             Mean action noise std: 2.71
          Mean value_function loss: 510.6206
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 74.5682
                       Mean reward: 480.33
               Mean episode length: 167.44
    Episode_Reward/reaching_object: 1.2405
     Episode_Reward/lifting_object: 92.5466
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.3333
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 2.27s
                      Time elapsed: 00:35:06
                               ETA: 00:39:14

################################################################################
                     [1m Learning iteration 945/2000 [0m                      

                       Computation: 43385 steps/s (collection: 2.155s, learning 0.111s)
             Mean action noise std: 2.71
          Mean value_function loss: 528.7091
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 74.5777
                       Mean reward: 423.78
               Mean episode length: 162.17
    Episode_Reward/reaching_object: 1.2151
     Episode_Reward/lifting_object: 89.6110
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 2.27s
                      Time elapsed: 00:35:08
                               ETA: 00:39:11

################################################################################
                     [1m Learning iteration 946/2000 [0m                      

                       Computation: 43950 steps/s (collection: 2.134s, learning 0.103s)
             Mean action noise std: 2.71
          Mean value_function loss: 504.7610
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 74.5806
                       Mean reward: 466.22
               Mean episode length: 175.69
    Episode_Reward/reaching_object: 1.2486
     Episode_Reward/lifting_object: 92.6878
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0723
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.2917
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 2.24s
                      Time elapsed: 00:35:11
                               ETA: 00:39:09

################################################################################
                     [1m Learning iteration 947/2000 [0m                      

                       Computation: 43772 steps/s (collection: 2.143s, learning 0.103s)
             Mean action noise std: 2.71
          Mean value_function loss: 507.6266
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 74.5827
                       Mean reward: 612.31
               Mean episode length: 195.75
    Episode_Reward/reaching_object: 1.2592
     Episode_Reward/lifting_object: 94.9985
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.2083
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 2.25s
                      Time elapsed: 00:35:13
                               ETA: 00:39:07

################################################################################
                     [1m Learning iteration 948/2000 [0m                      

                       Computation: 43544 steps/s (collection: 2.131s, learning 0.127s)
             Mean action noise std: 2.71
          Mean value_function loss: 481.3959
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 74.5869
                       Mean reward: 485.93
               Mean episode length: 167.72
    Episode_Reward/reaching_object: 1.2379
     Episode_Reward/lifting_object: 92.5549
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0717
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.9583
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 2.26s
                      Time elapsed: 00:35:15
                               ETA: 00:39:05

################################################################################
                     [1m Learning iteration 949/2000 [0m                      

                       Computation: 43542 steps/s (collection: 2.140s, learning 0.118s)
             Mean action noise std: 2.71
          Mean value_function loss: 486.6339
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 74.5896
                       Mean reward: 479.26
               Mean episode length: 168.68
    Episode_Reward/reaching_object: 1.2497
     Episode_Reward/lifting_object: 93.8968
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0714
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.3333
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 2.26s
                      Time elapsed: 00:35:17
                               ETA: 00:39:03

################################################################################
                     [1m Learning iteration 950/2000 [0m                      

                       Computation: 43586 steps/s (collection: 2.146s, learning 0.110s)
             Mean action noise std: 2.71
          Mean value_function loss: 513.8091
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 74.5924
                       Mean reward: 512.46
               Mean episode length: 174.43
    Episode_Reward/reaching_object: 1.2657
     Episode_Reward/lifting_object: 94.4190
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0709
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 2.26s
                      Time elapsed: 00:35:20
                               ETA: 00:39:00

################################################################################
                     [1m Learning iteration 951/2000 [0m                      

                       Computation: 43180 steps/s (collection: 2.173s, learning 0.104s)
             Mean action noise std: 2.71
          Mean value_function loss: 472.4940
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 74.5952
                       Mean reward: 482.62
               Mean episode length: 170.74
    Episode_Reward/reaching_object: 1.2488
     Episode_Reward/lifting_object: 91.9098
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0724
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.2500
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 2.28s
                      Time elapsed: 00:35:22
                               ETA: 00:38:58

################################################################################
                     [1m Learning iteration 952/2000 [0m                      

                       Computation: 44429 steps/s (collection: 2.117s, learning 0.096s)
             Mean action noise std: 2.71
          Mean value_function loss: 513.6853
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 74.5968
                       Mean reward: 559.19
               Mean episode length: 185.67
    Episode_Reward/reaching_object: 1.2755
     Episode_Reward/lifting_object: 95.0309
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0718
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.9167
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 2.21s
                      Time elapsed: 00:35:24
                               ETA: 00:38:56

################################################################################
                     [1m Learning iteration 953/2000 [0m                      

                       Computation: 43784 steps/s (collection: 2.144s, learning 0.101s)
             Mean action noise std: 2.71
          Mean value_function loss: 495.3714
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 74.5973
                       Mean reward: 455.20
               Mean episode length: 161.43
    Episode_Reward/reaching_object: 1.2387
     Episode_Reward/lifting_object: 93.1407
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0708
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.7083
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 2.25s
                      Time elapsed: 00:35:26
                               ETA: 00:38:54

################################################################################
                     [1m Learning iteration 954/2000 [0m                      

                       Computation: 43582 steps/s (collection: 2.160s, learning 0.096s)
             Mean action noise std: 2.71
          Mean value_function loss: 536.7394
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 74.5990
                       Mean reward: 484.94
               Mean episode length: 168.87
    Episode_Reward/reaching_object: 1.2816
     Episode_Reward/lifting_object: 97.4222
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0715
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 2.26s
                      Time elapsed: 00:35:29
                               ETA: 00:38:51

################################################################################
                     [1m Learning iteration 955/2000 [0m                      

                       Computation: 44571 steps/s (collection: 2.111s, learning 0.095s)
             Mean action noise std: 2.71
          Mean value_function loss: 504.5673
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 74.6015
                       Mean reward: 530.45
               Mean episode length: 184.41
    Episode_Reward/reaching_object: 1.2536
     Episode_Reward/lifting_object: 93.3531
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0720
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.7083
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 2.21s
                      Time elapsed: 00:35:31
                               ETA: 00:38:49

################################################################################
                     [1m Learning iteration 956/2000 [0m                      

                       Computation: 44260 steps/s (collection: 2.116s, learning 0.105s)
             Mean action noise std: 2.71
          Mean value_function loss: 484.9927
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 74.6021
                       Mean reward: 533.97
               Mean episode length: 177.43
    Episode_Reward/reaching_object: 1.2781
     Episode_Reward/lifting_object: 97.4381
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0717
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.1250
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 2.22s
                      Time elapsed: 00:35:33
                               ETA: 00:38:47

################################################################################
                     [1m Learning iteration 957/2000 [0m                      

                       Computation: 44318 steps/s (collection: 2.117s, learning 0.101s)
             Mean action noise std: 2.71
          Mean value_function loss: 484.9335
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 74.6025
                       Mean reward: 478.90
               Mean episode length: 168.29
    Episode_Reward/reaching_object: 1.2361
     Episode_Reward/lifting_object: 91.9717
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0702
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.4167
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 2.22s
                      Time elapsed: 00:35:35
                               ETA: 00:38:45

################################################################################
                     [1m Learning iteration 958/2000 [0m                      

                       Computation: 44320 steps/s (collection: 2.122s, learning 0.096s)
             Mean action noise std: 2.71
          Mean value_function loss: 503.8448
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 74.6033
                       Mean reward: 494.83
               Mean episode length: 171.83
    Episode_Reward/reaching_object: 1.2717
     Episode_Reward/lifting_object: 96.9545
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0699
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.8333
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 2.22s
                      Time elapsed: 00:35:37
                               ETA: 00:38:42

################################################################################
                     [1m Learning iteration 959/2000 [0m                      

                       Computation: 44177 steps/s (collection: 2.117s, learning 0.109s)
             Mean action noise std: 2.71
          Mean value_function loss: 481.7733
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 74.6037
                       Mean reward: 492.11
               Mean episode length: 173.25
    Episode_Reward/reaching_object: 1.3016
     Episode_Reward/lifting_object: 99.4228
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0716
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.1667
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 2.23s
                      Time elapsed: 00:35:40
                               ETA: 00:38:40

################################################################################
                     [1m Learning iteration 960/2000 [0m                      

                       Computation: 44151 steps/s (collection: 2.107s, learning 0.120s)
             Mean action noise std: 2.71
          Mean value_function loss: 517.1267
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 74.6043
                       Mean reward: 443.03
               Mean episode length: 156.14
    Episode_Reward/reaching_object: 1.2481
     Episode_Reward/lifting_object: 95.0313
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.3333
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 2.23s
                      Time elapsed: 00:35:42
                               ETA: 00:38:38

################################################################################
                     [1m Learning iteration 961/2000 [0m                      

                       Computation: 43702 steps/s (collection: 2.141s, learning 0.109s)
             Mean action noise std: 2.71
          Mean value_function loss: 541.3036
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 74.6057
                       Mean reward: 467.62
               Mean episode length: 163.95
    Episode_Reward/reaching_object: 1.1466
     Episode_Reward/lifting_object: 85.1793
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.5833
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 2.25s
                      Time elapsed: 00:35:44
                               ETA: 00:38:36

################################################################################
                     [1m Learning iteration 962/2000 [0m                      

                       Computation: 43929 steps/s (collection: 2.133s, learning 0.105s)
             Mean action noise std: 2.71
          Mean value_function loss: 588.0407
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 74.6082
                       Mean reward: 489.78
               Mean episode length: 173.25
    Episode_Reward/reaching_object: 1.2713
     Episode_Reward/lifting_object: 96.4087
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0690
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 19.2083
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 2.24s
                      Time elapsed: 00:35:46
                               ETA: 00:38:34

################################################################################
                     [1m Learning iteration 963/2000 [0m                      

                       Computation: 44966 steps/s (collection: 2.086s, learning 0.100s)
             Mean action noise std: 2.71
          Mean value_function loss: 563.3846
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 74.6125
                       Mean reward: 468.17
               Mean episode length: 169.96
    Episode_Reward/reaching_object: 1.2796
     Episode_Reward/lifting_object: 97.3323
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.5000
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 2.19s
                      Time elapsed: 00:35:49
                               ETA: 00:38:31

################################################################################
                     [1m Learning iteration 964/2000 [0m                      

                       Computation: 44700 steps/s (collection: 2.106s, learning 0.093s)
             Mean action noise std: 2.71
          Mean value_function loss: 579.6257
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 74.6178
                       Mean reward: 497.94
               Mean episode length: 174.71
    Episode_Reward/reaching_object: 1.2783
     Episode_Reward/lifting_object: 97.6570
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.9583
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 2.20s
                      Time elapsed: 00:35:51
                               ETA: 00:38:29

################################################################################
                     [1m Learning iteration 965/2000 [0m                      

                       Computation: 44683 steps/s (collection: 2.102s, learning 0.098s)
             Mean action noise std: 2.71
          Mean value_function loss: 546.7916
               Mean surrogate loss: 0.0097
                 Mean entropy loss: 74.6232
                       Mean reward: 396.81
               Mean episode length: 149.50
    Episode_Reward/reaching_object: 1.1672
     Episode_Reward/lifting_object: 87.2708
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 2.20s
                      Time elapsed: 00:35:53
                               ETA: 00:38:27

################################################################################
                     [1m Learning iteration 966/2000 [0m                      

                       Computation: 44463 steps/s (collection: 2.098s, learning 0.113s)
             Mean action noise std: 2.71
          Mean value_function loss: 474.8534
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 74.6238
                       Mean reward: 421.06
               Mean episode length: 160.06
    Episode_Reward/reaching_object: 1.1804
     Episode_Reward/lifting_object: 86.7419
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 2.21s
                      Time elapsed: 00:35:55
                               ETA: 00:38:25

################################################################################
                     [1m Learning iteration 967/2000 [0m                      

                       Computation: 45219 steps/s (collection: 2.075s, learning 0.099s)
             Mean action noise std: 2.71
          Mean value_function loss: 473.0332
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 74.6242
                       Mean reward: 506.24
               Mean episode length: 171.42
    Episode_Reward/reaching_object: 1.2847
     Episode_Reward/lifting_object: 97.7002
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 2.17s
                      Time elapsed: 00:35:57
                               ETA: 00:38:22

################################################################################
                     [1m Learning iteration 968/2000 [0m                      

                       Computation: 44574 steps/s (collection: 2.109s, learning 0.096s)
             Mean action noise std: 2.71
          Mean value_function loss: 492.0669
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 74.6249
                       Mean reward: 543.36
               Mean episode length: 182.06
    Episode_Reward/reaching_object: 1.3504
     Episode_Reward/lifting_object: 104.7119
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0712
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.0417
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 2.21s
                      Time elapsed: 00:36:00
                               ETA: 00:38:20

################################################################################
                     [1m Learning iteration 969/2000 [0m                      

                       Computation: 45247 steps/s (collection: 2.083s, learning 0.090s)
             Mean action noise std: 2.71
          Mean value_function loss: 460.3843
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 74.6258
                       Mean reward: 460.12
               Mean episode length: 166.69
    Episode_Reward/reaching_object: 1.3110
     Episode_Reward/lifting_object: 100.0413
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0700
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.5000
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 2.17s
                      Time elapsed: 00:36:02
                               ETA: 00:38:18

################################################################################
                     [1m Learning iteration 970/2000 [0m                      

                       Computation: 45290 steps/s (collection: 2.081s, learning 0.090s)
             Mean action noise std: 2.71
          Mean value_function loss: 480.3360
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 74.6266
                       Mean reward: 498.15
               Mean episode length: 173.46
    Episode_Reward/reaching_object: 1.3237
     Episode_Reward/lifting_object: 101.2548
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0708
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 2.17s
                      Time elapsed: 00:36:04
                               ETA: 00:38:15

################################################################################
                     [1m Learning iteration 971/2000 [0m                      

                       Computation: 45773 steps/s (collection: 2.058s, learning 0.090s)
             Mean action noise std: 2.71
          Mean value_function loss: 532.9836
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 74.6275
                       Mean reward: 516.39
               Mean episode length: 177.82
    Episode_Reward/reaching_object: 1.3604
     Episode_Reward/lifting_object: 104.2943
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0719
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.2917
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 2.15s
                      Time elapsed: 00:36:06
                               ETA: 00:38:13

################################################################################
                     [1m Learning iteration 972/2000 [0m                      

                       Computation: 45287 steps/s (collection: 2.077s, learning 0.094s)
             Mean action noise std: 2.71
          Mean value_function loss: 591.0380
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 74.6302
                       Mean reward: 573.96
               Mean episode length: 179.28
    Episode_Reward/reaching_object: 1.3807
     Episode_Reward/lifting_object: 108.4179
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 2.17s
                      Time elapsed: 00:36:08
                               ETA: 00:38:11

################################################################################
                     [1m Learning iteration 973/2000 [0m                      

                       Computation: 45413 steps/s (collection: 2.065s, learning 0.099s)
             Mean action noise std: 2.71
          Mean value_function loss: 539.8476
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 74.6358
                       Mean reward: 452.51
               Mean episode length: 159.00
    Episode_Reward/reaching_object: 1.3284
     Episode_Reward/lifting_object: 102.3049
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0699
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 16.3333
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 2.16s
                      Time elapsed: 00:36:10
                               ETA: 00:38:09

################################################################################
                     [1m Learning iteration 974/2000 [0m                      

                       Computation: 44714 steps/s (collection: 2.090s, learning 0.108s)
             Mean action noise std: 2.71
          Mean value_function loss: 550.0713
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 74.6404
                       Mean reward: 530.01
               Mean episode length: 176.39
    Episode_Reward/reaching_object: 1.2713
     Episode_Reward/lifting_object: 97.5407
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0686
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.9583
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 2.20s
                      Time elapsed: 00:36:13
                               ETA: 00:38:06

################################################################################
                     [1m Learning iteration 975/2000 [0m                      

                       Computation: 44863 steps/s (collection: 2.073s, learning 0.119s)
             Mean action noise std: 2.71
          Mean value_function loss: 479.9753
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 74.6445
                       Mean reward: 490.10
               Mean episode length: 172.72
    Episode_Reward/reaching_object: 1.2549
     Episode_Reward/lifting_object: 96.0621
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.7083
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 2.19s
                      Time elapsed: 00:36:15
                               ETA: 00:38:04

################################################################################
                     [1m Learning iteration 976/2000 [0m                      

                       Computation: 46003 steps/s (collection: 2.032s, learning 0.105s)
             Mean action noise std: 2.71
          Mean value_function loss: 513.2010
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 74.6483
                       Mean reward: 584.79
               Mean episode length: 185.26
    Episode_Reward/reaching_object: 1.3221
     Episode_Reward/lifting_object: 101.9740
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 2.14s
                      Time elapsed: 00:36:17
                               ETA: 00:38:02

################################################################################
                     [1m Learning iteration 977/2000 [0m                      

                       Computation: 46437 steps/s (collection: 2.023s, learning 0.094s)
             Mean action noise std: 2.72
          Mean value_function loss: 522.9676
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 74.6518
                       Mean reward: 458.95
               Mean episode length: 166.97
    Episode_Reward/reaching_object: 1.2402
     Episode_Reward/lifting_object: 93.4591
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.9583
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 2.12s
                      Time elapsed: 00:36:19
                               ETA: 00:37:59

################################################################################
                     [1m Learning iteration 978/2000 [0m                      

                       Computation: 45544 steps/s (collection: 2.065s, learning 0.093s)
             Mean action noise std: 2.72
          Mean value_function loss: 490.8100
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 74.6552
                       Mean reward: 496.78
               Mean episode length: 171.74
    Episode_Reward/reaching_object: 1.2132
     Episode_Reward/lifting_object: 91.0358
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 2.16s
                      Time elapsed: 00:36:21
                               ETA: 00:37:57

################################################################################
                     [1m Learning iteration 979/2000 [0m                      

                       Computation: 45891 steps/s (collection: 2.046s, learning 0.096s)
             Mean action noise std: 2.72
          Mean value_function loss: 508.4086
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 74.6582
                       Mean reward: 492.56
               Mean episode length: 166.79
    Episode_Reward/reaching_object: 1.2231
     Episode_Reward/lifting_object: 91.6623
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 16.3333
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 2.14s
                      Time elapsed: 00:36:23
                               ETA: 00:37:55

################################################################################
                     [1m Learning iteration 980/2000 [0m                      

                       Computation: 45709 steps/s (collection: 2.060s, learning 0.091s)
             Mean action noise std: 2.72
          Mean value_function loss: 514.1516
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.6612
                       Mean reward: 487.65
               Mean episode length: 168.76
    Episode_Reward/reaching_object: 1.2684
     Episode_Reward/lifting_object: 96.3166
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0699
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 16.4167
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 2.15s
                      Time elapsed: 00:36:25
                               ETA: 00:37:52

################################################################################
                     [1m Learning iteration 981/2000 [0m                      

                       Computation: 45648 steps/s (collection: 2.061s, learning 0.093s)
             Mean action noise std: 2.72
          Mean value_function loss: 504.7347
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 74.6662
                       Mean reward: 516.48
               Mean episode length: 172.06
    Episode_Reward/reaching_object: 1.2959
     Episode_Reward/lifting_object: 99.4444
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0695
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.1667
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 2.15s
                      Time elapsed: 00:36:28
                               ETA: 00:37:50

################################################################################
                     [1m Learning iteration 982/2000 [0m                      

                       Computation: 45305 steps/s (collection: 2.068s, learning 0.102s)
             Mean action noise std: 2.72
          Mean value_function loss: 499.2630
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 74.6713
                       Mean reward: 504.70
               Mean episode length: 179.14
    Episode_Reward/reaching_object: 1.3264
     Episode_Reward/lifting_object: 102.3116
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0710
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 2.17s
                      Time elapsed: 00:36:30
                               ETA: 00:37:48

################################################################################
                     [1m Learning iteration 983/2000 [0m                      

                       Computation: 45725 steps/s (collection: 2.063s, learning 0.087s)
             Mean action noise std: 2.72
          Mean value_function loss: 465.3509
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 74.6725
                       Mean reward: 549.24
               Mean episode length: 180.71
    Episode_Reward/reaching_object: 1.3120
     Episode_Reward/lifting_object: 100.7643
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0720
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 15.2500
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 2.15s
                      Time elapsed: 00:36:32
                               ETA: 00:37:45

################################################################################
                     [1m Learning iteration 984/2000 [0m                      

                       Computation: 44296 steps/s (collection: 2.113s, learning 0.106s)
             Mean action noise std: 2.72
          Mean value_function loss: 482.6671
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 74.6754
                       Mean reward: 535.89
               Mean episode length: 175.38
    Episode_Reward/reaching_object: 1.3023
     Episode_Reward/lifting_object: 99.9476
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 2.22s
                      Time elapsed: 00:36:34
                               ETA: 00:37:43

################################################################################
                     [1m Learning iteration 985/2000 [0m                      

                       Computation: 45452 steps/s (collection: 2.059s, learning 0.103s)
             Mean action noise std: 2.72
          Mean value_function loss: 474.2007
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 74.6806
                       Mean reward: 504.41
               Mean episode length: 168.96
    Episode_Reward/reaching_object: 1.3131
     Episode_Reward/lifting_object: 101.3400
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 2.16s
                      Time elapsed: 00:36:36
                               ETA: 00:37:41

################################################################################
                     [1m Learning iteration 986/2000 [0m                      

                       Computation: 45617 steps/s (collection: 2.065s, learning 0.090s)
             Mean action noise std: 2.72
          Mean value_function loss: 496.8329
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 74.6864
                       Mean reward: 494.51
               Mean episode length: 166.07
    Episode_Reward/reaching_object: 1.2470
     Episode_Reward/lifting_object: 94.4573
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0695
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 2.15s
                      Time elapsed: 00:36:39
                               ETA: 00:37:39

################################################################################
                     [1m Learning iteration 987/2000 [0m                      

                       Computation: 44128 steps/s (collection: 2.080s, learning 0.148s)
             Mean action noise std: 2.72
          Mean value_function loss: 496.9276
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 74.6886
                       Mean reward: 437.95
               Mean episode length: 164.60
    Episode_Reward/reaching_object: 1.2712
     Episode_Reward/lifting_object: 96.8876
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0715
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.4167
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 2.23s
                      Time elapsed: 00:36:41
                               ETA: 00:37:36

################################################################################
                     [1m Learning iteration 988/2000 [0m                      

                       Computation: 36640 steps/s (collection: 2.502s, learning 0.181s)
             Mean action noise std: 2.72
          Mean value_function loss: 475.5436
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 74.6897
                       Mean reward: 521.26
               Mean episode length: 175.53
    Episode_Reward/reaching_object: 1.3912
     Episode_Reward/lifting_object: 108.2157
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0724
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.3750
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 2.68s
                      Time elapsed: 00:36:43
                               ETA: 00:37:35

################################################################################
                     [1m Learning iteration 989/2000 [0m                      

                       Computation: 42050 steps/s (collection: 2.240s, learning 0.098s)
             Mean action noise std: 2.72
          Mean value_function loss: 460.1440
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 74.6904
                       Mean reward: 535.47
               Mean episode length: 177.80
    Episode_Reward/reaching_object: 1.2791
     Episode_Reward/lifting_object: 98.6707
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0707
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 2.34s
                      Time elapsed: 00:36:46
                               ETA: 00:37:33

################################################################################
                     [1m Learning iteration 990/2000 [0m                      

                       Computation: 44593 steps/s (collection: 2.107s, learning 0.098s)
             Mean action noise std: 2.72
          Mean value_function loss: 517.3451
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 74.6912
                       Mean reward: 551.38
               Mean episode length: 184.55
    Episode_Reward/reaching_object: 1.3657
     Episode_Reward/lifting_object: 106.4680
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0728
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.9583
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 2.20s
                      Time elapsed: 00:36:48
                               ETA: 00:37:30

################################################################################
                     [1m Learning iteration 991/2000 [0m                      

                       Computation: 43346 steps/s (collection: 2.176s, learning 0.092s)
             Mean action noise std: 2.72
          Mean value_function loss: 515.4310
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 74.6931
                       Mean reward: 491.82
               Mean episode length: 170.06
    Episode_Reward/reaching_object: 1.3330
     Episode_Reward/lifting_object: 102.7471
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0722
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 2.27s
                      Time elapsed: 00:36:50
                               ETA: 00:37:28

################################################################################
                     [1m Learning iteration 992/2000 [0m                      

                       Computation: 43514 steps/s (collection: 2.156s, learning 0.104s)
             Mean action noise std: 2.72
          Mean value_function loss: 479.8413
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 74.6955
                       Mean reward: 534.00
               Mean episode length: 181.24
    Episode_Reward/reaching_object: 1.3200
     Episode_Reward/lifting_object: 101.5977
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0734
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.2083
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 2.26s
                      Time elapsed: 00:36:52
                               ETA: 00:37:26

################################################################################
                     [1m Learning iteration 993/2000 [0m                      

                       Computation: 45132 steps/s (collection: 2.079s, learning 0.099s)
             Mean action noise std: 2.72
          Mean value_function loss: 448.5840
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 74.7007
                       Mean reward: 502.37
               Mean episode length: 172.87
    Episode_Reward/reaching_object: 1.3632
     Episode_Reward/lifting_object: 106.5234
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0743
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 2.18s
                      Time elapsed: 00:36:55
                               ETA: 00:37:24

################################################################################
                     [1m Learning iteration 994/2000 [0m                      

                       Computation: 43408 steps/s (collection: 2.162s, learning 0.103s)
             Mean action noise std: 2.72
          Mean value_function loss: 443.8963
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 74.7037
                       Mean reward: 578.71
               Mean episode length: 184.71
    Episode_Reward/reaching_object: 1.4174
     Episode_Reward/lifting_object: 111.2077
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0752
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.0833
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 2.26s
                      Time elapsed: 00:36:57
                               ETA: 00:37:21

################################################################################
                     [1m Learning iteration 995/2000 [0m                      

                       Computation: 42763 steps/s (collection: 2.144s, learning 0.155s)
             Mean action noise std: 2.72
          Mean value_function loss: 490.0541
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 74.7072
                       Mean reward: 485.01
               Mean episode length: 168.98
    Episode_Reward/reaching_object: 1.3204
     Episode_Reward/lifting_object: 100.6241
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0724
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 2.30s
                      Time elapsed: 00:36:59
                               ETA: 00:37:19

################################################################################
                     [1m Learning iteration 996/2000 [0m                      

                       Computation: 43343 steps/s (collection: 2.178s, learning 0.090s)
             Mean action noise std: 2.72
          Mean value_function loss: 489.3042
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 74.7110
                       Mean reward: 552.71
               Mean episode length: 183.42
    Episode_Reward/reaching_object: 1.3022
     Episode_Reward/lifting_object: 98.7667
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0727
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 2.27s
                      Time elapsed: 00:37:01
                               ETA: 00:37:17

################################################################################
                     [1m Learning iteration 997/2000 [0m                      

                       Computation: 44305 steps/s (collection: 2.121s, learning 0.098s)
             Mean action noise std: 2.72
          Mean value_function loss: 414.9789
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.7124
                       Mean reward: 499.58
               Mean episode length: 174.47
    Episode_Reward/reaching_object: 1.3520
     Episode_Reward/lifting_object: 104.5476
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0735
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 2.22s
                      Time elapsed: 00:37:04
                               ETA: 00:37:15

################################################################################
                     [1m Learning iteration 998/2000 [0m                      

                       Computation: 44606 steps/s (collection: 2.114s, learning 0.090s)
             Mean action noise std: 2.72
          Mean value_function loss: 465.4308
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 74.7150
                       Mean reward: 506.86
               Mean episode length: 179.68
    Episode_Reward/reaching_object: 1.3484
     Episode_Reward/lifting_object: 104.1269
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0739
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 2.20s
                      Time elapsed: 00:37:06
                               ETA: 00:37:13

################################################################################
                     [1m Learning iteration 999/2000 [0m                      

                       Computation: 44737 steps/s (collection: 2.095s, learning 0.103s)
             Mean action noise std: 2.72
          Mean value_function loss: 476.8346
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 74.7164
                       Mean reward: 498.62
               Mean episode length: 167.50
    Episode_Reward/reaching_object: 1.2678
     Episode_Reward/lifting_object: 96.9675
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0700
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.5000
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 2.20s
                      Time elapsed: 00:37:08
                               ETA: 00:37:10

################################################################################
                     [1m Learning iteration 1000/2000 [0m                     

                       Computation: 13516 steps/s (collection: 7.124s, learning 0.149s)
             Mean action noise std: 2.72
          Mean value_function loss: 469.6192
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 74.7177
                       Mean reward: 534.58
               Mean episode length: 177.57
    Episode_Reward/reaching_object: 1.3825
     Episode_Reward/lifting_object: 107.5835
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0727
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 7.27s
                      Time elapsed: 00:37:15
                               ETA: 00:37:13

################################################################################
                     [1m Learning iteration 1001/2000 [0m                     

                       Computation: 13699 steps/s (collection: 7.057s, learning 0.119s)
             Mean action noise std: 2.72
          Mean value_function loss: 546.8800
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 74.7205
                       Mean reward: 519.83
               Mean episode length: 169.95
    Episode_Reward/reaching_object: 1.2827
     Episode_Reward/lifting_object: 98.8583
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.9167
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 7.18s
                      Time elapsed: 00:37:23
                               ETA: 00:37:16

################################################################################
                     [1m Learning iteration 1002/2000 [0m                     

                       Computation: 14007 steps/s (collection: 6.910s, learning 0.108s)
             Mean action noise std: 2.72
          Mean value_function loss: 510.9452
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 74.7246
                       Mean reward: 509.72
               Mean episode length: 170.43
    Episode_Reward/reaching_object: 1.3194
     Episode_Reward/lifting_object: 102.4041
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0700
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.2917
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 7.02s
                      Time elapsed: 00:37:30
                               ETA: 00:37:18

################################################################################
                     [1m Learning iteration 1003/2000 [0m                     

                       Computation: 13867 steps/s (collection: 6.974s, learning 0.115s)
             Mean action noise std: 2.72
          Mean value_function loss: 544.4486
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 74.7292
                       Mean reward: 473.54
               Mean episode length: 165.32
    Episode_Reward/reaching_object: 1.3688
     Episode_Reward/lifting_object: 106.7851
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0700
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 7.09s
                      Time elapsed: 00:37:37
                               ETA: 00:37:21

################################################################################
                     [1m Learning iteration 1004/2000 [0m                     

                       Computation: 13763 steps/s (collection: 7.002s, learning 0.140s)
             Mean action noise std: 2.72
          Mean value_function loss: 530.7135
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 74.7320
                       Mean reward: 537.91
               Mean episode length: 169.68
    Episode_Reward/reaching_object: 1.3011
     Episode_Reward/lifting_object: 101.5530
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0686
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.0833
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 7.14s
                      Time elapsed: 00:37:44
                               ETA: 00:37:24

################################################################################
                     [1m Learning iteration 1005/2000 [0m                     

                       Computation: 13839 steps/s (collection: 6.990s, learning 0.113s)
             Mean action noise std: 2.72
          Mean value_function loss: 484.0581
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 74.7349
                       Mean reward: 493.28
               Mean episode length: 164.77
    Episode_Reward/reaching_object: 1.2308
     Episode_Reward/lifting_object: 94.8374
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 7.10s
                      Time elapsed: 00:37:51
                               ETA: 00:37:26

################################################################################
                     [1m Learning iteration 1006/2000 [0m                     

                       Computation: 14058 steps/s (collection: 6.864s, learning 0.128s)
             Mean action noise std: 2.72
          Mean value_function loss: 501.0101
               Mean surrogate loss: 0.0112
                 Mean entropy loss: 74.7405
                       Mean reward: 472.58
               Mean episode length: 164.27
    Episode_Reward/reaching_object: 1.2879
     Episode_Reward/lifting_object: 99.5056
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0686
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.9583
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 6.99s
                      Time elapsed: 00:37:58
                               ETA: 00:37:28

################################################################################
                     [1m Learning iteration 1007/2000 [0m                     

                       Computation: 13970 steps/s (collection: 6.920s, learning 0.117s)
             Mean action noise std: 2.72
          Mean value_function loss: 442.1787
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 74.7421
                       Mean reward: 465.09
               Mean episode length: 164.92
    Episode_Reward/reaching_object: 1.2744
     Episode_Reward/lifting_object: 98.1929
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0699
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 7.04s
                      Time elapsed: 00:38:05
                               ETA: 00:37:31

################################################################################
                     [1m Learning iteration 1008/2000 [0m                     

                       Computation: 15938 steps/s (collection: 6.079s, learning 0.089s)
             Mean action noise std: 2.72
          Mean value_function loss: 436.3405
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 74.7433
                       Mean reward: 627.35
               Mean episode length: 194.32
    Episode_Reward/reaching_object: 1.4315
     Episode_Reward/lifting_object: 113.1197
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 6.17s
                      Time elapsed: 00:38:11
                               ETA: 00:37:32

################################################################################
                     [1m Learning iteration 1009/2000 [0m                     

                       Computation: 43922 steps/s (collection: 2.126s, learning 0.112s)
             Mean action noise std: 2.73
          Mean value_function loss: 420.4257
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 74.7457
                       Mean reward: 564.41
               Mean episode length: 177.25
    Episode_Reward/reaching_object: 1.3662
     Episode_Reward/lifting_object: 107.6371
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0729
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.0833
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 2.24s
                      Time elapsed: 00:38:13
                               ETA: 00:37:30

################################################################################
                     [1m Learning iteration 1010/2000 [0m                     

                       Computation: 44537 steps/s (collection: 2.093s, learning 0.115s)
             Mean action noise std: 2.73
          Mean value_function loss: 376.7639
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 74.7470
                       Mean reward: 558.01
               Mean episode length: 179.26
    Episode_Reward/reaching_object: 1.4146
     Episode_Reward/lifting_object: 111.5066
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0751
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.9583
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 2.21s
                      Time elapsed: 00:38:16
                               ETA: 00:37:28

################################################################################
                     [1m Learning iteration 1011/2000 [0m                     

                       Computation: 43872 steps/s (collection: 2.123s, learning 0.118s)
             Mean action noise std: 2.73
          Mean value_function loss: 427.0484
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 74.7491
                       Mean reward: 527.03
               Mean episode length: 174.64
    Episode_Reward/reaching_object: 1.4220
     Episode_Reward/lifting_object: 113.3457
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0755
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 2.24s
                      Time elapsed: 00:38:18
                               ETA: 00:37:26

################################################################################
                     [1m Learning iteration 1012/2000 [0m                     

                       Computation: 44383 steps/s (collection: 2.095s, learning 0.120s)
             Mean action noise std: 2.73
          Mean value_function loss: 484.0558
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 74.7543
                       Mean reward: 570.50
               Mean episode length: 179.38
    Episode_Reward/reaching_object: 1.3995
     Episode_Reward/lifting_object: 110.2897
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0737
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 2.21s
                      Time elapsed: 00:38:20
                               ETA: 00:37:23

################################################################################
                     [1m Learning iteration 1013/2000 [0m                     

                       Computation: 44117 steps/s (collection: 2.113s, learning 0.115s)
             Mean action noise std: 2.73
          Mean value_function loss: 539.3391
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 74.7603
                       Mean reward: 491.86
               Mean episode length: 163.52
    Episode_Reward/reaching_object: 1.2579
     Episode_Reward/lifting_object: 96.9601
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.8333
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 2.23s
                      Time elapsed: 00:38:22
                               ETA: 00:37:21

################################################################################
                     [1m Learning iteration 1014/2000 [0m                     

                       Computation: 43814 steps/s (collection: 2.117s, learning 0.127s)
             Mean action noise std: 2.73
          Mean value_function loss: 515.4108
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 74.7679
                       Mean reward: 545.61
               Mean episode length: 177.99
    Episode_Reward/reaching_object: 1.2779
     Episode_Reward/lifting_object: 99.5755
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 17.2500
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 2.24s
                      Time elapsed: 00:38:24
                               ETA: 00:37:19

################################################################################
                     [1m Learning iteration 1015/2000 [0m                     

                       Computation: 42430 steps/s (collection: 2.149s, learning 0.168s)
             Mean action noise std: 2.73
          Mean value_function loss: 544.8564
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 74.7703
                       Mean reward: 466.45
               Mean episode length: 159.16
    Episode_Reward/reaching_object: 1.2079
     Episode_Reward/lifting_object: 93.7269
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.6250
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 2.32s
                      Time elapsed: 00:38:27
                               ETA: 00:37:16

################################################################################
                     [1m Learning iteration 1016/2000 [0m                     

                       Computation: 43767 steps/s (collection: 2.132s, learning 0.114s)
             Mean action noise std: 2.73
          Mean value_function loss: 530.5230
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 74.7724
                       Mean reward: 541.08
               Mean episode length: 166.00
    Episode_Reward/reaching_object: 1.2754
     Episode_Reward/lifting_object: 98.3351
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.4583
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 2.25s
                      Time elapsed: 00:38:29
                               ETA: 00:37:14

################################################################################
                     [1m Learning iteration 1017/2000 [0m                     

                       Computation: 43822 steps/s (collection: 2.124s, learning 0.119s)
             Mean action noise std: 2.73
          Mean value_function loss: 508.6352
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.7750
                       Mean reward: 515.56
               Mean episode length: 165.17
    Episode_Reward/reaching_object: 1.2928
     Episode_Reward/lifting_object: 100.2903
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 2.24s
                      Time elapsed: 00:38:31
                               ETA: 00:37:12

################################################################################
                     [1m Learning iteration 1018/2000 [0m                     

                       Computation: 41284 steps/s (collection: 2.227s, learning 0.155s)
             Mean action noise std: 2.73
          Mean value_function loss: 521.8945
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 74.7847
                       Mean reward: 502.92
               Mean episode length: 164.40
    Episode_Reward/reaching_object: 1.2595
     Episode_Reward/lifting_object: 98.7331
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.7083
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 2.38s
                      Time elapsed: 00:38:34
                               ETA: 00:37:10

################################################################################
                     [1m Learning iteration 1019/2000 [0m                     

                       Computation: 43262 steps/s (collection: 2.149s, learning 0.124s)
             Mean action noise std: 2.73
          Mean value_function loss: 481.1773
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 74.7985
                       Mean reward: 521.98
               Mean episode length: 178.93
    Episode_Reward/reaching_object: 1.3027
     Episode_Reward/lifting_object: 101.5670
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 2.27s
                      Time elapsed: 00:38:36
                               ETA: 00:37:07

################################################################################
                     [1m Learning iteration 1020/2000 [0m                     

                       Computation: 44280 steps/s (collection: 2.106s, learning 0.114s)
             Mean action noise std: 2.73
          Mean value_function loss: 434.3574
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 74.8069
                       Mean reward: 439.63
               Mean episode length: 157.35
    Episode_Reward/reaching_object: 1.3374
     Episode_Reward/lifting_object: 105.0195
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0704
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 2.22s
                      Time elapsed: 00:38:38
                               ETA: 00:37:05

################################################################################
                     [1m Learning iteration 1021/2000 [0m                     

                       Computation: 44135 steps/s (collection: 2.107s, learning 0.120s)
             Mean action noise std: 2.73
          Mean value_function loss: 437.7012
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 74.8122
                       Mean reward: 557.99
               Mean episode length: 175.01
    Episode_Reward/reaching_object: 1.3923
     Episode_Reward/lifting_object: 110.0818
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0712
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 2.23s
                      Time elapsed: 00:38:40
                               ETA: 00:37:03

################################################################################
                     [1m Learning iteration 1022/2000 [0m                     

                       Computation: 44596 steps/s (collection: 2.094s, learning 0.111s)
             Mean action noise std: 2.73
          Mean value_function loss: 459.9067
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 74.8185
                       Mean reward: 534.09
               Mean episode length: 177.52
    Episode_Reward/reaching_object: 1.4112
     Episode_Reward/lifting_object: 111.8315
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0722
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 2.20s
                      Time elapsed: 00:38:43
                               ETA: 00:37:00

################################################################################
                     [1m Learning iteration 1023/2000 [0m                     

                       Computation: 43471 steps/s (collection: 2.141s, learning 0.121s)
             Mean action noise std: 2.73
          Mean value_function loss: 425.2569
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 74.8211
                       Mean reward: 564.70
               Mean episode length: 179.63
    Episode_Reward/reaching_object: 1.4202
     Episode_Reward/lifting_object: 112.6796
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0737
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 2.26s
                      Time elapsed: 00:38:45
                               ETA: 00:36:58

################################################################################
                     [1m Learning iteration 1024/2000 [0m                     

                       Computation: 44694 steps/s (collection: 2.104s, learning 0.095s)
             Mean action noise std: 2.73
          Mean value_function loss: 423.8886
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 74.8236
                       Mean reward: 594.04
               Mean episode length: 190.24
    Episode_Reward/reaching_object: 1.4465
     Episode_Reward/lifting_object: 115.7258
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0747
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 2.20s
                      Time elapsed: 00:38:47
                               ETA: 00:36:56

################################################################################
                     [1m Learning iteration 1025/2000 [0m                     

                       Computation: 45453 steps/s (collection: 2.076s, learning 0.087s)
             Mean action noise std: 2.73
          Mean value_function loss: 407.5528
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 74.8269
                       Mean reward: 564.00
               Mean episode length: 179.75
    Episode_Reward/reaching_object: 1.4687
     Episode_Reward/lifting_object: 117.3018
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0748
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.6667
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 2.16s
                      Time elapsed: 00:38:49
                               ETA: 00:36:53

################################################################################
                     [1m Learning iteration 1026/2000 [0m                     

                       Computation: 42792 steps/s (collection: 2.155s, learning 0.143s)
             Mean action noise std: 2.73
          Mean value_function loss: 428.6426
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 74.8345
                       Mean reward: 568.36
               Mean episode length: 174.63
    Episode_Reward/reaching_object: 1.3949
     Episode_Reward/lifting_object: 110.1376
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0713
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.8333
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 2.30s
                      Time elapsed: 00:38:52
                               ETA: 00:36:51

################################################################################
                     [1m Learning iteration 1027/2000 [0m                     

                       Computation: 41743 steps/s (collection: 2.207s, learning 0.148s)
             Mean action noise std: 2.74
          Mean value_function loss: 456.2616
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 74.8427
                       Mean reward: 539.76
               Mean episode length: 176.54
    Episode_Reward/reaching_object: 1.4020
     Episode_Reward/lifting_object: 110.6333
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 2.35s
                      Time elapsed: 00:38:54
                               ETA: 00:36:49

################################################################################
                     [1m Learning iteration 1028/2000 [0m                     

                       Computation: 42881 steps/s (collection: 2.195s, learning 0.098s)
             Mean action noise std: 2.74
          Mean value_function loss: 437.3506
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 74.8505
                       Mean reward: 484.38
               Mean episode length: 167.75
    Episode_Reward/reaching_object: 1.3886
     Episode_Reward/lifting_object: 109.6059
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0745
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 2.29s
                      Time elapsed: 00:38:56
                               ETA: 00:36:47

################################################################################
                     [1m Learning iteration 1029/2000 [0m                     

                       Computation: 43766 steps/s (collection: 2.138s, learning 0.109s)
             Mean action noise std: 2.74
          Mean value_function loss: 502.5565
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 74.8547
                       Mean reward: 516.26
               Mean episode length: 169.01
    Episode_Reward/reaching_object: 1.3512
     Episode_Reward/lifting_object: 105.2180
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0715
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.1667
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 2.25s
                      Time elapsed: 00:38:58
                               ETA: 00:36:44

################################################################################
                     [1m Learning iteration 1030/2000 [0m                     

                       Computation: 43312 steps/s (collection: 2.181s, learning 0.089s)
             Mean action noise std: 2.74
          Mean value_function loss: 473.3163
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 74.8620
                       Mean reward: 553.69
               Mean episode length: 178.60
    Episode_Reward/reaching_object: 1.3365
     Episode_Reward/lifting_object: 104.5635
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0715
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.1667
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 2.27s
                      Time elapsed: 00:39:01
                               ETA: 00:36:42

################################################################################
                     [1m Learning iteration 1031/2000 [0m                     

                       Computation: 42884 steps/s (collection: 2.175s, learning 0.118s)
             Mean action noise std: 2.74
          Mean value_function loss: 508.6128
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 74.8685
                       Mean reward: 502.42
               Mean episode length: 164.55
    Episode_Reward/reaching_object: 1.3394
     Episode_Reward/lifting_object: 104.8364
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0705
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.9583
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 2.29s
                      Time elapsed: 00:39:03
                               ETA: 00:36:40

################################################################################
                     [1m Learning iteration 1032/2000 [0m                     

                       Computation: 44001 steps/s (collection: 2.123s, learning 0.112s)
             Mean action noise std: 2.74
          Mean value_function loss: 505.1592
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 74.8768
                       Mean reward: 565.64
               Mean episode length: 180.11
    Episode_Reward/reaching_object: 1.4272
     Episode_Reward/lifting_object: 112.5234
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0749
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 2.23s
                      Time elapsed: 00:39:05
                               ETA: 00:36:38

################################################################################
                     [1m Learning iteration 1033/2000 [0m                     

                       Computation: 43542 steps/s (collection: 2.144s, learning 0.114s)
             Mean action noise std: 2.74
          Mean value_function loss: 495.9838
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 74.8814
                       Mean reward: 591.26
               Mean episode length: 185.46
    Episode_Reward/reaching_object: 1.3555
     Episode_Reward/lifting_object: 107.2856
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0713
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.9583
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 2.26s
                      Time elapsed: 00:39:07
                               ETA: 00:36:35

################################################################################
                     [1m Learning iteration 1034/2000 [0m                     

                       Computation: 44748 steps/s (collection: 2.099s, learning 0.098s)
             Mean action noise std: 2.74
          Mean value_function loss: 493.2186
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 74.8860
                       Mean reward: 512.08
               Mean episode length: 172.60
    Episode_Reward/reaching_object: 1.3822
     Episode_Reward/lifting_object: 108.6450
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0724
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.2500
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 2.20s
                      Time elapsed: 00:39:10
                               ETA: 00:36:33

################################################################################
                     [1m Learning iteration 1035/2000 [0m                     

                       Computation: 42488 steps/s (collection: 2.214s, learning 0.100s)
             Mean action noise std: 2.74
          Mean value_function loss: 512.7873
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 74.8910
                       Mean reward: 563.71
               Mean episode length: 179.74
    Episode_Reward/reaching_object: 1.3728
     Episode_Reward/lifting_object: 108.6305
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 2.31s
                      Time elapsed: 00:39:12
                               ETA: 00:36:31

################################################################################
                     [1m Learning iteration 1036/2000 [0m                     

                       Computation: 44608 steps/s (collection: 2.111s, learning 0.093s)
             Mean action noise std: 2.74
          Mean value_function loss: 458.9709
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 74.8978
                       Mean reward: 504.84
               Mean episode length: 168.77
    Episode_Reward/reaching_object: 1.4063
     Episode_Reward/lifting_object: 110.6142
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0735
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 2.20s
                      Time elapsed: 00:39:14
                               ETA: 00:36:28

################################################################################
                     [1m Learning iteration 1037/2000 [0m                     

                       Computation: 42390 steps/s (collection: 2.187s, learning 0.132s)
             Mean action noise std: 2.74
          Mean value_function loss: 455.2362
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 74.9088
                       Mean reward: 522.85
               Mean episode length: 178.85
    Episode_Reward/reaching_object: 1.4437
     Episode_Reward/lifting_object: 115.2413
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0735
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 2.32s
                      Time elapsed: 00:39:16
                               ETA: 00:36:26

################################################################################
                     [1m Learning iteration 1038/2000 [0m                     

                       Computation: 44094 steps/s (collection: 2.139s, learning 0.091s)
             Mean action noise std: 2.74
          Mean value_function loss: 515.2645
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 74.9208
                       Mean reward: 526.26
               Mean episode length: 167.83
    Episode_Reward/reaching_object: 1.4066
     Episode_Reward/lifting_object: 111.1351
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0727
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.8333
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 2.23s
                      Time elapsed: 00:39:19
                               ETA: 00:36:24

################################################################################
                     [1m Learning iteration 1039/2000 [0m                     

                       Computation: 44562 steps/s (collection: 2.115s, learning 0.090s)
             Mean action noise std: 2.74
          Mean value_function loss: 474.3601
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 74.9262
                       Mean reward: 479.52
               Mean episode length: 166.15
    Episode_Reward/reaching_object: 1.3981
     Episode_Reward/lifting_object: 110.4128
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0725
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 2.21s
                      Time elapsed: 00:39:21
                               ETA: 00:36:22

################################################################################
                     [1m Learning iteration 1040/2000 [0m                     

                       Computation: 44104 steps/s (collection: 2.120s, learning 0.109s)
             Mean action noise std: 2.74
          Mean value_function loss: 458.6271
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 74.9310
                       Mean reward: 514.56
               Mean episode length: 170.22
    Episode_Reward/reaching_object: 1.3896
     Episode_Reward/lifting_object: 109.7320
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0714
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 2.23s
                      Time elapsed: 00:39:23
                               ETA: 00:36:19

################################################################################
                     [1m Learning iteration 1041/2000 [0m                     

                       Computation: 43975 steps/s (collection: 2.125s, learning 0.110s)
             Mean action noise std: 2.74
          Mean value_function loss: 476.7294
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 74.9335
                       Mean reward: 548.18
               Mean episode length: 176.09
    Episode_Reward/reaching_object: 1.3698
     Episode_Reward/lifting_object: 107.1761
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0720
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 2.24s
                      Time elapsed: 00:39:25
                               ETA: 00:36:17

################################################################################
                     [1m Learning iteration 1042/2000 [0m                     

                       Computation: 41211 steps/s (collection: 2.300s, learning 0.086s)
             Mean action noise std: 2.75
          Mean value_function loss: 437.1931
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 74.9364
                       Mean reward: 593.37
               Mean episode length: 188.70
    Episode_Reward/reaching_object: 1.4638
     Episode_Reward/lifting_object: 115.6852
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0748
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 2.39s
                      Time elapsed: 00:39:28
                               ETA: 00:36:15

################################################################################
                     [1m Learning iteration 1043/2000 [0m                     

                       Computation: 43566 steps/s (collection: 2.124s, learning 0.132s)
             Mean action noise std: 2.75
          Mean value_function loss: 440.6695
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 74.9433
                       Mean reward: 601.78
               Mean episode length: 191.42
    Episode_Reward/reaching_object: 1.4189
     Episode_Reward/lifting_object: 112.4383
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0733
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 2.26s
                      Time elapsed: 00:39:30
                               ETA: 00:36:12

################################################################################
                     [1m Learning iteration 1044/2000 [0m                     

                       Computation: 44535 steps/s (collection: 2.117s, learning 0.091s)
             Mean action noise std: 2.75
          Mean value_function loss: 451.4272
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 74.9517
                       Mean reward: 543.95
               Mean episode length: 175.26
    Episode_Reward/reaching_object: 1.4581
     Episode_Reward/lifting_object: 116.4626
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 2.21s
                      Time elapsed: 00:39:32
                               ETA: 00:36:10

################################################################################
                     [1m Learning iteration 1045/2000 [0m                     

                       Computation: 43399 steps/s (collection: 2.151s, learning 0.114s)
             Mean action noise std: 2.75
          Mean value_function loss: 454.8679
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 74.9575
                       Mean reward: 599.02
               Mean episode length: 181.72
    Episode_Reward/reaching_object: 1.4854
     Episode_Reward/lifting_object: 119.3527
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.1667
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 2.27s
                      Time elapsed: 00:39:35
                               ETA: 00:36:08

################################################################################
                     [1m Learning iteration 1046/2000 [0m                     

                       Computation: 43886 steps/s (collection: 2.126s, learning 0.114s)
             Mean action noise std: 2.75
          Mean value_function loss: 498.4538
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 74.9600
                       Mean reward: 548.53
               Mean episode length: 185.01
    Episode_Reward/reaching_object: 1.4656
     Episode_Reward/lifting_object: 116.9751
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0749
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 2.24s
                      Time elapsed: 00:39:37
                               ETA: 00:36:06

################################################################################
                     [1m Learning iteration 1047/2000 [0m                     

                       Computation: 45425 steps/s (collection: 2.064s, learning 0.101s)
             Mean action noise std: 2.75
          Mean value_function loss: 449.9494
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 74.9625
                       Mean reward: 582.71
               Mean episode length: 184.03
    Episode_Reward/reaching_object: 1.3563
     Episode_Reward/lifting_object: 106.6007
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 2.16s
                      Time elapsed: 00:39:39
                               ETA: 00:36:03

################################################################################
                     [1m Learning iteration 1048/2000 [0m                     

                       Computation: 41284 steps/s (collection: 2.239s, learning 0.142s)
             Mean action noise std: 2.75
          Mean value_function loss: 464.2679
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.9658
                       Mean reward: 614.34
               Mean episode length: 189.67
    Episode_Reward/reaching_object: 1.4562
     Episode_Reward/lifting_object: 115.9122
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 2.38s
                      Time elapsed: 00:39:41
                               ETA: 00:36:01

################################################################################
                     [1m Learning iteration 1049/2000 [0m                     

                       Computation: 42446 steps/s (collection: 2.208s, learning 0.108s)
             Mean action noise std: 2.75
          Mean value_function loss: 510.8403
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 74.9712
                       Mean reward: 565.19
               Mean episode length: 178.14
    Episode_Reward/reaching_object: 1.4089
     Episode_Reward/lifting_object: 111.4507
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.8333
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 2.32s
                      Time elapsed: 00:39:44
                               ETA: 00:35:59

################################################################################
                     [1m Learning iteration 1050/2000 [0m                     

                       Computation: 44769 steps/s (collection: 2.088s, learning 0.108s)
             Mean action noise std: 2.75
          Mean value_function loss: 456.9896
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 74.9777
                       Mean reward: 544.57
               Mean episode length: 172.56
    Episode_Reward/reaching_object: 1.3855
     Episode_Reward/lifting_object: 110.0846
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0713
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 2.20s
                      Time elapsed: 00:39:46
                               ETA: 00:35:56

################################################################################
                     [1m Learning iteration 1051/2000 [0m                     

                       Computation: 40578 steps/s (collection: 2.311s, learning 0.111s)
             Mean action noise std: 2.75
          Mean value_function loss: 434.7292
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.9856
                       Mean reward: 589.55
               Mean episode length: 185.30
    Episode_Reward/reaching_object: 1.4445
     Episode_Reward/lifting_object: 114.4124
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0749
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 2.42s
                      Time elapsed: 00:39:48
                               ETA: 00:35:54

################################################################################
                     [1m Learning iteration 1052/2000 [0m                     

                       Computation: 43697 steps/s (collection: 2.150s, learning 0.100s)
             Mean action noise std: 2.75
          Mean value_function loss: 468.2649
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 74.9910
                       Mean reward: 572.52
               Mean episode length: 177.14
    Episode_Reward/reaching_object: 1.4271
     Episode_Reward/lifting_object: 114.6695
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0728
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 2.25s
                      Time elapsed: 00:39:50
                               ETA: 00:35:52

################################################################################
                     [1m Learning iteration 1053/2000 [0m                     

                       Computation: 42704 steps/s (collection: 2.188s, learning 0.114s)
             Mean action noise std: 2.75
          Mean value_function loss: 452.7761
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 74.9957
                       Mean reward: 584.22
               Mean episode length: 183.20
    Episode_Reward/reaching_object: 1.4656
     Episode_Reward/lifting_object: 117.1522
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0743
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.2917
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 2.30s
                      Time elapsed: 00:39:53
                               ETA: 00:35:50

################################################################################
                     [1m Learning iteration 1054/2000 [0m                     

                       Computation: 42721 steps/s (collection: 2.167s, learning 0.134s)
             Mean action noise std: 2.75
          Mean value_function loss: 462.4891
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 75.0024
                       Mean reward: 576.78
               Mean episode length: 184.32
    Episode_Reward/reaching_object: 1.4544
     Episode_Reward/lifting_object: 116.1168
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0734
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 2.30s
                      Time elapsed: 00:39:55
                               ETA: 00:35:48

################################################################################
                     [1m Learning iteration 1055/2000 [0m                     

                       Computation: 41659 steps/s (collection: 2.260s, learning 0.100s)
             Mean action noise std: 2.75
          Mean value_function loss: 472.2349
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 75.0096
                       Mean reward: 633.34
               Mean episode length: 195.75
    Episode_Reward/reaching_object: 1.4598
     Episode_Reward/lifting_object: 117.0900
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0748
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 2.36s
                      Time elapsed: 00:39:57
                               ETA: 00:35:45

################################################################################
                     [1m Learning iteration 1056/2000 [0m                     

                       Computation: 45136 steps/s (collection: 2.089s, learning 0.089s)
             Mean action noise std: 2.75
          Mean value_function loss: 461.4052
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 75.0175
                       Mean reward: 573.37
               Mean episode length: 182.20
    Episode_Reward/reaching_object: 1.4220
     Episode_Reward/lifting_object: 113.2301
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 2.18s
                      Time elapsed: 00:40:00
                               ETA: 00:35:43

################################################################################
                     [1m Learning iteration 1057/2000 [0m                     

                       Computation: 43572 steps/s (collection: 2.168s, learning 0.089s)
             Mean action noise std: 2.75
          Mean value_function loss: 518.0366
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 75.0260
                       Mean reward: 559.23
               Mean episode length: 176.84
    Episode_Reward/reaching_object: 1.4457
     Episode_Reward/lifting_object: 115.1905
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0737
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 2.26s
                      Time elapsed: 00:40:02
                               ETA: 00:35:41

################################################################################
                     [1m Learning iteration 1058/2000 [0m                     

                       Computation: 42143 steps/s (collection: 2.193s, learning 0.140s)
             Mean action noise std: 2.76
          Mean value_function loss: 492.3300
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 75.0351
                       Mean reward: 580.64
               Mean episode length: 184.30
    Episode_Reward/reaching_object: 1.4371
     Episode_Reward/lifting_object: 115.1672
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 2.33s
                      Time elapsed: 00:40:04
                               ETA: 00:35:39

################################################################################
                     [1m Learning iteration 1059/2000 [0m                     

                       Computation: 42328 steps/s (collection: 2.203s, learning 0.120s)
             Mean action noise std: 2.76
          Mean value_function loss: 421.0202
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 75.0408
                       Mean reward: 552.79
               Mean episode length: 179.34
    Episode_Reward/reaching_object: 1.4595
     Episode_Reward/lifting_object: 117.0316
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0740
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 2.32s
                      Time elapsed: 00:40:07
                               ETA: 00:35:36

################################################################################
                     [1m Learning iteration 1060/2000 [0m                     

                       Computation: 44366 steps/s (collection: 2.108s, learning 0.108s)
             Mean action noise std: 2.76
          Mean value_function loss: 474.7437
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 75.0469
                       Mean reward: 547.16
               Mean episode length: 173.70
    Episode_Reward/reaching_object: 1.4149
     Episode_Reward/lifting_object: 112.7365
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0733
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 2.22s
                      Time elapsed: 00:40:09
                               ETA: 00:35:34

################################################################################
                     [1m Learning iteration 1061/2000 [0m                     

                       Computation: 44632 steps/s (collection: 2.096s, learning 0.106s)
             Mean action noise std: 2.76
          Mean value_function loss: 455.8448
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 75.0549
                       Mean reward: 667.61
               Mean episode length: 198.55
    Episode_Reward/reaching_object: 1.4629
     Episode_Reward/lifting_object: 118.0783
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0739
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 2.20s
                      Time elapsed: 00:40:11
                               ETA: 00:35:32

################################################################################
                     [1m Learning iteration 1062/2000 [0m                     

                       Computation: 42426 steps/s (collection: 2.175s, learning 0.142s)
             Mean action noise std: 2.76
          Mean value_function loss: 523.1937
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 75.0616
                       Mean reward: 574.62
               Mean episode length: 181.97
    Episode_Reward/reaching_object: 1.4183
     Episode_Reward/lifting_object: 113.2394
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0731
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 2.32s
                      Time elapsed: 00:40:13
                               ETA: 00:35:29

################################################################################
                     [1m Learning iteration 1063/2000 [0m                     

                       Computation: 43048 steps/s (collection: 2.170s, learning 0.114s)
             Mean action noise std: 2.76
          Mean value_function loss: 503.5953
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 75.0669
                       Mean reward: 517.88
               Mean episode length: 165.69
    Episode_Reward/reaching_object: 1.4391
     Episode_Reward/lifting_object: 115.3965
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0739
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 2.28s
                      Time elapsed: 00:40:16
                               ETA: 00:35:27

################################################################################
                     [1m Learning iteration 1064/2000 [0m                     

                       Computation: 41535 steps/s (collection: 2.209s, learning 0.158s)
             Mean action noise std: 2.76
          Mean value_function loss: 407.0854
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 75.0753
                       Mean reward: 587.27
               Mean episode length: 185.02
    Episode_Reward/reaching_object: 1.5018
     Episode_Reward/lifting_object: 121.7738
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0744
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 2.37s
                      Time elapsed: 00:40:18
                               ETA: 00:35:25

################################################################################
                     [1m Learning iteration 1065/2000 [0m                     

                       Computation: 43881 steps/s (collection: 2.137s, learning 0.103s)
             Mean action noise std: 2.76
          Mean value_function loss: 419.4553
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 75.0805
                       Mean reward: 581.14
               Mean episode length: 184.45
    Episode_Reward/reaching_object: 1.5104
     Episode_Reward/lifting_object: 122.4016
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0766
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 2.24s
                      Time elapsed: 00:40:20
                               ETA: 00:35:23

################################################################################
                     [1m Learning iteration 1066/2000 [0m                     

                       Computation: 41624 steps/s (collection: 2.253s, learning 0.109s)
             Mean action noise std: 2.76
          Mean value_function loss: 487.6145
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 75.0870
                       Mean reward: 645.06
               Mean episode length: 193.05
    Episode_Reward/reaching_object: 1.4841
     Episode_Reward/lifting_object: 119.4411
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0750
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.0833
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 2.36s
                      Time elapsed: 00:40:23
                               ETA: 00:35:20

################################################################################
                     [1m Learning iteration 1067/2000 [0m                     

                       Computation: 44307 steps/s (collection: 2.128s, learning 0.091s)
             Mean action noise std: 2.76
          Mean value_function loss: 460.8220
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 75.0978
                       Mean reward: 495.87
               Mean episode length: 166.34
    Episode_Reward/reaching_object: 1.4847
     Episode_Reward/lifting_object: 120.0559
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0761
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.7917
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 2.22s
                      Time elapsed: 00:40:25
                               ETA: 00:35:18

################################################################################
                     [1m Learning iteration 1068/2000 [0m                     

                       Computation: 44361 steps/s (collection: 2.123s, learning 0.093s)
             Mean action noise std: 2.76
          Mean value_function loss: 472.8630
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 75.1078
                       Mean reward: 572.77
               Mean episode length: 183.59
    Episode_Reward/reaching_object: 1.4832
     Episode_Reward/lifting_object: 119.7462
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0750
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.7917
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 2.22s
                      Time elapsed: 00:40:27
                               ETA: 00:35:16

################################################################################
                     [1m Learning iteration 1069/2000 [0m                     

                       Computation: 44664 steps/s (collection: 2.102s, learning 0.099s)
             Mean action noise std: 2.76
          Mean value_function loss: 495.4928
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 75.1157
                       Mean reward: 612.60
               Mean episode length: 190.51
    Episode_Reward/reaching_object: 1.5447
     Episode_Reward/lifting_object: 125.1902
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0762
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 2.20s
                      Time elapsed: 00:40:29
                               ETA: 00:35:14

################################################################################
                     [1m Learning iteration 1070/2000 [0m                     

                       Computation: 44497 steps/s (collection: 2.118s, learning 0.091s)
             Mean action noise std: 2.76
          Mean value_function loss: 466.7124
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 75.1199
                       Mean reward: 656.83
               Mean episode length: 192.30
    Episode_Reward/reaching_object: 1.5696
     Episode_Reward/lifting_object: 127.6602
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0778
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.9583
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 2.21s
                      Time elapsed: 00:40:31
                               ETA: 00:35:11

################################################################################
                     [1m Learning iteration 1071/2000 [0m                     

                       Computation: 44622 steps/s (collection: 2.116s, learning 0.087s)
             Mean action noise std: 2.77
          Mean value_function loss: 467.9920
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 75.1281
                       Mean reward: 596.18
               Mean episode length: 184.56
    Episode_Reward/reaching_object: 1.4633
     Episode_Reward/lifting_object: 117.3709
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0756
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 2.20s
                      Time elapsed: 00:40:34
                               ETA: 00:35:09

################################################################################
                     [1m Learning iteration 1072/2000 [0m                     

                       Computation: 44281 steps/s (collection: 2.132s, learning 0.088s)
             Mean action noise std: 2.77
          Mean value_function loss: 435.6884
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 75.1339
                       Mean reward: 647.15
               Mean episode length: 197.37
    Episode_Reward/reaching_object: 1.5416
     Episode_Reward/lifting_object: 125.3768
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0760
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 2.22s
                      Time elapsed: 00:40:36
                               ETA: 00:35:07

################################################################################
                     [1m Learning iteration 1073/2000 [0m                     

                       Computation: 44638 steps/s (collection: 2.114s, learning 0.089s)
             Mean action noise std: 2.77
          Mean value_function loss: 480.7663
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 75.1395
                       Mean reward: 654.17
               Mean episode length: 197.58
    Episode_Reward/reaching_object: 1.5205
     Episode_Reward/lifting_object: 122.7020
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0764
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 2.20s
                      Time elapsed: 00:40:38
                               ETA: 00:35:04

################################################################################
                     [1m Learning iteration 1074/2000 [0m                     

                       Computation: 44639 steps/s (collection: 2.105s, learning 0.097s)
             Mean action noise std: 2.77
          Mean value_function loss: 502.3091
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 75.1474
                       Mean reward: 637.15
               Mean episode length: 192.93
    Episode_Reward/reaching_object: 1.5156
     Episode_Reward/lifting_object: 122.2435
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0751
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 2.20s
                      Time elapsed: 00:40:40
                               ETA: 00:35:02

################################################################################
                     [1m Learning iteration 1075/2000 [0m                     

                       Computation: 44132 steps/s (collection: 2.135s, learning 0.092s)
             Mean action noise std: 2.77
          Mean value_function loss: 469.1164
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 75.1560
                       Mean reward: 528.42
               Mean episode length: 174.71
    Episode_Reward/reaching_object: 1.4490
     Episode_Reward/lifting_object: 115.5083
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0751
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 2.23s
                      Time elapsed: 00:40:42
                               ETA: 00:35:00

################################################################################
                     [1m Learning iteration 1076/2000 [0m                     

                       Computation: 44357 steps/s (collection: 2.129s, learning 0.087s)
             Mean action noise std: 2.77
          Mean value_function loss: 463.9994
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 75.1659
                       Mean reward: 661.89
               Mean episode length: 195.24
    Episode_Reward/reaching_object: 1.5393
     Episode_Reward/lifting_object: 124.5950
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0765
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 2.22s
                      Time elapsed: 00:40:45
                               ETA: 00:34:57

################################################################################
                     [1m Learning iteration 1077/2000 [0m                     

                       Computation: 44968 steps/s (collection: 2.081s, learning 0.105s)
             Mean action noise std: 2.77
          Mean value_function loss: 433.0708
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 75.1715
                       Mean reward: 569.92
               Mean episode length: 184.85
    Episode_Reward/reaching_object: 1.5049
     Episode_Reward/lifting_object: 121.0867
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0774
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.2083
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 2.19s
                      Time elapsed: 00:40:47
                               ETA: 00:34:55

################################################################################
                     [1m Learning iteration 1078/2000 [0m                     

                       Computation: 44636 steps/s (collection: 2.099s, learning 0.103s)
             Mean action noise std: 2.77
          Mean value_function loss: 474.6345
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 75.1778
                       Mean reward: 658.80
               Mean episode length: 197.46
    Episode_Reward/reaching_object: 1.5218
     Episode_Reward/lifting_object: 121.9717
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0768
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.7917
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 2.20s
                      Time elapsed: 00:40:49
                               ETA: 00:34:53

################################################################################
                     [1m Learning iteration 1079/2000 [0m                     

                       Computation: 45143 steps/s (collection: 2.091s, learning 0.087s)
             Mean action noise std: 2.77
          Mean value_function loss: 496.6697
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 75.1862
                       Mean reward: 538.16
               Mean episode length: 175.10
    Episode_Reward/reaching_object: 1.4746
     Episode_Reward/lifting_object: 118.3355
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0752
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.5417
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 2.18s
                      Time elapsed: 00:40:51
                               ETA: 00:34:50

################################################################################
                     [1m Learning iteration 1080/2000 [0m                     

                       Computation: 42609 steps/s (collection: 2.132s, learning 0.175s)
             Mean action noise std: 2.77
          Mean value_function loss: 488.7369
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 75.1970
                       Mean reward: 581.41
               Mean episode length: 183.09
    Episode_Reward/reaching_object: 1.4713
     Episode_Reward/lifting_object: 117.4167
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0747
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.7083
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 2.31s
                      Time elapsed: 00:40:54
                               ETA: 00:34:48

################################################################################
                     [1m Learning iteration 1081/2000 [0m                     

                       Computation: 43058 steps/s (collection: 2.185s, learning 0.098s)
             Mean action noise std: 2.77
          Mean value_function loss: 535.4264
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 75.2024
                       Mean reward: 511.99
               Mean episode length: 168.31
    Episode_Reward/reaching_object: 1.5060
     Episode_Reward/lifting_object: 121.4877
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 2.28s
                      Time elapsed: 00:40:56
                               ETA: 00:34:46

################################################################################
                     [1m Learning iteration 1082/2000 [0m                     

                       Computation: 44668 steps/s (collection: 2.110s, learning 0.091s)
             Mean action noise std: 2.77
          Mean value_function loss: 473.4058
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 75.2089
                       Mean reward: 633.18
               Mean episode length: 187.02
    Episode_Reward/reaching_object: 1.4769
     Episode_Reward/lifting_object: 119.1691
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0741
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 2.20s
                      Time elapsed: 00:40:58
                               ETA: 00:34:43

################################################################################
                     [1m Learning iteration 1083/2000 [0m                     

                       Computation: 44498 steps/s (collection: 2.121s, learning 0.088s)
             Mean action noise std: 2.77
          Mean value_function loss: 509.4368
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 75.2151
                       Mean reward: 607.75
               Mean episode length: 184.28
    Episode_Reward/reaching_object: 1.4550
     Episode_Reward/lifting_object: 116.1266
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0728
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 2.21s
                      Time elapsed: 00:41:00
                               ETA: 00:34:41

################################################################################
                     [1m Learning iteration 1084/2000 [0m                     

                       Computation: 42901 steps/s (collection: 2.173s, learning 0.119s)
             Mean action noise std: 2.77
          Mean value_function loss: 466.0823
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 75.2214
                       Mean reward: 656.12
               Mean episode length: 195.47
    Episode_Reward/reaching_object: 1.5464
     Episode_Reward/lifting_object: 125.8756
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0761
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 2.29s
                      Time elapsed: 00:41:02
                               ETA: 00:34:39

################################################################################
                     [1m Learning iteration 1085/2000 [0m                     

                       Computation: 40820 steps/s (collection: 2.297s, learning 0.111s)
             Mean action noise std: 2.78
          Mean value_function loss: 442.8103
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 75.2288
                       Mean reward: 611.11
               Mean episode length: 187.91
    Episode_Reward/reaching_object: 1.5226
     Episode_Reward/lifting_object: 122.8224
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0754
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 2.41s
                      Time elapsed: 00:41:05
                               ETA: 00:34:37

################################################################################
                     [1m Learning iteration 1086/2000 [0m                     

                       Computation: 44383 steps/s (collection: 2.124s, learning 0.091s)
             Mean action noise std: 2.78
          Mean value_function loss: 437.5419
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 75.2354
                       Mean reward: 617.61
               Mean episode length: 188.42
    Episode_Reward/reaching_object: 1.5370
     Episode_Reward/lifting_object: 125.1727
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0747
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 2.21s
                      Time elapsed: 00:41:07
                               ETA: 00:34:34

################################################################################
                     [1m Learning iteration 1087/2000 [0m                     

                       Computation: 45184 steps/s (collection: 2.084s, learning 0.092s)
             Mean action noise std: 2.78
          Mean value_function loss: 517.8001
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 75.2440
                       Mean reward: 650.34
               Mean episode length: 197.11
    Episode_Reward/reaching_object: 1.5025
     Episode_Reward/lifting_object: 121.5933
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0739
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 2.18s
                      Time elapsed: 00:41:09
                               ETA: 00:34:32

################################################################################
                     [1m Learning iteration 1088/2000 [0m                     

                       Computation: 44359 steps/s (collection: 2.089s, learning 0.127s)
             Mean action noise std: 2.78
          Mean value_function loss: 504.6652
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 75.2492
                       Mean reward: 542.04
               Mean episode length: 174.88
    Episode_Reward/reaching_object: 1.5043
     Episode_Reward/lifting_object: 122.0224
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0760
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 2.22s
                      Time elapsed: 00:41:12
                               ETA: 00:34:30

################################################################################
                     [1m Learning iteration 1089/2000 [0m                     

                       Computation: 43585 steps/s (collection: 2.108s, learning 0.148s)
             Mean action noise std: 2.78
          Mean value_function loss: 528.4896
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 75.2555
                       Mean reward: 583.46
               Mean episode length: 185.24
    Episode_Reward/reaching_object: 1.5153
     Episode_Reward/lifting_object: 122.0529
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0757
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.6667
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 2.26s
                      Time elapsed: 00:41:14
                               ETA: 00:34:27

################################################################################
                     [1m Learning iteration 1090/2000 [0m                     

                       Computation: 43224 steps/s (collection: 2.157s, learning 0.118s)
             Mean action noise std: 2.78
          Mean value_function loss: 493.8619
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 75.2600
                       Mean reward: 591.79
               Mean episode length: 184.15
    Episode_Reward/reaching_object: 1.4500
     Episode_Reward/lifting_object: 117.1084
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0734
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 2.27s
                      Time elapsed: 00:41:16
                               ETA: 00:34:25

################################################################################
                     [1m Learning iteration 1091/2000 [0m                     

                       Computation: 44035 steps/s (collection: 2.141s, learning 0.091s)
             Mean action noise std: 2.78
          Mean value_function loss: 504.3642
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 75.2652
                       Mean reward: 598.27
               Mean episode length: 182.89
    Episode_Reward/reaching_object: 1.5262
     Episode_Reward/lifting_object: 123.9308
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0745
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.5417
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 2.23s
                      Time elapsed: 00:41:18
                               ETA: 00:34:23

################################################################################
                     [1m Learning iteration 1092/2000 [0m                     

                       Computation: 41057 steps/s (collection: 2.271s, learning 0.124s)
             Mean action noise std: 2.78
          Mean value_function loss: 487.9074
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 75.2727
                       Mean reward: 575.93
               Mean episode length: 181.18
    Episode_Reward/reaching_object: 1.5276
     Episode_Reward/lifting_object: 123.5767
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0761
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 2.39s
                      Time elapsed: 00:41:21
                               ETA: 00:34:21

################################################################################
                     [1m Learning iteration 1093/2000 [0m                     

                       Computation: 43055 steps/s (collection: 2.186s, learning 0.098s)
             Mean action noise std: 2.78
          Mean value_function loss: 450.6308
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 75.2815
                       Mean reward: 598.57
               Mean episode length: 181.72
    Episode_Reward/reaching_object: 1.4221
     Episode_Reward/lifting_object: 113.4762
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0742
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 2.28s
                      Time elapsed: 00:41:23
                               ETA: 00:34:18

################################################################################
                     [1m Learning iteration 1094/2000 [0m                     

                       Computation: 41789 steps/s (collection: 2.250s, learning 0.103s)
             Mean action noise std: 2.78
          Mean value_function loss: 465.1878
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 75.2843
                       Mean reward: 595.05
               Mean episode length: 183.29
    Episode_Reward/reaching_object: 1.5030
     Episode_Reward/lifting_object: 122.0762
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0757
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 2.35s
                      Time elapsed: 00:41:25
                               ETA: 00:34:16

################################################################################
                     [1m Learning iteration 1095/2000 [0m                     

                       Computation: 41998 steps/s (collection: 2.181s, learning 0.159s)
             Mean action noise std: 2.78
          Mean value_function loss: 470.9780
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 75.2899
                       Mean reward: 635.80
               Mean episode length: 194.53
    Episode_Reward/reaching_object: 1.4728
     Episode_Reward/lifting_object: 118.4818
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0741
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 2.34s
                      Time elapsed: 00:41:28
                               ETA: 00:34:14

################################################################################
                     [1m Learning iteration 1096/2000 [0m                     

                       Computation: 44412 steps/s (collection: 2.114s, learning 0.100s)
             Mean action noise std: 2.78
          Mean value_function loss: 437.7880
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 75.2972
                       Mean reward: 694.44
               Mean episode length: 201.05
    Episode_Reward/reaching_object: 1.5831
     Episode_Reward/lifting_object: 128.8321
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0776
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 2.21s
                      Time elapsed: 00:41:30
                               ETA: 00:34:12

################################################################################
                     [1m Learning iteration 1097/2000 [0m                     

                       Computation: 43309 steps/s (collection: 2.176s, learning 0.094s)
             Mean action noise std: 2.78
          Mean value_function loss: 453.0574
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 75.3123
                       Mean reward: 644.07
               Mean episode length: 198.12
    Episode_Reward/reaching_object: 1.5828
     Episode_Reward/lifting_object: 129.1332
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0799
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 2.27s
                      Time elapsed: 00:41:32
                               ETA: 00:34:09

################################################################################
                     [1m Learning iteration 1098/2000 [0m                     

                       Computation: 43560 steps/s (collection: 2.149s, learning 0.108s)
             Mean action noise std: 2.79
          Mean value_function loss: 472.8156
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 75.3245
                       Mean reward: 718.45
               Mean episode length: 205.44
    Episode_Reward/reaching_object: 1.6192
     Episode_Reward/lifting_object: 132.7431
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0783
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 2.26s
                      Time elapsed: 00:41:34
                               ETA: 00:34:07

################################################################################
                     [1m Learning iteration 1099/2000 [0m                     

                       Computation: 43282 steps/s (collection: 2.122s, learning 0.150s)
             Mean action noise std: 2.79
          Mean value_function loss: 450.6464
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 75.3350
                       Mean reward: 671.03
               Mean episode length: 200.06
    Episode_Reward/reaching_object: 1.5767
     Episode_Reward/lifting_object: 128.4642
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0768
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 2.27s
                      Time elapsed: 00:41:37
                               ETA: 00:34:05

################################################################################
                     [1m Learning iteration 1100/2000 [0m                     

                       Computation: 44647 steps/s (collection: 2.110s, learning 0.092s)
             Mean action noise std: 2.79
          Mean value_function loss: 496.1077
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 75.3469
                       Mean reward: 670.06
               Mean episode length: 201.10
    Episode_Reward/reaching_object: 1.5772
     Episode_Reward/lifting_object: 128.5488
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0785
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 2.20s
                      Time elapsed: 00:41:39
                               ETA: 00:34:03

################################################################################
                     [1m Learning iteration 1101/2000 [0m                     

                       Computation: 43684 steps/s (collection: 2.151s, learning 0.100s)
             Mean action noise std: 2.79
          Mean value_function loss: 603.0430
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 75.3595
                       Mean reward: 651.08
               Mean episode length: 194.25
    Episode_Reward/reaching_object: 1.4977
     Episode_Reward/lifting_object: 120.7190
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0754
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 2.25s
                      Time elapsed: 00:41:41
                               ETA: 00:34:00

################################################################################
                     [1m Learning iteration 1102/2000 [0m                     

                       Computation: 43681 steps/s (collection: 2.155s, learning 0.095s)
             Mean action noise std: 2.79
          Mean value_function loss: 550.6836
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 75.3674
                       Mean reward: 631.46
               Mean episode length: 192.91
    Episode_Reward/reaching_object: 1.5986
     Episode_Reward/lifting_object: 130.3634
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0776
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 2.25s
                      Time elapsed: 00:41:43
                               ETA: 00:33:58

################################################################################
                     [1m Learning iteration 1103/2000 [0m                     

                       Computation: 43325 steps/s (collection: 2.103s, learning 0.166s)
             Mean action noise std: 2.79
          Mean value_function loss: 515.3992
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 75.3707
                       Mean reward: 590.09
               Mean episode length: 188.68
    Episode_Reward/reaching_object: 1.4723
     Episode_Reward/lifting_object: 117.7455
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0761
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 2.27s
                      Time elapsed: 00:41:46
                               ETA: 00:33:56

################################################################################
                     [1m Learning iteration 1104/2000 [0m                     

                       Computation: 44313 steps/s (collection: 2.113s, learning 0.106s)
             Mean action noise std: 2.79
          Mean value_function loss: 506.8872
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 75.3771
                       Mean reward: 606.87
               Mean episode length: 186.49
    Episode_Reward/reaching_object: 1.5214
     Episode_Reward/lifting_object: 123.1261
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0774
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 2.22s
                      Time elapsed: 00:41:48
                               ETA: 00:33:53

################################################################################
                     [1m Learning iteration 1105/2000 [0m                     

                       Computation: 42951 steps/s (collection: 2.188s, learning 0.101s)
             Mean action noise std: 2.79
          Mean value_function loss: 522.0751
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 75.3810
                       Mean reward: 629.20
               Mean episode length: 191.59
    Episode_Reward/reaching_object: 1.4950
     Episode_Reward/lifting_object: 120.8685
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0750
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 2.29s
                      Time elapsed: 00:41:50
                               ETA: 00:33:51

################################################################################
                     [1m Learning iteration 1106/2000 [0m                     

                       Computation: 41345 steps/s (collection: 2.255s, learning 0.123s)
             Mean action noise std: 2.79
          Mean value_function loss: 549.7856
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 75.3835
                       Mean reward: 589.63
               Mean episode length: 180.12
    Episode_Reward/reaching_object: 1.4665
     Episode_Reward/lifting_object: 118.1443
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.0417
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 2.38s
                      Time elapsed: 00:41:53
                               ETA: 00:33:49

################################################################################
                     [1m Learning iteration 1107/2000 [0m                     

                       Computation: 42670 steps/s (collection: 2.198s, learning 0.106s)
             Mean action noise std: 2.79
          Mean value_function loss: 482.9439
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 75.3864
                       Mean reward: 650.41
               Mean episode length: 190.71
    Episode_Reward/reaching_object: 1.4983
     Episode_Reward/lifting_object: 122.0308
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0758
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 2.30s
                      Time elapsed: 00:41:55
                               ETA: 00:33:47

################################################################################
                     [1m Learning iteration 1108/2000 [0m                     

                       Computation: 43223 steps/s (collection: 2.157s, learning 0.117s)
             Mean action noise std: 2.79
          Mean value_function loss: 513.0524
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 75.3902
                       Mean reward: 604.83
               Mean episode length: 188.25
    Episode_Reward/reaching_object: 1.4436
     Episode_Reward/lifting_object: 116.2177
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0745
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 2.27s
                      Time elapsed: 00:41:57
                               ETA: 00:33:44

################################################################################
                     [1m Learning iteration 1109/2000 [0m                     

                       Computation: 44747 steps/s (collection: 2.101s, learning 0.096s)
             Mean action noise std: 2.79
          Mean value_function loss: 477.4108
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 75.3956
                       Mean reward: 569.32
               Mean episode length: 178.67
    Episode_Reward/reaching_object: 1.4732
     Episode_Reward/lifting_object: 119.5162
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0751
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 2.20s
                      Time elapsed: 00:41:59
                               ETA: 00:33:42

################################################################################
                     [1m Learning iteration 1110/2000 [0m                     

                       Computation: 43808 steps/s (collection: 2.108s, learning 0.136s)
             Mean action noise std: 2.79
          Mean value_function loss: 496.5914
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 75.4016
                       Mean reward: 643.08
               Mean episode length: 192.97
    Episode_Reward/reaching_object: 1.5072
     Episode_Reward/lifting_object: 123.0287
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0757
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 2.24s
                      Time elapsed: 00:42:02
                               ETA: 00:33:40

################################################################################
                     [1m Learning iteration 1111/2000 [0m                     

                       Computation: 43547 steps/s (collection: 2.139s, learning 0.119s)
             Mean action noise std: 2.79
          Mean value_function loss: 541.8310
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 75.4070
                       Mean reward: 685.86
               Mean episode length: 201.10
    Episode_Reward/reaching_object: 1.4810
     Episode_Reward/lifting_object: 119.7313
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0758
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 2.26s
                      Time elapsed: 00:42:04
                               ETA: 00:33:38

################################################################################
                     [1m Learning iteration 1112/2000 [0m                     

                       Computation: 44167 steps/s (collection: 2.135s, learning 0.091s)
             Mean action noise std: 2.79
          Mean value_function loss: 496.7869
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 75.4104
                       Mean reward: 621.36
               Mean episode length: 188.79
    Episode_Reward/reaching_object: 1.5246
     Episode_Reward/lifting_object: 123.8614
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0751
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.3750
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 2.23s
                      Time elapsed: 00:42:06
                               ETA: 00:33:35

################################################################################
                     [1m Learning iteration 1113/2000 [0m                     

                       Computation: 43991 steps/s (collection: 2.134s, learning 0.101s)
             Mean action noise std: 2.80
          Mean value_function loss: 540.6905
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 75.4205
                       Mean reward: 607.26
               Mean episode length: 188.68
    Episode_Reward/reaching_object: 1.4726
     Episode_Reward/lifting_object: 118.9986
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0750
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 2.23s
                      Time elapsed: 00:42:08
                               ETA: 00:33:33

################################################################################
                     [1m Learning iteration 1114/2000 [0m                     

                       Computation: 42758 steps/s (collection: 2.152s, learning 0.147s)
             Mean action noise std: 2.80
          Mean value_function loss: 515.6768
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 75.4311
                       Mean reward: 613.59
               Mean episode length: 189.18
    Episode_Reward/reaching_object: 1.5332
     Episode_Reward/lifting_object: 124.5505
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0757
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 2.30s
                      Time elapsed: 00:42:11
                               ETA: 00:33:31

################################################################################
                     [1m Learning iteration 1115/2000 [0m                     

                       Computation: 43544 steps/s (collection: 2.150s, learning 0.108s)
             Mean action noise std: 2.80
          Mean value_function loss: 487.9618
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 75.4339
                       Mean reward: 616.52
               Mean episode length: 186.68
    Episode_Reward/reaching_object: 1.4676
     Episode_Reward/lifting_object: 118.3502
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0746
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.0833
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 2.26s
                      Time elapsed: 00:42:13
                               ETA: 00:33:28

################################################################################
                     [1m Learning iteration 1116/2000 [0m                     

                       Computation: 45136 steps/s (collection: 2.090s, learning 0.088s)
             Mean action noise std: 2.80
          Mean value_function loss: 522.1638
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 75.4358
                       Mean reward: 661.23
               Mean episode length: 197.22
    Episode_Reward/reaching_object: 1.5170
     Episode_Reward/lifting_object: 123.4364
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0748
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 2.18s
                      Time elapsed: 00:42:15
                               ETA: 00:33:26

################################################################################
                     [1m Learning iteration 1117/2000 [0m                     

                       Computation: 44715 steps/s (collection: 2.091s, learning 0.107s)
             Mean action noise std: 2.80
          Mean value_function loss: 535.0642
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 75.4399
                       Mean reward: 571.36
               Mean episode length: 177.89
    Episode_Reward/reaching_object: 1.4423
     Episode_Reward/lifting_object: 116.7128
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0717
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 2.20s
                      Time elapsed: 00:42:17
                               ETA: 00:33:24

################################################################################
                     [1m Learning iteration 1118/2000 [0m                     

                       Computation: 41747 steps/s (collection: 2.254s, learning 0.101s)
             Mean action noise std: 2.80
          Mean value_function loss: 509.9720
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 75.4516
                       Mean reward: 627.46
               Mean episode length: 189.63
    Episode_Reward/reaching_object: 1.5251
     Episode_Reward/lifting_object: 124.5461
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0747
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.9583
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 2.35s
                      Time elapsed: 00:42:20
                               ETA: 00:33:22

################################################################################
                     [1m Learning iteration 1119/2000 [0m                     

                       Computation: 43431 steps/s (collection: 2.112s, learning 0.151s)
             Mean action noise std: 2.80
          Mean value_function loss: 494.2463
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 75.4699
                       Mean reward: 561.12
               Mean episode length: 179.09
    Episode_Reward/reaching_object: 1.4245
     Episode_Reward/lifting_object: 114.3562
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0760
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 2.26s
                      Time elapsed: 00:42:22
                               ETA: 00:33:19

################################################################################
                     [1m Learning iteration 1120/2000 [0m                     

                       Computation: 44520 steps/s (collection: 2.119s, learning 0.089s)
             Mean action noise std: 2.80
          Mean value_function loss: 496.1748
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 75.4785
                       Mean reward: 626.17
               Mean episode length: 189.66
    Episode_Reward/reaching_object: 1.5320
     Episode_Reward/lifting_object: 124.2778
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0764
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 2.21s
                      Time elapsed: 00:42:24
                               ETA: 00:33:17

################################################################################
                     [1m Learning iteration 1121/2000 [0m                     

                       Computation: 43524 steps/s (collection: 2.164s, learning 0.095s)
             Mean action noise std: 2.80
          Mean value_function loss: 477.9279
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 75.4799
                       Mean reward: 612.31
               Mean episode length: 190.29
    Episode_Reward/reaching_object: 1.4766
     Episode_Reward/lifting_object: 119.9081
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0744
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 2.26s
                      Time elapsed: 00:42:26
                               ETA: 00:33:15

################################################################################
                     [1m Learning iteration 1122/2000 [0m                     

                       Computation: 44737 steps/s (collection: 2.102s, learning 0.096s)
             Mean action noise std: 2.80
          Mean value_function loss: 415.4093
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 75.4828
                       Mean reward: 620.08
               Mean episode length: 184.37
    Episode_Reward/reaching_object: 1.5612
     Episode_Reward/lifting_object: 127.4358
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0751
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 2.20s
                      Time elapsed: 00:42:28
                               ETA: 00:33:12

################################################################################
                     [1m Learning iteration 1123/2000 [0m                     

                       Computation: 42512 steps/s (collection: 2.165s, learning 0.148s)
             Mean action noise std: 2.80
          Mean value_function loss: 455.5614
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 75.4895
                       Mean reward: 595.79
               Mean episode length: 179.79
    Episode_Reward/reaching_object: 1.5395
     Episode_Reward/lifting_object: 124.7582
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0760
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 2.31s
                      Time elapsed: 00:42:31
                               ETA: 00:33:10

################################################################################
                     [1m Learning iteration 1124/2000 [0m                     

                       Computation: 44243 steps/s (collection: 2.115s, learning 0.107s)
             Mean action noise std: 2.80
          Mean value_function loss: 442.9563
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 75.4999
                       Mean reward: 652.65
               Mean episode length: 190.54
    Episode_Reward/reaching_object: 1.5578
     Episode_Reward/lifting_object: 126.9548
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0766
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 2.22s
                      Time elapsed: 00:42:33
                               ETA: 00:33:08

################################################################################
                     [1m Learning iteration 1125/2000 [0m                     

                       Computation: 44730 steps/s (collection: 2.108s, learning 0.089s)
             Mean action noise std: 2.80
          Mean value_function loss: 480.1862
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 75.5071
                       Mean reward: 655.33
               Mean episode length: 193.48
    Episode_Reward/reaching_object: 1.5989
     Episode_Reward/lifting_object: 130.2759
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0770
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 2.20s
                      Time elapsed: 00:42:35
                               ETA: 00:33:05

################################################################################
                     [1m Learning iteration 1126/2000 [0m                     

                       Computation: 44101 steps/s (collection: 2.142s, learning 0.087s)
             Mean action noise std: 2.81
          Mean value_function loss: 474.4522
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 75.5178
                       Mean reward: 678.41
               Mean episode length: 197.78
    Episode_Reward/reaching_object: 1.6079
     Episode_Reward/lifting_object: 131.5152
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0771
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 2.23s
                      Time elapsed: 00:42:37
                               ETA: 00:33:03

################################################################################
                     [1m Learning iteration 1127/2000 [0m                     

                       Computation: 44830 steps/s (collection: 2.104s, learning 0.089s)
             Mean action noise std: 2.81
          Mean value_function loss: 475.0846
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 75.5286
                       Mean reward: 690.55
               Mean episode length: 203.62
    Episode_Reward/reaching_object: 1.5973
     Episode_Reward/lifting_object: 130.4997
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0783
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 2.19s
                      Time elapsed: 00:42:40
                               ETA: 00:33:01

################################################################################
                     [1m Learning iteration 1128/2000 [0m                     

                       Computation: 42812 steps/s (collection: 2.145s, learning 0.152s)
             Mean action noise std: 2.81
          Mean value_function loss: 481.0637
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 75.5372
                       Mean reward: 681.39
               Mean episode length: 204.21
    Episode_Reward/reaching_object: 1.6174
     Episode_Reward/lifting_object: 132.3132
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0790
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 2.30s
                      Time elapsed: 00:42:42
                               ETA: 00:32:59

################################################################################
                     [1m Learning iteration 1129/2000 [0m                     

                       Computation: 43831 steps/s (collection: 2.129s, learning 0.114s)
             Mean action noise std: 2.81
          Mean value_function loss: 485.0411
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 75.5433
                       Mean reward: 632.64
               Mean episode length: 191.80
    Episode_Reward/reaching_object: 1.5796
     Episode_Reward/lifting_object: 128.6811
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0775
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 2.24s
                      Time elapsed: 00:42:44
                               ETA: 00:32:56

################################################################################
                     [1m Learning iteration 1130/2000 [0m                     

                       Computation: 44120 steps/s (collection: 2.133s, learning 0.096s)
             Mean action noise std: 2.81
          Mean value_function loss: 441.7750
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 75.5540
                       Mean reward: 640.94
               Mean episode length: 190.92
    Episode_Reward/reaching_object: 1.5713
     Episode_Reward/lifting_object: 127.7756
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0757
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 2.23s
                      Time elapsed: 00:42:46
                               ETA: 00:32:54

################################################################################
                     [1m Learning iteration 1131/2000 [0m                     

                       Computation: 45289 steps/s (collection: 2.082s, learning 0.089s)
             Mean action noise std: 2.81
          Mean value_function loss: 475.6261
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 75.5650
                       Mean reward: 607.61
               Mean episode length: 185.95
    Episode_Reward/reaching_object: 1.4926
     Episode_Reward/lifting_object: 120.4615
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0753
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 2.17s
                      Time elapsed: 00:42:49
                               ETA: 00:32:52

################################################################################
                     [1m Learning iteration 1132/2000 [0m                     

                       Computation: 43876 steps/s (collection: 2.118s, learning 0.123s)
             Mean action noise std: 2.81
          Mean value_function loss: 478.4915
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 75.5741
                       Mean reward: 670.30
               Mean episode length: 200.31
    Episode_Reward/reaching_object: 1.5680
     Episode_Reward/lifting_object: 127.4710
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0779
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 2.24s
                      Time elapsed: 00:42:51
                               ETA: 00:32:49

################################################################################
                     [1m Learning iteration 1133/2000 [0m                     

                       Computation: 44585 steps/s (collection: 2.106s, learning 0.099s)
             Mean action noise std: 2.81
          Mean value_function loss: 482.0542
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 75.5855
                       Mean reward: 528.37
               Mean episode length: 173.12
    Episode_Reward/reaching_object: 1.4791
     Episode_Reward/lifting_object: 118.4068
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0767
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 2.20s
                      Time elapsed: 00:42:53
                               ETA: 00:32:47

################################################################################
                     [1m Learning iteration 1134/2000 [0m                     

                       Computation: 40999 steps/s (collection: 2.258s, learning 0.140s)
             Mean action noise std: 2.81
          Mean value_function loss: 441.5471
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 75.5918
                       Mean reward: 625.09
               Mean episode length: 187.28
    Episode_Reward/reaching_object: 1.5959
     Episode_Reward/lifting_object: 129.9364
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0781
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 2.40s
                      Time elapsed: 00:42:55
                               ETA: 00:32:45

################################################################################
                     [1m Learning iteration 1135/2000 [0m                     

                       Computation: 43489 steps/s (collection: 2.171s, learning 0.089s)
             Mean action noise std: 2.81
          Mean value_function loss: 425.1209
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 75.5982
                       Mean reward: 702.54
               Mean episode length: 207.15
    Episode_Reward/reaching_object: 1.5553
     Episode_Reward/lifting_object: 125.6148
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0781
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 2.26s
                      Time elapsed: 00:42:58
                               ETA: 00:32:43

################################################################################
                     [1m Learning iteration 1136/2000 [0m                     

                       Computation: 41775 steps/s (collection: 2.220s, learning 0.133s)
             Mean action noise std: 2.81
          Mean value_function loss: 429.3423
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 75.6112
                       Mean reward: 711.06
               Mean episode length: 207.73
    Episode_Reward/reaching_object: 1.5824
     Episode_Reward/lifting_object: 129.0588
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0772
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 2.35s
                      Time elapsed: 00:43:00
                               ETA: 00:32:40

################################################################################
                     [1m Learning iteration 1137/2000 [0m                     

                       Computation: 44297 steps/s (collection: 2.121s, learning 0.098s)
             Mean action noise std: 2.82
          Mean value_function loss: 447.0239
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 75.6206
                       Mean reward: 638.76
               Mean episode length: 193.00
    Episode_Reward/reaching_object: 1.5805
     Episode_Reward/lifting_object: 129.3010
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0792
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 2.22s
                      Time elapsed: 00:43:02
                               ETA: 00:32:38

################################################################################
                     [1m Learning iteration 1138/2000 [0m                     

                       Computation: 41355 steps/s (collection: 2.273s, learning 0.105s)
             Mean action noise std: 2.82
          Mean value_function loss: 444.1360
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 75.6305
                       Mean reward: 648.85
               Mean episode length: 194.37
    Episode_Reward/reaching_object: 1.5475
     Episode_Reward/lifting_object: 125.2844
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0781
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 2.38s
                      Time elapsed: 00:43:05
                               ETA: 00:32:36

################################################################################
                     [1m Learning iteration 1139/2000 [0m                     

                       Computation: 43996 steps/s (collection: 2.135s, learning 0.100s)
             Mean action noise std: 2.82
          Mean value_function loss: 432.8994
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 75.6385
                       Mean reward: 671.27
               Mean episode length: 198.51
    Episode_Reward/reaching_object: 1.5990
     Episode_Reward/lifting_object: 131.0341
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0768
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 2.23s
                      Time elapsed: 00:43:07
                               ETA: 00:32:34

################################################################################
                     [1m Learning iteration 1140/2000 [0m                     

                       Computation: 43912 steps/s (collection: 2.126s, learning 0.113s)
             Mean action noise std: 2.82
          Mean value_function loss: 469.4075
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 75.6457
                       Mean reward: 591.98
               Mean episode length: 182.26
    Episode_Reward/reaching_object: 1.4728
     Episode_Reward/lifting_object: 119.7332
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0734
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 2.24s
                      Time elapsed: 00:43:09
                               ETA: 00:32:31

################################################################################
                     [1m Learning iteration 1141/2000 [0m                     

                       Computation: 44948 steps/s (collection: 2.101s, learning 0.087s)
             Mean action noise std: 2.82
          Mean value_function loss: 425.3959
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 75.6504
                       Mean reward: 573.13
               Mean episode length: 178.12
    Episode_Reward/reaching_object: 1.4661
     Episode_Reward/lifting_object: 118.5523
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0741
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 2.19s
                      Time elapsed: 00:43:11
                               ETA: 00:32:29

################################################################################
                     [1m Learning iteration 1142/2000 [0m                     

                       Computation: 43767 steps/s (collection: 2.137s, learning 0.109s)
             Mean action noise std: 2.82
          Mean value_function loss: 482.6205
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 75.6577
                       Mean reward: 603.38
               Mean episode length: 182.85
    Episode_Reward/reaching_object: 1.4963
     Episode_Reward/lifting_object: 121.5662
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 2.25s
                      Time elapsed: 00:43:14
                               ETA: 00:32:27

################################################################################
                     [1m Learning iteration 1143/2000 [0m                     

                       Computation: 44325 steps/s (collection: 2.119s, learning 0.099s)
             Mean action noise std: 2.82
          Mean value_function loss: 447.6873
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 75.6698
                       Mean reward: 562.66
               Mean episode length: 175.24
    Episode_Reward/reaching_object: 1.4849
     Episode_Reward/lifting_object: 120.0978
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0734
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 2.22s
                      Time elapsed: 00:43:16
                               ETA: 00:32:24

################################################################################
                     [1m Learning iteration 1144/2000 [0m                     

                       Computation: 43416 steps/s (collection: 2.178s, learning 0.087s)
             Mean action noise std: 2.82
          Mean value_function loss: 453.2929
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 75.6768
                       Mean reward: 632.93
               Mean episode length: 188.48
    Episode_Reward/reaching_object: 1.5326
     Episode_Reward/lifting_object: 125.1282
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0740
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 2.26s
                      Time elapsed: 00:43:18
                               ETA: 00:32:22

################################################################################
                     [1m Learning iteration 1145/2000 [0m                     

                       Computation: 40802 steps/s (collection: 2.289s, learning 0.121s)
             Mean action noise std: 2.82
          Mean value_function loss: 421.2474
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 75.6795
                       Mean reward: 685.36
               Mean episode length: 200.35
    Episode_Reward/reaching_object: 1.5915
     Episode_Reward/lifting_object: 131.0677
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0779
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 2.41s
                      Time elapsed: 00:43:20
                               ETA: 00:32:20

################################################################################
                     [1m Learning iteration 1146/2000 [0m                     

                       Computation: 43257 steps/s (collection: 2.172s, learning 0.100s)
             Mean action noise std: 2.82
          Mean value_function loss: 421.1333
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 75.6865
                       Mean reward: 651.59
               Mean episode length: 192.48
    Episode_Reward/reaching_object: 1.5788
     Episode_Reward/lifting_object: 129.1523
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0769
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 2.27s
                      Time elapsed: 00:43:23
                               ETA: 00:32:18

################################################################################
                     [1m Learning iteration 1147/2000 [0m                     

                       Computation: 42438 steps/s (collection: 2.172s, learning 0.144s)
             Mean action noise std: 2.82
          Mean value_function loss: 403.9729
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 75.6946
                       Mean reward: 573.48
               Mean episode length: 176.31
    Episode_Reward/reaching_object: 1.5536
     Episode_Reward/lifting_object: 127.3442
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0761
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 2.32s
                      Time elapsed: 00:43:25
                               ETA: 00:32:15

################################################################################
                     [1m Learning iteration 1148/2000 [0m                     

                       Computation: 42607 steps/s (collection: 2.197s, learning 0.111s)
             Mean action noise std: 2.83
          Mean value_function loss: 429.6103
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 75.7044
                       Mean reward: 645.02
               Mean episode length: 192.70
    Episode_Reward/reaching_object: 1.5652
     Episode_Reward/lifting_object: 127.9373
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0774
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 2.31s
                      Time elapsed: 00:43:27
                               ETA: 00:32:13

################################################################################
                     [1m Learning iteration 1149/2000 [0m                     

                       Computation: 43237 steps/s (collection: 2.157s, learning 0.116s)
             Mean action noise std: 2.83
          Mean value_function loss: 409.3121
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 75.7113
                       Mean reward: 656.42
               Mean episode length: 194.82
    Episode_Reward/reaching_object: 1.6196
     Episode_Reward/lifting_object: 133.3338
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0774
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 2.27s
                      Time elapsed: 00:43:30
                               ETA: 00:32:11

################################################################################
                     [1m Learning iteration 1150/2000 [0m                     

                       Computation: 43783 steps/s (collection: 2.134s, learning 0.111s)
             Mean action noise std: 2.83
          Mean value_function loss: 29220116.3000
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 75.7152
                       Mean reward: -5724.38
               Mean episode length: 201.94
    Episode_Reward/reaching_object: 1.5727
     Episode_Reward/lifting_object: 128.6716
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.3272
          Episode_Reward/joint_vel: -213.3219
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 2.25s
                      Time elapsed: 00:43:32
                               ETA: 00:32:09

################################################################################
                     [1m Learning iteration 1151/2000 [0m                     

                       Computation: 41082 steps/s (collection: 2.261s, learning 0.132s)
             Mean action noise std: 2.83
          Mean value_function loss: 540.6335
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 75.7173
                       Mean reward: 611.65
               Mean episode length: 180.97
    Episode_Reward/reaching_object: 1.4952
     Episode_Reward/lifting_object: 121.5777
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0725
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 2.39s
                      Time elapsed: 00:43:34
                               ETA: 00:32:06

################################################################################
                     [1m Learning iteration 1152/2000 [0m                     

                       Computation: 43865 steps/s (collection: 2.153s, learning 0.088s)
             Mean action noise std: 2.83
          Mean value_function loss: 508.4699
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 75.7243
                       Mean reward: 644.33
               Mean episode length: 191.10
    Episode_Reward/reaching_object: 1.4761
     Episode_Reward/lifting_object: 120.1847
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0718
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 2.24s
                      Time elapsed: 00:43:36
                               ETA: 00:32:04

################################################################################
                     [1m Learning iteration 1153/2000 [0m                     

                       Computation: 43213 steps/s (collection: 2.161s, learning 0.114s)
             Mean action noise std: 2.83
          Mean value_function loss: 458.4555
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 75.7320
                       Mean reward: 563.00
               Mean episode length: 173.61
    Episode_Reward/reaching_object: 1.4573
     Episode_Reward/lifting_object: 118.8067
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0708
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 2.27s
                      Time elapsed: 00:43:39
                               ETA: 00:32:02

################################################################################
                     [1m Learning iteration 1154/2000 [0m                     

                       Computation: 44877 steps/s (collection: 2.101s, learning 0.089s)
             Mean action noise std: 2.83
          Mean value_function loss: 466.9409
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 75.7364
                       Mean reward: 586.74
               Mean episode length: 180.00
    Episode_Reward/reaching_object: 1.5321
     Episode_Reward/lifting_object: 125.1889
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0734
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 2.19s
                      Time elapsed: 00:43:41
                               ETA: 00:32:00

################################################################################
                     [1m Learning iteration 1155/2000 [0m                     

                       Computation: 44659 steps/s (collection: 2.107s, learning 0.094s)
             Mean action noise std: 2.83
          Mean value_function loss: 416.1129
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 75.7444
                       Mean reward: 637.96
               Mean episode length: 186.05
    Episode_Reward/reaching_object: 1.5349
     Episode_Reward/lifting_object: 125.3869
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0749
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 2.20s
                      Time elapsed: 00:43:43
                               ETA: 00:31:57

################################################################################
                     [1m Learning iteration 1156/2000 [0m                     

                       Computation: 45086 steps/s (collection: 2.086s, learning 0.094s)
             Mean action noise std: 2.83
          Mean value_function loss: 447.5239
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 75.7533
                       Mean reward: 610.84
               Mean episode length: 182.77
    Episode_Reward/reaching_object: 1.5251
     Episode_Reward/lifting_object: 122.4639
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0745
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 2.18s
                      Time elapsed: 00:43:45
                               ETA: 00:31:55

################################################################################
                     [1m Learning iteration 1157/2000 [0m                     

                       Computation: 44222 steps/s (collection: 2.110s, learning 0.113s)
             Mean action noise std: 2.83
          Mean value_function loss: 403.4185
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 75.7580
                       Mean reward: 626.33
               Mean episode length: 188.43
    Episode_Reward/reaching_object: 1.5491
     Episode_Reward/lifting_object: 124.7715
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0741
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 2.22s
                      Time elapsed: 00:43:48
                               ETA: 00:31:53

################################################################################
                     [1m Learning iteration 1158/2000 [0m                     

                       Computation: 44551 steps/s (collection: 2.086s, learning 0.121s)
             Mean action noise std: 2.83
          Mean value_function loss: 468.0401
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 75.7647
                       Mean reward: 658.39
               Mean episode length: 197.75
    Episode_Reward/reaching_object: 1.5722
     Episode_Reward/lifting_object: 129.3895
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0768
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.9583
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 2.21s
                      Time elapsed: 00:43:50
                               ETA: 00:31:50

################################################################################
                     [1m Learning iteration 1159/2000 [0m                     

                       Computation: 44206 steps/s (collection: 2.129s, learning 0.095s)
             Mean action noise std: 2.83
          Mean value_function loss: 425.9856
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 75.7807
                       Mean reward: 657.58
               Mean episode length: 191.19
    Episode_Reward/reaching_object: 1.5083
     Episode_Reward/lifting_object: 123.6487
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0715
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 2.22s
                      Time elapsed: 00:43:52
                               ETA: 00:31:48

################################################################################
                     [1m Learning iteration 1160/2000 [0m                     

                       Computation: 45331 steps/s (collection: 2.074s, learning 0.095s)
             Mean action noise std: 2.83
          Mean value_function loss: 379.1417
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 75.7934
                       Mean reward: 700.23
               Mean episode length: 204.16
    Episode_Reward/reaching_object: 1.5878
     Episode_Reward/lifting_object: 130.7903
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0757
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 2.17s
                      Time elapsed: 00:43:54
                               ETA: 00:31:46

################################################################################
                     [1m Learning iteration 1161/2000 [0m                     

                       Computation: 44250 steps/s (collection: 2.114s, learning 0.107s)
             Mean action noise std: 2.84
          Mean value_function loss: 389.4904
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 75.8014
                       Mean reward: 681.67
               Mean episode length: 200.65
    Episode_Reward/reaching_object: 1.6248
     Episode_Reward/lifting_object: 134.1566
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0764
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 2.22s
                      Time elapsed: 00:43:56
                               ETA: 00:31:43

################################################################################
                     [1m Learning iteration 1162/2000 [0m                     

                       Computation: 45202 steps/s (collection: 2.073s, learning 0.102s)
             Mean action noise std: 2.84
          Mean value_function loss: 414.1122
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 75.8073
                       Mean reward: 701.22
               Mean episode length: 203.15
    Episode_Reward/reaching_object: 1.6418
     Episode_Reward/lifting_object: 136.5877
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0764
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 2.17s
                      Time elapsed: 00:43:59
                               ETA: 00:31:41

################################################################################
                     [1m Learning iteration 1163/2000 [0m                     

                       Computation: 45277 steps/s (collection: 2.070s, learning 0.101s)
             Mean action noise std: 2.84
          Mean value_function loss: 411.9215
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 75.8220
                       Mean reward: 679.89
               Mean episode length: 201.07
    Episode_Reward/reaching_object: 1.6158
     Episode_Reward/lifting_object: 134.2061
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0754
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 2.17s
                      Time elapsed: 00:44:01
                               ETA: 00:31:39

################################################################################
                     [1m Learning iteration 1164/2000 [0m                     

                       Computation: 44420 steps/s (collection: 2.095s, learning 0.118s)
             Mean action noise std: 2.84
          Mean value_function loss: 404.6286
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 75.8397
                       Mean reward: 713.97
               Mean episode length: 203.44
    Episode_Reward/reaching_object: 1.6112
     Episode_Reward/lifting_object: 132.6001
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0768
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 2.21s
                      Time elapsed: 00:44:03
                               ETA: 00:31:36

################################################################################
                     [1m Learning iteration 1165/2000 [0m                     

                       Computation: 44008 steps/s (collection: 2.116s, learning 0.118s)
             Mean action noise std: 2.84
          Mean value_function loss: 355.9714
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 75.8504
                       Mean reward: 691.21
               Mean episode length: 206.13
    Episode_Reward/reaching_object: 1.6299
     Episode_Reward/lifting_object: 134.6903
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0772
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 2.23s
                      Time elapsed: 00:44:05
                               ETA: 00:31:34

################################################################################
                     [1m Learning iteration 1166/2000 [0m                     

                       Computation: 43550 steps/s (collection: 2.148s, learning 0.109s)
             Mean action noise std: 2.84
          Mean value_function loss: 455.1902
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 75.8610
                       Mean reward: 635.80
               Mean episode length: 190.88
    Episode_Reward/reaching_object: 1.5672
     Episode_Reward/lifting_object: 128.6040
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0761
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 2.26s
                      Time elapsed: 00:44:07
                               ETA: 00:31:32

################################################################################
                     [1m Learning iteration 1167/2000 [0m                     

                       Computation: 43163 steps/s (collection: 2.170s, learning 0.108s)
             Mean action noise std: 2.84
          Mean value_function loss: 394.2218
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 75.8703
                       Mean reward: 608.35
               Mean episode length: 185.02
    Episode_Reward/reaching_object: 1.5781
     Episode_Reward/lifting_object: 129.8034
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0759
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 2.28s
                      Time elapsed: 00:44:10
                               ETA: 00:31:30

################################################################################
                     [1m Learning iteration 1168/2000 [0m                     

                       Computation: 45572 steps/s (collection: 2.071s, learning 0.086s)
             Mean action noise std: 2.84
          Mean value_function loss: 407.2078
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 75.8756
                       Mean reward: 636.11
               Mean episode length: 190.67
    Episode_Reward/reaching_object: 1.5624
     Episode_Reward/lifting_object: 128.0621
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0758
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 2.16s
                      Time elapsed: 00:44:12
                               ETA: 00:31:27

################################################################################
                     [1m Learning iteration 1169/2000 [0m                     

                       Computation: 44456 steps/s (collection: 2.096s, learning 0.116s)
             Mean action noise std: 2.84
          Mean value_function loss: 384.9396
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 75.8816
                       Mean reward: 663.61
               Mean episode length: 196.55
    Episode_Reward/reaching_object: 1.6154
     Episode_Reward/lifting_object: 133.7452
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0780
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 2.21s
                      Time elapsed: 00:44:14
                               ETA: 00:31:25

################################################################################
                     [1m Learning iteration 1170/2000 [0m                     

                       Computation: 44578 steps/s (collection: 2.114s, learning 0.092s)
             Mean action noise std: 2.84
          Mean value_function loss: 405.6822
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 75.8901
                       Mean reward: 652.77
               Mean episode length: 193.90
    Episode_Reward/reaching_object: 1.5888
     Episode_Reward/lifting_object: 130.6071
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0765
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 2.21s
                      Time elapsed: 00:44:16
                               ETA: 00:31:23

################################################################################
                     [1m Learning iteration 1171/2000 [0m                     

                       Computation: 44265 steps/s (collection: 2.114s, learning 0.107s)
             Mean action noise std: 2.85
          Mean value_function loss: 421.6389
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 75.8987
                       Mean reward: 654.00
               Mean episode length: 192.15
    Episode_Reward/reaching_object: 1.5098
     Episode_Reward/lifting_object: 124.0373
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0735
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 2.22s
                      Time elapsed: 00:44:18
                               ETA: 00:31:20

################################################################################
                     [1m Learning iteration 1172/2000 [0m                     

                       Computation: 43680 steps/s (collection: 2.138s, learning 0.113s)
             Mean action noise std: 2.85
          Mean value_function loss: 433.8279
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 75.9113
                       Mean reward: 632.40
               Mean episode length: 191.59
    Episode_Reward/reaching_object: 1.5786
     Episode_Reward/lifting_object: 129.9136
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0766
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 2.25s
                      Time elapsed: 00:44:21
                               ETA: 00:31:18

################################################################################
                     [1m Learning iteration 1173/2000 [0m                     

                       Computation: 44070 steps/s (collection: 2.118s, learning 0.113s)
             Mean action noise std: 2.85
          Mean value_function loss: 364.9001
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 75.9194
                       Mean reward: 701.97
               Mean episode length: 206.00
    Episode_Reward/reaching_object: 1.6181
     Episode_Reward/lifting_object: 133.9700
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0777
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 2.23s
                      Time elapsed: 00:44:23
                               ETA: 00:31:16

################################################################################
                     [1m Learning iteration 1174/2000 [0m                     

                       Computation: 43992 steps/s (collection: 2.114s, learning 0.121s)
             Mean action noise std: 2.85
          Mean value_function loss: 448.8665
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 75.9240
                       Mean reward: 643.81
               Mean episode length: 194.31
    Episode_Reward/reaching_object: 1.5145
     Episode_Reward/lifting_object: 123.8318
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0747
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 2.23s
                      Time elapsed: 00:44:25
                               ETA: 00:31:13

################################################################################
                     [1m Learning iteration 1175/2000 [0m                     

                       Computation: 43889 steps/s (collection: 2.112s, learning 0.128s)
             Mean action noise std: 2.85
          Mean value_function loss: 441.1305
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 75.9340
                       Mean reward: 600.64
               Mean episode length: 188.72
    Episode_Reward/reaching_object: 1.5258
     Episode_Reward/lifting_object: 125.0971
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0751
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 2.24s
                      Time elapsed: 00:44:27
                               ETA: 00:31:11

################################################################################
                     [1m Learning iteration 1176/2000 [0m                     

                       Computation: 43898 steps/s (collection: 2.145s, learning 0.094s)
             Mean action noise std: 2.85
          Mean value_function loss: 582.9737
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 75.9476
                       Mean reward: 628.39
               Mean episode length: 189.98
    Episode_Reward/reaching_object: 1.5297
     Episode_Reward/lifting_object: 124.9339
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0753
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.7917
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 2.24s
                      Time elapsed: 00:44:30
                               ETA: 00:31:09

################################################################################
                     [1m Learning iteration 1177/2000 [0m                     

                       Computation: 44357 steps/s (collection: 2.108s, learning 0.108s)
             Mean action noise std: 2.85
          Mean value_function loss: 399.0102
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 75.9565
                       Mean reward: 628.11
               Mean episode length: 189.31
    Episode_Reward/reaching_object: 1.5941
     Episode_Reward/lifting_object: 131.0014
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0766
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 2.22s
                      Time elapsed: 00:44:32
                               ETA: 00:31:07

################################################################################
                     [1m Learning iteration 1178/2000 [0m                     

                       Computation: 44704 steps/s (collection: 2.089s, learning 0.110s)
             Mean action noise std: 2.85
          Mean value_function loss: 419.5498
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 75.9672
                       Mean reward: 628.36
               Mean episode length: 187.86
    Episode_Reward/reaching_object: 1.5308
     Episode_Reward/lifting_object: 125.3628
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0748
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 2.20s
                      Time elapsed: 00:44:34
                               ETA: 00:31:04

################################################################################
                     [1m Learning iteration 1179/2000 [0m                     

                       Computation: 43779 steps/s (collection: 2.130s, learning 0.115s)
             Mean action noise std: 2.85
          Mean value_function loss: 390.6501
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 75.9795
                       Mean reward: 658.89
               Mean episode length: 198.27
    Episode_Reward/reaching_object: 1.5634
     Episode_Reward/lifting_object: 127.9652
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0758
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 2.25s
                      Time elapsed: 00:44:36
                               ETA: 00:31:02

################################################################################
                     [1m Learning iteration 1180/2000 [0m                     

                       Computation: 44271 steps/s (collection: 2.107s, learning 0.114s)
             Mean action noise std: 2.86
          Mean value_function loss: 397.1525
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 75.9937
                       Mean reward: 652.00
               Mean episode length: 193.76
    Episode_Reward/reaching_object: 1.5886
     Episode_Reward/lifting_object: 130.7647
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0770
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 2.22s
                      Time elapsed: 00:44:39
                               ETA: 00:31:00

################################################################################
                     [1m Learning iteration 1181/2000 [0m                     

                       Computation: 44742 steps/s (collection: 2.083s, learning 0.114s)
             Mean action noise std: 2.86
          Mean value_function loss: 374.8487
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 76.0043
                       Mean reward: 621.95
               Mean episode length: 192.15
    Episode_Reward/reaching_object: 1.5820
     Episode_Reward/lifting_object: 129.1792
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0777
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 2.20s
                      Time elapsed: 00:44:41
                               ETA: 00:30:57

################################################################################
                     [1m Learning iteration 1182/2000 [0m                     

                       Computation: 44655 steps/s (collection: 2.103s, learning 0.099s)
             Mean action noise std: 2.86
          Mean value_function loss: 422.8107
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 76.0167
                       Mean reward: 662.44
               Mean episode length: 197.47
    Episode_Reward/reaching_object: 1.6008
     Episode_Reward/lifting_object: 131.6908
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0776
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 2.20s
                      Time elapsed: 00:44:43
                               ETA: 00:30:55

################################################################################
                     [1m Learning iteration 1183/2000 [0m                     

                       Computation: 45342 steps/s (collection: 2.059s, learning 0.109s)
             Mean action noise std: 2.86
          Mean value_function loss: 391.9877
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 76.0337
                       Mean reward: 684.11
               Mean episode length: 201.05
    Episode_Reward/reaching_object: 1.6103
     Episode_Reward/lifting_object: 131.7115
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0788
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 2.17s
                      Time elapsed: 00:44:45
                               ETA: 00:30:53

################################################################################
                     [1m Learning iteration 1184/2000 [0m                     

                       Computation: 44806 steps/s (collection: 2.095s, learning 0.099s)
             Mean action noise std: 2.86
          Mean value_function loss: 376.9809
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 76.0401
                       Mean reward: 683.79
               Mean episode length: 203.56
    Episode_Reward/reaching_object: 1.6400
     Episode_Reward/lifting_object: 134.8534
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0790
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 2.19s
                      Time elapsed: 00:44:47
                               ETA: 00:30:50

################################################################################
                     [1m Learning iteration 1185/2000 [0m                     

                       Computation: 45287 steps/s (collection: 2.077s, learning 0.094s)
             Mean action noise std: 2.86
          Mean value_function loss: 396.5053
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 76.0440
                       Mean reward: 654.40
               Mean episode length: 198.06
    Episode_Reward/reaching_object: 1.6451
     Episode_Reward/lifting_object: 135.3569
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0794
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 2.17s
                      Time elapsed: 00:44:49
                               ETA: 00:30:48

################################################################################
                     [1m Learning iteration 1186/2000 [0m                     

                       Computation: 45656 steps/s (collection: 2.061s, learning 0.093s)
             Mean action noise std: 2.86
          Mean value_function loss: 425.1062
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 76.0573
                       Mean reward: 699.44
               Mean episode length: 203.82
    Episode_Reward/reaching_object: 1.5898
     Episode_Reward/lifting_object: 130.0001
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0775
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 2.15s
                      Time elapsed: 00:44:52
                               ETA: 00:30:46

################################################################################
                     [1m Learning iteration 1187/2000 [0m                     

                       Computation: 45476 steps/s (collection: 2.068s, learning 0.093s)
             Mean action noise std: 2.86
          Mean value_function loss: 440.8969
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 76.0695
                       Mean reward: 679.95
               Mean episode length: 197.66
    Episode_Reward/reaching_object: 1.6432
     Episode_Reward/lifting_object: 135.9531
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0783
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 2.16s
                      Time elapsed: 00:44:54
                               ETA: 00:30:43

################################################################################
                     [1m Learning iteration 1188/2000 [0m                     

                       Computation: 45482 steps/s (collection: 2.070s, learning 0.091s)
             Mean action noise std: 2.86
          Mean value_function loss: 420.6063
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 76.0728
                       Mean reward: 677.20
               Mean episode length: 198.92
    Episode_Reward/reaching_object: 1.6735
     Episode_Reward/lifting_object: 137.4108
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0799
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 2.16s
                      Time elapsed: 00:44:56
                               ETA: 00:30:41

################################################################################
                     [1m Learning iteration 1189/2000 [0m                     

                       Computation: 45114 steps/s (collection: 2.071s, learning 0.108s)
             Mean action noise std: 2.87
          Mean value_function loss: 429.9416
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 76.0840
                       Mean reward: 681.14
               Mean episode length: 198.53
    Episode_Reward/reaching_object: 1.5954
     Episode_Reward/lifting_object: 130.7975
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0766
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 2.18s
                      Time elapsed: 00:44:58
                               ETA: 00:30:39

################################################################################
                     [1m Learning iteration 1190/2000 [0m                     

                       Computation: 45010 steps/s (collection: 2.071s, learning 0.113s)
             Mean action noise std: 2.87
          Mean value_function loss: 416.2381
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 76.1010
                       Mean reward: 649.77
               Mean episode length: 195.11
    Episode_Reward/reaching_object: 1.6235
     Episode_Reward/lifting_object: 133.0328
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0782
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 2.18s
                      Time elapsed: 00:45:00
                               ETA: 00:30:36

################################################################################
                     [1m Learning iteration 1191/2000 [0m                     

                       Computation: 44471 steps/s (collection: 2.110s, learning 0.101s)
             Mean action noise std: 2.87
          Mean value_function loss: 416.7058
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 76.1176
                       Mean reward: 640.92
               Mean episode length: 198.73
    Episode_Reward/reaching_object: 1.6141
     Episode_Reward/lifting_object: 132.0923
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0776
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 2.21s
                      Time elapsed: 00:45:03
                               ETA: 00:30:34

################################################################################
                     [1m Learning iteration 1192/2000 [0m                     

                       Computation: 44030 steps/s (collection: 2.139s, learning 0.094s)
             Mean action noise std: 2.87
          Mean value_function loss: 417.4299
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 76.1316
                       Mean reward: 644.83
               Mean episode length: 191.30
    Episode_Reward/reaching_object: 1.5936
     Episode_Reward/lifting_object: 130.8328
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0762
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 2.23s
                      Time elapsed: 00:45:05
                               ETA: 00:30:32

################################################################################
                     [1m Learning iteration 1193/2000 [0m                     

                       Computation: 43384 steps/s (collection: 2.156s, learning 0.110s)
             Mean action noise std: 2.87
          Mean value_function loss: 415.3370
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 76.1417
                       Mean reward: 686.94
               Mean episode length: 201.86
    Episode_Reward/reaching_object: 1.6717
     Episode_Reward/lifting_object: 138.0525
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0800
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 2.27s
                      Time elapsed: 00:45:07
                               ETA: 00:30:29

################################################################################
                     [1m Learning iteration 1194/2000 [0m                     

                       Computation: 44600 steps/s (collection: 2.089s, learning 0.115s)
             Mean action noise std: 2.87
          Mean value_function loss: 353.0446
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 76.1454
                       Mean reward: 701.66
               Mean episode length: 205.37
    Episode_Reward/reaching_object: 1.6371
     Episode_Reward/lifting_object: 135.1973
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0785
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 2.20s
                      Time elapsed: 00:45:09
                               ETA: 00:30:27

################################################################################
                     [1m Learning iteration 1195/2000 [0m                     

                       Computation: 44154 steps/s (collection: 2.106s, learning 0.121s)
             Mean action noise std: 2.87
          Mean value_function loss: 369.2609
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 76.1533
                       Mean reward: 732.62
               Mean episode length: 206.35
    Episode_Reward/reaching_object: 1.7098
     Episode_Reward/lifting_object: 141.8092
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0797
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 2.23s
                      Time elapsed: 00:45:11
                               ETA: 00:30:25

################################################################################
                     [1m Learning iteration 1196/2000 [0m                     

                       Computation: 44979 steps/s (collection: 2.077s, learning 0.109s)
             Mean action noise std: 2.87
          Mean value_function loss: 385.8275
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 76.1647
                       Mean reward: 682.15
               Mean episode length: 205.90
    Episode_Reward/reaching_object: 1.6887
     Episode_Reward/lifting_object: 139.5522
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0798
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 2.19s
                      Time elapsed: 00:45:14
                               ETA: 00:30:23

################################################################################
                     [1m Learning iteration 1197/2000 [0m                     

                       Computation: 45171 steps/s (collection: 2.062s, learning 0.114s)
             Mean action noise std: 2.87
          Mean value_function loss: 397.7016
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 76.1760
                       Mean reward: 700.22
               Mean episode length: 204.12
    Episode_Reward/reaching_object: 1.6776
     Episode_Reward/lifting_object: 138.8612
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0799
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 2.18s
                      Time elapsed: 00:45:16
                               ETA: 00:30:20

################################################################################
                     [1m Learning iteration 1198/2000 [0m                     

                       Computation: 45072 steps/s (collection: 2.075s, learning 0.106s)
             Mean action noise std: 2.88
          Mean value_function loss: 367.1179
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 76.1858
                       Mean reward: 688.57
               Mean episode length: 204.42
    Episode_Reward/reaching_object: 1.6743
     Episode_Reward/lifting_object: 138.0479
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0815
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 2.18s
                      Time elapsed: 00:45:18
                               ETA: 00:30:18

################################################################################
                     [1m Learning iteration 1199/2000 [0m                     

                       Computation: 45494 steps/s (collection: 2.073s, learning 0.088s)
             Mean action noise std: 2.88
          Mean value_function loss: 397.9233
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 76.1986
                       Mean reward: 690.37
               Mean episode length: 207.80
    Episode_Reward/reaching_object: 1.6203
     Episode_Reward/lifting_object: 133.1109
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0791
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 2.16s
                      Time elapsed: 00:45:20
                               ETA: 00:30:16

################################################################################
                     [1m Learning iteration 1200/2000 [0m                     

                       Computation: 44890 steps/s (collection: 2.101s, learning 0.089s)
             Mean action noise std: 2.88
          Mean value_function loss: 385.1746
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 76.2161
                       Mean reward: 700.97
               Mean episode length: 205.07
    Episode_Reward/reaching_object: 1.6535
     Episode_Reward/lifting_object: 136.4452
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0795
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 2.19s
                      Time elapsed: 00:45:22
                               ETA: 00:30:13

################################################################################
                     [1m Learning iteration 1201/2000 [0m                     

                       Computation: 44572 steps/s (collection: 2.112s, learning 0.094s)
             Mean action noise std: 2.88
          Mean value_function loss: 349.0227
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 76.2304
                       Mean reward: 704.17
               Mean episode length: 207.98
    Episode_Reward/reaching_object: 1.6323
     Episode_Reward/lifting_object: 134.0449
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0797
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 2.21s
                      Time elapsed: 00:45:25
                               ETA: 00:30:11

################################################################################
                     [1m Learning iteration 1202/2000 [0m                     

                       Computation: 44777 steps/s (collection: 2.102s, learning 0.094s)
             Mean action noise std: 2.88
          Mean value_function loss: 376.5775
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 76.2428
                       Mean reward: 663.56
               Mean episode length: 200.01
    Episode_Reward/reaching_object: 1.6697
     Episode_Reward/lifting_object: 137.9501
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0796
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 2.20s
                      Time elapsed: 00:45:27
                               ETA: 00:30:09

################################################################################
                     [1m Learning iteration 1203/2000 [0m                     

                       Computation: 45885 steps/s (collection: 2.048s, learning 0.094s)
             Mean action noise std: 2.88
          Mean value_function loss: 366.6321
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 76.2542
                       Mean reward: 713.97
               Mean episode length: 211.56
    Episode_Reward/reaching_object: 1.6864
     Episode_Reward/lifting_object: 139.0518
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 2.14s
                      Time elapsed: 00:45:29
                               ETA: 00:30:06

################################################################################
                     [1m Learning iteration 1204/2000 [0m                     

                       Computation: 45586 steps/s (collection: 2.057s, learning 0.099s)
             Mean action noise std: 2.88
          Mean value_function loss: 392.1823
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 76.2700
                       Mean reward: 666.25
               Mean episode length: 200.35
    Episode_Reward/reaching_object: 1.6526
     Episode_Reward/lifting_object: 135.7538
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 2.16s
                      Time elapsed: 00:45:31
                               ETA: 00:30:04

################################################################################
                     [1m Learning iteration 1205/2000 [0m                     

                       Computation: 45084 steps/s (collection: 2.087s, learning 0.094s)
             Mean action noise std: 2.89
          Mean value_function loss: 406.2832
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 76.2796
                       Mean reward: 616.21
               Mean episode length: 193.99
    Episode_Reward/reaching_object: 1.6100
     Episode_Reward/lifting_object: 132.3324
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0800
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 2.18s
                      Time elapsed: 00:45:33
                               ETA: 00:30:02

################################################################################
                     [1m Learning iteration 1206/2000 [0m                     

                       Computation: 45048 steps/s (collection: 2.089s, learning 0.094s)
             Mean action noise std: 2.89
          Mean value_function loss: 353.9974
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 76.2828
                       Mean reward: 714.52
               Mean episode length: 210.00
    Episode_Reward/reaching_object: 1.7315
     Episode_Reward/lifting_object: 143.6920
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0823
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 2.18s
                      Time elapsed: 00:45:35
                               ETA: 00:29:59

################################################################################
                     [1m Learning iteration 1207/2000 [0m                     

                       Computation: 45410 steps/s (collection: 2.058s, learning 0.107s)
             Mean action noise std: 2.89
          Mean value_function loss: 335.1716
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 76.2912
                       Mean reward: 692.16
               Mean episode length: 206.81
    Episode_Reward/reaching_object: 1.6586
     Episode_Reward/lifting_object: 136.6440
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0822
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 2.16s
                      Time elapsed: 00:45:38
                               ETA: 00:29:57

################################################################################
                     [1m Learning iteration 1208/2000 [0m                     

                       Computation: 44303 steps/s (collection: 2.114s, learning 0.105s)
             Mean action noise std: 2.89
          Mean value_function loss: 388.2413
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 76.3097
                       Mean reward: 702.11
               Mean episode length: 202.42
    Episode_Reward/reaching_object: 1.6231
     Episode_Reward/lifting_object: 133.8086
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0790
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 2.22s
                      Time elapsed: 00:45:40
                               ETA: 00:29:55

################################################################################
                     [1m Learning iteration 1209/2000 [0m                     

                       Computation: 43617 steps/s (collection: 2.134s, learning 0.119s)
             Mean action noise std: 2.89
          Mean value_function loss: 393.9074
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 76.3400
                       Mean reward: 662.97
               Mean episode length: 195.97
    Episode_Reward/reaching_object: 1.6467
     Episode_Reward/lifting_object: 135.0253
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0798
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 2.25s
                      Time elapsed: 00:45:42
                               ETA: 00:29:52

################################################################################
                     [1m Learning iteration 1210/2000 [0m                     

                       Computation: 44602 steps/s (collection: 2.105s, learning 0.099s)
             Mean action noise std: 2.89
          Mean value_function loss: 413.8318
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 76.3533
                       Mean reward: 709.36
               Mean episode length: 202.57
    Episode_Reward/reaching_object: 1.6270
     Episode_Reward/lifting_object: 133.9971
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0801
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 2.20s
                      Time elapsed: 00:45:44
                               ETA: 00:29:50

################################################################################
                     [1m Learning iteration 1211/2000 [0m                     

                       Computation: 43866 steps/s (collection: 2.140s, learning 0.101s)
             Mean action noise std: 2.89
          Mean value_function loss: 420.8264
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 76.3591
                       Mean reward: 733.46
               Mean episode length: 210.52
    Episode_Reward/reaching_object: 1.6437
     Episode_Reward/lifting_object: 134.6423
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0813
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 2.24s
                      Time elapsed: 00:45:46
                               ETA: 00:29:48

################################################################################
                     [1m Learning iteration 1212/2000 [0m                     

                       Computation: 44003 steps/s (collection: 2.133s, learning 0.101s)
             Mean action noise std: 2.89
          Mean value_function loss: 410.1669
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 76.3689
                       Mean reward: 667.12
               Mean episode length: 195.49
    Episode_Reward/reaching_object: 1.6536
     Episode_Reward/lifting_object: 135.7797
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0822
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 2.23s
                      Time elapsed: 00:45:49
                               ETA: 00:29:45

################################################################################
                     [1m Learning iteration 1213/2000 [0m                     

                       Computation: 43772 steps/s (collection: 2.150s, learning 0.096s)
             Mean action noise std: 2.90
          Mean value_function loss: 354.4969
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 76.3797
                       Mean reward: 678.13
               Mean episode length: 199.80
    Episode_Reward/reaching_object: 1.6510
     Episode_Reward/lifting_object: 135.1119
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 2.25s
                      Time elapsed: 00:45:51
                               ETA: 00:29:43

################################################################################
                     [1m Learning iteration 1214/2000 [0m                     

                       Computation: 44452 steps/s (collection: 2.116s, learning 0.095s)
             Mean action noise std: 2.90
          Mean value_function loss: 358.6718
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 76.3859
                       Mean reward: 683.85
               Mean episode length: 203.64
    Episode_Reward/reaching_object: 1.6599
     Episode_Reward/lifting_object: 137.0336
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0827
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 2.21s
                      Time elapsed: 00:45:53
                               ETA: 00:29:41

################################################################################
                     [1m Learning iteration 1215/2000 [0m                     

                       Computation: 43096 steps/s (collection: 2.176s, learning 0.105s)
             Mean action noise std: 2.90
          Mean value_function loss: 381.2790
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 76.3976
                       Mean reward: 699.91
               Mean episode length: 203.51
    Episode_Reward/reaching_object: 1.6121
     Episode_Reward/lifting_object: 132.2461
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0798
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 2.28s
                      Time elapsed: 00:45:55
                               ETA: 00:29:39

################################################################################
                     [1m Learning iteration 1216/2000 [0m                     

                       Computation: 42385 steps/s (collection: 2.186s, learning 0.134s)
             Mean action noise std: 2.90
          Mean value_function loss: 354.2400
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 76.4185
                       Mean reward: 617.68
               Mean episode length: 189.56
    Episode_Reward/reaching_object: 1.6397
     Episode_Reward/lifting_object: 134.7689
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0828
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 2.32s
                      Time elapsed: 00:45:58
                               ETA: 00:29:36

################################################################################
                     [1m Learning iteration 1217/2000 [0m                     

                       Computation: 43533 steps/s (collection: 2.146s, learning 0.113s)
             Mean action noise std: 2.90
          Mean value_function loss: 366.8678
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 76.4308
                       Mean reward: 698.90
               Mean episode length: 207.23
    Episode_Reward/reaching_object: 1.6908
     Episode_Reward/lifting_object: 138.8777
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 2.26s
                      Time elapsed: 00:46:00
                               ETA: 00:29:34

################################################################################
                     [1m Learning iteration 1218/2000 [0m                     

                       Computation: 44690 steps/s (collection: 2.106s, learning 0.094s)
             Mean action noise std: 2.90
          Mean value_function loss: 364.1928
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 76.4498
                       Mean reward: 681.31
               Mean episode length: 202.17
    Episode_Reward/reaching_object: 1.6342
     Episode_Reward/lifting_object: 134.5104
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0803
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 2.20s
                      Time elapsed: 00:46:02
                               ETA: 00:29:32

################################################################################
                     [1m Learning iteration 1219/2000 [0m                     

                       Computation: 44673 steps/s (collection: 2.109s, learning 0.091s)
             Mean action noise std: 2.90
          Mean value_function loss: 382.1368
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 76.4604
                       Mean reward: 682.02
               Mean episode length: 199.75
    Episode_Reward/reaching_object: 1.6422
     Episode_Reward/lifting_object: 134.4305
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0820
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 2.20s
                      Time elapsed: 00:46:04
                               ETA: 00:29:30

################################################################################
                     [1m Learning iteration 1220/2000 [0m                     

                       Computation: 44000 steps/s (collection: 2.129s, learning 0.106s)
             Mean action noise std: 2.90
          Mean value_function loss: 389.2296
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 76.4614
                       Mean reward: 695.46
               Mean episode length: 203.07
    Episode_Reward/reaching_object: 1.6665
     Episode_Reward/lifting_object: 137.2603
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0821
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 2.23s
                      Time elapsed: 00:46:07
                               ETA: 00:29:27

################################################################################
                     [1m Learning iteration 1221/2000 [0m                     

                       Computation: 44241 steps/s (collection: 2.086s, learning 0.135s)
             Mean action noise std: 2.90
          Mean value_function loss: 358.2796
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 76.4640
                       Mean reward: 655.39
               Mean episode length: 193.23
    Episode_Reward/reaching_object: 1.6245
     Episode_Reward/lifting_object: 133.6945
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 2.22s
                      Time elapsed: 00:46:09
                               ETA: 00:29:25

################################################################################
                     [1m Learning iteration 1222/2000 [0m                     

                       Computation: 43407 steps/s (collection: 2.141s, learning 0.124s)
             Mean action noise std: 2.90
          Mean value_function loss: 407.7903
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 76.4687
                       Mean reward: 702.77
               Mean episode length: 206.99
    Episode_Reward/reaching_object: 1.6645
     Episode_Reward/lifting_object: 136.3157
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0817
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 2.26s
                      Time elapsed: 00:46:11
                               ETA: 00:29:23

################################################################################
                     [1m Learning iteration 1223/2000 [0m                     

                       Computation: 43488 steps/s (collection: 2.158s, learning 0.102s)
             Mean action noise std: 2.90
          Mean value_function loss: 445.0204
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 76.4722
                       Mean reward: 634.41
               Mean episode length: 190.12
    Episode_Reward/reaching_object: 1.5857
     Episode_Reward/lifting_object: 128.8622
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0787
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 2.26s
                      Time elapsed: 00:46:13
                               ETA: 00:29:20

################################################################################
                     [1m Learning iteration 1224/2000 [0m                     

                       Computation: 43863 steps/s (collection: 2.138s, learning 0.104s)
             Mean action noise std: 2.90
          Mean value_function loss: 412.1320
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 76.4744
                       Mean reward: 619.24
               Mean episode length: 189.07
    Episode_Reward/reaching_object: 1.5915
     Episode_Reward/lifting_object: 131.0920
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0776
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 2.24s
                      Time elapsed: 00:46:16
                               ETA: 00:29:18

################################################################################
                     [1m Learning iteration 1225/2000 [0m                     

                       Computation: 43034 steps/s (collection: 2.179s, learning 0.105s)
             Mean action noise std: 2.91
          Mean value_function loss: 451.9274
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 76.4778
                       Mean reward: 588.14
               Mean episode length: 182.04
    Episode_Reward/reaching_object: 1.5259
     Episode_Reward/lifting_object: 124.6891
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0768
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.2917
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 2.28s
                      Time elapsed: 00:46:18
                               ETA: 00:29:16

################################################################################
                     [1m Learning iteration 1226/2000 [0m                     

                       Computation: 43262 steps/s (collection: 2.164s, learning 0.108s)
             Mean action noise std: 2.91
          Mean value_function loss: 457.0021
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 76.4809
                       Mean reward: 658.37
               Mean episode length: 194.84
    Episode_Reward/reaching_object: 1.5882
     Episode_Reward/lifting_object: 130.1080
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0773
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 2.27s
                      Time elapsed: 00:46:20
                               ETA: 00:29:14

################################################################################
                     [1m Learning iteration 1227/2000 [0m                     

                       Computation: 43561 steps/s (collection: 2.154s, learning 0.102s)
             Mean action noise std: 2.91
          Mean value_function loss: 493.9468
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 76.4834
                       Mean reward: 588.26
               Mean episode length: 179.41
    Episode_Reward/reaching_object: 1.5084
     Episode_Reward/lifting_object: 123.0517
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0758
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 2.26s
                      Time elapsed: 00:46:22
                               ETA: 00:29:11

################################################################################
                     [1m Learning iteration 1228/2000 [0m                     

                       Computation: 43993 steps/s (collection: 2.127s, learning 0.107s)
             Mean action noise std: 2.91
          Mean value_function loss: 506.5335
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 76.4861
                       Mean reward: 676.61
               Mean episode length: 197.84
    Episode_Reward/reaching_object: 1.4854
     Episode_Reward/lifting_object: 121.4357
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0745
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 2.23s
                      Time elapsed: 00:46:25
                               ETA: 00:29:09

################################################################################
                     [1m Learning iteration 1229/2000 [0m                     

                       Computation: 43350 steps/s (collection: 2.170s, learning 0.098s)
             Mean action noise std: 2.91
          Mean value_function loss: 568.1385
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 76.4891
                       Mean reward: 649.98
               Mean episode length: 193.62
    Episode_Reward/reaching_object: 1.4902
     Episode_Reward/lifting_object: 121.6597
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0744
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.1667
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 2.27s
                      Time elapsed: 00:46:27
                               ETA: 00:29:07

################################################################################
                     [1m Learning iteration 1230/2000 [0m                     

                       Computation: 43749 steps/s (collection: 2.136s, learning 0.111s)
             Mean action noise std: 2.91
          Mean value_function loss: 563.8464
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 76.4915
                       Mean reward: 520.16
               Mean episode length: 168.08
    Episode_Reward/reaching_object: 1.3683
     Episode_Reward/lifting_object: 110.0611
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0703
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 2.25s
                      Time elapsed: 00:46:29
                               ETA: 00:29:04

################################################################################
                     [1m Learning iteration 1231/2000 [0m                     

                       Computation: 43534 steps/s (collection: 2.151s, learning 0.107s)
             Mean action noise std: 2.91
          Mean value_function loss: 587.8518
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 76.4936
                       Mean reward: 538.26
               Mean episode length: 174.70
    Episode_Reward/reaching_object: 1.3220
     Episode_Reward/lifting_object: 105.4434
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 15.9167
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 2.26s
                      Time elapsed: 00:46:31
                               ETA: 00:29:02

################################################################################
                     [1m Learning iteration 1232/2000 [0m                     

                       Computation: 43303 steps/s (collection: 2.171s, learning 0.099s)
             Mean action noise std: 2.91
          Mean value_function loss: 576.9430
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 76.4956
                       Mean reward: 515.38
               Mean episode length: 163.09
    Episode_Reward/reaching_object: 1.2958
     Episode_Reward/lifting_object: 103.4884
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 2.27s
                      Time elapsed: 00:46:34
                               ETA: 00:29:00

################################################################################
                     [1m Learning iteration 1233/2000 [0m                     

                       Computation: 43645 steps/s (collection: 2.153s, learning 0.099s)
             Mean action noise std: 2.91
          Mean value_function loss: 568.1473
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 76.5000
                       Mean reward: 455.61
               Mean episode length: 158.03
    Episode_Reward/reaching_object: 1.3097
     Episode_Reward/lifting_object: 104.2378
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0694
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 2.25s
                      Time elapsed: 00:46:36
                               ETA: 00:28:58

################################################################################
                     [1m Learning iteration 1234/2000 [0m                     

                       Computation: 42431 steps/s (collection: 2.198s, learning 0.119s)
             Mean action noise std: 2.91
          Mean value_function loss: 574.5299
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 76.5093
                       Mean reward: 586.01
               Mean episode length: 183.92
    Episode_Reward/reaching_object: 1.3711
     Episode_Reward/lifting_object: 110.9451
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0712
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 2.32s
                      Time elapsed: 00:46:38
                               ETA: 00:28:55

################################################################################
                     [1m Learning iteration 1235/2000 [0m                     

                       Computation: 42688 steps/s (collection: 2.175s, learning 0.128s)
             Mean action noise std: 2.91
          Mean value_function loss: 554.7841
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 76.5137
                       Mean reward: 598.98
               Mean episode length: 183.55
    Episode_Reward/reaching_object: 1.3833
     Episode_Reward/lifting_object: 111.4783
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0709
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.7917
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 2.30s
                      Time elapsed: 00:46:41
                               ETA: 00:28:53

################################################################################
                     [1m Learning iteration 1236/2000 [0m                     

                       Computation: 41593 steps/s (collection: 2.244s, learning 0.120s)
             Mean action noise std: 2.91
          Mean value_function loss: 567.5617
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 76.5189
                       Mean reward: 480.89
               Mean episode length: 163.91
    Episode_Reward/reaching_object: 1.2676
     Episode_Reward/lifting_object: 100.7736
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.3333
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 2.36s
                      Time elapsed: 00:46:43
                               ETA: 00:28:51

################################################################################
                     [1m Learning iteration 1237/2000 [0m                     

                       Computation: 43672 steps/s (collection: 2.152s, learning 0.099s)
             Mean action noise std: 2.91
          Mean value_function loss: 528.6267
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 76.5220
                       Mean reward: 549.68
               Mean episode length: 171.41
    Episode_Reward/reaching_object: 1.2744
     Episode_Reward/lifting_object: 101.4342
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 2.25s
                      Time elapsed: 00:46:45
                               ETA: 00:28:49

################################################################################
                     [1m Learning iteration 1238/2000 [0m                     

                       Computation: 44001 steps/s (collection: 2.138s, learning 0.097s)
             Mean action noise std: 2.91
          Mean value_function loss: 484.0800
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 76.5259
                       Mean reward: 591.48
               Mean episode length: 184.61
    Episode_Reward/reaching_object: 1.3508
     Episode_Reward/lifting_object: 108.6764
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0702
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 2.23s
                      Time elapsed: 00:46:47
                               ETA: 00:28:46

################################################################################
                     [1m Learning iteration 1239/2000 [0m                     

                       Computation: 43225 steps/s (collection: 2.174s, learning 0.101s)
             Mean action noise std: 2.91
          Mean value_function loss: 510.4667
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 76.5313
                       Mean reward: 632.00
               Mean episode length: 190.88
    Episode_Reward/reaching_object: 1.3838
     Episode_Reward/lifting_object: 112.0653
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0710
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 2.27s
                      Time elapsed: 00:46:50
                               ETA: 00:28:44

################################################################################
                     [1m Learning iteration 1240/2000 [0m                     

                       Computation: 43463 steps/s (collection: 2.153s, learning 0.109s)
             Mean action noise std: 2.91
          Mean value_function loss: 517.1486
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 76.5326
                       Mean reward: 470.80
               Mean episode length: 164.85
    Episode_Reward/reaching_object: 1.3453
     Episode_Reward/lifting_object: 107.8723
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0712
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.9583
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 2.26s
                      Time elapsed: 00:46:52
                               ETA: 00:28:42

################################################################################
                     [1m Learning iteration 1241/2000 [0m                     

                       Computation: 43150 steps/s (collection: 2.166s, learning 0.112s)
             Mean action noise std: 2.91
          Mean value_function loss: 512.5789
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 76.5345
                       Mean reward: 551.31
               Mean episode length: 175.58
    Episode_Reward/reaching_object: 1.3451
     Episode_Reward/lifting_object: 108.3406
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0695
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 2.28s
                      Time elapsed: 00:46:54
                               ETA: 00:28:40

################################################################################
                     [1m Learning iteration 1242/2000 [0m                     

                       Computation: 43280 steps/s (collection: 2.154s, learning 0.117s)
             Mean action noise std: 2.91
          Mean value_function loss: 489.5978
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 76.5380
                       Mean reward: 493.03
               Mean episode length: 168.12
    Episode_Reward/reaching_object: 1.3083
     Episode_Reward/lifting_object: 104.2113
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 2.27s
                      Time elapsed: 00:46:57
                               ETA: 00:28:37

################################################################################
                     [1m Learning iteration 1243/2000 [0m                     

                       Computation: 43786 steps/s (collection: 2.145s, learning 0.100s)
             Mean action noise std: 2.91
          Mean value_function loss: 470.4827
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 76.5406
                       Mean reward: 560.68
               Mean episode length: 184.87
    Episode_Reward/reaching_object: 1.3868
     Episode_Reward/lifting_object: 112.1027
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0721
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.0833
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 2.25s
                      Time elapsed: 00:46:59
                               ETA: 00:28:35

################################################################################
                     [1m Learning iteration 1244/2000 [0m                     

                       Computation: 43400 steps/s (collection: 2.165s, learning 0.100s)
             Mean action noise std: 2.91
          Mean value_function loss: 439.6960
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 76.5453
                       Mean reward: 556.55
               Mean episode length: 175.74
    Episode_Reward/reaching_object: 1.4119
     Episode_Reward/lifting_object: 113.4664
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0727
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 2.27s
                      Time elapsed: 00:47:01
                               ETA: 00:28:33

################################################################################
                     [1m Learning iteration 1245/2000 [0m                     

                       Computation: 43628 steps/s (collection: 2.144s, learning 0.110s)
             Mean action noise std: 2.92
          Mean value_function loss: 430.9608
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 76.5496
                       Mean reward: 673.49
               Mean episode length: 199.96
    Episode_Reward/reaching_object: 1.4515
     Episode_Reward/lifting_object: 116.9205
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0748
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 2.25s
                      Time elapsed: 00:47:03
                               ETA: 00:28:31

################################################################################
                     [1m Learning iteration 1246/2000 [0m                     

                       Computation: 43241 steps/s (collection: 2.171s, learning 0.103s)
             Mean action noise std: 2.92
          Mean value_function loss: 482.2397
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 76.5556
                       Mean reward: 586.22
               Mean episode length: 181.69
    Episode_Reward/reaching_object: 1.5070
     Episode_Reward/lifting_object: 123.4120
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0744
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.7083
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 2.27s
                      Time elapsed: 00:47:06
                               ETA: 00:28:28

################################################################################
                     [1m Learning iteration 1247/2000 [0m                     

                       Computation: 43458 steps/s (collection: 2.155s, learning 0.107s)
             Mean action noise std: 2.92
          Mean value_function loss: 484.6981
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 76.5648
                       Mean reward: 594.48
               Mean episode length: 184.96
    Episode_Reward/reaching_object: 1.4963
     Episode_Reward/lifting_object: 122.3530
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0735
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.0833
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 2.26s
                      Time elapsed: 00:47:08
                               ETA: 00:28:26

################################################################################
                     [1m Learning iteration 1248/2000 [0m                     

                       Computation: 43130 steps/s (collection: 2.147s, learning 0.132s)
             Mean action noise std: 2.92
          Mean value_function loss: 415.2169
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 76.5720
                       Mean reward: 631.32
               Mean episode length: 193.19
    Episode_Reward/reaching_object: 1.4852
     Episode_Reward/lifting_object: 119.5532
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0757
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 2.28s
                      Time elapsed: 00:47:10
                               ETA: 00:28:24

################################################################################
                     [1m Learning iteration 1249/2000 [0m                     

                       Computation: 42684 steps/s (collection: 2.165s, learning 0.138s)
             Mean action noise std: 2.92
          Mean value_function loss: 412.3226
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 76.5765
                       Mean reward: 632.92
               Mean episode length: 191.96
    Episode_Reward/reaching_object: 1.5685
     Episode_Reward/lifting_object: 127.8278
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0774
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 2.30s
                      Time elapsed: 00:47:12
                               ETA: 00:28:22

################################################################################
                     [1m Learning iteration 1250/2000 [0m                     

                       Computation: 43787 steps/s (collection: 2.142s, learning 0.103s)
             Mean action noise std: 2.92
          Mean value_function loss: 400.9525
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 76.5922
                       Mean reward: 601.49
               Mean episode length: 182.24
    Episode_Reward/reaching_object: 1.5425
     Episode_Reward/lifting_object: 125.1323
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0754
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 2.25s
                      Time elapsed: 00:47:15
                               ETA: 00:28:19

################################################################################
                     [1m Learning iteration 1251/2000 [0m                     

                       Computation: 43512 steps/s (collection: 2.149s, learning 0.111s)
             Mean action noise std: 2.92
          Mean value_function loss: 394.3439
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 76.6209
                       Mean reward: 661.93
               Mean episode length: 205.23
    Episode_Reward/reaching_object: 1.6268
     Episode_Reward/lifting_object: 132.3176
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0800
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 2.26s
                      Time elapsed: 00:47:17
                               ETA: 00:28:17

################################################################################
                     [1m Learning iteration 1252/2000 [0m                     

                       Computation: 43145 steps/s (collection: 2.174s, learning 0.105s)
             Mean action noise std: 2.93
          Mean value_function loss: 359.4255
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 76.6434
                       Mean reward: 682.30
               Mean episode length: 204.16
    Episode_Reward/reaching_object: 1.5972
     Episode_Reward/lifting_object: 129.6668
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0781
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 2.28s
                      Time elapsed: 00:47:19
                               ETA: 00:28:15

################################################################################
                     [1m Learning iteration 1253/2000 [0m                     

                       Computation: 42954 steps/s (collection: 2.183s, learning 0.106s)
             Mean action noise std: 2.93
          Mean value_function loss: 381.3794
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 76.6569
                       Mean reward: 652.15
               Mean episode length: 197.55
    Episode_Reward/reaching_object: 1.6103
     Episode_Reward/lifting_object: 131.2583
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0779
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 2.29s
                      Time elapsed: 00:47:22
                               ETA: 00:28:12

################################################################################
                     [1m Learning iteration 1254/2000 [0m                     

                       Computation: 43103 steps/s (collection: 2.164s, learning 0.117s)
             Mean action noise std: 2.93
          Mean value_function loss: 327.2872
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 76.6662
                       Mean reward: 719.19
               Mean episode length: 209.98
    Episode_Reward/reaching_object: 1.6714
     Episode_Reward/lifting_object: 136.5918
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0803
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 2.28s
                      Time elapsed: 00:47:24
                               ETA: 00:28:10

################################################################################
                     [1m Learning iteration 1255/2000 [0m                     

                       Computation: 43056 steps/s (collection: 2.182s, learning 0.101s)
             Mean action noise std: 2.93
          Mean value_function loss: 380.9258
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 76.6758
                       Mean reward: 737.69
               Mean episode length: 214.04
    Episode_Reward/reaching_object: 1.6211
     Episode_Reward/lifting_object: 131.6871
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0791
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 2.28s
                      Time elapsed: 00:47:26
                               ETA: 00:28:08

################################################################################
                     [1m Learning iteration 1256/2000 [0m                     

                       Computation: 43156 steps/s (collection: 2.174s, learning 0.104s)
             Mean action noise std: 2.93
          Mean value_function loss: 364.1472
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 76.6977
                       Mean reward: 766.33
               Mean episode length: 218.31
    Episode_Reward/reaching_object: 1.6926
     Episode_Reward/lifting_object: 138.4553
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0811
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 2.28s
                      Time elapsed: 00:47:28
                               ETA: 00:28:06

################################################################################
                     [1m Learning iteration 1257/2000 [0m                     

                       Computation: 42409 steps/s (collection: 2.221s, learning 0.097s)
             Mean action noise std: 2.93
          Mean value_function loss: 353.8124
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 76.7283
                       Mean reward: 639.79
               Mean episode length: 197.64
    Episode_Reward/reaching_object: 1.6514
     Episode_Reward/lifting_object: 134.3070
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0800
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 2.32s
                      Time elapsed: 00:47:31
                               ETA: 00:28:03

################################################################################
                     [1m Learning iteration 1258/2000 [0m                     

                       Computation: 43103 steps/s (collection: 2.169s, learning 0.111s)
             Mean action noise std: 2.94
          Mean value_function loss: 382.1516
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 76.7427
                       Mean reward: 631.33
               Mean episode length: 187.77
    Episode_Reward/reaching_object: 1.6184
     Episode_Reward/lifting_object: 131.2866
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0771
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 2.28s
                      Time elapsed: 00:47:33
                               ETA: 00:28:01

################################################################################
                     [1m Learning iteration 1259/2000 [0m                     

                       Computation: 43144 steps/s (collection: 2.175s, learning 0.103s)
             Mean action noise std: 2.94
          Mean value_function loss: 363.0092
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 76.7526
                       Mean reward: 687.11
               Mean episode length: 198.61
    Episode_Reward/reaching_object: 1.6908
     Episode_Reward/lifting_object: 139.1086
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0788
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 2.28s
                      Time elapsed: 00:47:35
                               ETA: 00:27:59

################################################################################
                     [1m Learning iteration 1260/2000 [0m                     

                       Computation: 42811 steps/s (collection: 2.186s, learning 0.110s)
             Mean action noise std: 2.94
          Mean value_function loss: 405.3260
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 76.7586
                       Mean reward: 592.07
               Mean episode length: 182.54
    Episode_Reward/reaching_object: 1.5754
     Episode_Reward/lifting_object: 128.5584
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0762
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 2.30s
                      Time elapsed: 00:47:38
                               ETA: 00:27:57

################################################################################
                     [1m Learning iteration 1261/2000 [0m                     

                       Computation: 43442 steps/s (collection: 2.149s, learning 0.114s)
             Mean action noise std: 2.94
          Mean value_function loss: 382.3907
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 76.7620
                       Mean reward: 691.88
               Mean episode length: 199.88
    Episode_Reward/reaching_object: 1.6119
     Episode_Reward/lifting_object: 131.6637
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0773
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 2.26s
                      Time elapsed: 00:47:40
                               ETA: 00:27:54

################################################################################
                     [1m Learning iteration 1262/2000 [0m                     

                       Computation: 42655 steps/s (collection: 2.176s, learning 0.129s)
             Mean action noise std: 2.94
          Mean value_function loss: 367.8589
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 76.7631
                       Mean reward: 686.33
               Mean episode length: 196.37
    Episode_Reward/reaching_object: 1.6041
     Episode_Reward/lifting_object: 132.2006
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0748
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 2.30s
                      Time elapsed: 00:47:42
                               ETA: 00:27:52

################################################################################
                     [1m Learning iteration 1263/2000 [0m                     

                       Computation: 43247 steps/s (collection: 2.155s, learning 0.118s)
             Mean action noise std: 2.94
          Mean value_function loss: 438.3647
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 76.7647
                       Mean reward: 634.94
               Mean episode length: 191.01
    Episode_Reward/reaching_object: 1.5172
     Episode_Reward/lifting_object: 123.4312
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0745
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.6667
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 2.27s
                      Time elapsed: 00:47:44
                               ETA: 00:27:50

################################################################################
                     [1m Learning iteration 1264/2000 [0m                     

                       Computation: 43363 steps/s (collection: 2.164s, learning 0.103s)
             Mean action noise std: 2.94
          Mean value_function loss: 367.1333
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 76.7661
                       Mean reward: 685.16
               Mean episode length: 201.06
    Episode_Reward/reaching_object: 1.6144
     Episode_Reward/lifting_object: 132.7533
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0773
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 2.27s
                      Time elapsed: 00:47:47
                               ETA: 00:27:48

################################################################################
                     [1m Learning iteration 1265/2000 [0m                     

                       Computation: 43431 steps/s (collection: 2.161s, learning 0.103s)
             Mean action noise std: 2.94
          Mean value_function loss: 411.3344
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 76.7694
                       Mean reward: 641.02
               Mean episode length: 188.82
    Episode_Reward/reaching_object: 1.5953
     Episode_Reward/lifting_object: 130.4955
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0785
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 2.26s
                      Time elapsed: 00:47:49
                               ETA: 00:27:45

################################################################################
                     [1m Learning iteration 1266/2000 [0m                     

                       Computation: 43517 steps/s (collection: 2.151s, learning 0.108s)
             Mean action noise std: 2.94
          Mean value_function loss: 410.6494
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 76.7816
                       Mean reward: 688.04
               Mean episode length: 206.07
    Episode_Reward/reaching_object: 1.6236
     Episode_Reward/lifting_object: 133.0550
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0802
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 2.26s
                      Time elapsed: 00:47:51
                               ETA: 00:27:43

################################################################################
                     [1m Learning iteration 1267/2000 [0m                     

                       Computation: 41810 steps/s (collection: 2.230s, learning 0.121s)
             Mean action noise std: 2.94
          Mean value_function loss: 333.1801
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 76.8040
                       Mean reward: 710.51
               Mean episode length: 206.91
    Episode_Reward/reaching_object: 1.6451
     Episode_Reward/lifting_object: 134.5189
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0808
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 2.35s
                      Time elapsed: 00:47:53
                               ETA: 00:27:41

################################################################################
                     [1m Learning iteration 1268/2000 [0m                     

                       Computation: 43008 steps/s (collection: 2.186s, learning 0.100s)
             Mean action noise std: 2.94
          Mean value_function loss: 318.6011
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 76.8209
                       Mean reward: 731.45
               Mean episode length: 207.66
    Episode_Reward/reaching_object: 1.6502
     Episode_Reward/lifting_object: 136.1248
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0806
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 2.29s
                      Time elapsed: 00:47:56
                               ETA: 00:27:39

################################################################################
                     [1m Learning iteration 1269/2000 [0m                     

                       Computation: 44101 steps/s (collection: 2.115s, learning 0.114s)
             Mean action noise std: 2.94
          Mean value_function loss: 304.4899
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 76.8298
                       Mean reward: 722.50
               Mean episode length: 208.65
    Episode_Reward/reaching_object: 1.6895
     Episode_Reward/lifting_object: 139.0906
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 2.23s
                      Time elapsed: 00:47:58
                               ETA: 00:27:36

################################################################################
                     [1m Learning iteration 1270/2000 [0m                     

                       Computation: 43948 steps/s (collection: 2.134s, learning 0.103s)
             Mean action noise std: 2.95
          Mean value_function loss: 314.3091
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 76.8429
                       Mean reward: 665.36
               Mean episode length: 198.02
    Episode_Reward/reaching_object: 1.6517
     Episode_Reward/lifting_object: 135.6376
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0523
          Episode_Reward/joint_vel: -0.0820
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 2.24s
                      Time elapsed: 00:48:00
                               ETA: 00:27:34

################################################################################
                     [1m Learning iteration 1271/2000 [0m                     

                       Computation: 44033 steps/s (collection: 2.129s, learning 0.104s)
             Mean action noise std: 2.95
          Mean value_function loss: 304.6598
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 76.8561
                       Mean reward: 701.42
               Mean episode length: 209.55
    Episode_Reward/reaching_object: 1.7073
     Episode_Reward/lifting_object: 139.5499
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0856
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 2.23s
                      Time elapsed: 00:48:02
                               ETA: 00:27:32

################################################################################
                     [1m Learning iteration 1272/2000 [0m                     

                       Computation: 43007 steps/s (collection: 2.166s, learning 0.120s)
             Mean action noise std: 2.95
          Mean value_function loss: 317.0523
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 76.8714
                       Mean reward: 706.75
               Mean episode length: 210.92
    Episode_Reward/reaching_object: 1.7092
     Episode_Reward/lifting_object: 140.1574
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0859
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 2.29s
                      Time elapsed: 00:48:05
                               ETA: 00:27:30

################################################################################
                     [1m Learning iteration 1273/2000 [0m                     

                       Computation: 43925 steps/s (collection: 2.134s, learning 0.104s)
             Mean action noise std: 2.95
          Mean value_function loss: 360.2775
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 76.8760
                       Mean reward: 716.95
               Mean episode length: 208.49
    Episode_Reward/reaching_object: 1.6764
     Episode_Reward/lifting_object: 137.5641
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0533
          Episode_Reward/joint_vel: -0.0850
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 2.24s
                      Time elapsed: 00:48:07
                               ETA: 00:27:27

################################################################################
                     [1m Learning iteration 1274/2000 [0m                     

                       Computation: 43520 steps/s (collection: 2.147s, learning 0.112s)
             Mean action noise std: 2.95
          Mean value_function loss: 351.9193
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 76.8802
                       Mean reward: 645.73
               Mean episode length: 195.86
    Episode_Reward/reaching_object: 1.6742
     Episode_Reward/lifting_object: 137.5813
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 2.26s
                      Time elapsed: 00:48:09
                               ETA: 00:27:25

################################################################################
                     [1m Learning iteration 1275/2000 [0m                     

                       Computation: 43824 steps/s (collection: 2.131s, learning 0.112s)
             Mean action noise std: 2.95
          Mean value_function loss: 415.1682
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 76.8897
                       Mean reward: 657.47
               Mean episode length: 197.58
    Episode_Reward/reaching_object: 1.5903
     Episode_Reward/lifting_object: 129.6585
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 2.24s
                      Time elapsed: 00:48:12
                               ETA: 00:27:23

################################################################################
                     [1m Learning iteration 1276/2000 [0m                     

                       Computation: 43874 steps/s (collection: 2.128s, learning 0.113s)
             Mean action noise std: 2.95
          Mean value_function loss: 422.3533
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 76.9028
                       Mean reward: 639.49
               Mean episode length: 198.80
    Episode_Reward/reaching_object: 1.6029
     Episode_Reward/lifting_object: 130.9986
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0521
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 2.24s
                      Time elapsed: 00:48:14
                               ETA: 00:27:20

################################################################################
                     [1m Learning iteration 1277/2000 [0m                     

                       Computation: 43467 steps/s (collection: 2.142s, learning 0.120s)
             Mean action noise std: 2.96
          Mean value_function loss: 382.1826
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 76.9200
                       Mean reward: 644.14
               Mean episode length: 197.10
    Episode_Reward/reaching_object: 1.6478
     Episode_Reward/lifting_object: 134.4141
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0853
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 2.26s
                      Time elapsed: 00:48:16
                               ETA: 00:27:18

################################################################################
                     [1m Learning iteration 1278/2000 [0m                     

                       Computation: 44315 steps/s (collection: 2.111s, learning 0.108s)
             Mean action noise std: 2.96
          Mean value_function loss: 354.2458
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 76.9290
                       Mean reward: 627.40
               Mean episode length: 196.69
    Episode_Reward/reaching_object: 1.6213
     Episode_Reward/lifting_object: 133.6504
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 2.22s
                      Time elapsed: 00:48:18
                               ETA: 00:27:16

################################################################################
                     [1m Learning iteration 1279/2000 [0m                     

                       Computation: 43699 steps/s (collection: 2.147s, learning 0.102s)
             Mean action noise std: 2.96
          Mean value_function loss: 336.5897
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 76.9375
                       Mean reward: 652.31
               Mean episode length: 203.61
    Episode_Reward/reaching_object: 1.6509
     Episode_Reward/lifting_object: 135.8971
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0533
          Episode_Reward/joint_vel: -0.0854
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 2.25s
                      Time elapsed: 00:48:20
                               ETA: 00:27:14

################################################################################
                     [1m Learning iteration 1280/2000 [0m                     

                       Computation: 43633 steps/s (collection: 2.145s, learning 0.108s)
             Mean action noise std: 2.96
          Mean value_function loss: 359.9627
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 76.9584
                       Mean reward: 663.49
               Mean episode length: 198.18
    Episode_Reward/reaching_object: 1.5711
     Episode_Reward/lifting_object: 127.7184
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0849
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 2.25s
                      Time elapsed: 00:48:23
                               ETA: 00:27:11

################################################################################
                     [1m Learning iteration 1281/2000 [0m                     

                       Computation: 43692 steps/s (collection: 2.138s, learning 0.112s)
             Mean action noise std: 2.96
          Mean value_function loss: 333.5692
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 76.9792
                       Mean reward: 611.54
               Mean episode length: 194.25
    Episode_Reward/reaching_object: 1.6255
     Episode_Reward/lifting_object: 132.6539
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0867
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 2.25s
                      Time elapsed: 00:48:25
                               ETA: 00:27:09

################################################################################
                     [1m Learning iteration 1282/2000 [0m                     

                       Computation: 43891 steps/s (collection: 2.134s, learning 0.106s)
             Mean action noise std: 2.96
          Mean value_function loss: 326.8098
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 76.9943
                       Mean reward: 656.75
               Mean episode length: 205.00
    Episode_Reward/reaching_object: 1.6037
     Episode_Reward/lifting_object: 130.7702
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0855
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 2.24s
                      Time elapsed: 00:48:27
                               ETA: 00:27:07

################################################################################
                     [1m Learning iteration 1283/2000 [0m                     

                       Computation: 42833 steps/s (collection: 2.181s, learning 0.114s)
             Mean action noise std: 2.97
          Mean value_function loss: 329.5233
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 77.0126
                       Mean reward: 685.51
               Mean episode length: 209.41
    Episode_Reward/reaching_object: 1.6831
     Episode_Reward/lifting_object: 138.3660
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0877
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 2.30s
                      Time elapsed: 00:48:30
                               ETA: 00:27:04

################################################################################
                     [1m Learning iteration 1284/2000 [0m                     

                       Computation: 43670 steps/s (collection: 2.152s, learning 0.099s)
             Mean action noise std: 2.97
          Mean value_function loss: 315.4865
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 77.0287
                       Mean reward: 678.87
               Mean episode length: 211.41
    Episode_Reward/reaching_object: 1.6655
     Episode_Reward/lifting_object: 137.0724
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0549
          Episode_Reward/joint_vel: -0.0872
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 2.25s
                      Time elapsed: 00:48:32
                               ETA: 00:27:02

################################################################################
                     [1m Learning iteration 1285/2000 [0m                     

                       Computation: 42708 steps/s (collection: 2.184s, learning 0.117s)
             Mean action noise std: 2.97
          Mean value_function loss: 314.9409
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 77.0468
                       Mean reward: 725.84
               Mean episode length: 211.95
    Episode_Reward/reaching_object: 1.6411
     Episode_Reward/lifting_object: 133.6306
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0863
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 2.30s
                      Time elapsed: 00:48:34
                               ETA: 00:27:00

################################################################################
                     [1m Learning iteration 1286/2000 [0m                     

                       Computation: 42085 steps/s (collection: 2.222s, learning 0.114s)
             Mean action noise std: 2.97
          Mean value_function loss: 315.1080
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 77.0707
                       Mean reward: 660.44
               Mean episode length: 198.27
    Episode_Reward/reaching_object: 1.6390
     Episode_Reward/lifting_object: 135.1176
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0848
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 2.34s
                      Time elapsed: 00:48:36
                               ETA: 00:26:58

################################################################################
                     [1m Learning iteration 1287/2000 [0m                     

                       Computation: 40246 steps/s (collection: 2.334s, learning 0.109s)
             Mean action noise std: 2.97
          Mean value_function loss: 273.6575
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 77.0921
                       Mean reward: 647.79
               Mean episode length: 201.72
    Episode_Reward/reaching_object: 1.6033
     Episode_Reward/lifting_object: 131.7163
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 2.44s
                      Time elapsed: 00:48:39
                               ETA: 00:26:56

################################################################################
                     [1m Learning iteration 1288/2000 [0m                     

                       Computation: 38879 steps/s (collection: 2.400s, learning 0.129s)
             Mean action noise std: 2.98
          Mean value_function loss: 299.1320
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 77.1059
                       Mean reward: 683.48
               Mean episode length: 205.92
    Episode_Reward/reaching_object: 1.6346
     Episode_Reward/lifting_object: 135.0601
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 2.53s
                      Time elapsed: 00:48:41
                               ETA: 00:26:53

################################################################################
                     [1m Learning iteration 1289/2000 [0m                     

                       Computation: 38041 steps/s (collection: 2.452s, learning 0.133s)
             Mean action noise std: 2.98
          Mean value_function loss: 272.3277
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 77.1105
                       Mean reward: 675.98
               Mean episode length: 203.81
    Episode_Reward/reaching_object: 1.6629
     Episode_Reward/lifting_object: 136.5762
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.0844
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 2.58s
                      Time elapsed: 00:48:44
                               ETA: 00:26:51

################################################################################
                     [1m Learning iteration 1290/2000 [0m                     

                       Computation: 33914 steps/s (collection: 2.752s, learning 0.147s)
             Mean action noise std: 2.98
          Mean value_function loss: 261.1058
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 77.1176
                       Mean reward: 705.09
               Mean episode length: 213.54
    Episode_Reward/reaching_object: 1.6859
     Episode_Reward/lifting_object: 138.5077
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 2.90s
                      Time elapsed: 00:48:47
                               ETA: 00:26:49

################################################################################
                     [1m Learning iteration 1291/2000 [0m                     

                       Computation: 38240 steps/s (collection: 2.423s, learning 0.148s)
             Mean action noise std: 2.98
          Mean value_function loss: 335.9114
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 77.1352
                       Mean reward: 667.27
               Mean episode length: 202.93
    Episode_Reward/reaching_object: 1.6397
     Episode_Reward/lifting_object: 133.9941
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0837
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 2.57s
                      Time elapsed: 00:48:49
                               ETA: 00:26:47

################################################################################
                     [1m Learning iteration 1292/2000 [0m                     

                       Computation: 40619 steps/s (collection: 2.302s, learning 0.118s)
             Mean action noise std: 2.98
          Mean value_function loss: 352.8166
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 77.1597
                       Mean reward: 585.28
               Mean episode length: 181.22
    Episode_Reward/reaching_object: 1.5985
     Episode_Reward/lifting_object: 129.8214
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0817
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 2.42s
                      Time elapsed: 00:48:52
                               ETA: 00:26:45

################################################################################
                     [1m Learning iteration 1293/2000 [0m                     

                       Computation: 34492 steps/s (collection: 2.669s, learning 0.181s)
             Mean action noise std: 2.98
          Mean value_function loss: 401.4450
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 77.1836
                       Mean reward: 680.17
               Mean episode length: 196.10
    Episode_Reward/reaching_object: 1.6674
     Episode_Reward/lifting_object: 137.1598
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0824
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 2.85s
                      Time elapsed: 00:48:55
                               ETA: 00:26:43

################################################################################
                     [1m Learning iteration 1294/2000 [0m                     

                       Computation: 37398 steps/s (collection: 2.513s, learning 0.116s)
             Mean action noise std: 2.99
          Mean value_function loss: 390.5782
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 77.2018
                       Mean reward: 640.20
               Mean episode length: 187.59
    Episode_Reward/reaching_object: 1.6726
     Episode_Reward/lifting_object: 137.6638
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0808
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 2.63s
                      Time elapsed: 00:48:57
                               ETA: 00:26:41

################################################################################
                     [1m Learning iteration 1295/2000 [0m                     

                       Computation: 39045 steps/s (collection: 2.408s, learning 0.110s)
             Mean action noise std: 2.99
          Mean value_function loss: 389.0698
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 77.2167
                       Mean reward: 696.40
               Mean episode length: 204.63
    Episode_Reward/reaching_object: 1.6655
     Episode_Reward/lifting_object: 137.0113
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0827
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 2.52s
                      Time elapsed: 00:49:00
                               ETA: 00:26:39

################################################################################
                     [1m Learning iteration 1296/2000 [0m                     

                       Computation: 42862 steps/s (collection: 2.190s, learning 0.103s)
             Mean action noise std: 2.99
          Mean value_function loss: 333.9010
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 77.2365
                       Mean reward: 634.44
               Mean episode length: 189.93
    Episode_Reward/reaching_object: 1.6189
     Episode_Reward/lifting_object: 132.9586
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0804
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 2.29s
                      Time elapsed: 00:49:02
                               ETA: 00:26:37

################################################################################
                     [1m Learning iteration 1297/2000 [0m                     

                       Computation: 40227 steps/s (collection: 2.260s, learning 0.184s)
             Mean action noise std: 2.99
          Mean value_function loss: 327.4599
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 77.2608
                       Mean reward: 703.42
               Mean episode length: 209.08
    Episode_Reward/reaching_object: 1.6228
     Episode_Reward/lifting_object: 133.5360
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0816
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 2.44s
                      Time elapsed: 00:49:05
                               ETA: 00:26:35

################################################################################
                     [1m Learning iteration 1298/2000 [0m                     

                       Computation: 42881 steps/s (collection: 2.186s, learning 0.107s)
             Mean action noise std: 3.00
          Mean value_function loss: 305.8019
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 77.2876
                       Mean reward: 756.95
               Mean episode length: 218.71
    Episode_Reward/reaching_object: 1.7095
     Episode_Reward/lifting_object: 142.2456
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0823
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 2.29s
                      Time elapsed: 00:49:07
                               ETA: 00:26:32

################################################################################
                     [1m Learning iteration 1299/2000 [0m                     

                       Computation: 43213 steps/s (collection: 2.172s, learning 0.103s)
             Mean action noise std: 3.00
          Mean value_function loss: 359.7971
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 77.2988
                       Mean reward: 735.06
               Mean episode length: 215.45
    Episode_Reward/reaching_object: 1.6604
     Episode_Reward/lifting_object: 136.8582
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.0816
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 2.27s
                      Time elapsed: 00:49:09
                               ETA: 00:26:30

################################################################################
                     [1m Learning iteration 1300/2000 [0m                     

                       Computation: 42049 steps/s (collection: 2.244s, learning 0.094s)
             Mean action noise std: 3.00
          Mean value_function loss: 326.2632
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 77.3166
                       Mean reward: 727.82
               Mean episode length: 212.09
    Episode_Reward/reaching_object: 1.6697
     Episode_Reward/lifting_object: 138.1267
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0806
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 2.34s
                      Time elapsed: 00:49:11
                               ETA: 00:26:28

################################################################################
                     [1m Learning iteration 1301/2000 [0m                     

                       Computation: 42772 steps/s (collection: 2.181s, learning 0.117s)
             Mean action noise std: 3.00
          Mean value_function loss: 385.1632
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 77.3488
                       Mean reward: 670.50
               Mean episode length: 196.83
    Episode_Reward/reaching_object: 1.5903
     Episode_Reward/lifting_object: 131.5771
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0793
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 2.30s
                      Time elapsed: 00:49:14
                               ETA: 00:26:26

################################################################################
                     [1m Learning iteration 1302/2000 [0m                     

                       Computation: 39225 steps/s (collection: 2.346s, learning 0.161s)
             Mean action noise std: 3.00
          Mean value_function loss: 350.1914
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 77.3789
                       Mean reward: 678.51
               Mean episode length: 202.78
    Episode_Reward/reaching_object: 1.7210
     Episode_Reward/lifting_object: 143.6191
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.0822
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 2.51s
                      Time elapsed: 00:49:16
                               ETA: 00:26:23

################################################################################
                     [1m Learning iteration 1303/2000 [0m                     

                       Computation: 42800 steps/s (collection: 2.150s, learning 0.147s)
             Mean action noise std: 3.01
          Mean value_function loss: 350.6790
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 77.4006
                       Mean reward: 736.18
               Mean episode length: 215.17
    Episode_Reward/reaching_object: 1.7206
     Episode_Reward/lifting_object: 143.6843
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.0819
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 2.30s
                      Time elapsed: 00:49:19
                               ETA: 00:26:21

################################################################################
                     [1m Learning iteration 1304/2000 [0m                     

                       Computation: 42308 steps/s (collection: 2.225s, learning 0.099s)
             Mean action noise std: 3.01
          Mean value_function loss: 350.4470
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 77.4220
                       Mean reward: 675.50
               Mean episode length: 197.71
    Episode_Reward/reaching_object: 1.7141
     Episode_Reward/lifting_object: 143.1219
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0825
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 2.32s
                      Time elapsed: 00:49:21
                               ETA: 00:26:19

################################################################################
                     [1m Learning iteration 1305/2000 [0m                     

                       Computation: 42107 steps/s (collection: 2.207s, learning 0.128s)
             Mean action noise std: 3.01
          Mean value_function loss: 344.6756
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 77.4314
                       Mean reward: 732.37
               Mean episode length: 211.97
    Episode_Reward/reaching_object: 1.7015
     Episode_Reward/lifting_object: 141.2120
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0825
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 2.33s
                      Time elapsed: 00:49:23
                               ETA: 00:26:17

################################################################################
                     [1m Learning iteration 1306/2000 [0m                     

                       Computation: 40653 steps/s (collection: 2.240s, learning 0.178s)
             Mean action noise std: 3.01
          Mean value_function loss: 360.4558
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 77.4473
                       Mean reward: 701.43
               Mean episode length: 200.16
    Episode_Reward/reaching_object: 1.7030
     Episode_Reward/lifting_object: 141.0249
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.0832
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 2.42s
                      Time elapsed: 00:49:26
                               ETA: 00:26:14

################################################################################
                     [1m Learning iteration 1307/2000 [0m                     

                       Computation: 42878 steps/s (collection: 2.195s, learning 0.098s)
             Mean action noise std: 3.01
          Mean value_function loss: 328.9286
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 77.4636
                       Mean reward: 678.07
               Mean episode length: 199.37
    Episode_Reward/reaching_object: 1.6129
     Episode_Reward/lifting_object: 132.8688
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 2.29s
                      Time elapsed: 00:49:28
                               ETA: 00:26:12

################################################################################
                     [1m Learning iteration 1308/2000 [0m                     

                       Computation: 42508 steps/s (collection: 2.201s, learning 0.112s)
             Mean action noise std: 3.02
          Mean value_function loss: 302.7790
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 77.4823
                       Mean reward: 689.40
               Mean episode length: 205.69
    Episode_Reward/reaching_object: 1.6425
     Episode_Reward/lifting_object: 135.4778
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0806
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 2.31s
                      Time elapsed: 00:49:30
                               ETA: 00:26:10

################################################################################
                     [1m Learning iteration 1309/2000 [0m                     

                       Computation: 43035 steps/s (collection: 2.151s, learning 0.133s)
             Mean action noise std: 3.02
          Mean value_function loss: 286.1995
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 77.5030
                       Mean reward: 692.94
               Mean episode length: 204.70
    Episode_Reward/reaching_object: 1.7092
     Episode_Reward/lifting_object: 142.1863
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 2.28s
                      Time elapsed: 00:49:33
                               ETA: 00:26:08

################################################################################
                     [1m Learning iteration 1310/2000 [0m                     

                       Computation: 40227 steps/s (collection: 2.306s, learning 0.138s)
             Mean action noise std: 3.02
          Mean value_function loss: 295.4488
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 77.5235
                       Mean reward: 666.40
               Mean episode length: 203.15
    Episode_Reward/reaching_object: 1.7058
     Episode_Reward/lifting_object: 140.8363
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0843
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 2.44s
                      Time elapsed: 00:49:35
                               ETA: 00:26:06

################################################################################
                     [1m Learning iteration 1311/2000 [0m                     

                       Computation: 41889 steps/s (collection: 2.226s, learning 0.121s)
             Mean action noise std: 3.02
          Mean value_function loss: 306.5826
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 77.5461
                       Mean reward: 743.47
               Mean episode length: 219.10
    Episode_Reward/reaching_object: 1.7183
     Episode_Reward/lifting_object: 142.4146
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 2.35s
                      Time elapsed: 00:49:37
                               ETA: 00:26:03

################################################################################
                     [1m Learning iteration 1312/2000 [0m                     

                       Computation: 41431 steps/s (collection: 2.242s, learning 0.131s)
             Mean action noise std: 3.02
          Mean value_function loss: 311.8788
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 77.5619
                       Mean reward: 655.95
               Mean episode length: 197.19
    Episode_Reward/reaching_object: 1.7094
     Episode_Reward/lifting_object: 141.2791
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0834
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 2.37s
                      Time elapsed: 00:49:40
                               ETA: 00:26:01

################################################################################
                     [1m Learning iteration 1313/2000 [0m                     

                       Computation: 42418 steps/s (collection: 2.225s, learning 0.092s)
             Mean action noise std: 3.03
          Mean value_function loss: 334.5345
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 77.5785
                       Mean reward: 740.54
               Mean episode length: 211.13
    Episode_Reward/reaching_object: 1.6926
     Episode_Reward/lifting_object: 139.2282
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0827
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 2.32s
                      Time elapsed: 00:49:42
                               ETA: 00:25:59

################################################################################
                     [1m Learning iteration 1314/2000 [0m                     

                       Computation: 42833 steps/s (collection: 2.195s, learning 0.100s)
             Mean action noise std: 3.03
          Mean value_function loss: 318.1542
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 77.5885
                       Mean reward: 744.60
               Mean episode length: 215.63
    Episode_Reward/reaching_object: 1.7143
     Episode_Reward/lifting_object: 142.0434
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0565
          Episode_Reward/joint_vel: -0.0823
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 2.30s
                      Time elapsed: 00:49:44
                               ETA: 00:25:57

################################################################################
                     [1m Learning iteration 1315/2000 [0m                     

                       Computation: 42352 steps/s (collection: 2.203s, learning 0.118s)
             Mean action noise std: 3.03
          Mean value_function loss: 355.4890
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 77.6121
                       Mean reward: 728.13
               Mean episode length: 208.05
    Episode_Reward/reaching_object: 1.7439
     Episode_Reward/lifting_object: 145.5390
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0577
          Episode_Reward/joint_vel: -0.0834
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 2.32s
                      Time elapsed: 00:49:47
                               ETA: 00:25:54

################################################################################
                     [1m Learning iteration 1316/2000 [0m                     

                       Computation: 42509 steps/s (collection: 2.196s, learning 0.116s)
             Mean action noise std: 3.03
          Mean value_function loss: 342.9308
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 77.6349
                       Mean reward: 736.33
               Mean episode length: 210.96
    Episode_Reward/reaching_object: 1.7143
     Episode_Reward/lifting_object: 141.7953
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 2.31s
                      Time elapsed: 00:49:49
                               ETA: 00:25:52

################################################################################
                     [1m Learning iteration 1317/2000 [0m                     

                       Computation: 41285 steps/s (collection: 2.286s, learning 0.096s)
             Mean action noise std: 3.03
          Mean value_function loss: 317.2324
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 77.6478
                       Mean reward: 648.09
               Mean episode length: 195.51
    Episode_Reward/reaching_object: 1.6587
     Episode_Reward/lifting_object: 136.7261
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0829
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 2.38s
                      Time elapsed: 00:49:51
                               ETA: 00:25:50

################################################################################
                     [1m Learning iteration 1318/2000 [0m                     

                       Computation: 42110 steps/s (collection: 2.211s, learning 0.123s)
             Mean action noise std: 3.04
          Mean value_function loss: 313.6162
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 77.6693
                       Mean reward: 642.12
               Mean episode length: 194.33
    Episode_Reward/reaching_object: 1.6736
     Episode_Reward/lifting_object: 138.8616
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 2.33s
                      Time elapsed: 00:49:54
                               ETA: 00:25:48

################################################################################
                     [1m Learning iteration 1319/2000 [0m                     

                       Computation: 41958 steps/s (collection: 2.214s, learning 0.129s)
             Mean action noise std: 3.04
          Mean value_function loss: 286.5193
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 77.6902
                       Mean reward: 671.17
               Mean episode length: 201.32
    Episode_Reward/reaching_object: 1.6839
     Episode_Reward/lifting_object: 139.4158
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0827
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 2.34s
                      Time elapsed: 00:49:56
                               ETA: 00:25:45

################################################################################
                     [1m Learning iteration 1320/2000 [0m                     

                       Computation: 41932 steps/s (collection: 2.247s, learning 0.097s)
             Mean action noise std: 3.04
          Mean value_function loss: 293.5055
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 77.7007
                       Mean reward: 734.75
               Mean episode length: 213.59
    Episode_Reward/reaching_object: 1.6901
     Episode_Reward/lifting_object: 140.4063
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0828
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 2.34s
                      Time elapsed: 00:49:58
                               ETA: 00:25:43

################################################################################
                     [1m Learning iteration 1321/2000 [0m                     

                       Computation: 40554 steps/s (collection: 2.245s, learning 0.179s)
             Mean action noise std: 3.04
          Mean value_function loss: 330.9525
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 77.7259
                       Mean reward: 760.26
               Mean episode length: 214.40
    Episode_Reward/reaching_object: 1.7381
     Episode_Reward/lifting_object: 145.2584
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0584
          Episode_Reward/joint_vel: -0.0851
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 2.42s
                      Time elapsed: 00:50:01
                               ETA: 00:25:41

################################################################################
                     [1m Learning iteration 1322/2000 [0m                     

                       Computation: 41913 steps/s (collection: 2.233s, learning 0.112s)
             Mean action noise std: 3.05
          Mean value_function loss: 325.0687
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 77.7504
                       Mean reward: 672.76
               Mean episode length: 206.23
    Episode_Reward/reaching_object: 1.7381
     Episode_Reward/lifting_object: 144.3991
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0590
          Episode_Reward/joint_vel: -0.0860
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 2.35s
                      Time elapsed: 00:50:03
                               ETA: 00:25:39

################################################################################
                     [1m Learning iteration 1323/2000 [0m                     

                       Computation: 42110 steps/s (collection: 2.210s, learning 0.124s)
             Mean action noise std: 3.05
          Mean value_function loss: 310.8601
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 77.7805
                       Mean reward: 715.04
               Mean episode length: 213.39
    Episode_Reward/reaching_object: 1.7191
     Episode_Reward/lifting_object: 142.9555
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0858
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 2.33s
                      Time elapsed: 00:50:05
                               ETA: 00:25:37

################################################################################
                     [1m Learning iteration 1324/2000 [0m                     

                       Computation: 40978 steps/s (collection: 2.214s, learning 0.185s)
             Mean action noise std: 3.05
          Mean value_function loss: 319.8431
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 77.7988
                       Mean reward: 699.97
               Mean episode length: 206.23
    Episode_Reward/reaching_object: 1.7264
     Episode_Reward/lifting_object: 144.1107
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0845
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 2.40s
                      Time elapsed: 00:50:08
                               ETA: 00:25:34

################################################################################
                     [1m Learning iteration 1325/2000 [0m                     

                       Computation: 41030 steps/s (collection: 2.273s, learning 0.123s)
             Mean action noise std: 3.05
          Mean value_function loss: 293.2185
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 77.8033
                       Mean reward: 763.40
               Mean episode length: 218.08
    Episode_Reward/reaching_object: 1.7431
     Episode_Reward/lifting_object: 145.3960
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0860
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 2.40s
                      Time elapsed: 00:50:10
                               ETA: 00:25:32

################################################################################
                     [1m Learning iteration 1326/2000 [0m                     

                       Computation: 41573 steps/s (collection: 2.218s, learning 0.147s)
             Mean action noise std: 3.05
          Mean value_function loss: 308.1089
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 77.8229
                       Mean reward: 724.76
               Mean episode length: 213.07
    Episode_Reward/reaching_object: 1.6926
     Episode_Reward/lifting_object: 140.2145
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0847
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 2.36s
                      Time elapsed: 00:50:13
                               ETA: 00:25:30

################################################################################
                     [1m Learning iteration 1327/2000 [0m                     

                       Computation: 40498 steps/s (collection: 2.316s, learning 0.111s)
             Mean action noise std: 3.06
          Mean value_function loss: 270.4862
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 77.8500
                       Mean reward: 748.16
               Mean episode length: 215.92
    Episode_Reward/reaching_object: 1.7281
     Episode_Reward/lifting_object: 143.5339
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0868
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 2.43s
                      Time elapsed: 00:50:15
                               ETA: 00:25:28

################################################################################
                     [1m Learning iteration 1328/2000 [0m                     

                       Computation: 41806 steps/s (collection: 2.260s, learning 0.091s)
             Mean action noise std: 3.06
          Mean value_function loss: 295.5371
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 77.8663
                       Mean reward: 737.07
               Mean episode length: 213.37
    Episode_Reward/reaching_object: 1.7111
     Episode_Reward/lifting_object: 141.7867
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0863
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 2.35s
                      Time elapsed: 00:50:17
                               ETA: 00:25:25

################################################################################
                     [1m Learning iteration 1329/2000 [0m                     

                       Computation: 41671 steps/s (collection: 2.215s, learning 0.144s)
             Mean action noise std: 3.06
          Mean value_function loss: 315.4513
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 77.8892
                       Mean reward: 674.57
               Mean episode length: 201.64
    Episode_Reward/reaching_object: 1.6567
     Episode_Reward/lifting_object: 136.8771
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0564
          Episode_Reward/joint_vel: -0.0825
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 2.36s
                      Time elapsed: 00:50:20
                               ETA: 00:25:23

################################################################################
                     [1m Learning iteration 1330/2000 [0m                     

                       Computation: 42587 steps/s (collection: 2.215s, learning 0.093s)
             Mean action noise std: 3.06
          Mean value_function loss: 291.6546
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 77.9012
                       Mean reward: 764.33
               Mean episode length: 218.39
    Episode_Reward/reaching_object: 1.7445
     Episode_Reward/lifting_object: 145.3307
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0590
          Episode_Reward/joint_vel: -0.0856
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 2.31s
                      Time elapsed: 00:50:22
                               ETA: 00:25:21

################################################################################
                     [1m Learning iteration 1331/2000 [0m                     

                       Computation: 42277 steps/s (collection: 2.225s, learning 0.100s)
             Mean action noise std: 3.06
          Mean value_function loss: 289.4115
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 77.9126
                       Mean reward: 722.56
               Mean episode length: 209.00
    Episode_Reward/reaching_object: 1.7217
     Episode_Reward/lifting_object: 142.4604
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 2.33s
                      Time elapsed: 00:50:24
                               ETA: 00:25:19

################################################################################
                     [1m Learning iteration 1332/2000 [0m                     

                       Computation: 42316 steps/s (collection: 2.200s, learning 0.123s)
             Mean action noise std: 3.06
          Mean value_function loss: 290.4306
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 77.9310
                       Mean reward: 726.55
               Mean episode length: 214.06
    Episode_Reward/reaching_object: 1.7710
     Episode_Reward/lifting_object: 145.9495
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0605
          Episode_Reward/joint_vel: -0.0884
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 2.32s
                      Time elapsed: 00:50:27
                               ETA: 00:25:17

################################################################################
                     [1m Learning iteration 1333/2000 [0m                     

                       Computation: 17891 steps/s (collection: 5.353s, learning 0.142s)
             Mean action noise std: 3.07
          Mean value_function loss: 270.5590
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 77.9514
                       Mean reward: 712.89
               Mean episode length: 207.13
    Episode_Reward/reaching_object: 1.7493
     Episode_Reward/lifting_object: 145.3118
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0865
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 5.49s
                      Time elapsed: 00:50:32
                               ETA: 00:25:16

################################################################################
                     [1m Learning iteration 1334/2000 [0m                     

                       Computation: 13561 steps/s (collection: 7.123s, learning 0.125s)
             Mean action noise std: 3.07
          Mean value_function loss: 312.0225
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 77.9683
                       Mean reward: 688.54
               Mean episode length: 208.37
    Episode_Reward/reaching_object: 1.6605
     Episode_Reward/lifting_object: 136.6393
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0855
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 7.25s
                      Time elapsed: 00:50:39
                               ETA: 00:25:16

################################################################################
                     [1m Learning iteration 1335/2000 [0m                     

                       Computation: 13500 steps/s (collection: 7.160s, learning 0.121s)
             Mean action noise std: 3.07
          Mean value_function loss: 351.2901
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 77.9838
                       Mean reward: 678.63
               Mean episode length: 203.70
    Episode_Reward/reaching_object: 1.6898
     Episode_Reward/lifting_object: 140.7211
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0832
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 7.28s
                      Time elapsed: 00:50:47
                               ETA: 00:25:16

################################################################################
                     [1m Learning iteration 1336/2000 [0m                     

                       Computation: 13652 steps/s (collection: 7.083s, learning 0.117s)
             Mean action noise std: 3.07
          Mean value_function loss: 361.1666
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 77.9959
                       Mean reward: 681.31
               Mean episode length: 203.67
    Episode_Reward/reaching_object: 1.6865
     Episode_Reward/lifting_object: 140.4393
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 7.20s
                      Time elapsed: 00:50:54
                               ETA: 00:25:16

################################################################################
                     [1m Learning iteration 1337/2000 [0m                     

                       Computation: 13587 steps/s (collection: 7.083s, learning 0.152s)
             Mean action noise std: 3.07
          Mean value_function loss: 315.2382
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 78.0145
                       Mean reward: 653.36
               Mean episode length: 198.29
    Episode_Reward/reaching_object: 1.6719
     Episode_Reward/lifting_object: 138.9541
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0821
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 7.23s
                      Time elapsed: 00:51:01
                               ETA: 00:25:17

################################################################################
                     [1m Learning iteration 1338/2000 [0m                     

                       Computation: 13655 steps/s (collection: 7.071s, learning 0.128s)
             Mean action noise std: 3.07
          Mean value_function loss: 297.8905
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 78.0330
                       Mean reward: 709.88
               Mean episode length: 209.11
    Episode_Reward/reaching_object: 1.6902
     Episode_Reward/lifting_object: 140.5176
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0582
          Episode_Reward/joint_vel: -0.0836
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 7.20s
                      Time elapsed: 00:51:08
                               ETA: 00:25:17

################################################################################
                     [1m Learning iteration 1339/2000 [0m                     

                       Computation: 13893 steps/s (collection: 6.945s, learning 0.130s)
             Mean action noise std: 3.07
          Mean value_function loss: 333.5556
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 78.0396
                       Mean reward: 722.76
               Mean episode length: 210.27
    Episode_Reward/reaching_object: 1.6962
     Episode_Reward/lifting_object: 141.4331
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0823
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 7.08s
                      Time elapsed: 00:51:15
                               ETA: 00:25:17

################################################################################
                     [1m Learning iteration 1340/2000 [0m                     

                       Computation: 14043 steps/s (collection: 6.881s, learning 0.120s)
             Mean action noise std: 3.08
          Mean value_function loss: 327.3388
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 78.0423
                       Mean reward: 704.71
               Mean episode length: 205.50
    Episode_Reward/reaching_object: 1.7053
     Episode_Reward/lifting_object: 141.7527
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0843
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 7.00s
                      Time elapsed: 00:51:22
                               ETA: 00:25:17

################################################################################
                     [1m Learning iteration 1341/2000 [0m                     

                       Computation: 12583 steps/s (collection: 7.690s, learning 0.122s)
             Mean action noise std: 3.08
          Mean value_function loss: 284.6586
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 78.0539
                       Mean reward: 711.80
               Mean episode length: 207.67
    Episode_Reward/reaching_object: 1.6484
     Episode_Reward/lifting_object: 136.3577
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0811
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 7.81s
                      Time elapsed: 00:51:30
                               ETA: 00:25:17

################################################################################
                     [1m Learning iteration 1342/2000 [0m                     

                       Computation: 43353 steps/s (collection: 2.160s, learning 0.108s)
             Mean action noise std: 3.08
          Mean value_function loss: 315.0976
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 78.0741
                       Mean reward: 751.91
               Mean episode length: 213.62
    Episode_Reward/reaching_object: 1.7275
     Episode_Reward/lifting_object: 141.6152
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 2.27s
                      Time elapsed: 00:51:33
                               ETA: 00:25:15

################################################################################
                     [1m Learning iteration 1343/2000 [0m                     

                       Computation: 37307 steps/s (collection: 2.505s, learning 0.130s)
             Mean action noise std: 3.08
          Mean value_function loss: 341.6126
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 78.0916
                       Mean reward: 747.80
               Mean episode length: 214.25
    Episode_Reward/reaching_object: 1.6944
     Episode_Reward/lifting_object: 140.6538
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0582
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 2.63s
                      Time elapsed: 00:51:35
                               ETA: 00:25:13

################################################################################
                     [1m Learning iteration 1344/2000 [0m                     

                       Computation: 42028 steps/s (collection: 2.238s, learning 0.101s)
             Mean action noise std: 3.08
          Mean value_function loss: 385.3171
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 78.1043
                       Mean reward: 723.77
               Mean episode length: 210.36
    Episode_Reward/reaching_object: 1.7524
     Episode_Reward/lifting_object: 145.4161
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 2.34s
                      Time elapsed: 00:51:38
                               ETA: 00:25:11

################################################################################
                     [1m Learning iteration 1345/2000 [0m                     

                       Computation: 45897 steps/s (collection: 2.053s, learning 0.089s)
             Mean action noise std: 3.08
          Mean value_function loss: 395.5776
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 78.1209
                       Mean reward: 743.76
               Mean episode length: 213.63
    Episode_Reward/reaching_object: 1.7400
     Episode_Reward/lifting_object: 144.6004
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0591
          Episode_Reward/joint_vel: -0.0836
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 2.14s
                      Time elapsed: 00:51:40
                               ETA: 00:25:08

################################################################################
                     [1m Learning iteration 1346/2000 [0m                     

                       Computation: 44945 steps/s (collection: 2.082s, learning 0.106s)
             Mean action noise std: 3.09
          Mean value_function loss: 378.6954
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 78.1406
                       Mean reward: 734.81
               Mean episode length: 211.99
    Episode_Reward/reaching_object: 1.7017
     Episode_Reward/lifting_object: 141.1878
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0591
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 2.19s
                      Time elapsed: 00:51:42
                               ETA: 00:25:06

################################################################################
                     [1m Learning iteration 1347/2000 [0m                     

                       Computation: 45301 steps/s (collection: 2.064s, learning 0.106s)
             Mean action noise std: 3.09
          Mean value_function loss: 367.8405
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 78.1514
                       Mean reward: 667.59
               Mean episode length: 199.97
    Episode_Reward/reaching_object: 1.6755
     Episode_Reward/lifting_object: 138.0226
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 2.17s
                      Time elapsed: 00:51:44
                               ETA: 00:25:03

################################################################################
                     [1m Learning iteration 1348/2000 [0m                     

                       Computation: 43964 steps/s (collection: 2.090s, learning 0.146s)
             Mean action noise std: 3.09
          Mean value_function loss: 352.2197
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 78.1655
                       Mean reward: 680.09
               Mean episode length: 198.48
    Episode_Reward/reaching_object: 1.6254
     Episode_Reward/lifting_object: 134.3922
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0808
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 2.24s
                      Time elapsed: 00:51:46
                               ETA: 00:25:01

################################################################################
                     [1m Learning iteration 1349/2000 [0m                     

                       Computation: 45017 steps/s (collection: 2.073s, learning 0.111s)
             Mean action noise std: 3.09
          Mean value_function loss: 367.2507
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 78.1867
                       Mean reward: 732.11
               Mean episode length: 212.62
    Episode_Reward/reaching_object: 1.6769
     Episode_Reward/lifting_object: 138.5460
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0820
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 2.18s
                      Time elapsed: 00:51:48
                               ETA: 00:24:59

################################################################################
                     [1m Learning iteration 1350/2000 [0m                     

                       Computation: 45706 steps/s (collection: 2.051s, learning 0.100s)
             Mean action noise std: 3.09
          Mean value_function loss: 328.5908
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 78.2052
                       Mean reward: 716.89
               Mean episode length: 206.40
    Episode_Reward/reaching_object: 1.6772
     Episode_Reward/lifting_object: 139.0463
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0577
          Episode_Reward/joint_vel: -0.0810
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 2.15s
                      Time elapsed: 00:51:51
                               ETA: 00:24:56

################################################################################
                     [1m Learning iteration 1351/2000 [0m                     

                       Computation: 45396 steps/s (collection: 2.051s, learning 0.115s)
             Mean action noise std: 3.09
          Mean value_function loss: 347.3678
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 78.2199
                       Mean reward: 711.36
               Mean episode length: 205.42
    Episode_Reward/reaching_object: 1.6754
     Episode_Reward/lifting_object: 137.8945
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 2.17s
                      Time elapsed: 00:51:53
                               ETA: 00:24:54

################################################################################
                     [1m Learning iteration 1352/2000 [0m                     

                       Computation: 45442 steps/s (collection: 2.062s, learning 0.101s)
             Mean action noise std: 3.10
          Mean value_function loss: 394.5744
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 78.2334
                       Mean reward: 710.99
               Mean episode length: 204.15
    Episode_Reward/reaching_object: 1.6722
     Episode_Reward/lifting_object: 137.5368
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0829
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 2.16s
                      Time elapsed: 00:51:55
                               ETA: 00:24:52

################################################################################
                     [1m Learning iteration 1353/2000 [0m                     

                       Computation: 45610 steps/s (collection: 2.052s, learning 0.103s)
             Mean action noise std: 3.10
          Mean value_function loss: 359.9740
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 78.2432
                       Mean reward: 726.82
               Mean episode length: 208.03
    Episode_Reward/reaching_object: 1.6843
     Episode_Reward/lifting_object: 138.7916
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0584
          Episode_Reward/joint_vel: -0.0819
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 2.16s
                      Time elapsed: 00:51:57
                               ETA: 00:24:49

################################################################################
                     [1m Learning iteration 1354/2000 [0m                     

                       Computation: 45871 steps/s (collection: 2.050s, learning 0.093s)
             Mean action noise std: 3.10
          Mean value_function loss: 355.4693
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 78.2512
                       Mean reward: 691.67
               Mean episode length: 198.22
    Episode_Reward/reaching_object: 1.6580
     Episode_Reward/lifting_object: 136.7480
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0575
          Episode_Reward/joint_vel: -0.0812
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 2.14s
                      Time elapsed: 00:51:59
                               ETA: 00:24:47

################################################################################
                     [1m Learning iteration 1355/2000 [0m                     

                       Computation: 44428 steps/s (collection: 2.047s, learning 0.166s)
             Mean action noise std: 3.10
          Mean value_function loss: 333.8358
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 78.2623
                       Mean reward: 684.43
               Mean episode length: 198.54
    Episode_Reward/reaching_object: 1.7150
     Episode_Reward/lifting_object: 142.7248
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0591
          Episode_Reward/joint_vel: -0.0837
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 2.21s
                      Time elapsed: 00:52:01
                               ETA: 00:24:44

################################################################################
                     [1m Learning iteration 1356/2000 [0m                     

                       Computation: 46066 steps/s (collection: 2.034s, learning 0.100s)
             Mean action noise std: 3.10
          Mean value_function loss: 300.2814
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 78.2738
                       Mean reward: 711.71
               Mean episode length: 203.99
    Episode_Reward/reaching_object: 1.6671
     Episode_Reward/lifting_object: 137.2782
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0831
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 2.13s
                      Time elapsed: 00:52:04
                               ETA: 00:24:42

################################################################################
                     [1m Learning iteration 1357/2000 [0m                     

                       Computation: 45524 steps/s (collection: 2.059s, learning 0.101s)
             Mean action noise std: 3.10
          Mean value_function loss: 293.9185
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 78.2896
                       Mean reward: 736.23
               Mean episode length: 217.16
    Episode_Reward/reaching_object: 1.6816
     Episode_Reward/lifting_object: 139.1733
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 2.16s
                      Time elapsed: 00:52:06
                               ETA: 00:24:40

################################################################################
                     [1m Learning iteration 1358/2000 [0m                     

                       Computation: 44803 steps/s (collection: 2.082s, learning 0.112s)
             Mean action noise std: 3.10
          Mean value_function loss: 295.0818
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 78.3032
                       Mean reward: 711.43
               Mean episode length: 207.21
    Episode_Reward/reaching_object: 1.7819
     Episode_Reward/lifting_object: 148.8123
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0612
          Episode_Reward/joint_vel: -0.0850
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 2.19s
                      Time elapsed: 00:52:08
                               ETA: 00:24:37

################################################################################
                     [1m Learning iteration 1359/2000 [0m                     

                       Computation: 44263 steps/s (collection: 2.086s, learning 0.135s)
             Mean action noise std: 3.11
          Mean value_function loss: 312.1755
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 78.3215
                       Mean reward: 683.41
               Mean episode length: 203.44
    Episode_Reward/reaching_object: 1.7061
     Episode_Reward/lifting_object: 141.1996
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 2.22s
                      Time elapsed: 00:52:10
                               ETA: 00:24:35

################################################################################
                     [1m Learning iteration 1360/2000 [0m                     

                       Computation: 44549 steps/s (collection: 2.069s, learning 0.138s)
             Mean action noise std: 3.11
          Mean value_function loss: 289.6222
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 78.3459
                       Mean reward: 670.78
               Mean episode length: 205.60
    Episode_Reward/reaching_object: 1.7120
     Episode_Reward/lifting_object: 141.3932
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0843
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 2.21s
                      Time elapsed: 00:52:12
                               ETA: 00:24:33

################################################################################
                     [1m Learning iteration 1361/2000 [0m                     

                       Computation: 45666 steps/s (collection: 2.061s, learning 0.092s)
             Mean action noise std: 3.11
          Mean value_function loss: 311.3480
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 78.3597
                       Mean reward: 700.31
               Mean episode length: 204.35
    Episode_Reward/reaching_object: 1.7000
     Episode_Reward/lifting_object: 140.3581
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0835
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 2.15s
                      Time elapsed: 00:52:14
                               ETA: 00:24:30

################################################################################
                     [1m Learning iteration 1362/2000 [0m                     

                       Computation: 44602 steps/s (collection: 2.097s, learning 0.107s)
             Mean action noise std: 3.11
          Mean value_function loss: 343.4907
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 78.3754
                       Mean reward: 715.75
               Mean episode length: 206.41
    Episode_Reward/reaching_object: 1.6731
     Episode_Reward/lifting_object: 138.5401
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 2.20s
                      Time elapsed: 00:52:17
                               ETA: 00:24:28

################################################################################
                     [1m Learning iteration 1363/2000 [0m                     

                       Computation: 45954 steps/s (collection: 2.048s, learning 0.091s)
             Mean action noise std: 3.11
          Mean value_function loss: 321.6937
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 78.3937
                       Mean reward: 720.83
               Mean episode length: 210.35
    Episode_Reward/reaching_object: 1.7184
     Episode_Reward/lifting_object: 141.9137
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0845
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 2.14s
                      Time elapsed: 00:52:19
                               ETA: 00:24:26

################################################################################
                     [1m Learning iteration 1364/2000 [0m                     

                       Computation: 45546 steps/s (collection: 2.063s, learning 0.095s)
             Mean action noise std: 3.11
          Mean value_function loss: 307.9318
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 78.4098
                       Mean reward: 689.87
               Mean episode length: 202.88
    Episode_Reward/reaching_object: 1.6835
     Episode_Reward/lifting_object: 138.7945
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0847
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 2.16s
                      Time elapsed: 00:52:21
                               ETA: 00:24:23

################################################################################
                     [1m Learning iteration 1365/2000 [0m                     

                       Computation: 44605 steps/s (collection: 2.111s, learning 0.093s)
             Mean action noise std: 3.12
          Mean value_function loss: 342.6814
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 78.4251
                       Mean reward: 697.81
               Mean episode length: 200.96
    Episode_Reward/reaching_object: 1.6753
     Episode_Reward/lifting_object: 138.4596
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 2.20s
                      Time elapsed: 00:52:23
                               ETA: 00:24:21

################################################################################
                     [1m Learning iteration 1366/2000 [0m                     

                       Computation: 44575 steps/s (collection: 2.065s, learning 0.140s)
             Mean action noise std: 3.12
          Mean value_function loss: 327.7389
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 78.4387
                       Mean reward: 705.03
               Mean episode length: 202.26
    Episode_Reward/reaching_object: 1.6592
     Episode_Reward/lifting_object: 136.3155
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0844
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 2.21s
                      Time elapsed: 00:52:25
                               ETA: 00:24:19

################################################################################
                     [1m Learning iteration 1367/2000 [0m                     

                       Computation: 45931 steps/s (collection: 2.035s, learning 0.105s)
             Mean action noise std: 3.12
          Mean value_function loss: 334.9185
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 78.4475
                       Mean reward: 771.10
               Mean episode length: 217.72
    Episode_Reward/reaching_object: 1.7433
     Episode_Reward/lifting_object: 144.7416
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0869
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 2.14s
                      Time elapsed: 00:52:28
                               ETA: 00:24:16

################################################################################
                     [1m Learning iteration 1368/2000 [0m                     

                       Computation: 45325 steps/s (collection: 2.078s, learning 0.091s)
             Mean action noise std: 3.12
          Mean value_function loss: 352.1575
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 78.4584
                       Mean reward: 654.57
               Mean episode length: 194.43
    Episode_Reward/reaching_object: 1.6050
     Episode_Reward/lifting_object: 131.8299
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0575
          Episode_Reward/joint_vel: -0.0834
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 2.17s
                      Time elapsed: 00:52:30
                               ETA: 00:24:14

################################################################################
                     [1m Learning iteration 1369/2000 [0m                     

                       Computation: 45639 steps/s (collection: 2.063s, learning 0.091s)
             Mean action noise std: 3.12
          Mean value_function loss: 352.8603
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 78.4784
                       Mean reward: 679.06
               Mean episode length: 202.62
    Episode_Reward/reaching_object: 1.6808
     Episode_Reward/lifting_object: 138.9045
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0596
          Episode_Reward/joint_vel: -0.0861
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 2.15s
                      Time elapsed: 00:52:32
                               ETA: 00:24:11

################################################################################
                     [1m Learning iteration 1370/2000 [0m                     

                       Computation: 42226 steps/s (collection: 2.196s, learning 0.132s)
             Mean action noise std: 3.13
          Mean value_function loss: 320.7792
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 78.4991
                       Mean reward: 686.53
               Mean episode length: 201.09
    Episode_Reward/reaching_object: 1.6338
     Episode_Reward/lifting_object: 134.8870
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 2.33s
                      Time elapsed: 00:52:34
                               ETA: 00:24:09

################################################################################
                     [1m Learning iteration 1371/2000 [0m                     

                       Computation: 44076 steps/s (collection: 2.104s, learning 0.126s)
             Mean action noise std: 3.13
          Mean value_function loss: 305.0817
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 78.5165
                       Mean reward: 703.72
               Mean episode length: 209.21
    Episode_Reward/reaching_object: 1.6808
     Episode_Reward/lifting_object: 138.1778
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0862
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 2.23s
                      Time elapsed: 00:52:36
                               ETA: 00:24:07

################################################################################
                     [1m Learning iteration 1372/2000 [0m                     

                       Computation: 45317 steps/s (collection: 2.067s, learning 0.103s)
             Mean action noise std: 3.13
          Mean value_function loss: 295.3757
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 78.5334
                       Mean reward: 748.03
               Mean episode length: 211.44
    Episode_Reward/reaching_object: 1.7146
     Episode_Reward/lifting_object: 142.8272
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0863
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 2.17s
                      Time elapsed: 00:52:39
                               ETA: 00:24:04

################################################################################
                     [1m Learning iteration 1373/2000 [0m                     

                       Computation: 44401 steps/s (collection: 2.109s, learning 0.105s)
             Mean action noise std: 3.13
          Mean value_function loss: 303.5550
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 78.5405
                       Mean reward: 702.76
               Mean episode length: 204.58
    Episode_Reward/reaching_object: 1.7501
     Episode_Reward/lifting_object: 145.9951
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 2.21s
                      Time elapsed: 00:52:41
                               ETA: 00:24:02

################################################################################
                     [1m Learning iteration 1374/2000 [0m                     

                       Computation: 45258 steps/s (collection: 2.079s, learning 0.093s)
             Mean action noise std: 3.13
          Mean value_function loss: 295.5314
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 78.5440
                       Mean reward: 696.09
               Mean episode length: 202.65
    Episode_Reward/reaching_object: 1.6676
     Episode_Reward/lifting_object: 137.5339
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0862
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 2.17s
                      Time elapsed: 00:52:43
                               ETA: 00:24:00

################################################################################
                     [1m Learning iteration 1375/2000 [0m                     

                       Computation: 45689 steps/s (collection: 2.062s, learning 0.089s)
             Mean action noise std: 3.13
          Mean value_function loss: 277.6566
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 78.5505
                       Mean reward: 709.21
               Mean episode length: 209.48
    Episode_Reward/reaching_object: 1.7320
     Episode_Reward/lifting_object: 143.6939
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0614
          Episode_Reward/joint_vel: -0.0871
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 2.15s
                      Time elapsed: 00:52:45
                               ETA: 00:23:57

################################################################################
                     [1m Learning iteration 1376/2000 [0m                     

                       Computation: 45538 steps/s (collection: 2.071s, learning 0.088s)
             Mean action noise std: 3.13
          Mean value_function loss: 281.6939
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 78.5662
                       Mean reward: 717.30
               Mean episode length: 206.03
    Episode_Reward/reaching_object: 1.7861
     Episode_Reward/lifting_object: 149.2746
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0862
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 2.16s
                      Time elapsed: 00:52:47
                               ETA: 00:23:55

################################################################################
                     [1m Learning iteration 1377/2000 [0m                     

                       Computation: 44955 steps/s (collection: 2.084s, learning 0.103s)
             Mean action noise std: 3.13
          Mean value_function loss: 354.0323
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 78.5797
                       Mean reward: 733.29
               Mean episode length: 208.57
    Episode_Reward/reaching_object: 1.6821
     Episode_Reward/lifting_object: 139.4855
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 2.19s
                      Time elapsed: 00:52:49
                               ETA: 00:23:53

################################################################################
                     [1m Learning iteration 1378/2000 [0m                     

                       Computation: 45775 steps/s (collection: 2.055s, learning 0.092s)
             Mean action noise std: 3.14
          Mean value_function loss: 344.4437
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 78.5924
                       Mean reward: 637.71
               Mean episode length: 190.39
    Episode_Reward/reaching_object: 1.6502
     Episode_Reward/lifting_object: 136.4084
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0837
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 2.15s
                      Time elapsed: 00:52:52
                               ETA: 00:23:50

################################################################################
                     [1m Learning iteration 1379/2000 [0m                     

                       Computation: 45193 steps/s (collection: 2.079s, learning 0.097s)
             Mean action noise std: 3.14
          Mean value_function loss: 306.9274
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 78.6091
                       Mean reward: 697.96
               Mean episode length: 201.73
    Episode_Reward/reaching_object: 1.7020
     Episode_Reward/lifting_object: 140.2365
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0847
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 2.18s
                      Time elapsed: 00:52:54
                               ETA: 00:23:48

################################################################################
                     [1m Learning iteration 1380/2000 [0m                     

                       Computation: 45199 steps/s (collection: 2.081s, learning 0.094s)
             Mean action noise std: 3.14
          Mean value_function loss: 316.5904
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 78.6239
                       Mean reward: 688.66
               Mean episode length: 201.71
    Episode_Reward/reaching_object: 1.7311
     Episode_Reward/lifting_object: 143.3809
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0612
          Episode_Reward/joint_vel: -0.0853
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 2.17s
                      Time elapsed: 00:52:56
                               ETA: 00:23:46

################################################################################
                     [1m Learning iteration 1381/2000 [0m                     

                       Computation: 46118 steps/s (collection: 2.040s, learning 0.092s)
             Mean action noise std: 3.14
          Mean value_function loss: 336.4654
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 78.6378
                       Mean reward: 677.89
               Mean episode length: 199.39
    Episode_Reward/reaching_object: 1.6758
     Episode_Reward/lifting_object: 138.3351
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 2.13s
                      Time elapsed: 00:52:58
                               ETA: 00:23:43

################################################################################
                     [1m Learning iteration 1382/2000 [0m                     

                       Computation: 44380 steps/s (collection: 2.124s, learning 0.091s)
             Mean action noise std: 3.14
          Mean value_function loss: 286.2072
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 78.6515
                       Mean reward: 698.60
               Mean episode length: 206.36
    Episode_Reward/reaching_object: 1.6732
     Episode_Reward/lifting_object: 138.1437
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0599
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 2.22s
                      Time elapsed: 00:53:00
                               ETA: 00:23:41

################################################################################
                     [1m Learning iteration 1383/2000 [0m                     

                       Computation: 43346 steps/s (collection: 2.128s, learning 0.140s)
             Mean action noise std: 3.15
          Mean value_function loss: 311.2849
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 78.6711
                       Mean reward: 759.79
               Mean episode length: 217.54
    Episode_Reward/reaching_object: 1.7163
     Episode_Reward/lifting_object: 141.9816
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0607
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 2.27s
                      Time elapsed: 00:53:03
                               ETA: 00:23:39

################################################################################
                     [1m Learning iteration 1384/2000 [0m                     

                       Computation: 45506 steps/s (collection: 2.070s, learning 0.090s)
             Mean action noise std: 3.15
          Mean value_function loss: 298.0027
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 78.6947
                       Mean reward: 740.75
               Mean episode length: 214.34
    Episode_Reward/reaching_object: 1.6925
     Episode_Reward/lifting_object: 140.6816
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0828
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 2.16s
                      Time elapsed: 00:53:05
                               ETA: 00:23:36

################################################################################
                     [1m Learning iteration 1385/2000 [0m                     

                       Computation: 45317 steps/s (collection: 2.079s, learning 0.090s)
             Mean action noise std: 3.15
          Mean value_function loss: 337.7351
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 78.7099
                       Mean reward: 653.71
               Mean episode length: 192.55
    Episode_Reward/reaching_object: 1.6564
     Episode_Reward/lifting_object: 137.1584
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0591
          Episode_Reward/joint_vel: -0.0817
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 2.17s
                      Time elapsed: 00:53:07
                               ETA: 00:23:34

################################################################################
                     [1m Learning iteration 1386/2000 [0m                     

                       Computation: 45613 steps/s (collection: 2.061s, learning 0.094s)
             Mean action noise std: 3.15
          Mean value_function loss: 341.0445
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 78.7170
                       Mean reward: 720.55
               Mean episode length: 208.01
    Episode_Reward/reaching_object: 1.6570
     Episode_Reward/lifting_object: 136.1832
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0828
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 2.16s
                      Time elapsed: 00:53:09
                               ETA: 00:23:31

################################################################################
                     [1m Learning iteration 1387/2000 [0m                     

                       Computation: 45038 steps/s (collection: 2.073s, learning 0.110s)
             Mean action noise std: 3.15
          Mean value_function loss: 302.2200
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 78.7363
                       Mean reward: 713.57
               Mean episode length: 207.53
    Episode_Reward/reaching_object: 1.6967
     Episode_Reward/lifting_object: 140.3679
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0607
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 2.18s
                      Time elapsed: 00:53:11
                               ETA: 00:23:29

################################################################################
                     [1m Learning iteration 1388/2000 [0m                     

                       Computation: 44833 steps/s (collection: 2.093s, learning 0.100s)
             Mean action noise std: 3.15
          Mean value_function loss: 282.5166
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 78.7579
                       Mean reward: 718.88
               Mean episode length: 210.01
    Episode_Reward/reaching_object: 1.7731
     Episode_Reward/lifting_object: 146.8214
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0870
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 2.19s
                      Time elapsed: 00:53:13
                               ETA: 00:23:27

################################################################################
                     [1m Learning iteration 1389/2000 [0m                     

                       Computation: 45034 steps/s (collection: 2.074s, learning 0.109s)
             Mean action noise std: 3.15
          Mean value_function loss: 315.1458
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 78.7613
                       Mean reward: 711.89
               Mean episode length: 204.88
    Episode_Reward/reaching_object: 1.7271
     Episode_Reward/lifting_object: 143.3774
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0614
          Episode_Reward/joint_vel: -0.0855
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 2.18s
                      Time elapsed: 00:53:16
                               ETA: 00:23:24

################################################################################
                     [1m Learning iteration 1390/2000 [0m                     

                       Computation: 44949 steps/s (collection: 2.097s, learning 0.090s)
             Mean action noise std: 3.16
          Mean value_function loss: 289.7074
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 78.7701
                       Mean reward: 689.59
               Mean episode length: 201.34
    Episode_Reward/reaching_object: 1.7059
     Episode_Reward/lifting_object: 140.4320
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0614
          Episode_Reward/joint_vel: -0.0859
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 2.19s
                      Time elapsed: 00:53:18
                               ETA: 00:23:22

################################################################################
                     [1m Learning iteration 1391/2000 [0m                     

                       Computation: 45002 steps/s (collection: 2.080s, learning 0.105s)
             Mean action noise std: 3.16
          Mean value_function loss: 306.5327
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 78.7836
                       Mean reward: 693.66
               Mean episode length: 200.88
    Episode_Reward/reaching_object: 1.7038
     Episode_Reward/lifting_object: 140.5775
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0605
          Episode_Reward/joint_vel: -0.0844
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 2.18s
                      Time elapsed: 00:53:20
                               ETA: 00:23:20

################################################################################
                     [1m Learning iteration 1392/2000 [0m                     

                       Computation: 45466 steps/s (collection: 2.056s, learning 0.107s)
             Mean action noise std: 3.16
          Mean value_function loss: 311.1925
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 78.7965
                       Mean reward: 740.90
               Mean episode length: 213.86
    Episode_Reward/reaching_object: 1.7367
     Episode_Reward/lifting_object: 143.6851
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0617
          Episode_Reward/joint_vel: -0.0849
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 2.16s
                      Time elapsed: 00:53:22
                               ETA: 00:23:17

################################################################################
                     [1m Learning iteration 1393/2000 [0m                     

                       Computation: 44935 steps/s (collection: 2.077s, learning 0.111s)
             Mean action noise std: 3.16
          Mean value_function loss: 333.3392
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 78.8096
                       Mean reward: 739.97
               Mean episode length: 210.54
    Episode_Reward/reaching_object: 1.6881
     Episode_Reward/lifting_object: 139.1300
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0599
          Episode_Reward/joint_vel: -0.0837
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 2.19s
                      Time elapsed: 00:53:24
                               ETA: 00:23:15

################################################################################
                     [1m Learning iteration 1394/2000 [0m                     

                       Computation: 45209 steps/s (collection: 2.080s, learning 0.094s)
             Mean action noise std: 3.16
          Mean value_function loss: 341.8317
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 78.8185
                       Mean reward: 695.86
               Mean episode length: 200.66
    Episode_Reward/reaching_object: 1.6663
     Episode_Reward/lifting_object: 136.9255
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0596
          Episode_Reward/joint_vel: -0.0837
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 2.17s
                      Time elapsed: 00:53:27
                               ETA: 00:23:13

################################################################################
                     [1m Learning iteration 1395/2000 [0m                     

                       Computation: 45448 steps/s (collection: 2.055s, learning 0.108s)
             Mean action noise std: 3.16
          Mean value_function loss: 287.9471
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 78.8288
                       Mean reward: 724.65
               Mean episode length: 207.38
    Episode_Reward/reaching_object: 1.7079
     Episode_Reward/lifting_object: 140.8318
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0614
          Episode_Reward/joint_vel: -0.0851
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 2.16s
                      Time elapsed: 00:53:29
                               ETA: 00:23:10

################################################################################
                     [1m Learning iteration 1396/2000 [0m                     

                       Computation: 44939 steps/s (collection: 2.081s, learning 0.106s)
             Mean action noise std: 3.17
          Mean value_function loss: 278.5743
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 78.8474
                       Mean reward: 732.57
               Mean episode length: 211.25
    Episode_Reward/reaching_object: 1.7528
     Episode_Reward/lifting_object: 145.0338
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0625
          Episode_Reward/joint_vel: -0.0870
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 2.19s
                      Time elapsed: 00:53:31
                               ETA: 00:23:08

################################################################################
                     [1m Learning iteration 1397/2000 [0m                     

                       Computation: 43599 steps/s (collection: 2.077s, learning 0.178s)
             Mean action noise std: 3.17
          Mean value_function loss: 321.7864
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 78.8655
                       Mean reward: 697.77
               Mean episode length: 203.41
    Episode_Reward/reaching_object: 1.6938
     Episode_Reward/lifting_object: 138.9739
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0861
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 2.25s
                      Time elapsed: 00:53:33
                               ETA: 00:23:06

################################################################################
                     [1m Learning iteration 1398/2000 [0m                     

                       Computation: 43977 steps/s (collection: 2.128s, learning 0.107s)
             Mean action noise std: 3.17
          Mean value_function loss: 328.9427
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 78.8838
                       Mean reward: 751.19
               Mean episode length: 213.59
    Episode_Reward/reaching_object: 1.7156
     Episode_Reward/lifting_object: 141.8045
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0613
          Episode_Reward/joint_vel: -0.0854
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 2.24s
                      Time elapsed: 00:53:35
                               ETA: 00:23:03

################################################################################
                     [1m Learning iteration 1399/2000 [0m                     

                       Computation: 44556 steps/s (collection: 2.073s, learning 0.133s)
             Mean action noise std: 3.17
          Mean value_function loss: 289.6546
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 78.9044
                       Mean reward: 750.55
               Mean episode length: 211.54
    Episode_Reward/reaching_object: 1.7304
     Episode_Reward/lifting_object: 142.8330
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0617
          Episode_Reward/joint_vel: -0.0862
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 2.21s
                      Time elapsed: 00:53:38
                               ETA: 00:23:01

################################################################################
                     [1m Learning iteration 1400/2000 [0m                     

                       Computation: 45169 steps/s (collection: 2.083s, learning 0.093s)
             Mean action noise std: 3.17
          Mean value_function loss: 317.6520
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 78.9219
                       Mean reward: 718.57
               Mean episode length: 206.89
    Episode_Reward/reaching_object: 1.7479
     Episode_Reward/lifting_object: 145.2213
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0622
          Episode_Reward/joint_vel: -0.0857
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 2.18s
                      Time elapsed: 00:53:40
                               ETA: 00:22:59

################################################################################
                     [1m Learning iteration 1401/2000 [0m                     

                       Computation: 44081 steps/s (collection: 2.124s, learning 0.106s)
             Mean action noise std: 3.18
          Mean value_function loss: 263.2167
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 78.9398
                       Mean reward: 732.42
               Mean episode length: 208.40
    Episode_Reward/reaching_object: 1.7607
     Episode_Reward/lifting_object: 146.6240
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0625
          Episode_Reward/joint_vel: -0.0861
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 2.23s
                      Time elapsed: 00:53:42
                               ETA: 00:22:56

################################################################################
                     [1m Learning iteration 1402/2000 [0m                     

                       Computation: 44247 steps/s (collection: 2.099s, learning 0.123s)
             Mean action noise std: 3.18
          Mean value_function loss: 301.5099
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 78.9490
                       Mean reward: 743.34
               Mean episode length: 214.06
    Episode_Reward/reaching_object: 1.7336
     Episode_Reward/lifting_object: 143.9733
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0622
          Episode_Reward/joint_vel: -0.0863
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 2.22s
                      Time elapsed: 00:53:44
                               ETA: 00:22:54

################################################################################
                     [1m Learning iteration 1403/2000 [0m                     

                       Computation: 44605 steps/s (collection: 2.089s, learning 0.115s)
             Mean action noise std: 3.18
          Mean value_function loss: 317.2119
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 78.9563
                       Mean reward: 747.64
               Mean episode length: 211.61
    Episode_Reward/reaching_object: 1.7488
     Episode_Reward/lifting_object: 144.9756
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0625
          Episode_Reward/joint_vel: -0.0872
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 2.20s
                      Time elapsed: 00:53:46
                               ETA: 00:22:52

################################################################################
                     [1m Learning iteration 1404/2000 [0m                     

                       Computation: 44260 steps/s (collection: 2.083s, learning 0.138s)
             Mean action noise std: 3.18
          Mean value_function loss: 302.0336
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 78.9662
                       Mean reward: 765.31
               Mean episode length: 218.75
    Episode_Reward/reaching_object: 1.7696
     Episode_Reward/lifting_object: 146.8665
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0888
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 2.22s
                      Time elapsed: 00:53:49
                               ETA: 00:22:49

################################################################################
                     [1m Learning iteration 1405/2000 [0m                     

                       Computation: 42898 steps/s (collection: 2.121s, learning 0.171s)
             Mean action noise std: 3.18
          Mean value_function loss: 326.2287
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 78.9820
                       Mean reward: 748.09
               Mean episode length: 209.95
    Episode_Reward/reaching_object: 1.7149
     Episode_Reward/lifting_object: 142.2576
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0849
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 2.29s
                      Time elapsed: 00:53:51
                               ETA: 00:22:47

################################################################################
                     [1m Learning iteration 1406/2000 [0m                     

                       Computation: 45046 steps/s (collection: 2.086s, learning 0.096s)
             Mean action noise std: 3.18
          Mean value_function loss: 324.6764
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 78.9986
                       Mean reward: 742.31
               Mean episode length: 214.55
    Episode_Reward/reaching_object: 1.6616
     Episode_Reward/lifting_object: 137.1172
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0604
          Episode_Reward/joint_vel: -0.0848
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 2.18s
                      Time elapsed: 00:53:53
                               ETA: 00:22:45

################################################################################
                     [1m Learning iteration 1407/2000 [0m                     

                       Computation: 45707 steps/s (collection: 2.064s, learning 0.087s)
             Mean action noise std: 3.18
          Mean value_function loss: 322.1680
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 79.0159
                       Mean reward: 665.41
               Mean episode length: 193.40
    Episode_Reward/reaching_object: 1.7474
     Episode_Reward/lifting_object: 145.5247
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0865
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 2.15s
                      Time elapsed: 00:53:55
                               ETA: 00:22:42

################################################################################
                     [1m Learning iteration 1408/2000 [0m                     

                       Computation: 44539 steps/s (collection: 2.104s, learning 0.104s)
             Mean action noise std: 3.19
          Mean value_function loss: 268.5223
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 79.0270
                       Mean reward: 774.99
               Mean episode length: 218.92
    Episode_Reward/reaching_object: 1.7983
     Episode_Reward/lifting_object: 149.8694
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0642
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 2.21s
                      Time elapsed: 00:53:57
                               ETA: 00:22:40

################################################################################
                     [1m Learning iteration 1409/2000 [0m                     

                       Computation: 43122 steps/s (collection: 2.108s, learning 0.172s)
             Mean action noise std: 3.19
          Mean value_function loss: 290.5773
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 79.0424
                       Mean reward: 663.96
               Mean episode length: 198.94
    Episode_Reward/reaching_object: 1.6883
     Episode_Reward/lifting_object: 139.3789
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0612
          Episode_Reward/joint_vel: -0.0847
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 2.28s
                      Time elapsed: 00:54:00
                               ETA: 00:22:38

################################################################################
                     [1m Learning iteration 1410/2000 [0m                     

                       Computation: 44508 steps/s (collection: 2.113s, learning 0.096s)
             Mean action noise std: 3.19
          Mean value_function loss: 310.9039
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 79.0514
                       Mean reward: 719.77
               Mean episode length: 205.66
    Episode_Reward/reaching_object: 1.6843
     Episode_Reward/lifting_object: 139.5453
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0607
          Episode_Reward/joint_vel: -0.0845
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 2.21s
                      Time elapsed: 00:54:02
                               ETA: 00:22:35

################################################################################
                     [1m Learning iteration 1411/2000 [0m                     

                       Computation: 45142 steps/s (collection: 2.078s, learning 0.100s)
             Mean action noise std: 3.19
          Mean value_function loss: 275.1531
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 79.0625
                       Mean reward: 713.46
               Mean episode length: 206.58
    Episode_Reward/reaching_object: 1.7578
     Episode_Reward/lifting_object: 145.8540
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0629
          Episode_Reward/joint_vel: -0.0861
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 2.18s
                      Time elapsed: 00:54:04
                               ETA: 00:22:33

################################################################################
                     [1m Learning iteration 1412/2000 [0m                     

                       Computation: 45026 steps/s (collection: 2.071s, learning 0.113s)
             Mean action noise std: 3.19
          Mean value_function loss: 290.8041
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 79.0786
                       Mean reward: 756.82
               Mean episode length: 215.30
    Episode_Reward/reaching_object: 1.7464
     Episode_Reward/lifting_object: 145.0741
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 2.18s
                      Time elapsed: 00:54:06
                               ETA: 00:22:31

################################################################################
                     [1m Learning iteration 1413/2000 [0m                     

                       Computation: 44885 steps/s (collection: 2.068s, learning 0.122s)
             Mean action noise std: 3.19
          Mean value_function loss: 271.2366
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 79.0969
                       Mean reward: 768.19
               Mean episode length: 219.57
    Episode_Reward/reaching_object: 1.7958
     Episode_Reward/lifting_object: 149.7113
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0643
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 2.19s
                      Time elapsed: 00:54:08
                               ETA: 00:22:28

################################################################################
                     [1m Learning iteration 1414/2000 [0m                     

                       Computation: 42701 steps/s (collection: 2.161s, learning 0.141s)
             Mean action noise std: 3.20
          Mean value_function loss: 310.2669
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 79.1157
                       Mean reward: 752.28
               Mean episode length: 212.53
    Episode_Reward/reaching_object: 1.7502
     Episode_Reward/lifting_object: 145.6047
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0868
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 2.30s
                      Time elapsed: 00:54:11
                               ETA: 00:22:26

################################################################################
                     [1m Learning iteration 1415/2000 [0m                     

                       Computation: 44558 steps/s (collection: 2.089s, learning 0.117s)
             Mean action noise std: 3.20
          Mean value_function loss: 276.6477
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 79.1387
                       Mean reward: 741.33
               Mean episode length: 210.13
    Episode_Reward/reaching_object: 1.7202
     Episode_Reward/lifting_object: 142.4258
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0622
          Episode_Reward/joint_vel: -0.0861
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 2.21s
                      Time elapsed: 00:54:13
                               ETA: 00:22:24

################################################################################
                     [1m Learning iteration 1416/2000 [0m                     

                       Computation: 44696 steps/s (collection: 2.097s, learning 0.102s)
             Mean action noise std: 3.20
          Mean value_function loss: 315.2833
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 79.1615
                       Mean reward: 647.67
               Mean episode length: 188.25
    Episode_Reward/reaching_object: 1.7101
     Episode_Reward/lifting_object: 141.8094
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0857
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 2.20s
                      Time elapsed: 00:54:15
                               ETA: 00:22:21

################################################################################
                     [1m Learning iteration 1417/2000 [0m                     

                       Computation: 44985 steps/s (collection: 2.093s, learning 0.092s)
             Mean action noise std: 3.20
          Mean value_function loss: 287.9452
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 79.1749
                       Mean reward: 704.71
               Mean episode length: 206.57
    Episode_Reward/reaching_object: 1.7488
     Episode_Reward/lifting_object: 145.4633
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0865
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 2.19s
                      Time elapsed: 00:54:17
                               ETA: 00:22:19

################################################################################
                     [1m Learning iteration 1418/2000 [0m                     

                       Computation: 45821 steps/s (collection: 2.051s, learning 0.095s)
             Mean action noise std: 3.21
          Mean value_function loss: 238.2071
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 79.1937
                       Mean reward: 754.69
               Mean episode length: 212.81
    Episode_Reward/reaching_object: 1.7769
     Episode_Reward/lifting_object: 148.1605
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 2.15s
                      Time elapsed: 00:54:20
                               ETA: 00:22:17

################################################################################
                     [1m Learning iteration 1419/2000 [0m                     

                       Computation: 45161 steps/s (collection: 2.069s, learning 0.108s)
             Mean action noise std: 3.21
          Mean value_function loss: 316.4363
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 79.2071
                       Mean reward: 664.60
               Mean episode length: 200.47
    Episode_Reward/reaching_object: 1.7120
     Episode_Reward/lifting_object: 142.1916
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0625
          Episode_Reward/joint_vel: -0.0873
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 2.18s
                      Time elapsed: 00:54:22
                               ETA: 00:22:14

################################################################################
                     [1m Learning iteration 1420/2000 [0m                     

                       Computation: 41192 steps/s (collection: 2.252s, learning 0.135s)
             Mean action noise std: 3.21
          Mean value_function loss: 262.5170
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 79.2113
                       Mean reward: 793.83
               Mean episode length: 221.27
    Episode_Reward/reaching_object: 1.7662
     Episode_Reward/lifting_object: 147.3395
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0882
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 2.39s
                      Time elapsed: 00:54:24
                               ETA: 00:22:12

################################################################################
                     [1m Learning iteration 1421/2000 [0m                     

                       Computation: 44405 steps/s (collection: 2.103s, learning 0.111s)
             Mean action noise std: 3.21
          Mean value_function loss: 303.2003
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 79.2131
                       Mean reward: 688.85
               Mean episode length: 199.04
    Episode_Reward/reaching_object: 1.7303
     Episode_Reward/lifting_object: 144.8170
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0848
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 2.21s
                      Time elapsed: 00:54:26
                               ETA: 00:22:10

################################################################################
                     [1m Learning iteration 1422/2000 [0m                     

                       Computation: 44799 steps/s (collection: 2.089s, learning 0.106s)
             Mean action noise std: 3.21
          Mean value_function loss: 272.7448
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 79.2205
                       Mean reward: 731.31
               Mean episode length: 211.59
    Episode_Reward/reaching_object: 1.7200
     Episode_Reward/lifting_object: 143.6008
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0634
          Episode_Reward/joint_vel: -0.0863
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 2.19s
                      Time elapsed: 00:54:29
                               ETA: 00:22:07

################################################################################
                     [1m Learning iteration 1423/2000 [0m                     

                       Computation: 43549 steps/s (collection: 2.147s, learning 0.110s)
             Mean action noise std: 3.21
          Mean value_function loss: 301.5278
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 79.2291
                       Mean reward: 719.01
               Mean episode length: 209.30
    Episode_Reward/reaching_object: 1.7864
     Episode_Reward/lifting_object: 149.6662
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0878
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 2.26s
                      Time elapsed: 00:54:31
                               ETA: 00:22:05

################################################################################
                     [1m Learning iteration 1424/2000 [0m                     

                       Computation: 42311 steps/s (collection: 2.094s, learning 0.229s)
             Mean action noise std: 3.21
          Mean value_function loss: 299.4944
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 79.2393
                       Mean reward: 763.30
               Mean episode length: 215.09
    Episode_Reward/reaching_object: 1.7653
     Episode_Reward/lifting_object: 147.5589
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0867
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 2.32s
                      Time elapsed: 00:54:33
                               ETA: 00:22:03

################################################################################
                     [1m Learning iteration 1425/2000 [0m                     

                       Computation: 42977 steps/s (collection: 2.153s, learning 0.134s)
             Mean action noise std: 3.21
          Mean value_function loss: 282.6687
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 79.2573
                       Mean reward: 738.58
               Mean episode length: 209.61
    Episode_Reward/reaching_object: 1.8138
     Episode_Reward/lifting_object: 152.8818
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0660
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 2.29s
                      Time elapsed: 00:54:35
                               ETA: 00:22:00

################################################################################
                     [1m Learning iteration 1426/2000 [0m                     

                       Computation: 42115 steps/s (collection: 2.238s, learning 0.096s)
             Mean action noise std: 3.21
          Mean value_function loss: 308.2867
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 79.2727
                       Mean reward: 769.66
               Mean episode length: 216.54
    Episode_Reward/reaching_object: 1.7398
     Episode_Reward/lifting_object: 145.4998
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0857
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 2.33s
                      Time elapsed: 00:54:38
                               ETA: 00:21:58

################################################################################
                     [1m Learning iteration 1427/2000 [0m                     

                       Computation: 43510 steps/s (collection: 2.139s, learning 0.120s)
             Mean action noise std: 3.22
          Mean value_function loss: 300.3289
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 79.2867
                       Mean reward: 740.73
               Mean episode length: 215.30
    Episode_Reward/reaching_object: 1.7795
     Episode_Reward/lifting_object: 148.4911
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0658
          Episode_Reward/joint_vel: -0.0899
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 2.26s
                      Time elapsed: 00:54:40
                               ETA: 00:21:56

################################################################################
                     [1m Learning iteration 1428/2000 [0m                     

                       Computation: 44956 steps/s (collection: 2.093s, learning 0.094s)
             Mean action noise std: 3.22
          Mean value_function loss: 274.5632
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 79.3097
                       Mean reward: 733.58
               Mean episode length: 213.18
    Episode_Reward/reaching_object: 1.7606
     Episode_Reward/lifting_object: 146.7587
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0651
          Episode_Reward/joint_vel: -0.0879
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 2.19s
                      Time elapsed: 00:54:42
                               ETA: 00:21:53

################################################################################
                     [1m Learning iteration 1429/2000 [0m                     

                       Computation: 45374 steps/s (collection: 2.069s, learning 0.098s)
             Mean action noise std: 3.22
          Mean value_function loss: 294.8174
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 79.3267
                       Mean reward: 734.71
               Mean episode length: 210.51
    Episode_Reward/reaching_object: 1.7137
     Episode_Reward/lifting_object: 143.0020
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0847
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 2.17s
                      Time elapsed: 00:54:44
                               ETA: 00:21:51

################################################################################
                     [1m Learning iteration 1430/2000 [0m                     

                       Computation: 45215 steps/s (collection: 2.070s, learning 0.105s)
             Mean action noise std: 3.22
          Mean value_function loss: 275.6033
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 79.3491
                       Mean reward: 748.56
               Mean episode length: 216.02
    Episode_Reward/reaching_object: 1.7817
     Episode_Reward/lifting_object: 149.1439
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 2.17s
                      Time elapsed: 00:54:46
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 1431/2000 [0m                     

                       Computation: 42948 steps/s (collection: 2.184s, learning 0.105s)
             Mean action noise std: 3.23
          Mean value_function loss: 244.7858
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 79.3765
                       Mean reward: 791.19
               Mean episode length: 223.44
    Episode_Reward/reaching_object: 1.7940
     Episode_Reward/lifting_object: 149.2708
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0661
          Episode_Reward/joint_vel: -0.0904
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 2.29s
                      Time elapsed: 00:54:49
                               ETA: 00:21:46

################################################################################
                     [1m Learning iteration 1432/2000 [0m                     

                       Computation: 44537 steps/s (collection: 2.114s, learning 0.094s)
             Mean action noise std: 3.23
          Mean value_function loss: 281.2028
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 79.3971
                       Mean reward: 740.00
               Mean episode length: 211.04
    Episode_Reward/reaching_object: 1.7249
     Episode_Reward/lifting_object: 143.7449
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0877
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 2.21s
                      Time elapsed: 00:54:51
                               ETA: 00:21:44

################################################################################
                     [1m Learning iteration 1433/2000 [0m                     

                       Computation: 45404 steps/s (collection: 2.077s, learning 0.088s)
             Mean action noise std: 3.23
          Mean value_function loss: 270.5206
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 79.4061
                       Mean reward: 825.13
               Mean episode length: 227.57
    Episode_Reward/reaching_object: 1.7849
     Episode_Reward/lifting_object: 148.7652
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0658
          Episode_Reward/joint_vel: -0.0889
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 2.17s
                      Time elapsed: 00:54:53
                               ETA: 00:21:42

################################################################################
                     [1m Learning iteration 1434/2000 [0m                     

                       Computation: 44988 steps/s (collection: 2.095s, learning 0.090s)
             Mean action noise std: 3.23
          Mean value_function loss: 253.5404
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 79.4272
                       Mean reward: 785.63
               Mean episode length: 220.19
    Episode_Reward/reaching_object: 1.8141
     Episode_Reward/lifting_object: 151.8606
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0665
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 2.19s
                      Time elapsed: 00:54:55
                               ETA: 00:21:39

################################################################################
                     [1m Learning iteration 1435/2000 [0m                     

                       Computation: 43909 steps/s (collection: 2.148s, learning 0.091s)
             Mean action noise std: 3.23
          Mean value_function loss: 298.4486
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 79.4490
                       Mean reward: 758.43
               Mean episode length: 217.67
    Episode_Reward/reaching_object: 1.8017
     Episode_Reward/lifting_object: 151.2443
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0667
          Episode_Reward/joint_vel: -0.0898
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 2.24s
                      Time elapsed: 00:54:58
                               ETA: 00:21:37

################################################################################
                     [1m Learning iteration 1436/2000 [0m                     

                       Computation: 45684 steps/s (collection: 2.065s, learning 0.087s)
             Mean action noise std: 3.24
          Mean value_function loss: 278.9913
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 79.4634
                       Mean reward: 713.90
               Mean episode length: 207.68
    Episode_Reward/reaching_object: 1.7405
     Episode_Reward/lifting_object: 145.4518
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0645
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 2.15s
                      Time elapsed: 00:55:00
                               ETA: 00:21:35

################################################################################
                     [1m Learning iteration 1437/2000 [0m                     

                       Computation: 44570 steps/s (collection: 2.109s, learning 0.097s)
             Mean action noise std: 3.24
          Mean value_function loss: 317.9865
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 79.4761
                       Mean reward: 753.95
               Mean episode length: 213.25
    Episode_Reward/reaching_object: 1.7611
     Episode_Reward/lifting_object: 146.9787
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0864
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 2.21s
                      Time elapsed: 00:55:02
                               ETA: 00:21:32

################################################################################
                     [1m Learning iteration 1438/2000 [0m                     

                       Computation: 45488 steps/s (collection: 2.054s, learning 0.107s)
             Mean action noise std: 3.24
          Mean value_function loss: 324.1285
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 79.4960
                       Mean reward: 738.71
               Mean episode length: 209.49
    Episode_Reward/reaching_object: 1.7179
     Episode_Reward/lifting_object: 143.5723
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0645
          Episode_Reward/joint_vel: -0.0875
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 2.16s
                      Time elapsed: 00:55:04
                               ETA: 00:21:30

################################################################################
                     [1m Learning iteration 1439/2000 [0m                     

                       Computation: 45354 steps/s (collection: 2.055s, learning 0.112s)
             Mean action noise std: 3.24
          Mean value_function loss: 349.9703
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 79.5123
                       Mean reward: 732.04
               Mean episode length: 209.95
    Episode_Reward/reaching_object: 1.7555
     Episode_Reward/lifting_object: 147.2144
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.0880
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 2.17s
                      Time elapsed: 00:55:06
                               ETA: 00:21:28

################################################################################
                     [1m Learning iteration 1440/2000 [0m                     

                       Computation: 45597 steps/s (collection: 2.060s, learning 0.096s)
             Mean action noise std: 3.24
          Mean value_function loss: 316.7613
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 79.5195
                       Mean reward: 726.24
               Mean episode length: 207.75
    Episode_Reward/reaching_object: 1.7068
     Episode_Reward/lifting_object: 142.4101
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0859
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 2.16s
                      Time elapsed: 00:55:08
                               ETA: 00:21:25

################################################################################
                     [1m Learning iteration 1441/2000 [0m                     

                       Computation: 44362 steps/s (collection: 2.107s, learning 0.109s)
             Mean action noise std: 3.24
          Mean value_function loss: 288.6641
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 79.5289
                       Mean reward: 685.76
               Mean episode length: 203.40
    Episode_Reward/reaching_object: 1.7241
     Episode_Reward/lifting_object: 143.5607
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0653
          Episode_Reward/joint_vel: -0.0899
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 2.22s
                      Time elapsed: 00:55:11
                               ETA: 00:21:23

################################################################################
                     [1m Learning iteration 1442/2000 [0m                     

                       Computation: 45431 steps/s (collection: 2.046s, learning 0.118s)
             Mean action noise std: 3.25
          Mean value_function loss: 298.0818
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 79.5508
                       Mean reward: 743.10
               Mean episode length: 214.96
    Episode_Reward/reaching_object: 1.7344
     Episode_Reward/lifting_object: 144.7540
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0657
          Episode_Reward/joint_vel: -0.0897
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 2.16s
                      Time elapsed: 00:55:13
                               ETA: 00:21:21

################################################################################
                     [1m Learning iteration 1443/2000 [0m                     

                       Computation: 44676 steps/s (collection: 2.093s, learning 0.107s)
             Mean action noise std: 3.25
          Mean value_function loss: 321.1507
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 79.5702
                       Mean reward: 704.39
               Mean episode length: 207.11
    Episode_Reward/reaching_object: 1.7212
     Episode_Reward/lifting_object: 143.0607
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0657
          Episode_Reward/joint_vel: -0.0918
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 2.20s
                      Time elapsed: 00:55:15
                               ETA: 00:21:18

################################################################################
                     [1m Learning iteration 1444/2000 [0m                     

                       Computation: 44832 steps/s (collection: 2.097s, learning 0.096s)
             Mean action noise std: 3.25
          Mean value_function loss: 291.4196
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 79.5816
                       Mean reward: 747.00
               Mean episode length: 214.00
    Episode_Reward/reaching_object: 1.7916
     Episode_Reward/lifting_object: 151.3358
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0668
          Episode_Reward/joint_vel: -0.0906
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 2.19s
                      Time elapsed: 00:55:17
                               ETA: 00:21:16

################################################################################
                     [1m Learning iteration 1445/2000 [0m                     

                       Computation: 45742 steps/s (collection: 2.048s, learning 0.101s)
             Mean action noise std: 3.25
          Mean value_function loss: 72034514329.6000
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 79.5891
                       Mean reward: 755.04
               Mean episode length: 216.47
    Episode_Reward/reaching_object: 1.7740
     Episode_Reward/lifting_object: 149.5083
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -152.4572
          Episode_Reward/joint_vel: -17584.4590
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 2.15s
                      Time elapsed: 00:55:19
                               ETA: 00:21:14

################################################################################
                     [1m Learning iteration 1446/2000 [0m                     

                       Computation: 45838 steps/s (collection: 2.053s, learning 0.092s)
             Mean action noise std: 3.25
          Mean value_function loss: 247.2692
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 79.5935
                       Mean reward: 739.80
               Mean episode length: 215.34
    Episode_Reward/reaching_object: 1.7870
     Episode_Reward/lifting_object: 150.2568
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0676
          Episode_Reward/joint_vel: -0.0930
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 2.14s
                      Time elapsed: 00:55:21
                               ETA: 00:21:11

################################################################################
                     [1m Learning iteration 1447/2000 [0m                     

                       Computation: 44938 steps/s (collection: 2.066s, learning 0.121s)
             Mean action noise std: 3.25
          Mean value_function loss: 250.8788
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 79.6055
                       Mean reward: 774.04
               Mean episode length: 222.23
    Episode_Reward/reaching_object: 1.8294
     Episode_Reward/lifting_object: 153.2312
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0695
          Episode_Reward/joint_vel: -0.0955
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 2.19s
                      Time elapsed: 00:55:24
                               ETA: 00:21:09

################################################################################
                     [1m Learning iteration 1448/2000 [0m                     

                       Computation: 45932 steps/s (collection: 2.040s, learning 0.101s)
             Mean action noise std: 3.25
          Mean value_function loss: 246.9253
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 79.6242
                       Mean reward: 728.00
               Mean episode length: 210.67
    Episode_Reward/reaching_object: 1.7211
     Episode_Reward/lifting_object: 143.7813
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0662
          Episode_Reward/joint_vel: -0.0907
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 2.14s
                      Time elapsed: 00:55:26
                               ETA: 00:21:07

################################################################################
                     [1m Learning iteration 1449/2000 [0m                     

                       Computation: 45850 steps/s (collection: 2.046s, learning 0.098s)
             Mean action noise std: 3.26
          Mean value_function loss: 234.9094
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 79.6363
                       Mean reward: 744.51
               Mean episode length: 213.05
    Episode_Reward/reaching_object: 1.7220
     Episode_Reward/lifting_object: 143.7868
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0657
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 2.14s
                      Time elapsed: 00:55:28
                               ETA: 00:21:04

################################################################################
                     [1m Learning iteration 1450/2000 [0m                     

                       Computation: 45856 steps/s (collection: 2.044s, learning 0.100s)
             Mean action noise std: 3.26
          Mean value_function loss: 235.6731
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 79.6434
                       Mean reward: 789.68
               Mean episode length: 231.24
    Episode_Reward/reaching_object: 1.8226
     Episode_Reward/lifting_object: 152.6918
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.0938
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 2.14s
                      Time elapsed: 00:55:30
                               ETA: 00:21:02

################################################################################
                     [1m Learning iteration 1451/2000 [0m                     

                       Computation: 44581 steps/s (collection: 2.108s, learning 0.097s)
             Mean action noise std: 3.26
          Mean value_function loss: 243.6692
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 79.6495
                       Mean reward: 716.10
               Mean episode length: 204.81
    Episode_Reward/reaching_object: 1.7657
     Episode_Reward/lifting_object: 147.3810
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0670
          Episode_Reward/joint_vel: -0.0914
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 2.21s
                      Time elapsed: 00:55:32
                               ETA: 00:21:00

################################################################################
                     [1m Learning iteration 1452/2000 [0m                     

                       Computation: 42901 steps/s (collection: 2.175s, learning 0.116s)
             Mean action noise std: 3.26
          Mean value_function loss: 262.8272
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 79.6548
                       Mean reward: 767.32
               Mean episode length: 216.78
    Episode_Reward/reaching_object: 1.7141
     Episode_Reward/lifting_object: 143.1909
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0656
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 2.29s
                      Time elapsed: 00:55:35
                               ETA: 00:20:57

################################################################################
                     [1m Learning iteration 1453/2000 [0m                     

                       Computation: 45039 steps/s (collection: 2.064s, learning 0.119s)
             Mean action noise std: 3.26
          Mean value_function loss: 267.5152
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 79.6620
                       Mean reward: 677.83
               Mean episode length: 200.61
    Episode_Reward/reaching_object: 1.7216
     Episode_Reward/lifting_object: 142.7991
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0652
          Episode_Reward/joint_vel: -0.0900
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 2.18s
                      Time elapsed: 00:55:37
                               ETA: 00:20:55

################################################################################
                     [1m Learning iteration 1454/2000 [0m                     

                       Computation: 44638 steps/s (collection: 2.091s, learning 0.111s)
             Mean action noise std: 3.26
          Mean value_function loss: 276.3541
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 79.6755
                       Mean reward: 791.20
               Mean episode length: 224.76
    Episode_Reward/reaching_object: 1.8347
     Episode_Reward/lifting_object: 154.0650
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0694
          Episode_Reward/joint_vel: -0.0936
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 2.20s
                      Time elapsed: 00:55:39
                               ETA: 00:20:53

################################################################################
                     [1m Learning iteration 1455/2000 [0m                     

                       Computation: 44863 steps/s (collection: 2.081s, learning 0.111s)
             Mean action noise std: 3.26
          Mean value_function loss: 310.2283
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 79.6907
                       Mean reward: 725.27
               Mean episode length: 210.96
    Episode_Reward/reaching_object: 1.7644
     Episode_Reward/lifting_object: 147.8813
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0669
          Episode_Reward/joint_vel: -0.0909
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 2.19s
                      Time elapsed: 00:55:41
                               ETA: 00:20:50

################################################################################
                     [1m Learning iteration 1456/2000 [0m                     

                       Computation: 45066 steps/s (collection: 2.072s, learning 0.109s)
             Mean action noise std: 3.26
          Mean value_function loss: 317.9408
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 79.7093
                       Mean reward: 762.55
               Mean episode length: 217.10
    Episode_Reward/reaching_object: 1.7510
     Episode_Reward/lifting_object: 146.2200
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0663
          Episode_Reward/joint_vel: -0.0913
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 2.18s
                      Time elapsed: 00:55:43
                               ETA: 00:20:48

################################################################################
                     [1m Learning iteration 1457/2000 [0m                     

                       Computation: 44239 steps/s (collection: 2.109s, learning 0.114s)
             Mean action noise std: 3.27
          Mean value_function loss: 235.7908
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 79.7187
                       Mean reward: 767.70
               Mean episode length: 219.10
    Episode_Reward/reaching_object: 1.8642
     Episode_Reward/lifting_object: 157.2104
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0701
          Episode_Reward/joint_vel: -0.0937
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 2.22s
                      Time elapsed: 00:55:46
                               ETA: 00:20:46

################################################################################
                     [1m Learning iteration 1458/2000 [0m                     

                       Computation: 44487 steps/s (collection: 2.115s, learning 0.095s)
             Mean action noise std: 3.27
          Mean value_function loss: 285.8576
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 79.7325
                       Mean reward: 735.24
               Mean episode length: 208.57
    Episode_Reward/reaching_object: 1.7294
     Episode_Reward/lifting_object: 144.7759
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0666
          Episode_Reward/joint_vel: -0.0904
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 2.21s
                      Time elapsed: 00:55:48
                               ETA: 00:20:43

################################################################################
                     [1m Learning iteration 1459/2000 [0m                     

                       Computation: 44587 steps/s (collection: 2.106s, learning 0.099s)
             Mean action noise std: 3.27
          Mean value_function loss: 262.9529
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 79.7513
                       Mean reward: 717.35
               Mean episode length: 211.22
    Episode_Reward/reaching_object: 1.7473
     Episode_Reward/lifting_object: 146.1136
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0664
          Episode_Reward/joint_vel: -0.0901
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 2.20s
                      Time elapsed: 00:55:50
                               ETA: 00:20:41

################################################################################
                     [1m Learning iteration 1460/2000 [0m                     

                       Computation: 44914 steps/s (collection: 2.096s, learning 0.093s)
             Mean action noise std: 3.27
          Mean value_function loss: 317.3566
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 79.7669
                       Mean reward: 681.15
               Mean episode length: 202.01
    Episode_Reward/reaching_object: 1.7727
     Episode_Reward/lifting_object: 148.2167
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0677
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 2.19s
                      Time elapsed: 00:55:52
                               ETA: 00:20:39

################################################################################
                     [1m Learning iteration 1461/2000 [0m                     

                       Computation: 45427 steps/s (collection: 2.049s, learning 0.115s)
             Mean action noise std: 3.27
          Mean value_function loss: 302.4176
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 79.7763
                       Mean reward: 758.39
               Mean episode length: 213.68
    Episode_Reward/reaching_object: 1.7539
     Episode_Reward/lifting_object: 146.6464
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0665
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 2.16s
                      Time elapsed: 00:55:54
                               ETA: 00:20:36

################################################################################
                     [1m Learning iteration 1462/2000 [0m                     

                       Computation: 44565 steps/s (collection: 2.107s, learning 0.099s)
             Mean action noise std: 3.28
          Mean value_function loss: 300.6634
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 79.7948
                       Mean reward: 744.43
               Mean episode length: 213.44
    Episode_Reward/reaching_object: 1.7570
     Episode_Reward/lifting_object: 146.0224
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0910
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 2.21s
                      Time elapsed: 00:55:57
                               ETA: 00:20:34

################################################################################
                     [1m Learning iteration 1463/2000 [0m                     

                       Computation: 45403 steps/s (collection: 2.054s, learning 0.111s)
             Mean action noise std: 3.28
          Mean value_function loss: 276.6589
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 79.8181
                       Mean reward: 741.74
               Mean episode length: 212.27
    Episode_Reward/reaching_object: 1.7821
     Episode_Reward/lifting_object: 149.0685
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0677
          Episode_Reward/joint_vel: -0.0917
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 2.17s
                      Time elapsed: 00:55:59
                               ETA: 00:20:32

################################################################################
                     [1m Learning iteration 1464/2000 [0m                     

                       Computation: 44872 steps/s (collection: 2.097s, learning 0.094s)
             Mean action noise std: 3.28
          Mean value_function loss: 288.0168
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 79.8325
                       Mean reward: 745.92
               Mean episode length: 213.62
    Episode_Reward/reaching_object: 1.7660
     Episode_Reward/lifting_object: 147.4156
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0900
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 2.19s
                      Time elapsed: 00:56:01
                               ETA: 00:20:29

################################################################################
                     [1m Learning iteration 1465/2000 [0m                     

                       Computation: 44421 steps/s (collection: 2.069s, learning 0.144s)
             Mean action noise std: 3.28
          Mean value_function loss: 283.3687
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 79.8464
                       Mean reward: 774.09
               Mean episode length: 218.50
    Episode_Reward/reaching_object: 1.7894
     Episode_Reward/lifting_object: 150.0413
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0679
          Episode_Reward/joint_vel: -0.0921
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 2.21s
                      Time elapsed: 00:56:03
                               ETA: 00:20:27

################################################################################
                     [1m Learning iteration 1466/2000 [0m                     

                       Computation: 44321 steps/s (collection: 2.123s, learning 0.095s)
             Mean action noise std: 3.28
          Mean value_function loss: 292.3655
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 79.8564
                       Mean reward: 721.35
               Mean episode length: 207.49
    Episode_Reward/reaching_object: 1.7371
     Episode_Reward/lifting_object: 145.3341
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0661
          Episode_Reward/joint_vel: -0.0891
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 2.22s
                      Time elapsed: 00:56:05
                               ETA: 00:20:25

################################################################################
                     [1m Learning iteration 1467/2000 [0m                     

                       Computation: 45871 steps/s (collection: 2.050s, learning 0.093s)
             Mean action noise std: 3.28
          Mean value_function loss: 285.1076
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 79.8732
                       Mean reward: 707.31
               Mean episode length: 208.37
    Episode_Reward/reaching_object: 1.8138
     Episode_Reward/lifting_object: 152.3173
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0688
          Episode_Reward/joint_vel: -0.0913
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 2.14s
                      Time elapsed: 00:56:07
                               ETA: 00:20:22

################################################################################
                     [1m Learning iteration 1468/2000 [0m                     

                       Computation: 45726 steps/s (collection: 2.058s, learning 0.092s)
             Mean action noise std: 3.29
          Mean value_function loss: 367.3250
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 79.8987
                       Mean reward: 683.98
               Mean episode length: 198.75
    Episode_Reward/reaching_object: 1.6734
     Episode_Reward/lifting_object: 139.2495
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.0869
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 2.15s
                      Time elapsed: 00:56:10
                               ETA: 00:20:20

################################################################################
                     [1m Learning iteration 1469/2000 [0m                     

                       Computation: 45291 steps/s (collection: 2.075s, learning 0.095s)
             Mean action noise std: 3.29
          Mean value_function loss: 291.3077
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 79.9186
                       Mean reward: 763.96
               Mean episode length: 217.48
    Episode_Reward/reaching_object: 1.8104
     Episode_Reward/lifting_object: 151.7216
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0683
          Episode_Reward/joint_vel: -0.0922
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 2.17s
                      Time elapsed: 00:56:12
                               ETA: 00:20:18

################################################################################
                     [1m Learning iteration 1470/2000 [0m                     

                       Computation: 44187 steps/s (collection: 2.114s, learning 0.110s)
             Mean action noise std: 3.29
          Mean value_function loss: 332.1678
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 79.9224
                       Mean reward: 729.03
               Mean episode length: 206.22
    Episode_Reward/reaching_object: 1.7109
     Episode_Reward/lifting_object: 143.0159
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0652
          Episode_Reward/joint_vel: -0.0881
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 2.22s
                      Time elapsed: 00:56:14
                               ETA: 00:20:15

################################################################################
                     [1m Learning iteration 1471/2000 [0m                     

                       Computation: 44793 steps/s (collection: 2.073s, learning 0.121s)
             Mean action noise std: 3.29
          Mean value_function loss: 368.3645
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 79.9366
                       Mean reward: 663.91
               Mean episode length: 194.42
    Episode_Reward/reaching_object: 1.6844
     Episode_Reward/lifting_object: 139.6391
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0884
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 2.19s
                      Time elapsed: 00:56:16
                               ETA: 00:20:13

################################################################################
                     [1m Learning iteration 1472/2000 [0m                     

                       Computation: 44343 steps/s (collection: 2.124s, learning 0.093s)
             Mean action noise std: 3.29
          Mean value_function loss: 294.6068
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 79.9633
                       Mean reward: 764.06
               Mean episode length: 213.84
    Episode_Reward/reaching_object: 1.7414
     Episode_Reward/lifting_object: 145.5525
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0662
          Episode_Reward/joint_vel: -0.0892
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 2.22s
                      Time elapsed: 00:56:18
                               ETA: 00:20:11

################################################################################
                     [1m Learning iteration 1473/2000 [0m                     

                       Computation: 44703 steps/s (collection: 2.079s, learning 0.120s)
             Mean action noise std: 3.30
          Mean value_function loss: 322.6095
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 79.9792
                       Mean reward: 729.41
               Mean episode length: 208.03
    Episode_Reward/reaching_object: 1.7069
     Episode_Reward/lifting_object: 142.9470
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0653
          Episode_Reward/joint_vel: -0.0877
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 2.20s
                      Time elapsed: 00:56:21
                               ETA: 00:20:08

################################################################################
                     [1m Learning iteration 1474/2000 [0m                     

                       Computation: 44539 steps/s (collection: 2.075s, learning 0.132s)
             Mean action noise std: 3.30
          Mean value_function loss: 312.7263
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 79.9939
                       Mean reward: 691.07
               Mean episode length: 200.02
    Episode_Reward/reaching_object: 1.6909
     Episode_Reward/lifting_object: 141.5328
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0648
          Episode_Reward/joint_vel: -0.0868
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 2.21s
                      Time elapsed: 00:56:23
                               ETA: 00:20:06

################################################################################
                     [1m Learning iteration 1475/2000 [0m                     

                       Computation: 43873 steps/s (collection: 2.133s, learning 0.108s)
             Mean action noise std: 3.30
          Mean value_function loss: 305.3367
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 80.0113
                       Mean reward: 778.14
               Mean episode length: 216.89
    Episode_Reward/reaching_object: 1.7660
     Episode_Reward/lifting_object: 148.6524
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0668
          Episode_Reward/joint_vel: -0.0885
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 2.24s
                      Time elapsed: 00:56:25
                               ETA: 00:20:04

################################################################################
                     [1m Learning iteration 1476/2000 [0m                     

                       Computation: 44597 steps/s (collection: 2.112s, learning 0.093s)
             Mean action noise std: 3.30
          Mean value_function loss: 340.4582
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 80.0370
                       Mean reward: 804.75
               Mean episode length: 222.18
    Episode_Reward/reaching_object: 1.7446
     Episode_Reward/lifting_object: 146.6013
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0665
          Episode_Reward/joint_vel: -0.0886
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 2.20s
                      Time elapsed: 00:56:27
                               ETA: 00:20:01

################################################################################
                     [1m Learning iteration 1477/2000 [0m                     

                       Computation: 43836 steps/s (collection: 2.134s, learning 0.108s)
             Mean action noise std: 3.30
          Mean value_function loss: 345.1248
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 80.0518
                       Mean reward: 681.61
               Mean episode length: 197.31
    Episode_Reward/reaching_object: 1.7336
     Episode_Reward/lifting_object: 144.8984
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0667
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 2.24s
                      Time elapsed: 00:56:30
                               ETA: 00:19:59

################################################################################
                     [1m Learning iteration 1478/2000 [0m                     

                       Computation: 43694 steps/s (collection: 2.134s, learning 0.116s)
             Mean action noise std: 3.31
          Mean value_function loss: 294.4794
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 80.0730
                       Mean reward: 764.93
               Mean episode length: 218.65
    Episode_Reward/reaching_object: 1.7505
     Episode_Reward/lifting_object: 146.3771
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.0920
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 2.25s
                      Time elapsed: 00:56:32
                               ETA: 00:19:57

################################################################################
                     [1m Learning iteration 1479/2000 [0m                     

                       Computation: 43774 steps/s (collection: 2.143s, learning 0.103s)
             Mean action noise std: 3.31
          Mean value_function loss: 297.6945
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 80.0939
                       Mean reward: 724.69
               Mean episode length: 202.98
    Episode_Reward/reaching_object: 1.7160
     Episode_Reward/lifting_object: 143.2395
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0662
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 2.25s
                      Time elapsed: 00:56:34
                               ETA: 00:19:54

################################################################################
                     [1m Learning iteration 1480/2000 [0m                     

                       Computation: 42561 steps/s (collection: 2.181s, learning 0.129s)
             Mean action noise std: 3.31
          Mean value_function loss: 271.7578
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 80.1144
                       Mean reward: 770.53
               Mean episode length: 217.18
    Episode_Reward/reaching_object: 1.7676
     Episode_Reward/lifting_object: 148.4516
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0919
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 2.31s
                      Time elapsed: 00:56:36
                               ETA: 00:19:52

################################################################################
                     [1m Learning iteration 1481/2000 [0m                     

                       Computation: 42610 steps/s (collection: 2.161s, learning 0.146s)
             Mean action noise std: 3.31
          Mean value_function loss: 285.8484
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 80.1309
                       Mean reward: 787.14
               Mean episode length: 218.53
    Episode_Reward/reaching_object: 1.7466
     Episode_Reward/lifting_object: 147.2143
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0677
          Episode_Reward/joint_vel: -0.0902
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 2.31s
                      Time elapsed: 00:56:39
                               ETA: 00:19:50

################################################################################
                     [1m Learning iteration 1482/2000 [0m                     

                       Computation: 44638 steps/s (collection: 2.100s, learning 0.103s)
             Mean action noise std: 3.32
          Mean value_function loss: 292.7417
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 80.1529
                       Mean reward: 676.78
               Mean episode length: 198.94
    Episode_Reward/reaching_object: 1.6663
     Episode_Reward/lifting_object: 138.6234
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0663
          Episode_Reward/joint_vel: -0.0910
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 2.20s
                      Time elapsed: 00:56:41
                               ETA: 00:19:48

################################################################################
                     [1m Learning iteration 1483/2000 [0m                     

                       Computation: 45270 steps/s (collection: 2.082s, learning 0.090s)
             Mean action noise std: 3.32
          Mean value_function loss: 283.1498
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 80.1730
                       Mean reward: 712.24
               Mean episode length: 210.47
    Episode_Reward/reaching_object: 1.7843
     Episode_Reward/lifting_object: 149.8281
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0694
          Episode_Reward/joint_vel: -0.0922
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 2.17s
                      Time elapsed: 00:56:43
                               ETA: 00:19:45

################################################################################
                     [1m Learning iteration 1484/2000 [0m                     

                       Computation: 45380 steps/s (collection: 2.066s, learning 0.101s)
             Mean action noise std: 3.32
          Mean value_function loss: 304.0581
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 80.1857
                       Mean reward: 732.66
               Mean episode length: 210.71
    Episode_Reward/reaching_object: 1.7110
     Episode_Reward/lifting_object: 143.3413
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 2.17s
                      Time elapsed: 00:56:45
                               ETA: 00:19:43

################################################################################
                     [1m Learning iteration 1485/2000 [0m                     

                       Computation: 44327 steps/s (collection: 2.115s, learning 0.103s)
             Mean action noise std: 3.32
          Mean value_function loss: 263.7647
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 80.2082
                       Mean reward: 691.21
               Mean episode length: 199.63
    Episode_Reward/reaching_object: 1.7129
     Episode_Reward/lifting_object: 144.2061
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0675
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 2.22s
                      Time elapsed: 00:56:47
                               ETA: 00:19:41

################################################################################
                     [1m Learning iteration 1486/2000 [0m                     

                       Computation: 44180 steps/s (collection: 2.115s, learning 0.110s)
             Mean action noise std: 3.33
          Mean value_function loss: 299.4454
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 80.2282
                       Mean reward: 744.62
               Mean episode length: 208.78
    Episode_Reward/reaching_object: 1.7636
     Episode_Reward/lifting_object: 148.8751
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0889
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 2.23s
                      Time elapsed: 00:56:50
                               ETA: 00:19:38

################################################################################
                     [1m Learning iteration 1487/2000 [0m                     

                       Computation: 44565 steps/s (collection: 2.115s, learning 0.091s)
             Mean action noise std: 3.33
          Mean value_function loss: 245.0477
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 80.2502
                       Mean reward: 798.95
               Mean episode length: 225.23
    Episode_Reward/reaching_object: 1.8416
     Episode_Reward/lifting_object: 156.1624
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0709
          Episode_Reward/joint_vel: -0.0906
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 2.21s
                      Time elapsed: 00:56:52
                               ETA: 00:19:36

################################################################################
                     [1m Learning iteration 1488/2000 [0m                     

                       Computation: 44386 steps/s (collection: 2.122s, learning 0.093s)
             Mean action noise std: 3.33
          Mean value_function loss: 332.2721
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 80.2653
                       Mean reward: 668.46
               Mean episode length: 199.76
    Episode_Reward/reaching_object: 1.6511
     Episode_Reward/lifting_object: 137.1614
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0659
          Episode_Reward/joint_vel: -0.0881
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 2.21s
                      Time elapsed: 00:56:54
                               ETA: 00:19:34

################################################################################
                     [1m Learning iteration 1489/2000 [0m                     

                       Computation: 44628 steps/s (collection: 2.114s, learning 0.089s)
             Mean action noise std: 3.33
          Mean value_function loss: 272.1929
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 80.2738
                       Mean reward: 728.62
               Mean episode length: 211.35
    Episode_Reward/reaching_object: 1.7814
     Episode_Reward/lifting_object: 149.2051
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0694
          Episode_Reward/joint_vel: -0.0911
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 2.20s
                      Time elapsed: 00:56:56
                               ETA: 00:19:31

################################################################################
                     [1m Learning iteration 1490/2000 [0m                     

                       Computation: 44304 steps/s (collection: 2.084s, learning 0.135s)
             Mean action noise std: 3.33
          Mean value_function loss: 250.7597
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 80.2850
                       Mean reward: 789.84
               Mean episode length: 223.27
    Episode_Reward/reaching_object: 1.7643
     Episode_Reward/lifting_object: 148.1056
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 2.22s
                      Time elapsed: 00:56:58
                               ETA: 00:19:29

################################################################################
                     [1m Learning iteration 1491/2000 [0m                     

                       Computation: 44670 steps/s (collection: 2.079s, learning 0.121s)
             Mean action noise std: 3.33
          Mean value_function loss: 287.1197
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 80.2998
                       Mean reward: 768.49
               Mean episode length: 215.53
    Episode_Reward/reaching_object: 1.6771
     Episode_Reward/lifting_object: 140.4413
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0662
          Episode_Reward/joint_vel: -0.0858
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 2.20s
                      Time elapsed: 00:57:01
                               ETA: 00:19:27

################################################################################
                     [1m Learning iteration 1492/2000 [0m                     

                       Computation: 45665 steps/s (collection: 2.054s, learning 0.099s)
             Mean action noise std: 3.33
          Mean value_function loss: 283.1899
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 80.3145
                       Mean reward: 781.51
               Mean episode length: 218.54
    Episode_Reward/reaching_object: 1.7902
     Episode_Reward/lifting_object: 151.0167
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0697
          Episode_Reward/joint_vel: -0.0897
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 2.15s
                      Time elapsed: 00:57:03
                               ETA: 00:19:24

################################################################################
                     [1m Learning iteration 1493/2000 [0m                     

                       Computation: 44813 steps/s (collection: 2.101s, learning 0.093s)
             Mean action noise std: 3.34
          Mean value_function loss: 331.9268
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 80.3316
                       Mean reward: 752.47
               Mean episode length: 211.29
    Episode_Reward/reaching_object: 1.7539
     Episode_Reward/lifting_object: 147.7208
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0680
          Episode_Reward/joint_vel: -0.0880
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 2.19s
                      Time elapsed: 00:57:05
                               ETA: 00:19:22

################################################################################
                     [1m Learning iteration 1494/2000 [0m                     

                       Computation: 45478 steps/s (collection: 2.072s, learning 0.090s)
             Mean action noise std: 3.34
          Mean value_function loss: 299.6662
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 80.3532
                       Mean reward: 752.52
               Mean episode length: 211.92
    Episode_Reward/reaching_object: 1.7395
     Episode_Reward/lifting_object: 145.5322
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 2.16s
                      Time elapsed: 00:57:07
                               ETA: 00:19:20

################################################################################
                     [1m Learning iteration 1495/2000 [0m                     

                       Computation: 44448 steps/s (collection: 2.092s, learning 0.120s)
             Mean action noise std: 3.34
          Mean value_function loss: 269.5817
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 80.3703
                       Mean reward: 755.08
               Mean episode length: 213.85
    Episode_Reward/reaching_object: 1.7789
     Episode_Reward/lifting_object: 149.0747
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0696
          Episode_Reward/joint_vel: -0.0900
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 2.21s
                      Time elapsed: 00:57:09
                               ETA: 00:19:17

################################################################################
                     [1m Learning iteration 1496/2000 [0m                     

                       Computation: 41376 steps/s (collection: 2.223s, learning 0.153s)
             Mean action noise std: 3.34
          Mean value_function loss: 310.7949
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 80.3826
                       Mean reward: 791.94
               Mean episode length: 219.00
    Episode_Reward/reaching_object: 1.7527
     Episode_Reward/lifting_object: 147.2869
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0687
          Episode_Reward/joint_vel: -0.0901
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 2.38s
                      Time elapsed: 00:57:12
                               ETA: 00:19:15

################################################################################
                     [1m Learning iteration 1497/2000 [0m                     

                       Computation: 43745 steps/s (collection: 2.136s, learning 0.111s)
             Mean action noise std: 3.34
          Mean value_function loss: 288.1131
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 80.3885
                       Mean reward: 704.70
               Mean episode length: 206.91
    Episode_Reward/reaching_object: 1.7377
     Episode_Reward/lifting_object: 145.9932
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0689
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 2.25s
                      Time elapsed: 00:57:14
                               ETA: 00:19:13

################################################################################
                     [1m Learning iteration 1498/2000 [0m                     

                       Computation: 44803 steps/s (collection: 2.102s, learning 0.093s)
             Mean action noise std: 3.34
          Mean value_function loss: 311.9131
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 80.4000
                       Mean reward: 706.46
               Mean episode length: 207.92
    Episode_Reward/reaching_object: 1.7481
     Episode_Reward/lifting_object: 146.9435
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0693
          Episode_Reward/joint_vel: -0.0900
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 2.19s
                      Time elapsed: 00:57:16
                               ETA: 00:19:10

################################################################################
                     [1m Learning iteration 1499/2000 [0m                     

                       Computation: 43455 steps/s (collection: 2.132s, learning 0.131s)
             Mean action noise std: 3.35
          Mean value_function loss: 318.2948
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 80.4188
                       Mean reward: 711.22
               Mean episode length: 206.56
    Episode_Reward/reaching_object: 1.7620
     Episode_Reward/lifting_object: 147.3422
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0696
          Episode_Reward/joint_vel: -0.0906
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 2.26s
                      Time elapsed: 00:57:18
                               ETA: 00:19:08

################################################################################
                     [1m Learning iteration 1500/2000 [0m                     

                       Computation: 43215 steps/s (collection: 2.154s, learning 0.121s)
             Mean action noise std: 3.35
          Mean value_function loss: 293.1732
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 80.4362
                       Mean reward: 691.35
               Mean episode length: 202.89
    Episode_Reward/reaching_object: 1.6965
     Episode_Reward/lifting_object: 141.1733
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0683
          Episode_Reward/joint_vel: -0.0900
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 2.27s
                      Time elapsed: 00:57:21
                               ETA: 00:19:06

################################################################################
                     [1m Learning iteration 1501/2000 [0m                     

                       Computation: 44622 steps/s (collection: 2.107s, learning 0.096s)
             Mean action noise std: 3.35
          Mean value_function loss: 323.9780
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 80.4474
                       Mean reward: 688.93
               Mean episode length: 200.45
    Episode_Reward/reaching_object: 1.7154
     Episode_Reward/lifting_object: 143.3275
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.0886
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 2.20s
                      Time elapsed: 00:57:23
                               ETA: 00:19:03

################################################################################
                     [1m Learning iteration 1502/2000 [0m                     

                       Computation: 45042 steps/s (collection: 2.084s, learning 0.099s)
             Mean action noise std: 3.35
          Mean value_function loss: 292.6981
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 80.4578
                       Mean reward: 791.63
               Mean episode length: 218.06
    Episode_Reward/reaching_object: 1.7603
     Episode_Reward/lifting_object: 147.4043
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0696
          Episode_Reward/joint_vel: -0.0904
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 2.18s
                      Time elapsed: 00:57:25
                               ETA: 00:19:01

################################################################################
                     [1m Learning iteration 1503/2000 [0m                     

                       Computation: 45132 steps/s (collection: 2.068s, learning 0.110s)
             Mean action noise std: 3.35
          Mean value_function loss: 306.2062
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 80.4758
                       Mean reward: 725.92
               Mean episode length: 207.14
    Episode_Reward/reaching_object: 1.7308
     Episode_Reward/lifting_object: 145.7587
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0683
          Episode_Reward/joint_vel: -0.0874
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 2.18s
                      Time elapsed: 00:57:27
                               ETA: 00:18:59

################################################################################
                     [1m Learning iteration 1504/2000 [0m                     

                       Computation: 44674 steps/s (collection: 2.089s, learning 0.112s)
             Mean action noise std: 3.35
          Mean value_function loss: 333.7042
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 80.4891
                       Mean reward: 690.49
               Mean episode length: 201.24
    Episode_Reward/reaching_object: 1.7440
     Episode_Reward/lifting_object: 145.7690
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0694
          Episode_Reward/joint_vel: -0.0905
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 2.20s
                      Time elapsed: 00:57:30
                               ETA: 00:18:57

################################################################################
                     [1m Learning iteration 1505/2000 [0m                     

                       Computation: 43794 steps/s (collection: 2.126s, learning 0.119s)
             Mean action noise std: 3.36
          Mean value_function loss: 300.3895
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 80.4944
                       Mean reward: 755.70
               Mean episode length: 214.58
    Episode_Reward/reaching_object: 1.7257
     Episode_Reward/lifting_object: 145.1915
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.0862
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 2.24s
                      Time elapsed: 00:57:32
                               ETA: 00:18:54

################################################################################
                     [1m Learning iteration 1506/2000 [0m                     

                       Computation: 41511 steps/s (collection: 2.276s, learning 0.092s)
             Mean action noise std: 3.36
          Mean value_function loss: 314.8178
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 80.5012
                       Mean reward: 685.81
               Mean episode length: 199.97
    Episode_Reward/reaching_object: 1.7975
     Episode_Reward/lifting_object: 151.9137
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0708
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 2.37s
                      Time elapsed: 00:57:34
                               ETA: 00:18:52

################################################################################
                     [1m Learning iteration 1507/2000 [0m                     

                       Computation: 44538 steps/s (collection: 2.097s, learning 0.111s)
             Mean action noise std: 3.36
          Mean value_function loss: 316.5256
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 80.5078
                       Mean reward: 700.48
               Mean episode length: 201.31
    Episode_Reward/reaching_object: 1.7375
     Episode_Reward/lifting_object: 146.1474
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0689
          Episode_Reward/joint_vel: -0.0878
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 2.21s
                      Time elapsed: 00:57:36
                               ETA: 00:18:50

################################################################################
                     [1m Learning iteration 1508/2000 [0m                     

                       Computation: 44301 steps/s (collection: 2.111s, learning 0.108s)
             Mean action noise std: 3.36
          Mean value_function loss: 302.9027
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 80.5129
                       Mean reward: 760.08
               Mean episode length: 215.04
    Episode_Reward/reaching_object: 1.7601
     Episode_Reward/lifting_object: 148.1838
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0698
          Episode_Reward/joint_vel: -0.0878
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 2.22s
                      Time elapsed: 00:57:39
                               ETA: 00:18:47

################################################################################
                     [1m Learning iteration 1509/2000 [0m                     

                       Computation: 44381 steps/s (collection: 2.078s, learning 0.137s)
             Mean action noise std: 3.36
          Mean value_function loss: 318.5741
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 80.5181
                       Mean reward: 712.15
               Mean episode length: 207.00
    Episode_Reward/reaching_object: 1.7284
     Episode_Reward/lifting_object: 145.1674
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0687
          Episode_Reward/joint_vel: -0.0874
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 2.21s
                      Time elapsed: 00:57:41
                               ETA: 00:18:45

################################################################################
                     [1m Learning iteration 1510/2000 [0m                     

                       Computation: 45456 steps/s (collection: 2.072s, learning 0.090s)
             Mean action noise std: 3.36
          Mean value_function loss: 314.7287
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 80.5332
                       Mean reward: 748.91
               Mean episode length: 208.01
    Episode_Reward/reaching_object: 1.6956
     Episode_Reward/lifting_object: 141.9840
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0674
          Episode_Reward/joint_vel: -0.0851
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 2.16s
                      Time elapsed: 00:57:43
                               ETA: 00:18:43

################################################################################
                     [1m Learning iteration 1511/2000 [0m                     

                       Computation: 44083 steps/s (collection: 2.119s, learning 0.111s)
             Mean action noise std: 3.36
          Mean value_function loss: 291.4684
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 80.5482
                       Mean reward: 711.37
               Mean episode length: 204.13
    Episode_Reward/reaching_object: 1.7330
     Episode_Reward/lifting_object: 145.0674
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0688
          Episode_Reward/joint_vel: -0.0874
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 2.23s
                      Time elapsed: 00:57:45
                               ETA: 00:18:40

################################################################################
                     [1m Learning iteration 1512/2000 [0m                     

                       Computation: 44733 steps/s (collection: 2.092s, learning 0.106s)
             Mean action noise std: 3.36
          Mean value_function loss: 263.9174
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 80.5573
                       Mean reward: 730.16
               Mean episode length: 211.40
    Episode_Reward/reaching_object: 1.7351
     Episode_Reward/lifting_object: 145.0060
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.0879
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 2.20s
                      Time elapsed: 00:57:47
                               ETA: 00:18:38

################################################################################
                     [1m Learning iteration 1513/2000 [0m                     

                       Computation: 44085 steps/s (collection: 2.108s, learning 0.122s)
             Mean action noise std: 3.36
          Mean value_function loss: 292.4352
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 80.5695
                       Mean reward: 743.55
               Mean episode length: 208.65
    Episode_Reward/reaching_object: 1.7627
     Episode_Reward/lifting_object: 148.1812
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.0867
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 2.23s
                      Time elapsed: 00:57:50
                               ETA: 00:18:36

################################################################################
                     [1m Learning iteration 1514/2000 [0m                     

                       Computation: 43058 steps/s (collection: 2.102s, learning 0.181s)
             Mean action noise std: 3.37
          Mean value_function loss: 283.2731
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 80.5801
                       Mean reward: 733.06
               Mean episode length: 208.11
    Episode_Reward/reaching_object: 1.7578
     Episode_Reward/lifting_object: 147.6338
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0696
          Episode_Reward/joint_vel: -0.0873
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 2.28s
                      Time elapsed: 00:57:52
                               ETA: 00:18:33

################################################################################
                     [1m Learning iteration 1515/2000 [0m                     

                       Computation: 45740 steps/s (collection: 2.045s, learning 0.105s)
             Mean action noise std: 3.37
          Mean value_function loss: 322.1945
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 80.5961
                       Mean reward: 748.97
               Mean episode length: 209.82
    Episode_Reward/reaching_object: 1.7603
     Episode_Reward/lifting_object: 147.0852
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0695
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 2.15s
                      Time elapsed: 00:57:54
                               ETA: 00:18:31

################################################################################
                     [1m Learning iteration 1516/2000 [0m                     

                       Computation: 45759 steps/s (collection: 2.059s, learning 0.090s)
             Mean action noise std: 3.37
          Mean value_function loss: 291.6011
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 80.6164
                       Mean reward: 763.99
               Mean episode length: 216.73
    Episode_Reward/reaching_object: 1.8050
     Episode_Reward/lifting_object: 151.5811
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0718
          Episode_Reward/joint_vel: -0.0914
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 2.15s
                      Time elapsed: 00:57:56
                               ETA: 00:18:29

################################################################################
                     [1m Learning iteration 1517/2000 [0m                     

                       Computation: 45902 steps/s (collection: 2.042s, learning 0.100s)
             Mean action noise std: 3.37
          Mean value_function loss: 306.6027
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 80.6385
                       Mean reward: 788.32
               Mean episode length: 219.30
    Episode_Reward/reaching_object: 1.7728
     Episode_Reward/lifting_object: 148.3384
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0698
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 2.14s
                      Time elapsed: 00:57:58
                               ETA: 00:18:26

################################################################################
                     [1m Learning iteration 1518/2000 [0m                     

                       Computation: 44479 steps/s (collection: 2.117s, learning 0.094s)
             Mean action noise std: 3.38
          Mean value_function loss: 270.8160
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 80.6532
                       Mean reward: 763.42
               Mean episode length: 216.16
    Episode_Reward/reaching_object: 1.7710
     Episode_Reward/lifting_object: 148.8741
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0701
          Episode_Reward/joint_vel: -0.0889
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 2.21s
                      Time elapsed: 00:58:01
                               ETA: 00:18:24

################################################################################
                     [1m Learning iteration 1519/2000 [0m                     

                       Computation: 44349 steps/s (collection: 2.085s, learning 0.132s)
             Mean action noise std: 3.38
          Mean value_function loss: 267.4383
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 80.6669
                       Mean reward: 656.23
               Mean episode length: 195.53
    Episode_Reward/reaching_object: 1.7511
     Episode_Reward/lifting_object: 146.3530
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0698
          Episode_Reward/joint_vel: -0.0902
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 2.22s
                      Time elapsed: 00:58:03
                               ETA: 00:18:22

################################################################################
                     [1m Learning iteration 1520/2000 [0m                     

                       Computation: 44854 steps/s (collection: 2.074s, learning 0.118s)
             Mean action noise std: 3.38
          Mean value_function loss: 296.6286
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 80.6792
                       Mean reward: 731.70
               Mean episode length: 210.46
    Episode_Reward/reaching_object: 1.7920
     Episode_Reward/lifting_object: 150.9543
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0713
          Episode_Reward/joint_vel: -0.0914
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 2.19s
                      Time elapsed: 00:58:05
                               ETA: 00:18:19

################################################################################
                     [1m Learning iteration 1521/2000 [0m                     

                       Computation: 43867 steps/s (collection: 2.117s, learning 0.124s)
             Mean action noise std: 3.38
          Mean value_function loss: 327.1500
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 80.6997
                       Mean reward: 764.60
               Mean episode length: 214.76
    Episode_Reward/reaching_object: 1.7570
     Episode_Reward/lifting_object: 147.0824
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0703
          Episode_Reward/joint_vel: -0.0902
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 2.24s
                      Time elapsed: 00:58:07
                               ETA: 00:18:17

################################################################################
                     [1m Learning iteration 1522/2000 [0m                     

                       Computation: 44457 steps/s (collection: 2.100s, learning 0.111s)
             Mean action noise std: 3.38
          Mean value_function loss: 263.0007
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 80.7179
                       Mean reward: 696.46
               Mean episode length: 201.97
    Episode_Reward/reaching_object: 1.7589
     Episode_Reward/lifting_object: 147.4413
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0705
          Episode_Reward/joint_vel: -0.0908
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 2.21s
                      Time elapsed: 00:58:09
                               ETA: 00:18:15

################################################################################
                     [1m Learning iteration 1523/2000 [0m                     

                       Computation: 44794 steps/s (collection: 2.104s, learning 0.090s)
             Mean action noise std: 3.38
          Mean value_function loss: 256.5536
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 80.7337
                       Mean reward: 736.82
               Mean episode length: 212.42
    Episode_Reward/reaching_object: 1.7790
     Episode_Reward/lifting_object: 149.8038
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0718
          Episode_Reward/joint_vel: -0.0915
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 2.19s
                      Time elapsed: 00:58:12
                               ETA: 00:18:12

################################################################################
                     [1m Learning iteration 1524/2000 [0m                     

                       Computation: 43625 steps/s (collection: 2.080s, learning 0.174s)
             Mean action noise std: 3.39
          Mean value_function loss: 257.8446
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 80.7469
                       Mean reward: 770.58
               Mean episode length: 218.54
    Episode_Reward/reaching_object: 1.7849
     Episode_Reward/lifting_object: 150.0750
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0720
          Episode_Reward/joint_vel: -0.0930
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 2.25s
                      Time elapsed: 00:58:14
                               ETA: 00:18:10

################################################################################
                     [1m Learning iteration 1525/2000 [0m                     

                       Computation: 44436 steps/s (collection: 2.119s, learning 0.093s)
             Mean action noise std: 3.39
          Mean value_function loss: 286.0329
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 80.7669
                       Mean reward: 780.12
               Mean episode length: 219.27
    Episode_Reward/reaching_object: 1.8283
     Episode_Reward/lifting_object: 154.8283
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0731
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 2.21s
                      Time elapsed: 00:58:16
                               ETA: 00:18:08

################################################################################
                     [1m Learning iteration 1526/2000 [0m                     

                       Computation: 44898 steps/s (collection: 2.101s, learning 0.089s)
             Mean action noise std: 3.39
          Mean value_function loss: 286.8264
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 80.7855
                       Mean reward: 723.91
               Mean episode length: 208.39
    Episode_Reward/reaching_object: 1.7407
     Episode_Reward/lifting_object: 146.5861
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0715
          Episode_Reward/joint_vel: -0.0920
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 2.19s
                      Time elapsed: 00:58:18
                               ETA: 00:18:06

################################################################################
                     [1m Learning iteration 1527/2000 [0m                     

                       Computation: 44630 steps/s (collection: 2.110s, learning 0.093s)
             Mean action noise std: 3.39
          Mean value_function loss: 254.0263
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 80.8068
                       Mean reward: 747.22
               Mean episode length: 211.69
    Episode_Reward/reaching_object: 1.7795
     Episode_Reward/lifting_object: 150.3367
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0724
          Episode_Reward/joint_vel: -0.0936
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 2.20s
                      Time elapsed: 00:58:20
                               ETA: 00:18:03

################################################################################
                     [1m Learning iteration 1528/2000 [0m                     

                       Computation: 44899 steps/s (collection: 2.086s, learning 0.103s)
             Mean action noise std: 3.40
          Mean value_function loss: 311.6695
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 80.8288
                       Mean reward: 713.30
               Mean episode length: 207.20
    Episode_Reward/reaching_object: 1.7306
     Episode_Reward/lifting_object: 145.9233
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0711
          Episode_Reward/joint_vel: -0.0918
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 2.19s
                      Time elapsed: 00:58:23
                               ETA: 00:18:01

################################################################################
                     [1m Learning iteration 1529/2000 [0m                     

                       Computation: 44848 steps/s (collection: 2.076s, learning 0.116s)
             Mean action noise std: 3.40
          Mean value_function loss: 264.6091
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 80.8405
                       Mean reward: 725.95
               Mean episode length: 214.56
    Episode_Reward/reaching_object: 1.7189
     Episode_Reward/lifting_object: 143.3268
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0711
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 2.19s
                      Time elapsed: 00:58:25
                               ETA: 00:17:59

################################################################################
                     [1m Learning iteration 1530/2000 [0m                     

                       Computation: 44877 steps/s (collection: 2.074s, learning 0.116s)
             Mean action noise std: 3.40
          Mean value_function loss: 231.8305
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 80.8619
                       Mean reward: 765.28
               Mean episode length: 219.87
    Episode_Reward/reaching_object: 1.7848
     Episode_Reward/lifting_object: 150.2700
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0732
          Episode_Reward/joint_vel: -0.0942
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 2.19s
                      Time elapsed: 00:58:27
                               ETA: 00:17:56

################################################################################
                     [1m Learning iteration 1531/2000 [0m                     

                       Computation: 43940 steps/s (collection: 2.146s, learning 0.092s)
             Mean action noise std: 3.40
          Mean value_function loss: 287.4264
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 80.8898
                       Mean reward: 763.50
               Mean episode length: 214.46
    Episode_Reward/reaching_object: 1.7477
     Episode_Reward/lifting_object: 147.3477
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0715
          Episode_Reward/joint_vel: -0.0916
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 2.24s
                      Time elapsed: 00:58:29
                               ETA: 00:17:54

################################################################################
                     [1m Learning iteration 1532/2000 [0m                     

                       Computation: 44899 steps/s (collection: 2.102s, learning 0.088s)
             Mean action noise std: 3.41
          Mean value_function loss: 302.5482
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 80.9128
                       Mean reward: 700.34
               Mean episode length: 199.53
    Episode_Reward/reaching_object: 1.7348
     Episode_Reward/lifting_object: 146.2760
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0704
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 2.19s
                      Time elapsed: 00:58:31
                               ETA: 00:17:52

################################################################################
                     [1m Learning iteration 1533/2000 [0m                     

                       Computation: 42114 steps/s (collection: 2.245s, learning 0.089s)
             Mean action noise std: 3.41
          Mean value_function loss: 297.7243
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 80.9343
                       Mean reward: 724.12
               Mean episode length: 206.68
    Episode_Reward/reaching_object: 1.7455
     Episode_Reward/lifting_object: 146.7401
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0720
          Episode_Reward/joint_vel: -0.0930
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 2.33s
                      Time elapsed: 00:58:34
                               ETA: 00:17:49

################################################################################
                     [1m Learning iteration 1534/2000 [0m                     

                       Computation: 45597 steps/s (collection: 2.067s, learning 0.089s)
             Mean action noise std: 3.41
          Mean value_function loss: 271.5304
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 80.9539
                       Mean reward: 739.04
               Mean episode length: 211.03
    Episode_Reward/reaching_object: 1.7990
     Episode_Reward/lifting_object: 151.4023
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0740
          Episode_Reward/joint_vel: -0.0952
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 2.16s
                      Time elapsed: 00:58:36
                               ETA: 00:17:47

################################################################################
                     [1m Learning iteration 1535/2000 [0m                     

                       Computation: 44624 steps/s (collection: 2.100s, learning 0.103s)
             Mean action noise std: 3.41
          Mean value_function loss: 303.3447
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 80.9705
                       Mean reward: 746.22
               Mean episode length: 211.11
    Episode_Reward/reaching_object: 1.7728
     Episode_Reward/lifting_object: 148.8060
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0726
          Episode_Reward/joint_vel: -0.0921
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 2.20s
                      Time elapsed: 00:58:38
                               ETA: 00:17:45

################################################################################
                     [1m Learning iteration 1536/2000 [0m                     

                       Computation: 42911 steps/s (collection: 2.169s, learning 0.122s)
             Mean action noise std: 3.42
          Mean value_function loss: 248.2444
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 80.9827
                       Mean reward: 723.50
               Mean episode length: 210.10
    Episode_Reward/reaching_object: 1.7906
     Episode_Reward/lifting_object: 151.0672
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0728
          Episode_Reward/joint_vel: -0.0920
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 2.29s
                      Time elapsed: 00:58:40
                               ETA: 00:17:42

################################################################################
                     [1m Learning iteration 1537/2000 [0m                     

                       Computation: 43717 steps/s (collection: 2.132s, learning 0.117s)
             Mean action noise std: 3.42
          Mean value_function loss: 277.6985
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 80.9975
                       Mean reward: 727.69
               Mean episode length: 207.87
    Episode_Reward/reaching_object: 1.6925
     Episode_Reward/lifting_object: 141.3421
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0702
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 2.25s
                      Time elapsed: 00:58:43
                               ETA: 00:17:40

################################################################################
                     [1m Learning iteration 1538/2000 [0m                     

                       Computation: 43092 steps/s (collection: 2.182s, learning 0.099s)
             Mean action noise std: 3.42
          Mean value_function loss: 265.0080
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 81.0127
                       Mean reward: 736.04
               Mean episode length: 212.05
    Episode_Reward/reaching_object: 1.8185
     Episode_Reward/lifting_object: 152.6172
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0740
          Episode_Reward/joint_vel: -0.0946
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 2.28s
                      Time elapsed: 00:58:45
                               ETA: 00:17:38

################################################################################
                     [1m Learning iteration 1539/2000 [0m                     

                       Computation: 44472 steps/s (collection: 2.115s, learning 0.095s)
             Mean action noise std: 3.42
          Mean value_function loss: 286.6032
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 81.0250
                       Mean reward: 807.64
               Mean episode length: 223.81
    Episode_Reward/reaching_object: 1.7751
     Episode_Reward/lifting_object: 149.4546
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0736
          Episode_Reward/joint_vel: -0.0937
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 2.21s
                      Time elapsed: 00:58:47
                               ETA: 00:17:36

################################################################################
                     [1m Learning iteration 1540/2000 [0m                     

                       Computation: 44670 steps/s (collection: 2.111s, learning 0.090s)
             Mean action noise std: 3.42
          Mean value_function loss: 267.9055
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 81.0395
                       Mean reward: 754.85
               Mean episode length: 214.13
    Episode_Reward/reaching_object: 1.7826
     Episode_Reward/lifting_object: 149.9021
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0733
          Episode_Reward/joint_vel: -0.0930
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 2.20s
                      Time elapsed: 00:58:49
                               ETA: 00:17:33

################################################################################
                     [1m Learning iteration 1541/2000 [0m                     

                       Computation: 44287 steps/s (collection: 2.112s, learning 0.108s)
             Mean action noise std: 3.42
          Mean value_function loss: 321.7463
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 81.0515
                       Mean reward: 786.63
               Mean episode length: 218.44
    Episode_Reward/reaching_object: 1.7271
     Episode_Reward/lifting_object: 143.9240
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0719
          Episode_Reward/joint_vel: -0.0940
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 2.22s
                      Time elapsed: 00:58:52
                               ETA: 00:17:31

################################################################################
                     [1m Learning iteration 1542/2000 [0m                     

                       Computation: 44616 steps/s (collection: 2.111s, learning 0.093s)
             Mean action noise std: 3.43
          Mean value_function loss: 273.9399
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 81.0704
                       Mean reward: 733.07
               Mean episode length: 212.14
    Episode_Reward/reaching_object: 1.7795
     Episode_Reward/lifting_object: 149.3061
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0740
          Episode_Reward/joint_vel: -0.0940
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 2.20s
                      Time elapsed: 00:58:54
                               ETA: 00:17:29

################################################################################
                     [1m Learning iteration 1543/2000 [0m                     

                       Computation: 43168 steps/s (collection: 2.173s, learning 0.105s)
             Mean action noise std: 3.43
          Mean value_function loss: 297.2311
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 81.0835
                       Mean reward: 792.31
               Mean episode length: 218.93
    Episode_Reward/reaching_object: 1.8183
     Episode_Reward/lifting_object: 152.8080
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0753
          Episode_Reward/joint_vel: -0.0968
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 2.28s
                      Time elapsed: 00:58:56
                               ETA: 00:17:26

################################################################################
                     [1m Learning iteration 1544/2000 [0m                     

                       Computation: 42831 steps/s (collection: 2.193s, learning 0.103s)
             Mean action noise std: 3.43
          Mean value_function loss: 237.9614
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 81.0927
                       Mean reward: 729.53
               Mean episode length: 210.89
    Episode_Reward/reaching_object: 1.7613
     Episode_Reward/lifting_object: 147.8468
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0743
          Episode_Reward/joint_vel: -0.0953
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 2.30s
                      Time elapsed: 00:58:58
                               ETA: 00:17:24

################################################################################
                     [1m Learning iteration 1545/2000 [0m                     

                       Computation: 44497 steps/s (collection: 2.115s, learning 0.094s)
             Mean action noise std: 3.43
          Mean value_function loss: 250.2403
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 81.1042
                       Mean reward: 714.19
               Mean episode length: 207.50
    Episode_Reward/reaching_object: 1.7289
     Episode_Reward/lifting_object: 144.6084
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0733
          Episode_Reward/joint_vel: -0.0943
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 2.21s
                      Time elapsed: 00:59:01
                               ETA: 00:17:22

################################################################################
                     [1m Learning iteration 1546/2000 [0m                     

                       Computation: 44626 steps/s (collection: 2.112s, learning 0.091s)
             Mean action noise std: 3.43
          Mean value_function loss: 252.3009
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 81.1182
                       Mean reward: 706.69
               Mean episode length: 201.36
    Episode_Reward/reaching_object: 1.7748
     Episode_Reward/lifting_object: 148.5808
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0738
          Episode_Reward/joint_vel: -0.0943
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 2.20s
                      Time elapsed: 00:59:03
                               ETA: 00:17:19

################################################################################
                     [1m Learning iteration 1547/2000 [0m                     

                       Computation: 45069 steps/s (collection: 2.090s, learning 0.092s)
             Mean action noise std: 3.43
          Mean value_function loss: 267.5544
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 81.1340
                       Mean reward: 693.88
               Mean episode length: 203.49
    Episode_Reward/reaching_object: 1.7558
     Episode_Reward/lifting_object: 146.3144
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0731
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 2.18s
                      Time elapsed: 00:59:05
                               ETA: 00:17:17

################################################################################
                     [1m Learning iteration 1548/2000 [0m                     

                       Computation: 45290 steps/s (collection: 2.077s, learning 0.094s)
             Mean action noise std: 3.44
          Mean value_function loss: 251.3807
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 81.1549
                       Mean reward: 764.27
               Mean episode length: 219.38
    Episode_Reward/reaching_object: 1.7717
     Episode_Reward/lifting_object: 148.0771
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0756
          Episode_Reward/joint_vel: -0.0965
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 2.17s
                      Time elapsed: 00:59:07
                               ETA: 00:17:15

################################################################################
                     [1m Learning iteration 1549/2000 [0m                     

                       Computation: 45041 steps/s (collection: 2.084s, learning 0.099s)
             Mean action noise std: 3.44
          Mean value_function loss: 314.8781
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 81.1788
                       Mean reward: 781.35
               Mean episode length: 218.62
    Episode_Reward/reaching_object: 1.7617
     Episode_Reward/lifting_object: 146.4305
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0742
          Episode_Reward/joint_vel: -0.0950
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 2.18s
                      Time elapsed: 00:59:09
                               ETA: 00:17:12

################################################################################
                     [1m Learning iteration 1550/2000 [0m                     

                       Computation: 45374 steps/s (collection: 2.070s, learning 0.097s)
             Mean action noise std: 3.44
          Mean value_function loss: 337.7817
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 81.2019
                       Mean reward: 717.07
               Mean episode length: 207.68
    Episode_Reward/reaching_object: 1.7022
     Episode_Reward/lifting_object: 141.1636
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0724
          Episode_Reward/joint_vel: -0.0925
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 2.17s
                      Time elapsed: 00:59:11
                               ETA: 00:17:10

################################################################################
                     [1m Learning iteration 1551/2000 [0m                     

                       Computation: 44990 steps/s (collection: 2.086s, learning 0.099s)
             Mean action noise std: 3.44
          Mean value_function loss: 271.2194
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 81.2167
                       Mean reward: 717.14
               Mean episode length: 211.53
    Episode_Reward/reaching_object: 1.7207
     Episode_Reward/lifting_object: 143.6427
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0731
          Episode_Reward/joint_vel: -0.0926
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 2.18s
                      Time elapsed: 00:59:14
                               ETA: 00:17:08

################################################################################
                     [1m Learning iteration 1552/2000 [0m                     

                       Computation: 44586 steps/s (collection: 2.098s, learning 0.107s)
             Mean action noise std: 3.45
          Mean value_function loss: 351.6064
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 81.2326
                       Mean reward: 715.64
               Mean episode length: 204.65
    Episode_Reward/reaching_object: 1.7595
     Episode_Reward/lifting_object: 146.7578
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0732
          Episode_Reward/joint_vel: -0.0924
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 2.20s
                      Time elapsed: 00:59:16
                               ETA: 00:17:05

################################################################################
                     [1m Learning iteration 1553/2000 [0m                     

                       Computation: 44347 steps/s (collection: 2.119s, learning 0.098s)
             Mean action noise std: 3.45
          Mean value_function loss: 310.1948
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 81.2439
                       Mean reward: 676.60
               Mean episode length: 199.70
    Episode_Reward/reaching_object: 1.7442
     Episode_Reward/lifting_object: 144.8672
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0734
          Episode_Reward/joint_vel: -0.0932
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 2.22s
                      Time elapsed: 00:59:18
                               ETA: 00:17:03

################################################################################
                     [1m Learning iteration 1554/2000 [0m                     

                       Computation: 44616 steps/s (collection: 2.105s, learning 0.098s)
             Mean action noise std: 3.45
          Mean value_function loss: 312.7944
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 81.2622
                       Mean reward: 752.20
               Mean episode length: 216.74
    Episode_Reward/reaching_object: 1.8098
     Episode_Reward/lifting_object: 151.9827
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0747
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 2.20s
                      Time elapsed: 00:59:20
                               ETA: 00:17:01

################################################################################
                     [1m Learning iteration 1555/2000 [0m                     

                       Computation: 43970 steps/s (collection: 2.099s, learning 0.137s)
             Mean action noise std: 3.45
          Mean value_function loss: 312.7760
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 81.2788
                       Mean reward: 731.97
               Mean episode length: 213.00
    Episode_Reward/reaching_object: 1.6629
     Episode_Reward/lifting_object: 138.3648
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0719
          Episode_Reward/joint_vel: -0.0927
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 2.24s
                      Time elapsed: 00:59:23
                               ETA: 00:16:58

################################################################################
                     [1m Learning iteration 1556/2000 [0m                     

                       Computation: 43681 steps/s (collection: 2.129s, learning 0.122s)
             Mean action noise std: 3.45
          Mean value_function loss: 304.2843
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 81.2868
                       Mean reward: 721.42
               Mean episode length: 208.85
    Episode_Reward/reaching_object: 1.7450
     Episode_Reward/lifting_object: 146.4238
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0926
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 2.25s
                      Time elapsed: 00:59:25
                               ETA: 00:16:56

################################################################################
                     [1m Learning iteration 1557/2000 [0m                     

                       Computation: 43998 steps/s (collection: 2.141s, learning 0.094s)
             Mean action noise std: 3.45
          Mean value_function loss: 229.8477
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 81.3009
                       Mean reward: 766.70
               Mean episode length: 216.54
    Episode_Reward/reaching_object: 1.7529
     Episode_Reward/lifting_object: 147.0244
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0747
          Episode_Reward/joint_vel: -0.0948
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 2.23s
                      Time elapsed: 00:59:27
                               ETA: 00:16:54

################################################################################
                     [1m Learning iteration 1558/2000 [0m                     

                       Computation: 41990 steps/s (collection: 2.154s, learning 0.187s)
             Mean action noise std: 3.46
          Mean value_function loss: 239.9185
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 81.3181
                       Mean reward: 756.15
               Mean episode length: 215.00
    Episode_Reward/reaching_object: 1.7907
     Episode_Reward/lifting_object: 150.9560
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0760
          Episode_Reward/joint_vel: -0.0952
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 2.34s
                      Time elapsed: 00:59:29
                               ETA: 00:16:52

################################################################################
                     [1m Learning iteration 1559/2000 [0m                     

                       Computation: 44307 steps/s (collection: 2.124s, learning 0.095s)
             Mean action noise std: 3.46
          Mean value_function loss: 259.2760
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 81.3360
                       Mean reward: 724.81
               Mean episode length: 211.78
    Episode_Reward/reaching_object: 1.8115
     Episode_Reward/lifting_object: 153.0485
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0765
          Episode_Reward/joint_vel: -0.0958
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 2.22s
                      Time elapsed: 00:59:32
                               ETA: 00:16:49

################################################################################
                     [1m Learning iteration 1560/2000 [0m                     

                       Computation: 43521 steps/s (collection: 2.162s, learning 0.097s)
             Mean action noise std: 3.46
          Mean value_function loss: 276.9220
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 81.3550
                       Mean reward: 756.27
               Mean episode length: 214.29
    Episode_Reward/reaching_object: 1.7670
     Episode_Reward/lifting_object: 148.8331
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0752
          Episode_Reward/joint_vel: -0.0952
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 2.26s
                      Time elapsed: 00:59:34
                               ETA: 00:16:47

################################################################################
                     [1m Learning iteration 1561/2000 [0m                     

                       Computation: 44732 steps/s (collection: 2.097s, learning 0.101s)
             Mean action noise std: 3.46
          Mean value_function loss: 225.2237
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 81.3755
                       Mean reward: 801.28
               Mean episode length: 225.18
    Episode_Reward/reaching_object: 1.8333
     Episode_Reward/lifting_object: 154.9404
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0779
          Episode_Reward/joint_vel: -0.0979
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 2.20s
                      Time elapsed: 00:59:36
                               ETA: 00:16:45

################################################################################
                     [1m Learning iteration 1562/2000 [0m                     

                       Computation: 44646 steps/s (collection: 2.107s, learning 0.095s)
             Mean action noise std: 3.47
          Mean value_function loss: 285.0600
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 81.3895
                       Mean reward: 769.12
               Mean episode length: 217.89
    Episode_Reward/reaching_object: 1.7632
     Episode_Reward/lifting_object: 149.1930
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0754
          Episode_Reward/joint_vel: -0.0951
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 2.20s
                      Time elapsed: 00:59:38
                               ETA: 00:16:42

################################################################################
                     [1m Learning iteration 1563/2000 [0m                     

                       Computation: 42248 steps/s (collection: 2.183s, learning 0.144s)
             Mean action noise std: 3.47
          Mean value_function loss: 266.5682
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 81.4135
                       Mean reward: 794.18
               Mean episode length: 226.56
    Episode_Reward/reaching_object: 1.8006
     Episode_Reward/lifting_object: 151.5584
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0766
          Episode_Reward/joint_vel: -0.0960
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 2.33s
                      Time elapsed: 00:59:41
                               ETA: 00:16:40

################################################################################
                     [1m Learning iteration 1564/2000 [0m                     

                       Computation: 43957 steps/s (collection: 2.119s, learning 0.118s)
             Mean action noise std: 3.47
          Mean value_function loss: 234.5882
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 81.4369
                       Mean reward: 788.21
               Mean episode length: 225.75
    Episode_Reward/reaching_object: 1.8072
     Episode_Reward/lifting_object: 152.3262
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0769
          Episode_Reward/joint_vel: -0.0979
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 2.24s
                      Time elapsed: 00:59:43
                               ETA: 00:16:38

################################################################################
                     [1m Learning iteration 1565/2000 [0m                     

                       Computation: 44506 steps/s (collection: 2.108s, learning 0.101s)
             Mean action noise std: 3.47
          Mean value_function loss: 247.4628
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 81.4526
                       Mean reward: 760.73
               Mean episode length: 219.30
    Episode_Reward/reaching_object: 1.7384
     Episode_Reward/lifting_object: 144.8646
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0756
          Episode_Reward/joint_vel: -0.0977
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 2.21s
                      Time elapsed: 00:59:45
                               ETA: 00:16:35

################################################################################
                     [1m Learning iteration 1566/2000 [0m                     

                       Computation: 44062 steps/s (collection: 2.125s, learning 0.107s)
             Mean action noise std: 3.48
          Mean value_function loss: 230.4368
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 81.4666
                       Mean reward: 718.11
               Mean episode length: 207.43
    Episode_Reward/reaching_object: 1.7232
     Episode_Reward/lifting_object: 143.3236
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0745
          Episode_Reward/joint_vel: -0.0961
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 2.23s
                      Time elapsed: 00:59:47
                               ETA: 00:16:33

################################################################################
                     [1m Learning iteration 1567/2000 [0m                     

                       Computation: 43057 steps/s (collection: 2.128s, learning 0.156s)
             Mean action noise std: 3.48
          Mean value_function loss: 264.7615
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 81.4924
                       Mean reward: 777.49
               Mean episode length: 221.44
    Episode_Reward/reaching_object: 1.7927
     Episode_Reward/lifting_object: 150.8835
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0763
          Episode_Reward/joint_vel: -0.0958
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 2.28s
                      Time elapsed: 00:59:49
                               ETA: 00:16:31

################################################################################
                     [1m Learning iteration 1568/2000 [0m                     

                       Computation: 43476 steps/s (collection: 2.170s, learning 0.092s)
             Mean action noise std: 3.48
          Mean value_function loss: 220.5206
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 81.5200
                       Mean reward: 737.44
               Mean episode length: 214.11
    Episode_Reward/reaching_object: 1.7490
     Episode_Reward/lifting_object: 146.6405
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0748
          Episode_Reward/joint_vel: -0.0945
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 2.26s
                      Time elapsed: 00:59:52
                               ETA: 00:16:29

################################################################################
                     [1m Learning iteration 1569/2000 [0m                     

                       Computation: 43659 steps/s (collection: 2.144s, learning 0.108s)
             Mean action noise std: 3.48
          Mean value_function loss: 228.3092
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 81.5372
                       Mean reward: 756.22
               Mean episode length: 217.23
    Episode_Reward/reaching_object: 1.7796
     Episode_Reward/lifting_object: 149.4860
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0762
          Episode_Reward/joint_vel: -0.0953
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 2.25s
                      Time elapsed: 00:59:54
                               ETA: 00:16:26

################################################################################
                     [1m Learning iteration 1570/2000 [0m                     

                       Computation: 43058 steps/s (collection: 2.184s, learning 0.099s)
             Mean action noise std: 3.49
          Mean value_function loss: 273.2162
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 81.5584
                       Mean reward: 708.72
               Mean episode length: 205.11
    Episode_Reward/reaching_object: 1.7482
     Episode_Reward/lifting_object: 146.5994
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0757
          Episode_Reward/joint_vel: -0.0954
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 2.28s
                      Time elapsed: 00:59:56
                               ETA: 00:16:24

################################################################################
                     [1m Learning iteration 1571/2000 [0m                     

                       Computation: 43196 steps/s (collection: 2.159s, learning 0.117s)
             Mean action noise std: 3.49
          Mean value_function loss: 262.9174
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 81.5794
                       Mean reward: 756.93
               Mean episode length: 214.31
    Episode_Reward/reaching_object: 1.7469
     Episode_Reward/lifting_object: 145.8933
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0755
          Episode_Reward/joint_vel: -0.0943
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 2.28s
                      Time elapsed: 00:59:59
                               ETA: 00:16:22

################################################################################
                     [1m Learning iteration 1572/2000 [0m                     

                       Computation: 43851 steps/s (collection: 2.134s, learning 0.108s)
             Mean action noise std: 3.49
          Mean value_function loss: 257.2256
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 81.5970
                       Mean reward: 769.57
               Mean episode length: 220.67
    Episode_Reward/reaching_object: 1.7926
     Episode_Reward/lifting_object: 149.9955
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0766
          Episode_Reward/joint_vel: -0.0964
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 2.24s
                      Time elapsed: 01:00:01
                               ETA: 00:16:19

################################################################################
                     [1m Learning iteration 1573/2000 [0m                     

                       Computation: 43667 steps/s (collection: 2.147s, learning 0.104s)
             Mean action noise std: 3.49
          Mean value_function loss: 280.9849
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 81.6176
                       Mean reward: 745.64
               Mean episode length: 209.66
    Episode_Reward/reaching_object: 1.7905
     Episode_Reward/lifting_object: 150.8017
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0759
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 2.25s
                      Time elapsed: 01:00:03
                               ETA: 00:16:17

################################################################################
                     [1m Learning iteration 1574/2000 [0m                     

                       Computation: 43805 steps/s (collection: 2.134s, learning 0.110s)
             Mean action noise std: 3.49
          Mean value_function loss: 287.6737
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 81.6262
                       Mean reward: 745.87
               Mean episode length: 214.94
    Episode_Reward/reaching_object: 1.7574
     Episode_Reward/lifting_object: 147.8253
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0752
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 2.24s
                      Time elapsed: 01:00:05
                               ETA: 00:16:15

################################################################################
                     [1m Learning iteration 1575/2000 [0m                     

                       Computation: 43218 steps/s (collection: 2.150s, learning 0.125s)
             Mean action noise std: 3.49
          Mean value_function loss: 281.3141
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 81.6323
                       Mean reward: 730.20
               Mean episode length: 210.94
    Episode_Reward/reaching_object: 1.6943
     Episode_Reward/lifting_object: 141.9492
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0728
          Episode_Reward/joint_vel: -0.0908
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 2.27s
                      Time elapsed: 01:00:08
                               ETA: 00:16:12

################################################################################
                     [1m Learning iteration 1576/2000 [0m                     

                       Computation: 44370 steps/s (collection: 2.123s, learning 0.093s)
             Mean action noise std: 3.50
          Mean value_function loss: 291.9485
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 81.6417
                       Mean reward: 749.80
               Mean episode length: 214.51
    Episode_Reward/reaching_object: 1.7108
     Episode_Reward/lifting_object: 143.6190
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0744
          Episode_Reward/joint_vel: -0.0942
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 2.22s
                      Time elapsed: 01:00:10
                               ETA: 00:16:10

################################################################################
                     [1m Learning iteration 1577/2000 [0m                     

                       Computation: 44050 steps/s (collection: 2.134s, learning 0.098s)
             Mean action noise std: 3.50
          Mean value_function loss: 255.7474
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 81.6537
                       Mean reward: 756.26
               Mean episode length: 213.38
    Episode_Reward/reaching_object: 1.7475
     Episode_Reward/lifting_object: 147.1880
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0750
          Episode_Reward/joint_vel: -0.0921
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 2.23s
                      Time elapsed: 01:00:12
                               ETA: 00:16:08

################################################################################
                     [1m Learning iteration 1578/2000 [0m                     

                       Computation: 44217 steps/s (collection: 2.129s, learning 0.094s)
             Mean action noise std: 3.50
          Mean value_function loss: 257.6375
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 81.6621
                       Mean reward: 762.10
               Mean episode length: 216.64
    Episode_Reward/reaching_object: 1.7656
     Episode_Reward/lifting_object: 149.0923
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0764
          Episode_Reward/joint_vel: -0.0955
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 2.22s
                      Time elapsed: 01:00:14
                               ETA: 00:16:06

################################################################################
                     [1m Learning iteration 1579/2000 [0m                     

                       Computation: 43952 steps/s (collection: 2.123s, learning 0.113s)
             Mean action noise std: 3.50
          Mean value_function loss: 312.7595
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 81.6734
                       Mean reward: 731.40
               Mean episode length: 211.26
    Episode_Reward/reaching_object: 1.7194
     Episode_Reward/lifting_object: 144.3318
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0749
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 2.24s
                      Time elapsed: 01:00:16
                               ETA: 00:16:03

################################################################################
                     [1m Learning iteration 1580/2000 [0m                     

                       Computation: 43186 steps/s (collection: 2.172s, learning 0.105s)
             Mean action noise std: 3.50
          Mean value_function loss: 255.9720
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 81.6861
                       Mean reward: 743.30
               Mean episode length: 211.30
    Episode_Reward/reaching_object: 1.7426
     Episode_Reward/lifting_object: 147.0301
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0760
          Episode_Reward/joint_vel: -0.0947
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 2.28s
                      Time elapsed: 01:00:19
                               ETA: 00:16:01

################################################################################
                     [1m Learning iteration 1581/2000 [0m                     

                       Computation: 44179 steps/s (collection: 2.122s, learning 0.104s)
             Mean action noise std: 3.50
          Mean value_function loss: 248.7334
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 81.6994
                       Mean reward: 737.46
               Mean episode length: 214.28
    Episode_Reward/reaching_object: 1.8029
     Episode_Reward/lifting_object: 152.5061
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0781
          Episode_Reward/joint_vel: -0.0954
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 155516928
                    Iteration time: 2.23s
                      Time elapsed: 01:00:21
                               ETA: 00:15:59

################################################################################
                     [1m Learning iteration 1582/2000 [0m                     

                       Computation: 44232 steps/s (collection: 2.117s, learning 0.105s)
             Mean action noise std: 3.50
          Mean value_function loss: 248.1521
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 81.7074
                       Mean reward: 761.11
               Mean episode length: 219.95
    Episode_Reward/reaching_object: 1.7915
     Episode_Reward/lifting_object: 150.5190
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0776
          Episode_Reward/joint_vel: -0.0956
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 155615232
                    Iteration time: 2.22s
                      Time elapsed: 01:00:23
                               ETA: 00:15:56

################################################################################
                     [1m Learning iteration 1583/2000 [0m                     

                       Computation: 43429 steps/s (collection: 2.115s, learning 0.149s)
             Mean action noise std: 3.51
          Mean value_function loss: 276.8260
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 81.7178
                       Mean reward: 774.28
               Mean episode length: 217.66
    Episode_Reward/reaching_object: 1.7839
     Episode_Reward/lifting_object: 151.2558
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0779
          Episode_Reward/joint_vel: -0.0964
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 155713536
                    Iteration time: 2.26s
                      Time elapsed: 01:00:25
                               ETA: 00:15:54

################################################################################
                     [1m Learning iteration 1584/2000 [0m                     

                       Computation: 44116 steps/s (collection: 2.117s, learning 0.111s)
             Mean action noise std: 3.51
          Mean value_function loss: 286.9905
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 81.7373
                       Mean reward: 736.25
               Mean episode length: 208.58
    Episode_Reward/reaching_object: 1.7576
     Episode_Reward/lifting_object: 148.8824
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0766
          Episode_Reward/joint_vel: -0.0944
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 155811840
                    Iteration time: 2.23s
                      Time elapsed: 01:00:28
                               ETA: 00:15:52

################################################################################
                     [1m Learning iteration 1585/2000 [0m                     

                       Computation: 44268 steps/s (collection: 2.124s, learning 0.097s)
             Mean action noise std: 3.51
          Mean value_function loss: 299.5345
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 81.7520
                       Mean reward: 747.93
               Mean episode length: 211.18
    Episode_Reward/reaching_object: 1.7179
     Episode_Reward/lifting_object: 145.0082
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0750
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 155910144
                    Iteration time: 2.22s
                      Time elapsed: 01:00:30
                               ETA: 00:15:49

################################################################################
                     [1m Learning iteration 1586/2000 [0m                     

                       Computation: 43566 steps/s (collection: 2.151s, learning 0.106s)
             Mean action noise std: 3.51
          Mean value_function loss: 236.3635
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 81.7675
                       Mean reward: 730.19
               Mean episode length: 210.61
    Episode_Reward/reaching_object: 1.7931
     Episode_Reward/lifting_object: 151.6652
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0776
          Episode_Reward/joint_vel: -0.0949
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 156008448
                    Iteration time: 2.26s
                      Time elapsed: 01:00:32
                               ETA: 00:15:47

################################################################################
                     [1m Learning iteration 1587/2000 [0m                     

                       Computation: 43044 steps/s (collection: 2.171s, learning 0.112s)
             Mean action noise std: 3.51
          Mean value_function loss: 252.3274
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 81.7802
                       Mean reward: 747.92
               Mean episode length: 215.48
    Episode_Reward/reaching_object: 1.7413
     Episode_Reward/lifting_object: 146.8109
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0765
          Episode_Reward/joint_vel: -0.0935
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 156106752
                    Iteration time: 2.28s
                      Time elapsed: 01:00:34
                               ETA: 00:15:45

################################################################################
                     [1m Learning iteration 1588/2000 [0m                     

                       Computation: 43221 steps/s (collection: 2.163s, learning 0.112s)
             Mean action noise std: 3.51
          Mean value_function loss: 250.1030
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 81.7901
                       Mean reward: 741.11
               Mean episode length: 211.61
    Episode_Reward/reaching_object: 1.7243
     Episode_Reward/lifting_object: 144.8295
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0758
          Episode_Reward/joint_vel: -0.0936
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 156205056
                    Iteration time: 2.27s
                      Time elapsed: 01:00:37
                               ETA: 00:15:43

################################################################################
                     [1m Learning iteration 1589/2000 [0m                     

                       Computation: 43081 steps/s (collection: 2.176s, learning 0.106s)
             Mean action noise std: 3.52
          Mean value_function loss: 287.6968
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 81.7985
                       Mean reward: 743.26
               Mean episode length: 212.59
    Episode_Reward/reaching_object: 1.7393
     Episode_Reward/lifting_object: 146.5549
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0767
          Episode_Reward/joint_vel: -0.0957
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 156303360
                    Iteration time: 2.28s
                      Time elapsed: 01:00:39
                               ETA: 00:15:40

################################################################################
                     [1m Learning iteration 1590/2000 [0m                     

                       Computation: 43375 steps/s (collection: 2.166s, learning 0.100s)
             Mean action noise std: 3.52
          Mean value_function loss: 253.2603
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 81.8121
                       Mean reward: 794.56
               Mean episode length: 224.91
    Episode_Reward/reaching_object: 1.7578
     Episode_Reward/lifting_object: 148.5565
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0773
          Episode_Reward/joint_vel: -0.0939
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 156401664
                    Iteration time: 2.27s
                      Time elapsed: 01:00:41
                               ETA: 00:15:38

################################################################################
                     [1m Learning iteration 1591/2000 [0m                     

                       Computation: 43044 steps/s (collection: 2.153s, learning 0.131s)
             Mean action noise std: 3.52
          Mean value_function loss: 265.4094
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 81.8266
                       Mean reward: 761.39
               Mean episode length: 215.67
    Episode_Reward/reaching_object: 1.7160
     Episode_Reward/lifting_object: 144.7566
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0755
          Episode_Reward/joint_vel: -0.0930
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 156499968
                    Iteration time: 2.28s
                      Time elapsed: 01:00:44
                               ETA: 00:15:36

################################################################################
                     [1m Learning iteration 1592/2000 [0m                     

                       Computation: 43381 steps/s (collection: 2.159s, learning 0.107s)
             Mean action noise std: 3.52
          Mean value_function loss: 256.4791
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 81.8336
                       Mean reward: 814.90
               Mean episode length: 229.36
    Episode_Reward/reaching_object: 1.7879
     Episode_Reward/lifting_object: 151.3879
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0781
          Episode_Reward/joint_vel: -0.0952
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 156598272
                    Iteration time: 2.27s
                      Time elapsed: 01:00:46
                               ETA: 00:15:33

################################################################################
                     [1m Learning iteration 1593/2000 [0m                     

                       Computation: 44332 steps/s (collection: 2.121s, learning 0.096s)
             Mean action noise std: 3.52
          Mean value_function loss: 283.8771
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 81.8443
                       Mean reward: 721.99
               Mean episode length: 205.92
    Episode_Reward/reaching_object: 1.7244
     Episode_Reward/lifting_object: 146.0044
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0757
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 156696576
                    Iteration time: 2.22s
                      Time elapsed: 01:00:48
                               ETA: 00:15:31

################################################################################
                     [1m Learning iteration 1594/2000 [0m                     

                       Computation: 44055 steps/s (collection: 2.137s, learning 0.094s)
             Mean action noise std: 3.52
          Mean value_function loss: 263.7970
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 81.8581
                       Mean reward: 690.84
               Mean episode length: 201.55
    Episode_Reward/reaching_object: 1.7780
     Episode_Reward/lifting_object: 150.6584
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0778
          Episode_Reward/joint_vel: -0.0957
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 156794880
                    Iteration time: 2.23s
                      Time elapsed: 01:00:50
                               ETA: 00:15:29

################################################################################
                     [1m Learning iteration 1595/2000 [0m                     

                       Computation: 42951 steps/s (collection: 2.159s, learning 0.130s)
             Mean action noise std: 3.53
          Mean value_function loss: 273.7623
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 81.8719
                       Mean reward: 682.67
               Mean episode length: 202.37
    Episode_Reward/reaching_object: 1.7500
     Episode_Reward/lifting_object: 147.9050
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0780
          Episode_Reward/joint_vel: -0.0972
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 156893184
                    Iteration time: 2.29s
                      Time elapsed: 01:00:53
                               ETA: 00:15:26

################################################################################
                     [1m Learning iteration 1596/2000 [0m                     

                       Computation: 42205 steps/s (collection: 2.193s, learning 0.136s)
             Mean action noise std: 3.53
          Mean value_function loss: 283.6576
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 81.8855
                       Mean reward: 770.13
               Mean episode length: 218.92
    Episode_Reward/reaching_object: 1.7662
     Episode_Reward/lifting_object: 149.4578
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0781
          Episode_Reward/joint_vel: -0.0969
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 156991488
                    Iteration time: 2.33s
                      Time elapsed: 01:00:55
                               ETA: 00:15:24

################################################################################
                     [1m Learning iteration 1597/2000 [0m                     

                       Computation: 42801 steps/s (collection: 2.175s, learning 0.122s)
             Mean action noise std: 3.53
          Mean value_function loss: 241.7635
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 81.8964
                       Mean reward: 672.71
               Mean episode length: 199.87
    Episode_Reward/reaching_object: 1.7065
     Episode_Reward/lifting_object: 143.9842
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0768
          Episode_Reward/joint_vel: -0.0960
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 157089792
                    Iteration time: 2.30s
                      Time elapsed: 01:00:57
                               ETA: 00:15:22

################################################################################
                     [1m Learning iteration 1598/2000 [0m                     

                       Computation: 43732 steps/s (collection: 2.153s, learning 0.095s)
             Mean action noise std: 3.53
          Mean value_function loss: 255.2516
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 81.9026
                       Mean reward: 732.26
               Mean episode length: 215.79
    Episode_Reward/reaching_object: 1.7548
     Episode_Reward/lifting_object: 148.5791
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0785
          Episode_Reward/joint_vel: -0.0976
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 157188096
                    Iteration time: 2.25s
                      Time elapsed: 01:00:59
                               ETA: 00:15:20

################################################################################
                     [1m Learning iteration 1599/2000 [0m                     

                       Computation: 44308 steps/s (collection: 2.122s, learning 0.096s)
             Mean action noise std: 3.53
          Mean value_function loss: 258.9674
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 81.9103
                       Mean reward: 741.36
               Mean episode length: 213.56
    Episode_Reward/reaching_object: 1.7426
     Episode_Reward/lifting_object: 147.4559
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0775
          Episode_Reward/joint_vel: -0.0967
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 157286400
                    Iteration time: 2.22s
                      Time elapsed: 01:01:02
                               ETA: 00:15:17

################################################################################
                     [1m Learning iteration 1600/2000 [0m                     

                       Computation: 42209 steps/s (collection: 2.174s, learning 0.155s)
             Mean action noise std: 3.53
          Mean value_function loss: 265.1535
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 81.9155
                       Mean reward: 721.81
               Mean episode length: 210.37
    Episode_Reward/reaching_object: 1.7459
     Episode_Reward/lifting_object: 147.8342
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0775
          Episode_Reward/joint_vel: -0.0959
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 157384704
                    Iteration time: 2.33s
                      Time elapsed: 01:01:04
                               ETA: 00:15:15

################################################################################
                     [1m Learning iteration 1601/2000 [0m                     

                       Computation: 42283 steps/s (collection: 2.201s, learning 0.124s)
             Mean action noise std: 3.53
          Mean value_function loss: 314.9616
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 81.9253
                       Mean reward: 722.19
               Mean episode length: 210.98
    Episode_Reward/reaching_object: 1.7179
     Episode_Reward/lifting_object: 144.6160
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0773
          Episode_Reward/joint_vel: -0.0971
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 157483008
                    Iteration time: 2.32s
                      Time elapsed: 01:01:06
                               ETA: 00:15:13

################################################################################
                     [1m Learning iteration 1602/2000 [0m                     

                       Computation: 43505 steps/s (collection: 2.160s, learning 0.100s)
             Mean action noise std: 3.54
          Mean value_function loss: 214.9960
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 81.9412
                       Mean reward: 736.45
               Mean episode length: 210.89
    Episode_Reward/reaching_object: 1.7650
     Episode_Reward/lifting_object: 149.8202
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0787
          Episode_Reward/joint_vel: -0.0966
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 157581312
                    Iteration time: 2.26s
                      Time elapsed: 01:01:09
                               ETA: 00:15:10

################################################################################
                     [1m Learning iteration 1603/2000 [0m                     

                       Computation: 43596 steps/s (collection: 2.148s, learning 0.107s)
             Mean action noise std: 3.54
          Mean value_function loss: 233.8009
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 81.9589
                       Mean reward: 706.63
               Mean episode length: 212.73
    Episode_Reward/reaching_object: 1.7620
     Episode_Reward/lifting_object: 149.5274
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0787
          Episode_Reward/joint_vel: -0.0976
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 157679616
                    Iteration time: 2.25s
                      Time elapsed: 01:01:11
                               ETA: 00:15:08

################################################################################
                     [1m Learning iteration 1604/2000 [0m                     

                       Computation: 42728 steps/s (collection: 2.182s, learning 0.119s)
             Mean action noise std: 3.54
          Mean value_function loss: 234.4006
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 81.9704
                       Mean reward: 779.18
               Mean episode length: 222.76
    Episode_Reward/reaching_object: 1.7523
     Episode_Reward/lifting_object: 147.8671
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0788
          Episode_Reward/joint_vel: -0.0969
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 157777920
                    Iteration time: 2.30s
                      Time elapsed: 01:01:13
                               ETA: 00:15:06

################################################################################
                     [1m Learning iteration 1605/2000 [0m                     

                       Computation: 44107 steps/s (collection: 2.119s, learning 0.110s)
             Mean action noise std: 3.54
          Mean value_function loss: 245.9116
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 81.9850
                       Mean reward: 734.26
               Mean episode length: 208.29
    Episode_Reward/reaching_object: 1.7976
     Episode_Reward/lifting_object: 152.7202
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0793
          Episode_Reward/joint_vel: -0.0969
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 157876224
                    Iteration time: 2.23s
                      Time elapsed: 01:01:15
                               ETA: 00:15:04

################################################################################
                     [1m Learning iteration 1606/2000 [0m                     

                       Computation: 42664 steps/s (collection: 2.194s, learning 0.111s)
             Mean action noise std: 3.54
          Mean value_function loss: 256.8891
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 81.9927
                       Mean reward: 771.18
               Mean episode length: 221.55
    Episode_Reward/reaching_object: 1.7124
     Episode_Reward/lifting_object: 144.3723
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0777
          Episode_Reward/joint_vel: -0.0958
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 157974528
                    Iteration time: 2.30s
                      Time elapsed: 01:01:18
                               ETA: 00:15:01

################################################################################
                     [1m Learning iteration 1607/2000 [0m                     

                       Computation: 42857 steps/s (collection: 2.133s, learning 0.161s)
             Mean action noise std: 3.54
          Mean value_function loss: 299.8676
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 82.0032
                       Mean reward: 750.67
               Mean episode length: 211.74
    Episode_Reward/reaching_object: 1.7427
     Episode_Reward/lifting_object: 147.2291
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0769
          Episode_Reward/joint_vel: -0.0943
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 158072832
                    Iteration time: 2.29s
                      Time elapsed: 01:01:20
                               ETA: 00:14:59

################################################################################
                     [1m Learning iteration 1608/2000 [0m                     

                       Computation: 43066 steps/s (collection: 2.176s, learning 0.107s)
             Mean action noise std: 3.54
          Mean value_function loss: 268.3729
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 82.0122
                       Mean reward: 734.17
               Mean episode length: 212.82
    Episode_Reward/reaching_object: 1.7587
     Episode_Reward/lifting_object: 148.1228
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0788
          Episode_Reward/joint_vel: -0.0958
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 158171136
                    Iteration time: 2.28s
                      Time elapsed: 01:01:22
                               ETA: 00:14:57

################################################################################
                     [1m Learning iteration 1609/2000 [0m                     

                       Computation: 43919 steps/s (collection: 2.130s, learning 0.109s)
             Mean action noise std: 3.55
          Mean value_function loss: 279.5463
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 82.0299
                       Mean reward: 703.43
               Mean episode length: 204.46
    Episode_Reward/reaching_object: 1.7489
     Episode_Reward/lifting_object: 146.6900
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0779
          Episode_Reward/joint_vel: -0.0950
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 158269440
                    Iteration time: 2.24s
                      Time elapsed: 01:01:24
                               ETA: 00:14:54

################################################################################
                     [1m Learning iteration 1610/2000 [0m                     

                       Computation: 43197 steps/s (collection: 2.153s, learning 0.123s)
             Mean action noise std: 3.55
          Mean value_function loss: 223.7083
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 82.0451
                       Mean reward: 752.23
               Mean episode length: 212.97
    Episode_Reward/reaching_object: 1.7831
     Episode_Reward/lifting_object: 149.9642
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0788
          Episode_Reward/joint_vel: -0.0950
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 158367744
                    Iteration time: 2.28s
                      Time elapsed: 01:01:27
                               ETA: 00:14:52

################################################################################
                     [1m Learning iteration 1611/2000 [0m                     

                       Computation: 43309 steps/s (collection: 2.169s, learning 0.101s)
             Mean action noise std: 3.55
          Mean value_function loss: 238.3258
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 82.0578
                       Mean reward: 776.11
               Mean episode length: 220.13
    Episode_Reward/reaching_object: 1.7628
     Episode_Reward/lifting_object: 148.1382
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0789
          Episode_Reward/joint_vel: -0.0956
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 158466048
                    Iteration time: 2.27s
                      Time elapsed: 01:01:29
                               ETA: 00:14:50

################################################################################
                     [1m Learning iteration 1612/2000 [0m                     

                       Computation: 43425 steps/s (collection: 2.148s, learning 0.116s)
             Mean action noise std: 3.55
          Mean value_function loss: 230.8934
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 82.0746
                       Mean reward: 781.21
               Mean episode length: 219.74
    Episode_Reward/reaching_object: 1.7645
     Episode_Reward/lifting_object: 148.5001
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0782
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 158564352
                    Iteration time: 2.26s
                      Time elapsed: 01:01:31
                               ETA: 00:14:48

################################################################################
                     [1m Learning iteration 1613/2000 [0m                     

                       Computation: 43247 steps/s (collection: 2.170s, learning 0.103s)
             Mean action noise std: 3.55
          Mean value_function loss: 266.7545
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 82.0878
                       Mean reward: 732.50
               Mean episode length: 207.69
    Episode_Reward/reaching_object: 1.7100
     Episode_Reward/lifting_object: 143.6692
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0756
          Episode_Reward/joint_vel: -0.0910
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 158662656
                    Iteration time: 2.27s
                      Time elapsed: 01:01:34
                               ETA: 00:14:45

################################################################################
                     [1m Learning iteration 1614/2000 [0m                     

                       Computation: 43883 steps/s (collection: 2.127s, learning 0.114s)
             Mean action noise std: 3.55
          Mean value_function loss: 249.1677
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 82.0975
                       Mean reward: 793.28
               Mean episode length: 221.08
    Episode_Reward/reaching_object: 1.7844
     Episode_Reward/lifting_object: 151.1112
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0786
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 158760960
                    Iteration time: 2.24s
                      Time elapsed: 01:01:36
                               ETA: 00:14:43

################################################################################
                     [1m Learning iteration 1615/2000 [0m                     

                       Computation: 43629 steps/s (collection: 2.148s, learning 0.106s)
             Mean action noise std: 3.56
          Mean value_function loss: 266.2794
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 82.1140
                       Mean reward: 781.65
               Mean episode length: 219.15
    Episode_Reward/reaching_object: 1.8008
     Episode_Reward/lifting_object: 151.4472
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0790
          Episode_Reward/joint_vel: -0.0937
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158859264
                    Iteration time: 2.25s
                      Time elapsed: 01:01:38
                               ETA: 00:14:41

################################################################################
                     [1m Learning iteration 1616/2000 [0m                     

                       Computation: 43594 steps/s (collection: 2.157s, learning 0.098s)
             Mean action noise std: 3.56
          Mean value_function loss: 256.7904
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 82.1269
                       Mean reward: 709.47
               Mean episode length: 207.11
    Episode_Reward/reaching_object: 1.7448
     Episode_Reward/lifting_object: 145.7033
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0782
          Episode_Reward/joint_vel: -0.0942
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 158957568
                    Iteration time: 2.25s
                      Time elapsed: 01:01:40
                               ETA: 00:14:38

################################################################################
                     [1m Learning iteration 1617/2000 [0m                     

                       Computation: 43572 steps/s (collection: 2.143s, learning 0.113s)
             Mean action noise std: 3.56
          Mean value_function loss: 238.3831
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 82.1342
                       Mean reward: 762.25
               Mean episode length: 217.18
    Episode_Reward/reaching_object: 1.7620
     Episode_Reward/lifting_object: 147.2551
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0785
          Episode_Reward/joint_vel: -0.0943
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 159055872
                    Iteration time: 2.26s
                      Time elapsed: 01:01:43
                               ETA: 00:14:36

################################################################################
                     [1m Learning iteration 1618/2000 [0m                     

                       Computation: 42743 steps/s (collection: 2.180s, learning 0.120s)
             Mean action noise std: 3.56
          Mean value_function loss: 265.6878
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 82.1483
                       Mean reward: 736.34
               Mean episode length: 212.34
    Episode_Reward/reaching_object: 1.7519
     Episode_Reward/lifting_object: 146.5957
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0778
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 159154176
                    Iteration time: 2.30s
                      Time elapsed: 01:01:45
                               ETA: 00:14:34

################################################################################
                     [1m Learning iteration 1619/2000 [0m                     

                       Computation: 43127 steps/s (collection: 2.142s, learning 0.137s)
             Mean action noise std: 3.56
          Mean value_function loss: 266.8340
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 82.1668
                       Mean reward: 689.34
               Mean episode length: 198.75
    Episode_Reward/reaching_object: 1.7900
     Episode_Reward/lifting_object: 151.4839
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0787
          Episode_Reward/joint_vel: -0.0919
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 159252480
                    Iteration time: 2.28s
                      Time elapsed: 01:01:47
                               ETA: 00:14:31

################################################################################
                     [1m Learning iteration 1620/2000 [0m                     

                       Computation: 42617 steps/s (collection: 2.213s, learning 0.094s)
             Mean action noise std: 3.56
          Mean value_function loss: 227.2382
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 82.1786
                       Mean reward: 757.97
               Mean episode length: 213.64
    Episode_Reward/reaching_object: 1.7594
     Episode_Reward/lifting_object: 147.2687
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0782
          Episode_Reward/joint_vel: -0.0927
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 159350784
                    Iteration time: 2.31s
                      Time elapsed: 01:01:49
                               ETA: 00:14:29

################################################################################
                     [1m Learning iteration 1621/2000 [0m                     

                       Computation: 43834 steps/s (collection: 2.140s, learning 0.103s)
             Mean action noise std: 3.56
          Mean value_function loss: 283.8956
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 82.1869
                       Mean reward: 721.19
               Mean episode length: 204.85
    Episode_Reward/reaching_object: 1.7224
     Episode_Reward/lifting_object: 143.6613
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0773
          Episode_Reward/joint_vel: -0.0918
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 159449088
                    Iteration time: 2.24s
                      Time elapsed: 01:01:52
                               ETA: 00:14:27

################################################################################
                     [1m Learning iteration 1622/2000 [0m                     

                       Computation: 41617 steps/s (collection: 2.158s, learning 0.205s)
             Mean action noise std: 3.57
          Mean value_function loss: 239.3774
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 82.1977
                       Mean reward: 782.93
               Mean episode length: 217.15
    Episode_Reward/reaching_object: 1.8141
     Episode_Reward/lifting_object: 153.0788
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0808
          Episode_Reward/joint_vel: -0.0953
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 159547392
                    Iteration time: 2.36s
                      Time elapsed: 01:01:54
                               ETA: 00:14:25

################################################################################
                     [1m Learning iteration 1623/2000 [0m                     

                       Computation: 42952 steps/s (collection: 2.161s, learning 0.128s)
             Mean action noise std: 3.57
          Mean value_function loss: 279.4146
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 82.2109
                       Mean reward: 778.50
               Mean episode length: 219.34
    Episode_Reward/reaching_object: 1.7308
     Episode_Reward/lifting_object: 145.2454
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0771
          Episode_Reward/joint_vel: -0.0904
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 159645696
                    Iteration time: 2.29s
                      Time elapsed: 01:01:56
                               ETA: 00:14:22

################################################################################
                     [1m Learning iteration 1624/2000 [0m                     

                       Computation: 44218 steps/s (collection: 2.119s, learning 0.104s)
             Mean action noise std: 3.57
          Mean value_function loss: 279.8083
               Mean surrogate loss: 0.0124
                 Mean entropy loss: 82.2208
                       Mean reward: 762.30
               Mean episode length: 215.97
    Episode_Reward/reaching_object: 1.7426
     Episode_Reward/lifting_object: 146.3919
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0779
          Episode_Reward/joint_vel: -0.0924
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 159744000
                    Iteration time: 2.22s
                      Time elapsed: 01:01:59
                               ETA: 00:14:20

################################################################################
                     [1m Learning iteration 1625/2000 [0m                     

                       Computation: 43296 steps/s (collection: 2.172s, learning 0.098s)
             Mean action noise std: 3.57
          Mean value_function loss: 236.4650
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 82.2228
                       Mean reward: 797.21
               Mean episode length: 225.50
    Episode_Reward/reaching_object: 1.7736
     Episode_Reward/lifting_object: 149.5598
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0788
          Episode_Reward/joint_vel: -0.0925
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 159842304
                    Iteration time: 2.27s
                      Time elapsed: 01:02:01
                               ETA: 00:14:18

################################################################################
                     [1m Learning iteration 1626/2000 [0m                     

                       Computation: 43785 steps/s (collection: 2.150s, learning 0.095s)
             Mean action noise std: 3.57
          Mean value_function loss: 297.5393
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 82.2277
                       Mean reward: 699.70
               Mean episode length: 198.19
    Episode_Reward/reaching_object: 1.7392
     Episode_Reward/lifting_object: 145.7104
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0768
          Episode_Reward/joint_vel: -0.0904
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 159940608
                    Iteration time: 2.25s
                      Time elapsed: 01:02:03
                               ETA: 00:14:15

################################################################################
                     [1m Learning iteration 1627/2000 [0m                     

                       Computation: 42489 steps/s (collection: 2.150s, learning 0.164s)
             Mean action noise std: 3.57
          Mean value_function loss: 281.8044
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 82.2369
                       Mean reward: 717.25
               Mean episode length: 204.65
    Episode_Reward/reaching_object: 1.7380
     Episode_Reward/lifting_object: 145.4005
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0781
          Episode_Reward/joint_vel: -0.0911
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160038912
                    Iteration time: 2.31s
                      Time elapsed: 01:02:05
                               ETA: 00:14:13

################################################################################
                     [1m Learning iteration 1628/2000 [0m                     

                       Computation: 42730 steps/s (collection: 2.199s, learning 0.102s)
             Mean action noise std: 3.57
          Mean value_function loss: 272.0284
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 82.2487
                       Mean reward: 792.63
               Mean episode length: 217.98
    Episode_Reward/reaching_object: 1.7650
     Episode_Reward/lifting_object: 148.2442
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0789
          Episode_Reward/joint_vel: -0.0919
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 160137216
                    Iteration time: 2.30s
                      Time elapsed: 01:02:08
                               ETA: 00:14:11

################################################################################
                     [1m Learning iteration 1629/2000 [0m                     

                       Computation: 42266 steps/s (collection: 2.171s, learning 0.155s)
             Mean action noise std: 3.57
          Mean value_function loss: 275.9346
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 82.2583
                       Mean reward: 801.70
               Mean episode length: 222.59
    Episode_Reward/reaching_object: 1.8195
     Episode_Reward/lifting_object: 153.7345
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0802
          Episode_Reward/joint_vel: -0.0915
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 160235520
                    Iteration time: 2.33s
                      Time elapsed: 01:02:10
                               ETA: 00:14:09

################################################################################
                     [1m Learning iteration 1630/2000 [0m                     

                       Computation: 38952 steps/s (collection: 2.373s, learning 0.151s)
             Mean action noise std: 3.58
          Mean value_function loss: 276.9219
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 82.2683
                       Mean reward: 748.90
               Mean episode length: 209.60
    Episode_Reward/reaching_object: 1.8168
     Episode_Reward/lifting_object: 153.8553
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0800
          Episode_Reward/joint_vel: -0.0913
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 160333824
                    Iteration time: 2.52s
                      Time elapsed: 01:02:13
                               ETA: 00:14:06

################################################################################
                     [1m Learning iteration 1631/2000 [0m                     

                       Computation: 38664 steps/s (collection: 2.398s, learning 0.145s)
             Mean action noise std: 3.58
          Mean value_function loss: 250.4578
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 82.2822
                       Mean reward: 818.35
               Mean episode length: 224.88
    Episode_Reward/reaching_object: 1.8363
     Episode_Reward/lifting_object: 155.4919
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0810
          Episode_Reward/joint_vel: -0.0917
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 160432128
                    Iteration time: 2.54s
                      Time elapsed: 01:02:15
                               ETA: 00:14:04

################################################################################
                     [1m Learning iteration 1632/2000 [0m                     

                       Computation: 39489 steps/s (collection: 2.323s, learning 0.166s)
             Mean action noise std: 3.58
          Mean value_function loss: 271.7506
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 82.3034
                       Mean reward: 773.78
               Mean episode length: 216.99
    Episode_Reward/reaching_object: 1.7938
     Episode_Reward/lifting_object: 150.4470
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0793
          Episode_Reward/joint_vel: -0.0912
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 160530432
                    Iteration time: 2.49s
                      Time elapsed: 01:02:18
                               ETA: 00:14:02

################################################################################
                     [1m Learning iteration 1633/2000 [0m                     

                       Computation: 38483 steps/s (collection: 2.385s, learning 0.169s)
             Mean action noise std: 3.58
          Mean value_function loss: 303.6028
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 82.3256
                       Mean reward: 754.84
               Mean episode length: 211.03
    Episode_Reward/reaching_object: 1.7527
     Episode_Reward/lifting_object: 147.4601
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0782
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 160628736
                    Iteration time: 2.55s
                      Time elapsed: 01:02:20
                               ETA: 00:14:00

################################################################################
                     [1m Learning iteration 1634/2000 [0m                     

                       Computation: 39181 steps/s (collection: 2.370s, learning 0.138s)
             Mean action noise std: 3.59
          Mean value_function loss: 326.1650
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 82.3493
                       Mean reward: 750.46
               Mean episode length: 207.58
    Episode_Reward/reaching_object: 1.7361
     Episode_Reward/lifting_object: 146.0119
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0773
          Episode_Reward/joint_vel: -0.0892
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 160727040
                    Iteration time: 2.51s
                      Time elapsed: 01:02:23
                               ETA: 00:13:57

################################################################################
                     [1m Learning iteration 1635/2000 [0m                     

                       Computation: 39928 steps/s (collection: 2.318s, learning 0.144s)
             Mean action noise std: 3.59
          Mean value_function loss: 285.6788
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 82.3680
                       Mean reward: 797.17
               Mean episode length: 220.06
    Episode_Reward/reaching_object: 1.7580
     Episode_Reward/lifting_object: 148.1207
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0795
          Episode_Reward/joint_vel: -0.0909
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 160825344
                    Iteration time: 2.46s
                      Time elapsed: 01:02:25
                               ETA: 00:13:55

################################################################################
                     [1m Learning iteration 1636/2000 [0m                     

                       Computation: 41363 steps/s (collection: 2.244s, learning 0.133s)
             Mean action noise std: 3.59
          Mean value_function loss: 346.6751
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 82.3814
                       Mean reward: 695.92
               Mean episode length: 197.56
    Episode_Reward/reaching_object: 1.7085
     Episode_Reward/lifting_object: 143.3608
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0770
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 160923648
                    Iteration time: 2.38s
                      Time elapsed: 01:02:27
                               ETA: 00:13:53

################################################################################
                     [1m Learning iteration 1637/2000 [0m                     

                       Computation: 42486 steps/s (collection: 2.199s, learning 0.115s)
             Mean action noise std: 3.59
          Mean value_function loss: 294.3005
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 82.4015
                       Mean reward: 780.82
               Mean episode length: 217.34
    Episode_Reward/reaching_object: 1.8302
     Episode_Reward/lifting_object: 154.6100
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0811
          Episode_Reward/joint_vel: -0.0924
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 161021952
                    Iteration time: 2.31s
                      Time elapsed: 01:02:30
                               ETA: 00:13:51

################################################################################
                     [1m Learning iteration 1638/2000 [0m                     

                       Computation: 42784 steps/s (collection: 2.195s, learning 0.102s)
             Mean action noise std: 3.59
          Mean value_function loss: 283.4398
               Mean surrogate loss: 0.0091
                 Mean entropy loss: 82.4108
                       Mean reward: 790.11
               Mean episode length: 216.58
    Episode_Reward/reaching_object: 1.8060
     Episode_Reward/lifting_object: 152.9481
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0804
          Episode_Reward/joint_vel: -0.0923
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 161120256
                    Iteration time: 2.30s
                      Time elapsed: 01:02:32
                               ETA: 00:13:48

################################################################################
                     [1m Learning iteration 1639/2000 [0m                     

                       Computation: 42098 steps/s (collection: 2.197s, learning 0.138s)
             Mean action noise std: 3.59
          Mean value_function loss: 251.8562
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 82.4119
                       Mean reward: 766.39
               Mean episode length: 217.57
    Episode_Reward/reaching_object: 1.7920
     Episode_Reward/lifting_object: 151.6892
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0813
          Episode_Reward/joint_vel: -0.0935
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 161218560
                    Iteration time: 2.34s
                      Time elapsed: 01:02:34
                               ETA: 00:13:46

################################################################################
                     [1m Learning iteration 1640/2000 [0m                     

                       Computation: 37402 steps/s (collection: 2.523s, learning 0.105s)
             Mean action noise std: 3.59
          Mean value_function loss: 271.4336
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 82.4133
                       Mean reward: 691.71
               Mean episode length: 201.64
    Episode_Reward/reaching_object: 1.7355
     Episode_Reward/lifting_object: 145.6999
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0795
          Episode_Reward/joint_vel: -0.0936
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 161316864
                    Iteration time: 2.63s
                      Time elapsed: 01:02:37
                               ETA: 00:13:44

################################################################################
                     [1m Learning iteration 1641/2000 [0m                     

                       Computation: 41079 steps/s (collection: 2.253s, learning 0.140s)
             Mean action noise std: 3.59
          Mean value_function loss: 262.1922
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 82.4156
                       Mean reward: 709.14
               Mean episode length: 206.50
    Episode_Reward/reaching_object: 1.7431
     Episode_Reward/lifting_object: 146.0175
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0793
          Episode_Reward/joint_vel: -0.0925
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 161415168
                    Iteration time: 2.39s
                      Time elapsed: 01:02:39
                               ETA: 00:13:42

################################################################################
                     [1m Learning iteration 1642/2000 [0m                     

                       Computation: 43729 steps/s (collection: 2.150s, learning 0.098s)
             Mean action noise std: 3.59
          Mean value_function loss: 251.9859
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 82.4180
                       Mean reward: 661.47
               Mean episode length: 196.22
    Episode_Reward/reaching_object: 1.7253
     Episode_Reward/lifting_object: 143.7549
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0793
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 161513472
                    Iteration time: 2.25s
                      Time elapsed: 01:02:42
                               ETA: 00:13:39

################################################################################
                     [1m Learning iteration 1643/2000 [0m                     

                       Computation: 44009 steps/s (collection: 2.124s, learning 0.110s)
             Mean action noise std: 3.59
          Mean value_function loss: 223.9358
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 82.4201
                       Mean reward: 739.75
               Mean episode length: 209.62
    Episode_Reward/reaching_object: 1.7832
     Episode_Reward/lifting_object: 150.2924
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0812
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 161611776
                    Iteration time: 2.23s
                      Time elapsed: 01:02:44
                               ETA: 00:13:37

################################################################################
                     [1m Learning iteration 1644/2000 [0m                     

                       Computation: 43495 steps/s (collection: 2.144s, learning 0.116s)
             Mean action noise std: 3.59
          Mean value_function loss: 270.9466
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 82.4226
                       Mean reward: 773.48
               Mean episode length: 218.83
    Episode_Reward/reaching_object: 1.8025
     Episode_Reward/lifting_object: 151.4852
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0813
          Episode_Reward/joint_vel: -0.0929
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 161710080
                    Iteration time: 2.26s
                      Time elapsed: 01:02:46
                               ETA: 00:13:35

################################################################################
                     [1m Learning iteration 1645/2000 [0m                     

                       Computation: 43725 steps/s (collection: 2.147s, learning 0.101s)
             Mean action noise std: 3.59
          Mean value_function loss: 236.7899
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 82.4253
                       Mean reward: 804.70
               Mean episode length: 225.51
    Episode_Reward/reaching_object: 1.8075
     Episode_Reward/lifting_object: 152.0804
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0815
          Episode_Reward/joint_vel: -0.0929
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 161808384
                    Iteration time: 2.25s
                      Time elapsed: 01:02:48
                               ETA: 00:13:32

################################################################################
                     [1m Learning iteration 1646/2000 [0m                     

                       Computation: 44264 steps/s (collection: 2.127s, learning 0.094s)
             Mean action noise std: 3.59
          Mean value_function loss: 247.7565
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 82.4275
                       Mean reward: 767.26
               Mean episode length: 218.75
    Episode_Reward/reaching_object: 1.7538
     Episode_Reward/lifting_object: 146.4665
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0798
          Episode_Reward/joint_vel: -0.0913
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 161906688
                    Iteration time: 2.22s
                      Time elapsed: 01:02:51
                               ETA: 00:13:30

################################################################################
                     [1m Learning iteration 1647/2000 [0m                     

                       Computation: 43851 steps/s (collection: 2.143s, learning 0.099s)
             Mean action noise std: 3.59
          Mean value_function loss: 270.4134
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 82.4305
                       Mean reward: 755.46
               Mean episode length: 211.96
    Episode_Reward/reaching_object: 1.7592
     Episode_Reward/lifting_object: 147.3560
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0792
          Episode_Reward/joint_vel: -0.0898
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 162004992
                    Iteration time: 2.24s
                      Time elapsed: 01:02:53
                               ETA: 00:13:28

################################################################################
                     [1m Learning iteration 1648/2000 [0m                     

                       Computation: 43782 steps/s (collection: 2.141s, learning 0.104s)
             Mean action noise std: 3.60
          Mean value_function loss: 327.6368
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 82.4348
                       Mean reward: 741.37
               Mean episode length: 211.49
    Episode_Reward/reaching_object: 1.7586
     Episode_Reward/lifting_object: 146.6747
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0794
          Episode_Reward/joint_vel: -0.0909
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 162103296
                    Iteration time: 2.25s
                      Time elapsed: 01:02:55
                               ETA: 00:13:25

################################################################################
                     [1m Learning iteration 1649/2000 [0m                     

                       Computation: 43883 steps/s (collection: 2.134s, learning 0.106s)
             Mean action noise std: 3.60
          Mean value_function loss: 321.9770
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 82.4472
                       Mean reward: 748.40
               Mean episode length: 208.34
    Episode_Reward/reaching_object: 1.7036
     Episode_Reward/lifting_object: 142.3628
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0769
          Episode_Reward/joint_vel: -0.0871
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 162201600
                    Iteration time: 2.24s
                      Time elapsed: 01:02:57
                               ETA: 00:13:23

################################################################################
                     [1m Learning iteration 1650/2000 [0m                     

                       Computation: 44071 steps/s (collection: 2.128s, learning 0.103s)
             Mean action noise std: 3.60
          Mean value_function loss: 316.5778
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 82.4646
                       Mean reward: 641.72
               Mean episode length: 190.70
    Episode_Reward/reaching_object: 1.7036
     Episode_Reward/lifting_object: 142.7059
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0770
          Episode_Reward/joint_vel: -0.0874
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 162299904
                    Iteration time: 2.23s
                      Time elapsed: 01:03:00
                               ETA: 00:13:21

################################################################################
                     [1m Learning iteration 1651/2000 [0m                     

                       Computation: 44123 steps/s (collection: 2.128s, learning 0.100s)
             Mean action noise std: 3.60
          Mean value_function loss: 341.6791
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 82.4771
                       Mean reward: 711.17
               Mean episode length: 199.78
    Episode_Reward/reaching_object: 1.6882
     Episode_Reward/lifting_object: 140.8340
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0759
          Episode_Reward/joint_vel: -0.0854
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 162398208
                    Iteration time: 2.23s
                      Time elapsed: 01:03:02
                               ETA: 00:13:19

################################################################################
                     [1m Learning iteration 1652/2000 [0m                     

                       Computation: 43413 steps/s (collection: 2.154s, learning 0.110s)
             Mean action noise std: 3.60
          Mean value_function loss: 311.0048
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 82.4823
                       Mean reward: 701.88
               Mean episode length: 198.65
    Episode_Reward/reaching_object: 1.6914
     Episode_Reward/lifting_object: 141.8611
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0764
          Episode_Reward/joint_vel: -0.0843
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 162496512
                    Iteration time: 2.26s
                      Time elapsed: 01:03:04
                               ETA: 00:13:16

################################################################################
                     [1m Learning iteration 1653/2000 [0m                     

                       Computation: 44140 steps/s (collection: 2.130s, learning 0.097s)
             Mean action noise std: 3.60
          Mean value_function loss: 328.5651
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 82.4891
                       Mean reward: 715.81
               Mean episode length: 204.61
    Episode_Reward/reaching_object: 1.7275
     Episode_Reward/lifting_object: 144.2402
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0775
          Episode_Reward/joint_vel: -0.0877
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 162594816
                    Iteration time: 2.23s
                      Time elapsed: 01:03:06
                               ETA: 00:13:14

################################################################################
                     [1m Learning iteration 1654/2000 [0m                     

                       Computation: 43993 steps/s (collection: 2.131s, learning 0.103s)
             Mean action noise std: 3.61
          Mean value_function loss: 313.0587
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 82.5060
                       Mean reward: 737.46
               Mean episode length: 208.68
    Episode_Reward/reaching_object: 1.6995
     Episode_Reward/lifting_object: 141.4101
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0769
          Episode_Reward/joint_vel: -0.0870
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 162693120
                    Iteration time: 2.23s
                      Time elapsed: 01:03:09
                               ETA: 00:13:12

################################################################################
                     [1m Learning iteration 1655/2000 [0m                     

                       Computation: 43987 steps/s (collection: 2.136s, learning 0.099s)
             Mean action noise std: 3.61
          Mean value_function loss: 284.4159
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 82.5269
                       Mean reward: 667.22
               Mean episode length: 196.62
    Episode_Reward/reaching_object: 1.7148
     Episode_Reward/lifting_object: 143.2096
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0773
          Episode_Reward/joint_vel: -0.0865
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 162791424
                    Iteration time: 2.23s
                      Time elapsed: 01:03:11
                               ETA: 00:13:09

################################################################################
                     [1m Learning iteration 1656/2000 [0m                     

                       Computation: 44040 steps/s (collection: 2.115s, learning 0.117s)
             Mean action noise std: 3.61
          Mean value_function loss: 301.5364
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 82.5416
                       Mean reward: 721.09
               Mean episode length: 203.74
    Episode_Reward/reaching_object: 1.7407
     Episode_Reward/lifting_object: 145.1673
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0789
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 162889728
                    Iteration time: 2.23s
                      Time elapsed: 01:03:13
                               ETA: 00:13:07

################################################################################
                     [1m Learning iteration 1657/2000 [0m                     

                       Computation: 43559 steps/s (collection: 2.150s, learning 0.107s)
             Mean action noise std: 3.61
          Mean value_function loss: 286.4003
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 82.5496
                       Mean reward: 765.21
               Mean episode length: 212.18
    Episode_Reward/reaching_object: 1.7238
     Episode_Reward/lifting_object: 144.4785
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0776
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 162988032
                    Iteration time: 2.26s
                      Time elapsed: 01:03:15
                               ETA: 00:13:05

################################################################################
                     [1m Learning iteration 1658/2000 [0m                     

                       Computation: 43986 steps/s (collection: 2.124s, learning 0.111s)
             Mean action noise std: 3.61
          Mean value_function loss: 282.0846
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 82.5666
                       Mean reward: 732.43
               Mean episode length: 206.86
    Episode_Reward/reaching_object: 1.7396
     Episode_Reward/lifting_object: 145.1278
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0788
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 163086336
                    Iteration time: 2.23s
                      Time elapsed: 01:03:18
                               ETA: 00:13:02

################################################################################
                     [1m Learning iteration 1659/2000 [0m                     

                       Computation: 43537 steps/s (collection: 2.144s, learning 0.114s)
             Mean action noise std: 3.61
          Mean value_function loss: 343.1063
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 82.5842
                       Mean reward: 729.16
               Mean episode length: 207.90
    Episode_Reward/reaching_object: 1.7776
     Episode_Reward/lifting_object: 148.0427
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0800
          Episode_Reward/joint_vel: -0.0906
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 163184640
                    Iteration time: 2.26s
                      Time elapsed: 01:03:20
                               ETA: 00:13:00

################################################################################
                     [1m Learning iteration 1660/2000 [0m                     

                       Computation: 43725 steps/s (collection: 2.136s, learning 0.113s)
             Mean action noise std: 3.61
          Mean value_function loss: 318.0830
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 82.5889
                       Mean reward: 699.04
               Mean episode length: 198.71
    Episode_Reward/reaching_object: 1.7472
     Episode_Reward/lifting_object: 145.2309
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0788
          Episode_Reward/joint_vel: -0.0888
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 163282944
                    Iteration time: 2.25s
                      Time elapsed: 01:03:22
                               ETA: 00:12:58

################################################################################
                     [1m Learning iteration 1661/2000 [0m                     

                       Computation: 43515 steps/s (collection: 2.149s, learning 0.110s)
             Mean action noise std: 3.62
          Mean value_function loss: 254.3410
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 82.5994
                       Mean reward: 740.63
               Mean episode length: 212.38
    Episode_Reward/reaching_object: 1.7298
     Episode_Reward/lifting_object: 143.7309
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0790
          Episode_Reward/joint_vel: -0.0885
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 163381248
                    Iteration time: 2.26s
                      Time elapsed: 01:03:24
                               ETA: 00:12:56

################################################################################
                     [1m Learning iteration 1662/2000 [0m                     

                       Computation: 44169 steps/s (collection: 2.123s, learning 0.103s)
             Mean action noise std: 3.62
          Mean value_function loss: 274.7981
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 82.6171
                       Mean reward: 734.64
               Mean episode length: 208.09
    Episode_Reward/reaching_object: 1.7519
     Episode_Reward/lifting_object: 146.0320
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0792
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 163479552
                    Iteration time: 2.23s
                      Time elapsed: 01:03:27
                               ETA: 00:12:53

################################################################################
                     [1m Learning iteration 1663/2000 [0m                     

                       Computation: 44300 steps/s (collection: 2.122s, learning 0.097s)
             Mean action noise std: 3.62
          Mean value_function loss: 286.3520
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 82.6319
                       Mean reward: 776.18
               Mean episode length: 217.06
    Episode_Reward/reaching_object: 1.7886
     Episode_Reward/lifting_object: 148.2562
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0810
          Episode_Reward/joint_vel: -0.0908
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 163577856
                    Iteration time: 2.22s
                      Time elapsed: 01:03:29
                               ETA: 00:12:51

################################################################################
                     [1m Learning iteration 1664/2000 [0m                     

                       Computation: 44224 steps/s (collection: 2.126s, learning 0.097s)
             Mean action noise std: 3.62
          Mean value_function loss: 274.1332
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 82.6405
                       Mean reward: 732.38
               Mean episode length: 206.10
    Episode_Reward/reaching_object: 1.7551
     Episode_Reward/lifting_object: 146.0550
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0792
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 163676160
                    Iteration time: 2.22s
                      Time elapsed: 01:03:31
                               ETA: 00:12:49

################################################################################
                     [1m Learning iteration 1665/2000 [0m                     

                       Computation: 44236 steps/s (collection: 2.118s, learning 0.105s)
             Mean action noise std: 3.62
          Mean value_function loss: 311.3732
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 82.6571
                       Mean reward: 756.13
               Mean episode length: 211.88
    Episode_Reward/reaching_object: 1.7495
     Episode_Reward/lifting_object: 144.9940
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0801
          Episode_Reward/joint_vel: -0.0911
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 163774464
                    Iteration time: 2.22s
                      Time elapsed: 01:03:33
                               ETA: 00:12:46

################################################################################
                     [1m Learning iteration 1666/2000 [0m                     

                       Computation: 27128 steps/s (collection: 3.526s, learning 0.098s)
             Mean action noise std: 3.63
          Mean value_function loss: 300.3772
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 82.6780
                       Mean reward: 694.11
               Mean episode length: 197.97
    Episode_Reward/reaching_object: 1.7402
     Episode_Reward/lifting_object: 145.0498
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0790
          Episode_Reward/joint_vel: -0.0884
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 163872768
                    Iteration time: 3.62s
                      Time elapsed: 01:03:37
                               ETA: 00:12:44

################################################################################
                     [1m Learning iteration 1667/2000 [0m                     

                       Computation: 14411 steps/s (collection: 6.696s, learning 0.125s)
             Mean action noise std: 3.63
          Mean value_function loss: 311.9076
               Mean surrogate loss: 0.0100
                 Mean entropy loss: 82.7010
                       Mean reward: 794.62
               Mean episode length: 219.85
    Episode_Reward/reaching_object: 1.8238
     Episode_Reward/lifting_object: 152.5099
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0824
          Episode_Reward/joint_vel: -0.0919
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 163971072
                    Iteration time: 6.82s
                      Time elapsed: 01:03:44
                               ETA: 00:12:43

################################################################################
                     [1m Learning iteration 1668/2000 [0m                     

                       Computation: 14165 steps/s (collection: 6.820s, learning 0.120s)
             Mean action noise std: 3.63
          Mean value_function loss: 272.7213
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 82.7049
                       Mean reward: 701.32
               Mean episode length: 201.00
    Episode_Reward/reaching_object: 1.7685
     Episode_Reward/lifting_object: 147.9019
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0810
          Episode_Reward/joint_vel: -0.0914
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 164069376
                    Iteration time: 6.94s
                      Time elapsed: 01:03:51
                               ETA: 00:12:42

################################################################################
                     [1m Learning iteration 1669/2000 [0m                     

                       Computation: 13372 steps/s (collection: 7.102s, learning 0.249s)
             Mean action noise std: 3.63
          Mean value_function loss: 317.7335
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 82.7114
                       Mean reward: 703.70
               Mean episode length: 199.66
    Episode_Reward/reaching_object: 1.7398
     Episode_Reward/lifting_object: 146.4023
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0793
          Episode_Reward/joint_vel: -0.0885
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 164167680
                    Iteration time: 7.35s
                      Time elapsed: 01:03:58
                               ETA: 00:12:40

################################################################################
                     [1m Learning iteration 1670/2000 [0m                     

                       Computation: 13019 steps/s (collection: 7.419s, learning 0.131s)
             Mean action noise std: 3.63
          Mean value_function loss: 271.6754
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 82.7211
                       Mean reward: 723.50
               Mean episode length: 207.22
    Episode_Reward/reaching_object: 1.7232
     Episode_Reward/lifting_object: 144.5095
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0806
          Episode_Reward/joint_vel: -0.0907
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 164265984
                    Iteration time: 7.55s
                      Time elapsed: 01:04:05
                               ETA: 00:12:39

################################################################################
                     [1m Learning iteration 1671/2000 [0m                     

                       Computation: 13921 steps/s (collection: 6.949s, learning 0.113s)
             Mean action noise std: 3.63
          Mean value_function loss: 314.9964
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 82.7349
                       Mean reward: 695.56
               Mean episode length: 202.96
    Episode_Reward/reaching_object: 1.6652
     Episode_Reward/lifting_object: 138.6037
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0779
          Episode_Reward/joint_vel: -0.0889
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 164364288
                    Iteration time: 7.06s
                      Time elapsed: 01:04:13
                               ETA: 00:12:38

################################################################################
                     [1m Learning iteration 1672/2000 [0m                     

                       Computation: 14101 steps/s (collection: 6.850s, learning 0.122s)
             Mean action noise std: 3.64
          Mean value_function loss: 255.0158
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 82.7502
                       Mean reward: 747.67
               Mean episode length: 209.07
    Episode_Reward/reaching_object: 1.7607
     Episode_Reward/lifting_object: 148.8595
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0811
          Episode_Reward/joint_vel: -0.0901
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 164462592
                    Iteration time: 6.97s
                      Time elapsed: 01:04:19
                               ETA: 00:12:36

################################################################################
                     [1m Learning iteration 1673/2000 [0m                     

                       Computation: 14070 steps/s (collection: 6.861s, learning 0.126s)
             Mean action noise std: 3.64
          Mean value_function loss: 287.0489
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 82.7671
                       Mean reward: 709.23
               Mean episode length: 201.51
    Episode_Reward/reaching_object: 1.7040
     Episode_Reward/lifting_object: 142.7135
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0792
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 164560896
                    Iteration time: 6.99s
                      Time elapsed: 01:04:26
                               ETA: 00:12:35

################################################################################
                     [1m Learning iteration 1674/2000 [0m                     

                       Computation: 13990 steps/s (collection: 6.913s, learning 0.113s)
             Mean action noise std: 3.64
          Mean value_function loss: 277.7639
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 82.7785
                       Mean reward: 690.63
               Mean episode length: 197.98
    Episode_Reward/reaching_object: 1.7270
     Episode_Reward/lifting_object: 144.9937
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0805
          Episode_Reward/joint_vel: -0.0915
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 164659200
                    Iteration time: 7.03s
                      Time elapsed: 01:04:34
                               ETA: 00:12:33

################################################################################
                     [1m Learning iteration 1675/2000 [0m                     

                       Computation: 24174 steps/s (collection: 3.971s, learning 0.095s)
             Mean action noise std: 3.64
          Mean value_function loss: 282.8229
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 82.7872
                       Mean reward: 765.53
               Mean episode length: 213.17
    Episode_Reward/reaching_object: 1.7554
     Episode_Reward/lifting_object: 148.1856
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0814
          Episode_Reward/joint_vel: -0.0909
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 164757504
                    Iteration time: 4.07s
                      Time elapsed: 01:04:38
                               ETA: 00:12:32

################################################################################
                     [1m Learning iteration 1676/2000 [0m                     

                       Computation: 47242 steps/s (collection: 1.990s, learning 0.091s)
             Mean action noise std: 3.64
          Mean value_function loss: 264.1281
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 82.8052
                       Mean reward: 791.65
               Mean episode length: 220.75
    Episode_Reward/reaching_object: 1.7879
     Episode_Reward/lifting_object: 151.4200
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0833
          Episode_Reward/joint_vel: -0.0931
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 164855808
                    Iteration time: 2.08s
                      Time elapsed: 01:04:40
                               ETA: 00:12:29

################################################################################
                     [1m Learning iteration 1677/2000 [0m                     

                       Computation: 47155 steps/s (collection: 1.993s, learning 0.092s)
             Mean action noise std: 3.65
          Mean value_function loss: 316.7463
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 82.8220
                       Mean reward: 759.82
               Mean episode length: 216.41
    Episode_Reward/reaching_object: 1.7136
     Episode_Reward/lifting_object: 144.4323
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0811
          Episode_Reward/joint_vel: -0.0921
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 164954112
                    Iteration time: 2.08s
                      Time elapsed: 01:04:42
                               ETA: 00:12:27

################################################################################
                     [1m Learning iteration 1678/2000 [0m                     

                       Computation: 45334 steps/s (collection: 2.076s, learning 0.093s)
             Mean action noise std: 3.65
          Mean value_function loss: 292.9824
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 82.8344
                       Mean reward: 731.07
               Mean episode length: 207.08
    Episode_Reward/reaching_object: 1.7893
     Episode_Reward/lifting_object: 150.9686
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0831
          Episode_Reward/joint_vel: -0.0935
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 165052416
                    Iteration time: 2.17s
                      Time elapsed: 01:04:44
                               ETA: 00:12:24

################################################################################
                     [1m Learning iteration 1679/2000 [0m                     

                       Computation: 46986 steps/s (collection: 1.992s, learning 0.100s)
             Mean action noise std: 3.65
          Mean value_function loss: 296.9587
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 82.8526
                       Mean reward: 688.90
               Mean episode length: 198.55
    Episode_Reward/reaching_object: 1.7168
     Episode_Reward/lifting_object: 144.1543
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0807
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 165150720
                    Iteration time: 2.09s
                      Time elapsed: 01:04:46
                               ETA: 00:12:22

################################################################################
                     [1m Learning iteration 1680/2000 [0m                     

                       Computation: 46512 steps/s (collection: 2.010s, learning 0.103s)
             Mean action noise std: 3.65
          Mean value_function loss: 301.8104
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 82.8793
                       Mean reward: 722.13
               Mean episode length: 209.17
    Episode_Reward/reaching_object: 1.7067
     Episode_Reward/lifting_object: 142.7124
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0815
          Episode_Reward/joint_vel: -0.0943
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 165249024
                    Iteration time: 2.11s
                      Time elapsed: 01:04:48
                               ETA: 00:12:20

################################################################################
                     [1m Learning iteration 1681/2000 [0m                     

                       Computation: 46772 steps/s (collection: 2.006s, learning 0.096s)
             Mean action noise std: 3.66
          Mean value_function loss: 279.0356
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 82.9048
                       Mean reward: 680.52
               Mean episode length: 202.79
    Episode_Reward/reaching_object: 1.7369
     Episode_Reward/lifting_object: 145.9352
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0824
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 165347328
                    Iteration time: 2.10s
                      Time elapsed: 01:04:50
                               ETA: 00:12:17

################################################################################
                     [1m Learning iteration 1682/2000 [0m                     

                       Computation: 46401 steps/s (collection: 2.028s, learning 0.091s)
             Mean action noise std: 3.66
          Mean value_function loss: 245.0044
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 82.9089
                       Mean reward: 769.69
               Mean episode length: 215.08
    Episode_Reward/reaching_object: 1.7322
     Episode_Reward/lifting_object: 146.3588
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0821
          Episode_Reward/joint_vel: -0.0932
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 165445632
                    Iteration time: 2.12s
                      Time elapsed: 01:04:52
                               ETA: 00:12:15

################################################################################
                     [1m Learning iteration 1683/2000 [0m                     

                       Computation: 46870 steps/s (collection: 2.004s, learning 0.094s)
             Mean action noise std: 3.66
          Mean value_function loss: 256.4719
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 82.9170
                       Mean reward: 760.37
               Mean episode length: 217.05
    Episode_Reward/reaching_object: 1.6851
     Episode_Reward/lifting_object: 141.3570
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0812
          Episode_Reward/joint_vel: -0.0940
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 165543936
                    Iteration time: 2.10s
                      Time elapsed: 01:04:54
                               ETA: 00:12:13

################################################################################
                     [1m Learning iteration 1684/2000 [0m                     

                       Computation: 46759 steps/s (collection: 2.007s, learning 0.096s)
             Mean action noise std: 3.66
          Mean value_function loss: 250.5244
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 82.9277
                       Mean reward: 745.35
               Mean episode length: 212.06
    Episode_Reward/reaching_object: 1.7242
     Episode_Reward/lifting_object: 145.3059
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0821
          Episode_Reward/joint_vel: -0.0943
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 165642240
                    Iteration time: 2.10s
                      Time elapsed: 01:04:57
                               ETA: 00:12:10

################################################################################
                     [1m Learning iteration 1685/2000 [0m                     

                       Computation: 46500 steps/s (collection: 1.998s, learning 0.116s)
             Mean action noise std: 3.66
          Mean value_function loss: 262.6009
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 82.9461
                       Mean reward: 764.98
               Mean episode length: 217.70
    Episode_Reward/reaching_object: 1.7581
     Episode_Reward/lifting_object: 148.0687
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0835
          Episode_Reward/joint_vel: -0.0969
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 165740544
                    Iteration time: 2.11s
                      Time elapsed: 01:04:59
                               ETA: 00:12:08

################################################################################
                     [1m Learning iteration 1686/2000 [0m                     

                       Computation: 46193 steps/s (collection: 2.010s, learning 0.118s)
             Mean action noise std: 3.66
          Mean value_function loss: 328.5547
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 82.9632
                       Mean reward: 785.72
               Mean episode length: 217.55
    Episode_Reward/reaching_object: 1.7207
     Episode_Reward/lifting_object: 145.0617
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0815
          Episode_Reward/joint_vel: -0.0926
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 165838848
                    Iteration time: 2.13s
                      Time elapsed: 01:05:01
                               ETA: 00:12:06

################################################################################
                     [1m Learning iteration 1687/2000 [0m                     

                       Computation: 46603 steps/s (collection: 1.991s, learning 0.118s)
             Mean action noise std: 3.66
          Mean value_function loss: 302.2019
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 82.9739
                       Mean reward: 717.89
               Mean episode length: 205.22
    Episode_Reward/reaching_object: 1.6546
     Episode_Reward/lifting_object: 138.2250
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0798
          Episode_Reward/joint_vel: -0.0939
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 165937152
                    Iteration time: 2.11s
                      Time elapsed: 01:05:03
                               ETA: 00:12:03

################################################################################
                     [1m Learning iteration 1688/2000 [0m                     

                       Computation: 46464 steps/s (collection: 2.012s, learning 0.104s)
             Mean action noise std: 3.67
          Mean value_function loss: 305.3021
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 82.9862
                       Mean reward: 723.48
               Mean episode length: 207.30
    Episode_Reward/reaching_object: 1.7454
     Episode_Reward/lifting_object: 147.6299
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0825
          Episode_Reward/joint_vel: -0.0949
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 166035456
                    Iteration time: 2.12s
                      Time elapsed: 01:05:05
                               ETA: 00:12:01

################################################################################
                     [1m Learning iteration 1689/2000 [0m                     

                       Computation: 45912 steps/s (collection: 2.022s, learning 0.119s)
             Mean action noise std: 3.67
          Mean value_function loss: 296.0222
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 82.9965
                       Mean reward: 720.81
               Mean episode length: 207.39
    Episode_Reward/reaching_object: 1.7089
     Episode_Reward/lifting_object: 144.0414
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0813
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 166133760
                    Iteration time: 2.14s
                      Time elapsed: 01:05:07
                               ETA: 00:11:59

################################################################################
                     [1m Learning iteration 1690/2000 [0m                     

                       Computation: 46725 steps/s (collection: 2.001s, learning 0.103s)
             Mean action noise std: 3.67
          Mean value_function loss: 290.0882
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 83.0081
                       Mean reward: 790.34
               Mean episode length: 223.92
    Episode_Reward/reaching_object: 1.7928
     Episode_Reward/lifting_object: 151.4129
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0850
          Episode_Reward/joint_vel: -0.0968
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 166232064
                    Iteration time: 2.10s
                      Time elapsed: 01:05:09
                               ETA: 00:11:56

################################################################################
                     [1m Learning iteration 1691/2000 [0m                     

                       Computation: 46697 steps/s (collection: 2.000s, learning 0.106s)
             Mean action noise std: 3.67
          Mean value_function loss: 299.2696
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 83.0216
                       Mean reward: 700.42
               Mean episode length: 200.83
    Episode_Reward/reaching_object: 1.7617
     Episode_Reward/lifting_object: 148.7372
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0836
          Episode_Reward/joint_vel: -0.0953
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 166330368
                    Iteration time: 2.11s
                      Time elapsed: 01:05:11
                               ETA: 00:11:54

################################################################################
                     [1m Learning iteration 1692/2000 [0m                     

                       Computation: 46922 steps/s (collection: 2.002s, learning 0.093s)
             Mean action noise std: 3.67
          Mean value_function loss: 321.7379
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 83.0380
                       Mean reward: 715.88
               Mean episode length: 202.99
    Episode_Reward/reaching_object: 1.7263
     Episode_Reward/lifting_object: 145.8155
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0824
          Episode_Reward/joint_vel: -0.0948
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 166428672
                    Iteration time: 2.10s
                      Time elapsed: 01:05:13
                               ETA: 00:11:52

################################################################################
                     [1m Learning iteration 1693/2000 [0m                     

                       Computation: 46505 steps/s (collection: 2.020s, learning 0.094s)
             Mean action noise std: 3.68
          Mean value_function loss: 288.8503
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 83.0531
                       Mean reward: 741.02
               Mean episode length: 208.78
    Episode_Reward/reaching_object: 1.6975
     Episode_Reward/lifting_object: 142.6100
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0818
          Episode_Reward/joint_vel: -0.0954
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166526976
                    Iteration time: 2.11s
                      Time elapsed: 01:05:16
                               ETA: 00:11:49

################################################################################
                     [1m Learning iteration 1694/2000 [0m                     

                       Computation: 46973 steps/s (collection: 1.999s, learning 0.094s)
             Mean action noise std: 3.68
          Mean value_function loss: 277.7927
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 83.0637
                       Mean reward: 743.76
               Mean episode length: 211.72
    Episode_Reward/reaching_object: 1.7219
     Episode_Reward/lifting_object: 145.0816
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0835
          Episode_Reward/joint_vel: -0.0956
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 166625280
                    Iteration time: 2.09s
                      Time elapsed: 01:05:18
                               ETA: 00:11:47

################################################################################
                     [1m Learning iteration 1695/2000 [0m                     

                       Computation: 47022 steps/s (collection: 2.000s, learning 0.090s)
             Mean action noise std: 3.68
          Mean value_function loss: 283.1914
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 83.0698
                       Mean reward: 746.52
               Mean episode length: 214.07
    Episode_Reward/reaching_object: 1.7365
     Episode_Reward/lifting_object: 146.3508
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0832
          Episode_Reward/joint_vel: -0.0954
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 166723584
                    Iteration time: 2.09s
                      Time elapsed: 01:05:20
                               ETA: 00:11:44

################################################################################
                     [1m Learning iteration 1696/2000 [0m                     

                       Computation: 46566 steps/s (collection: 2.017s, learning 0.094s)
             Mean action noise std: 3.68
          Mean value_function loss: 303.6730
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 83.0801
                       Mean reward: 722.74
               Mean episode length: 207.02
    Episode_Reward/reaching_object: 1.7131
     Episode_Reward/lifting_object: 144.1337
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0822
          Episode_Reward/joint_vel: -0.0943
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 166821888
                    Iteration time: 2.11s
                      Time elapsed: 01:05:22
                               ETA: 00:11:42

################################################################################
                     [1m Learning iteration 1697/2000 [0m                     

                       Computation: 46761 steps/s (collection: 2.010s, learning 0.093s)
             Mean action noise std: 3.68
          Mean value_function loss: 315.7400
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 83.0891
                       Mean reward: 719.14
               Mean episode length: 207.37
    Episode_Reward/reaching_object: 1.7183
     Episode_Reward/lifting_object: 144.3035
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0833
          Episode_Reward/joint_vel: -0.0963
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 166920192
                    Iteration time: 2.10s
                      Time elapsed: 01:05:24
                               ETA: 00:11:40

################################################################################
                     [1m Learning iteration 1698/2000 [0m                     

                       Computation: 45999 steps/s (collection: 2.034s, learning 0.103s)
             Mean action noise std: 3.68
          Mean value_function loss: 286.1751
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 83.0974
                       Mean reward: 780.23
               Mean episode length: 215.31
    Episode_Reward/reaching_object: 1.7899
     Episode_Reward/lifting_object: 152.1164
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0843
          Episode_Reward/joint_vel: -0.0937
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 167018496
                    Iteration time: 2.14s
                      Time elapsed: 01:05:26
                               ETA: 00:11:37

################################################################################
                     [1m Learning iteration 1699/2000 [0m                     

                       Computation: 46487 steps/s (collection: 2.021s, learning 0.094s)
             Mean action noise std: 3.68
          Mean value_function loss: 277.9143
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 83.1130
                       Mean reward: 710.00
               Mean episode length: 203.22
    Episode_Reward/reaching_object: 1.7457
     Episode_Reward/lifting_object: 147.4427
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0834
          Episode_Reward/joint_vel: -0.0948
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 167116800
                    Iteration time: 2.11s
                      Time elapsed: 01:05:28
                               ETA: 00:11:35

################################################################################
                     [1m Learning iteration 1700/2000 [0m                     

                       Computation: 46784 steps/s (collection: 2.006s, learning 0.095s)
             Mean action noise std: 3.69
          Mean value_function loss: 268.6504
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 83.1318
                       Mean reward: 769.25
               Mean episode length: 214.92
    Episode_Reward/reaching_object: 1.6960
     Episode_Reward/lifting_object: 142.4378
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0820
          Episode_Reward/joint_vel: -0.0933
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 167215104
                    Iteration time: 2.10s
                      Time elapsed: 01:05:30
                               ETA: 00:11:33

################################################################################
                     [1m Learning iteration 1701/2000 [0m                     

                       Computation: 46308 steps/s (collection: 2.017s, learning 0.106s)
             Mean action noise std: 3.69
          Mean value_function loss: 264.4590
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 83.1415
                       Mean reward: 699.51
               Mean episode length: 200.15
    Episode_Reward/reaching_object: 1.7417
     Episode_Reward/lifting_object: 146.7680
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0835
          Episode_Reward/joint_vel: -0.0955
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 167313408
                    Iteration time: 2.12s
                      Time elapsed: 01:05:32
                               ETA: 00:11:30

################################################################################
                     [1m Learning iteration 1702/2000 [0m                     

                       Computation: 46275 steps/s (collection: 2.020s, learning 0.104s)
             Mean action noise std: 3.69
          Mean value_function loss: 232.8409
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 83.1546
                       Mean reward: 718.58
               Mean episode length: 208.16
    Episode_Reward/reaching_object: 1.7611
     Episode_Reward/lifting_object: 149.0299
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0844
          Episode_Reward/joint_vel: -0.0949
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 167411712
                    Iteration time: 2.12s
                      Time elapsed: 01:05:35
                               ETA: 00:11:28

################################################################################
                     [1m Learning iteration 1703/2000 [0m                     

                       Computation: 46265 steps/s (collection: 2.021s, learning 0.104s)
             Mean action noise std: 3.69
          Mean value_function loss: 234.3319
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 83.1757
                       Mean reward: 771.68
               Mean episode length: 216.54
    Episode_Reward/reaching_object: 1.7108
     Episode_Reward/lifting_object: 143.8805
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0830
          Episode_Reward/joint_vel: -0.0947
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 167510016
                    Iteration time: 2.12s
                      Time elapsed: 01:05:37
                               ETA: 00:11:26

################################################################################
                     [1m Learning iteration 1704/2000 [0m                     

                       Computation: 46885 steps/s (collection: 1.992s, learning 0.105s)
             Mean action noise std: 3.70
          Mean value_function loss: 275.3677
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 83.1980
                       Mean reward: 755.39
               Mean episode length: 211.57
    Episode_Reward/reaching_object: 1.7517
     Episode_Reward/lifting_object: 147.9992
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0837
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 167608320
                    Iteration time: 2.10s
                      Time elapsed: 01:05:39
                               ETA: 00:11:23

################################################################################
                     [1m Learning iteration 1705/2000 [0m                     

                       Computation: 46833 steps/s (collection: 1.992s, learning 0.107s)
             Mean action noise std: 3.70
          Mean value_function loss: 326.4848
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 83.2231
                       Mean reward: 718.33
               Mean episode length: 204.67
    Episode_Reward/reaching_object: 1.6997
     Episode_Reward/lifting_object: 143.2112
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0819
          Episode_Reward/joint_vel: -0.0916
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 167706624
                    Iteration time: 2.10s
                      Time elapsed: 01:05:41
                               ETA: 00:11:21

################################################################################
                     [1m Learning iteration 1706/2000 [0m                     

                       Computation: 46624 steps/s (collection: 2.005s, learning 0.103s)
             Mean action noise std: 3.70
          Mean value_function loss: 280.9800
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 83.2495
                       Mean reward: 761.05
               Mean episode length: 217.25
    Episode_Reward/reaching_object: 1.7317
     Episode_Reward/lifting_object: 145.6810
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0838
          Episode_Reward/joint_vel: -0.0951
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 167804928
                    Iteration time: 2.11s
                      Time elapsed: 01:05:43
                               ETA: 00:11:19

################################################################################
                     [1m Learning iteration 1707/2000 [0m                     

                       Computation: 46231 steps/s (collection: 2.017s, learning 0.110s)
             Mean action noise std: 3.71
          Mean value_function loss: 264.7994
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 83.2693
                       Mean reward: 743.44
               Mean episode length: 211.01
    Episode_Reward/reaching_object: 1.7343
     Episode_Reward/lifting_object: 145.9998
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0838
          Episode_Reward/joint_vel: -0.0949
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 167903232
                    Iteration time: 2.13s
                      Time elapsed: 01:05:45
                               ETA: 00:11:16

################################################################################
                     [1m Learning iteration 1708/2000 [0m                     

                       Computation: 46799 steps/s (collection: 1.989s, learning 0.112s)
             Mean action noise std: 3.71
          Mean value_function loss: 240.7748
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 83.2869
                       Mean reward: 735.56
               Mean episode length: 209.26
    Episode_Reward/reaching_object: 1.7814
     Episode_Reward/lifting_object: 150.2541
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0857
          Episode_Reward/joint_vel: -0.0968
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 168001536
                    Iteration time: 2.10s
                      Time elapsed: 01:05:47
                               ETA: 00:11:14

################################################################################
                     [1m Learning iteration 1709/2000 [0m                     

                       Computation: 46002 steps/s (collection: 2.019s, learning 0.118s)
             Mean action noise std: 3.71
          Mean value_function loss: 236.6987
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 83.2984
                       Mean reward: 707.61
               Mean episode length: 210.84
    Episode_Reward/reaching_object: 1.7644
     Episode_Reward/lifting_object: 148.6634
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0863
          Episode_Reward/joint_vel: -0.0971
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 168099840
                    Iteration time: 2.14s
                      Time elapsed: 01:05:49
                               ETA: 00:11:12

################################################################################
                     [1m Learning iteration 1710/2000 [0m                     

                       Computation: 46752 steps/s (collection: 2.009s, learning 0.094s)
             Mean action noise std: 3.71
          Mean value_function loss: 274.7393
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 83.3079
                       Mean reward: 709.62
               Mean episode length: 205.33
    Episode_Reward/reaching_object: 1.7492
     Episode_Reward/lifting_object: 147.5369
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0851
          Episode_Reward/joint_vel: -0.0963
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 168198144
                    Iteration time: 2.10s
                      Time elapsed: 01:05:51
                               ETA: 00:11:09

################################################################################
                     [1m Learning iteration 1711/2000 [0m                     

                       Computation: 46141 steps/s (collection: 2.037s, learning 0.093s)
             Mean action noise std: 3.71
          Mean value_function loss: 254.2716
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 83.3185
                       Mean reward: 733.29
               Mean episode length: 210.94
    Episode_Reward/reaching_object: 1.7990
     Episode_Reward/lifting_object: 152.4068
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0876
          Episode_Reward/joint_vel: -0.0982
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 168296448
                    Iteration time: 2.13s
                      Time elapsed: 01:05:54
                               ETA: 00:11:07

################################################################################
                     [1m Learning iteration 1712/2000 [0m                     

                       Computation: 46356 steps/s (collection: 2.029s, learning 0.091s)
             Mean action noise std: 3.71
          Mean value_function loss: 270.0910
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 83.3390
                       Mean reward: 771.54
               Mean episode length: 219.29
    Episode_Reward/reaching_object: 1.7470
     Episode_Reward/lifting_object: 147.6913
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0857
          Episode_Reward/joint_vel: -0.0956
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 168394752
                    Iteration time: 2.12s
                      Time elapsed: 01:05:56
                               ETA: 00:11:05

################################################################################
                     [1m Learning iteration 1713/2000 [0m                     

                       Computation: 47035 steps/s (collection: 2.001s, learning 0.089s)
             Mean action noise std: 3.72
          Mean value_function loss: 266.3414
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 83.3518
                       Mean reward: 738.11
               Mean episode length: 207.91
    Episode_Reward/reaching_object: 1.7506
     Episode_Reward/lifting_object: 148.5889
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0854
          Episode_Reward/joint_vel: -0.0952
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168493056
                    Iteration time: 2.09s
                      Time elapsed: 01:05:58
                               ETA: 00:11:02

################################################################################
                     [1m Learning iteration 1714/2000 [0m                     

                       Computation: 46179 steps/s (collection: 2.017s, learning 0.112s)
             Mean action noise std: 3.72
          Mean value_function loss: 257.0970
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 83.3696
                       Mean reward: 761.81
               Mean episode length: 214.67
    Episode_Reward/reaching_object: 1.7017
     Episode_Reward/lifting_object: 143.4369
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0831
          Episode_Reward/joint_vel: -0.0925
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168591360
                    Iteration time: 2.13s
                      Time elapsed: 01:06:00
                               ETA: 00:11:00

################################################################################
                     [1m Learning iteration 1715/2000 [0m                     

                       Computation: 46580 steps/s (collection: 2.005s, learning 0.105s)
             Mean action noise std: 3.72
          Mean value_function loss: 276.7607
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 83.3884
                       Mean reward: 772.65
               Mean episode length: 216.28
    Episode_Reward/reaching_object: 1.8025
     Episode_Reward/lifting_object: 152.6229
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0869
          Episode_Reward/joint_vel: -0.0970
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 168689664
                    Iteration time: 2.11s
                      Time elapsed: 01:06:02
                               ETA: 00:10:58

################################################################################
                     [1m Learning iteration 1716/2000 [0m                     

                       Computation: 45900 steps/s (collection: 2.031s, learning 0.111s)
             Mean action noise std: 3.72
          Mean value_function loss: 291.8239
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 83.4042
                       Mean reward: 718.02
               Mean episode length: 208.98
    Episode_Reward/reaching_object: 1.7250
     Episode_Reward/lifting_object: 145.3903
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0850
          Episode_Reward/joint_vel: -0.0965
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 168787968
                    Iteration time: 2.14s
                      Time elapsed: 01:06:04
                               ETA: 00:10:55

################################################################################
                     [1m Learning iteration 1717/2000 [0m                     

                       Computation: 45947 steps/s (collection: 2.047s, learning 0.093s)
             Mean action noise std: 3.73
          Mean value_function loss: 277.7140
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 83.4216
                       Mean reward: 780.97
               Mean episode length: 219.40
    Episode_Reward/reaching_object: 1.7562
     Episode_Reward/lifting_object: 148.7889
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0856
          Episode_Reward/joint_vel: -0.0955
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 168886272
                    Iteration time: 2.14s
                      Time elapsed: 01:06:06
                               ETA: 00:10:53

################################################################################
                     [1m Learning iteration 1718/2000 [0m                     

                       Computation: 46709 steps/s (collection: 2.013s, learning 0.092s)
             Mean action noise std: 3.73
          Mean value_function loss: 265.0238
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 83.4405
                       Mean reward: 760.54
               Mean episode length: 212.50
    Episode_Reward/reaching_object: 1.7719
     Episode_Reward/lifting_object: 149.8091
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0861
          Episode_Reward/joint_vel: -0.0961
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 168984576
                    Iteration time: 2.10s
                      Time elapsed: 01:06:08
                               ETA: 00:10:51

################################################################################
                     [1m Learning iteration 1719/2000 [0m                     

                       Computation: 46115 steps/s (collection: 2.038s, learning 0.094s)
             Mean action noise std: 3.73
          Mean value_function loss: 268.1403
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 83.4544
                       Mean reward: 652.13
               Mean episode length: 192.39
    Episode_Reward/reaching_object: 1.7164
     Episode_Reward/lifting_object: 145.1528
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0842
          Episode_Reward/joint_vel: -0.0950
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 169082880
                    Iteration time: 2.13s
                      Time elapsed: 01:06:11
                               ETA: 00:10:48

################################################################################
                     [1m Learning iteration 1720/2000 [0m                     

                       Computation: 46102 steps/s (collection: 2.027s, learning 0.106s)
             Mean action noise std: 3.73
          Mean value_function loss: 246.8189
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 83.4712
                       Mean reward: 723.36
               Mean episode length: 206.47
    Episode_Reward/reaching_object: 1.7154
     Episode_Reward/lifting_object: 144.1902
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0850
          Episode_Reward/joint_vel: -0.0964
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 169181184
                    Iteration time: 2.13s
                      Time elapsed: 01:06:13
                               ETA: 00:10:46

################################################################################
                     [1m Learning iteration 1721/2000 [0m                     

                       Computation: 46451 steps/s (collection: 2.024s, learning 0.093s)
             Mean action noise std: 3.73
          Mean value_function loss: 254.3147
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 83.4807
                       Mean reward: 701.36
               Mean episode length: 206.69
    Episode_Reward/reaching_object: 1.7704
     Episode_Reward/lifting_object: 149.1769
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0868
          Episode_Reward/joint_vel: -0.0979
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 169279488
                    Iteration time: 2.12s
                      Time elapsed: 01:06:15
                               ETA: 00:10:44

################################################################################
                     [1m Learning iteration 1722/2000 [0m                     

                       Computation: 46267 steps/s (collection: 2.018s, learning 0.107s)
             Mean action noise std: 3.74
          Mean value_function loss: 271.7854
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 83.4962
                       Mean reward: 705.83
               Mean episode length: 207.59
    Episode_Reward/reaching_object: 1.7247
     Episode_Reward/lifting_object: 145.2572
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0851
          Episode_Reward/joint_vel: -0.0950
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 169377792
                    Iteration time: 2.12s
                      Time elapsed: 01:06:17
                               ETA: 00:10:41

################################################################################
                     [1m Learning iteration 1723/2000 [0m                     

                       Computation: 46108 steps/s (collection: 2.035s, learning 0.097s)
             Mean action noise std: 3.74
          Mean value_function loss: 251.5990
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 83.5119
                       Mean reward: 790.06
               Mean episode length: 221.32
    Episode_Reward/reaching_object: 1.8019
     Episode_Reward/lifting_object: 152.2813
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0887
          Episode_Reward/joint_vel: -0.0981
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 169476096
                    Iteration time: 2.13s
                      Time elapsed: 01:06:19
                               ETA: 00:10:39

################################################################################
                     [1m Learning iteration 1724/2000 [0m                     

                       Computation: 46645 steps/s (collection: 2.010s, learning 0.097s)
             Mean action noise std: 3.74
          Mean value_function loss: 247.8084
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 83.5249
                       Mean reward: 752.98
               Mean episode length: 211.75
    Episode_Reward/reaching_object: 1.7681
     Episode_Reward/lifting_object: 149.1669
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0871
          Episode_Reward/joint_vel: -0.0971
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 169574400
                    Iteration time: 2.11s
                      Time elapsed: 01:06:21
                               ETA: 00:10:37

################################################################################
                     [1m Learning iteration 1725/2000 [0m                     

                       Computation: 46299 steps/s (collection: 2.027s, learning 0.097s)
             Mean action noise std: 3.74
          Mean value_function loss: 258.9788
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 83.5443
                       Mean reward: 796.71
               Mean episode length: 224.09
    Episode_Reward/reaching_object: 1.6963
     Episode_Reward/lifting_object: 142.5015
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0843
          Episode_Reward/joint_vel: -0.0945
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 169672704
                    Iteration time: 2.12s
                      Time elapsed: 01:06:23
                               ETA: 00:10:34

################################################################################
                     [1m Learning iteration 1726/2000 [0m                     

                       Computation: 46526 steps/s (collection: 2.019s, learning 0.094s)
             Mean action noise std: 3.75
          Mean value_function loss: 275.7949
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 83.5723
                       Mean reward: 791.14
               Mean episode length: 218.62
    Episode_Reward/reaching_object: 1.8299
     Episode_Reward/lifting_object: 155.7835
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0887
          Episode_Reward/joint_vel: -0.0966
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 169771008
                    Iteration time: 2.11s
                      Time elapsed: 01:06:25
                               ETA: 00:10:32

################################################################################
                     [1m Learning iteration 1727/2000 [0m                     

                       Computation: 46341 steps/s (collection: 2.020s, learning 0.102s)
             Mean action noise std: 3.75
          Mean value_function loss: 282.7349
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 83.5958
                       Mean reward: 820.29
               Mean episode length: 227.49
    Episode_Reward/reaching_object: 1.8104
     Episode_Reward/lifting_object: 154.0288
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.0961
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 169869312
                    Iteration time: 2.12s
                      Time elapsed: 01:06:28
                               ETA: 00:10:30

################################################################################
                     [1m Learning iteration 1728/2000 [0m                     

                       Computation: 46046 steps/s (collection: 2.028s, learning 0.107s)
             Mean action noise std: 3.75
          Mean value_function loss: 286.9421
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 83.6057
                       Mean reward: 685.72
               Mean episode length: 202.56
    Episode_Reward/reaching_object: 1.6811
     Episode_Reward/lifting_object: 141.1334
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0847
          Episode_Reward/joint_vel: -0.0945
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 169967616
                    Iteration time: 2.13s
                      Time elapsed: 01:06:30
                               ETA: 00:10:27

################################################################################
                     [1m Learning iteration 1729/2000 [0m                     

                       Computation: 46780 steps/s (collection: 2.007s, learning 0.094s)
             Mean action noise std: 3.75
          Mean value_function loss: 280.6162
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 83.6164
                       Mean reward: 730.18
               Mean episode length: 210.90
    Episode_Reward/reaching_object: 1.7343
     Episode_Reward/lifting_object: 146.5137
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0862
          Episode_Reward/joint_vel: -0.0949
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 170065920
                    Iteration time: 2.10s
                      Time elapsed: 01:06:32
                               ETA: 00:10:25

################################################################################
                     [1m Learning iteration 1730/2000 [0m                     

                       Computation: 46365 steps/s (collection: 2.013s, learning 0.107s)
             Mean action noise std: 3.75
          Mean value_function loss: 241.9592
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 83.6345
                       Mean reward: 775.71
               Mean episode length: 218.06
    Episode_Reward/reaching_object: 1.7195
     Episode_Reward/lifting_object: 145.2129
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0862
          Episode_Reward/joint_vel: -0.0948
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 170164224
                    Iteration time: 2.12s
                      Time elapsed: 01:06:34
                               ETA: 00:10:23

################################################################################
                     [1m Learning iteration 1731/2000 [0m                     

                       Computation: 46049 steps/s (collection: 2.029s, learning 0.106s)
             Mean action noise std: 3.76
          Mean value_function loss: 230.9153
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 83.6491
                       Mean reward: 748.00
               Mean episode length: 218.80
    Episode_Reward/reaching_object: 1.7862
     Episode_Reward/lifting_object: 150.4420
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0890
          Episode_Reward/joint_vel: -0.0981
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 170262528
                    Iteration time: 2.13s
                      Time elapsed: 01:06:36
                               ETA: 00:10:20

################################################################################
                     [1m Learning iteration 1732/2000 [0m                     

                       Computation: 46745 steps/s (collection: 2.014s, learning 0.089s)
             Mean action noise std: 3.76
          Mean value_function loss: 194.9198
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 83.6582
                       Mean reward: 743.51
               Mean episode length: 210.41
    Episode_Reward/reaching_object: 1.7757
     Episode_Reward/lifting_object: 150.5532
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.0954
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 170360832
                    Iteration time: 2.10s
                      Time elapsed: 01:06:38
                               ETA: 00:10:18

################################################################################
                     [1m Learning iteration 1733/2000 [0m                     

                       Computation: 46285 steps/s (collection: 2.028s, learning 0.096s)
             Mean action noise std: 3.76
          Mean value_function loss: 256.9468
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 83.6738
                       Mean reward: 814.88
               Mean episode length: 225.09
    Episode_Reward/reaching_object: 1.8121
     Episode_Reward/lifting_object: 152.7884
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0894
          Episode_Reward/joint_vel: -0.0973
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 170459136
                    Iteration time: 2.12s
                      Time elapsed: 01:06:40
                               ETA: 00:10:16

################################################################################
                     [1m Learning iteration 1734/2000 [0m                     

                       Computation: 45618 steps/s (collection: 2.052s, learning 0.103s)
             Mean action noise std: 3.76
          Mean value_function loss: 246.8775
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 83.6963
                       Mean reward: 734.38
               Mean episode length: 206.59
    Episode_Reward/reaching_object: 1.7113
     Episode_Reward/lifting_object: 143.5926
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0855
          Episode_Reward/joint_vel: -0.0935
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 170557440
                    Iteration time: 2.15s
                      Time elapsed: 01:06:42
                               ETA: 00:10:13

################################################################################
                     [1m Learning iteration 1735/2000 [0m                     

                       Computation: 46455 steps/s (collection: 2.017s, learning 0.099s)
             Mean action noise std: 3.76
          Mean value_function loss: 261.0600
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 83.7180
                       Mean reward: 782.06
               Mean episode length: 216.54
    Episode_Reward/reaching_object: 1.7992
     Episode_Reward/lifting_object: 152.3317
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0891
          Episode_Reward/joint_vel: -0.0963
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 170655744
                    Iteration time: 2.12s
                      Time elapsed: 01:06:45
                               ETA: 00:10:11

################################################################################
                     [1m Learning iteration 1736/2000 [0m                     

                       Computation: 46200 steps/s (collection: 2.011s, learning 0.117s)
             Mean action noise std: 3.77
          Mean value_function loss: 243.5553
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 83.7318
                       Mean reward: 787.56
               Mean episode length: 219.63
    Episode_Reward/reaching_object: 1.7940
     Episode_Reward/lifting_object: 151.6361
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0887
          Episode_Reward/joint_vel: -0.0955
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 170754048
                    Iteration time: 2.13s
                      Time elapsed: 01:06:47
                               ETA: 00:10:09

################################################################################
                     [1m Learning iteration 1737/2000 [0m                     

                       Computation: 46136 steps/s (collection: 2.018s, learning 0.113s)
             Mean action noise std: 3.77
          Mean value_function loss: 270.9021
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 83.7451
                       Mean reward: 797.23
               Mean episode length: 221.18
    Episode_Reward/reaching_object: 1.7809
     Episode_Reward/lifting_object: 150.4443
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0891
          Episode_Reward/joint_vel: -0.0957
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 170852352
                    Iteration time: 2.13s
                      Time elapsed: 01:06:49
                               ETA: 00:10:06

################################################################################
                     [1m Learning iteration 1738/2000 [0m                     

                       Computation: 46074 steps/s (collection: 2.039s, learning 0.095s)
             Mean action noise std: 3.77
          Mean value_function loss: 291.4562
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 83.7520
                       Mean reward: 706.79
               Mean episode length: 204.12
    Episode_Reward/reaching_object: 1.7665
     Episode_Reward/lifting_object: 149.2922
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0878
          Episode_Reward/joint_vel: -0.0948
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 170950656
                    Iteration time: 2.13s
                      Time elapsed: 01:06:51
                               ETA: 00:10:04

################################################################################
                     [1m Learning iteration 1739/2000 [0m                     

                       Computation: 45840 steps/s (collection: 2.047s, learning 0.098s)
             Mean action noise std: 3.77
          Mean value_function loss: 280.1777
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 83.7645
                       Mean reward: 755.68
               Mean episode length: 211.48
    Episode_Reward/reaching_object: 1.7443
     Episode_Reward/lifting_object: 147.6287
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0868
          Episode_Reward/joint_vel: -0.0920
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 171048960
                    Iteration time: 2.14s
                      Time elapsed: 01:06:53
                               ETA: 00:10:02

################################################################################
                     [1m Learning iteration 1740/2000 [0m                     

                       Computation: 46601 steps/s (collection: 2.011s, learning 0.099s)
             Mean action noise std: 3.77
          Mean value_function loss: 219.1570
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 83.7851
                       Mean reward: 774.58
               Mean episode length: 220.04
    Episode_Reward/reaching_object: 1.8185
     Episode_Reward/lifting_object: 153.7137
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0910
          Episode_Reward/joint_vel: -0.0972
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 171147264
                    Iteration time: 2.11s
                      Time elapsed: 01:06:55
                               ETA: 00:09:59

################################################################################
                     [1m Learning iteration 1741/2000 [0m                     

                       Computation: 46393 steps/s (collection: 2.027s, learning 0.092s)
             Mean action noise std: 3.77
          Mean value_function loss: 237.6297
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 83.8023
                       Mean reward: 749.18
               Mean episode length: 213.86
    Episode_Reward/reaching_object: 1.7839
     Episode_Reward/lifting_object: 151.3788
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.0945
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 171245568
                    Iteration time: 2.12s
                      Time elapsed: 01:06:57
                               ETA: 00:09:57

################################################################################
                     [1m Learning iteration 1742/2000 [0m                     

                       Computation: 46255 steps/s (collection: 2.030s, learning 0.096s)
             Mean action noise std: 3.78
          Mean value_function loss: 245.7675
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 83.8130
                       Mean reward: 740.98
               Mean episode length: 212.70
    Episode_Reward/reaching_object: 1.7438
     Episode_Reward/lifting_object: 147.6209
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0879
          Episode_Reward/joint_vel: -0.0942
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 171343872
                    Iteration time: 2.13s
                      Time elapsed: 01:06:59
                               ETA: 00:09:55

################################################################################
                     [1m Learning iteration 1743/2000 [0m                     

                       Computation: 46325 steps/s (collection: 2.025s, learning 0.097s)
             Mean action noise std: 3.78
          Mean value_function loss: 228.0765
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 83.8261
                       Mean reward: 731.17
               Mean episode length: 206.71
    Episode_Reward/reaching_object: 1.8048
     Episode_Reward/lifting_object: 152.8750
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0891
          Episode_Reward/joint_vel: -0.0939
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 171442176
                    Iteration time: 2.12s
                      Time elapsed: 01:07:02
                               ETA: 00:09:52

################################################################################
                     [1m Learning iteration 1744/2000 [0m                     

                       Computation: 45804 steps/s (collection: 2.037s, learning 0.110s)
             Mean action noise std: 3.78
          Mean value_function loss: 243.6855
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 83.8431
                       Mean reward: 747.13
               Mean episode length: 212.00
    Episode_Reward/reaching_object: 1.7448
     Episode_Reward/lifting_object: 147.3583
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0877
          Episode_Reward/joint_vel: -0.0938
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 171540480
                    Iteration time: 2.15s
                      Time elapsed: 01:07:04
                               ETA: 00:09:50

################################################################################
                     [1m Learning iteration 1745/2000 [0m                     

                       Computation: 45896 steps/s (collection: 2.048s, learning 0.094s)
             Mean action noise std: 3.78
          Mean value_function loss: 239.4066
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 83.8602
                       Mean reward: 782.11
               Mean episode length: 217.94
    Episode_Reward/reaching_object: 1.8459
     Episode_Reward/lifting_object: 156.4207
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0915
          Episode_Reward/joint_vel: -0.0967
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 171638784
                    Iteration time: 2.14s
                      Time elapsed: 01:07:06
                               ETA: 00:09:48

################################################################################
                     [1m Learning iteration 1746/2000 [0m                     

                       Computation: 46732 steps/s (collection: 2.015s, learning 0.089s)
             Mean action noise std: 3.78
          Mean value_function loss: 286.6314
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 83.8687
                       Mean reward: 814.40
               Mean episode length: 224.95
    Episode_Reward/reaching_object: 1.7751
     Episode_Reward/lifting_object: 150.2968
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0887
          Episode_Reward/joint_vel: -0.0933
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 171737088
                    Iteration time: 2.10s
                      Time elapsed: 01:07:08
                               ETA: 00:09:45

################################################################################
                     [1m Learning iteration 1747/2000 [0m                     

                       Computation: 46371 steps/s (collection: 2.026s, learning 0.094s)
             Mean action noise std: 3.78
          Mean value_function loss: 240.8406
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 83.8795
                       Mean reward: 705.65
               Mean episode length: 204.37
    Episode_Reward/reaching_object: 1.8112
     Episode_Reward/lifting_object: 153.0632
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0910
          Episode_Reward/joint_vel: -0.0967
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 171835392
                    Iteration time: 2.12s
                      Time elapsed: 01:07:10
                               ETA: 00:09:43

################################################################################
                     [1m Learning iteration 1748/2000 [0m                     

                       Computation: 45593 steps/s (collection: 2.042s, learning 0.114s)
             Mean action noise std: 3.79
          Mean value_function loss: 248.1236
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 83.8821
                       Mean reward: 795.52
               Mean episode length: 222.85
    Episode_Reward/reaching_object: 1.7666
     Episode_Reward/lifting_object: 149.7602
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0891
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 171933696
                    Iteration time: 2.16s
                      Time elapsed: 01:07:12
                               ETA: 00:09:41

################################################################################
                     [1m Learning iteration 1749/2000 [0m                     

                       Computation: 46085 steps/s (collection: 2.034s, learning 0.099s)
             Mean action noise std: 3.79
          Mean value_function loss: 287.0084
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 83.8916
                       Mean reward: 728.29
               Mean episode length: 205.93
    Episode_Reward/reaching_object: 1.8206
     Episode_Reward/lifting_object: 154.7756
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0898
          Episode_Reward/joint_vel: -0.0925
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 172032000
                    Iteration time: 2.13s
                      Time elapsed: 01:07:14
                               ETA: 00:09:38

################################################################################
                     [1m Learning iteration 1750/2000 [0m                     

                       Computation: 45000 steps/s (collection: 2.065s, learning 0.120s)
             Mean action noise std: 3.79
          Mean value_function loss: 266.0918
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 83.9052
                       Mean reward: 750.03
               Mean episode length: 210.55
    Episode_Reward/reaching_object: 1.7821
     Episode_Reward/lifting_object: 150.9673
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0887
          Episode_Reward/joint_vel: -0.0926
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 172130304
                    Iteration time: 2.18s
                      Time elapsed: 01:07:17
                               ETA: 00:09:36

################################################################################
                     [1m Learning iteration 1751/2000 [0m                     

                       Computation: 46088 steps/s (collection: 2.036s, learning 0.097s)
             Mean action noise std: 3.79
          Mean value_function loss: 269.6697
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 83.9173
                       Mean reward: 698.46
               Mean episode length: 204.15
    Episode_Reward/reaching_object: 1.7325
     Episode_Reward/lifting_object: 145.9624
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0876
          Episode_Reward/joint_vel: -0.0916
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 172228608
                    Iteration time: 2.13s
                      Time elapsed: 01:07:19
                               ETA: 00:09:34

################################################################################
                     [1m Learning iteration 1752/2000 [0m                     

                       Computation: 45724 steps/s (collection: 2.059s, learning 0.091s)
             Mean action noise std: 3.79
          Mean value_function loss: 257.8053
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 83.9359
                       Mean reward: 796.02
               Mean episode length: 221.29
    Episode_Reward/reaching_object: 1.7763
     Episode_Reward/lifting_object: 150.3732
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0891
          Episode_Reward/joint_vel: -0.0931
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 172326912
                    Iteration time: 2.15s
                      Time elapsed: 01:07:21
                               ETA: 00:09:31

################################################################################
                     [1m Learning iteration 1753/2000 [0m                     

                       Computation: 45818 steps/s (collection: 2.053s, learning 0.093s)
             Mean action noise std: 3.79
          Mean value_function loss: 315.7216
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 83.9480
                       Mean reward: 763.90
               Mean episode length: 214.98
    Episode_Reward/reaching_object: 1.7277
     Episode_Reward/lifting_object: 144.3016
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0873
          Episode_Reward/joint_vel: -0.0925
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 172425216
                    Iteration time: 2.15s
                      Time elapsed: 01:07:23
                               ETA: 00:09:29

################################################################################
                     [1m Learning iteration 1754/2000 [0m                     

                       Computation: 46307 steps/s (collection: 2.034s, learning 0.089s)
             Mean action noise std: 3.79
          Mean value_function loss: 285.8936
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 83.9537
                       Mean reward: 749.10
               Mean episode length: 209.96
    Episode_Reward/reaching_object: 1.7876
     Episode_Reward/lifting_object: 150.6842
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0888
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 172523520
                    Iteration time: 2.12s
                      Time elapsed: 01:07:25
                               ETA: 00:09:27

################################################################################
                     [1m Learning iteration 1755/2000 [0m                     

                       Computation: 46207 steps/s (collection: 2.039s, learning 0.089s)
             Mean action noise std: 3.80
          Mean value_function loss: 229.8468
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 83.9650
                       Mean reward: 689.49
               Mean episode length: 199.37
    Episode_Reward/reaching_object: 1.7402
     Episode_Reward/lifting_object: 147.1151
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0875
          Episode_Reward/joint_vel: -0.0921
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 172621824
                    Iteration time: 2.13s
                      Time elapsed: 01:07:27
                               ETA: 00:09:24

################################################################################
                     [1m Learning iteration 1756/2000 [0m                     

                       Computation: 46502 steps/s (collection: 2.022s, learning 0.092s)
             Mean action noise std: 3.80
          Mean value_function loss: 226.1014
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 83.9766
                       Mean reward: 796.21
               Mean episode length: 222.92
    Episode_Reward/reaching_object: 1.8051
     Episode_Reward/lifting_object: 152.5946
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0900
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 172720128
                    Iteration time: 2.11s
                      Time elapsed: 01:07:29
                               ETA: 00:09:22

################################################################################
                     [1m Learning iteration 1757/2000 [0m                     

                       Computation: 45639 steps/s (collection: 2.050s, learning 0.104s)
             Mean action noise std: 3.80
          Mean value_function loss: 263.9069
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 83.9807
                       Mean reward: 763.98
               Mean episode length: 214.74
    Episode_Reward/reaching_object: 1.8095
     Episode_Reward/lifting_object: 153.2113
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0905
          Episode_Reward/joint_vel: -0.0943
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 172818432
                    Iteration time: 2.15s
                      Time elapsed: 01:07:31
                               ETA: 00:09:20

################################################################################
                     [1m Learning iteration 1758/2000 [0m                     

                       Computation: 45620 steps/s (collection: 2.032s, learning 0.123s)
             Mean action noise std: 3.80
          Mean value_function loss: 229.0691
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 83.9872
                       Mean reward: 747.50
               Mean episode length: 211.68
    Episode_Reward/reaching_object: 1.8497
     Episode_Reward/lifting_object: 156.5193
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0920
          Episode_Reward/joint_vel: -0.0948
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 172916736
                    Iteration time: 2.15s
                      Time elapsed: 01:07:34
                               ETA: 00:09:17

################################################################################
                     [1m Learning iteration 1759/2000 [0m                     

                       Computation: 45282 steps/s (collection: 2.049s, learning 0.122s)
             Mean action noise std: 3.80
          Mean value_function loss: 272.4811
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 84.0038
                       Mean reward: 722.27
               Mean episode length: 209.60
    Episode_Reward/reaching_object: 1.7399
     Episode_Reward/lifting_object: 146.0507
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0887
          Episode_Reward/joint_vel: -0.0939
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 173015040
                    Iteration time: 2.17s
                      Time elapsed: 01:07:36
                               ETA: 00:09:15

################################################################################
                     [1m Learning iteration 1760/2000 [0m                     

                       Computation: 44908 steps/s (collection: 2.078s, learning 0.111s)
             Mean action noise std: 3.80
          Mean value_function loss: 249.9295
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 84.0218
                       Mean reward: 740.10
               Mean episode length: 206.86
    Episode_Reward/reaching_object: 1.7842
     Episode_Reward/lifting_object: 150.6380
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0901
          Episode_Reward/joint_vel: -0.0936
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 173113344
                    Iteration time: 2.19s
                      Time elapsed: 01:07:38
                               ETA: 00:09:13

################################################################################
                     [1m Learning iteration 1761/2000 [0m                     

                       Computation: 45514 steps/s (collection: 2.055s, learning 0.105s)
             Mean action noise std: 3.81
          Mean value_function loss: 259.1398
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 84.0337
                       Mean reward: 720.27
               Mean episode length: 203.11
    Episode_Reward/reaching_object: 1.7610
     Episode_Reward/lifting_object: 147.9557
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0882
          Episode_Reward/joint_vel: -0.0909
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 173211648
                    Iteration time: 2.16s
                      Time elapsed: 01:07:40
                               ETA: 00:09:10

################################################################################
                     [1m Learning iteration 1762/2000 [0m                     

                       Computation: 45435 steps/s (collection: 2.056s, learning 0.108s)
             Mean action noise std: 3.81
          Mean value_function loss: 287.0261
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 84.0520
                       Mean reward: 778.87
               Mean episode length: 215.20
    Episode_Reward/reaching_object: 1.8084
     Episode_Reward/lifting_object: 153.0010
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0898
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173309952
                    Iteration time: 2.16s
                      Time elapsed: 01:07:42
                               ETA: 00:09:08

################################################################################
                     [1m Learning iteration 1763/2000 [0m                     

                       Computation: 46288 steps/s (collection: 2.033s, learning 0.091s)
             Mean action noise std: 3.81
          Mean value_function loss: 262.8475
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 84.0679
                       Mean reward: 818.13
               Mean episode length: 227.55
    Episode_Reward/reaching_object: 1.7847
     Episode_Reward/lifting_object: 150.1603
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0904
          Episode_Reward/joint_vel: -0.0956
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 173408256
                    Iteration time: 2.12s
                      Time elapsed: 01:07:44
                               ETA: 00:09:06

################################################################################
                     [1m Learning iteration 1764/2000 [0m                     

                       Computation: 46200 steps/s (collection: 2.036s, learning 0.092s)
             Mean action noise std: 3.81
          Mean value_function loss: 272.1262
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 84.0888
                       Mean reward: 829.60
               Mean episode length: 225.92
    Episode_Reward/reaching_object: 1.8391
     Episode_Reward/lifting_object: 155.6684
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0915
          Episode_Reward/joint_vel: -0.0936
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 173506560
                    Iteration time: 2.13s
                      Time elapsed: 01:07:47
                               ETA: 00:09:03

################################################################################
                     [1m Learning iteration 1765/2000 [0m                     

                       Computation: 46074 steps/s (collection: 2.039s, learning 0.095s)
             Mean action noise std: 3.82
          Mean value_function loss: 311.7150
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 84.1142
                       Mean reward: 696.97
               Mean episode length: 198.39
    Episode_Reward/reaching_object: 1.6904
     Episode_Reward/lifting_object: 141.5241
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0862
          Episode_Reward/joint_vel: -0.0905
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 173604864
                    Iteration time: 2.13s
                      Time elapsed: 01:07:49
                               ETA: 00:09:01

################################################################################
                     [1m Learning iteration 1766/2000 [0m                     

                       Computation: 46029 steps/s (collection: 2.036s, learning 0.100s)
             Mean action noise std: 3.82
          Mean value_function loss: 303.5997
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 84.1295
                       Mean reward: 783.67
               Mean episode length: 218.03
    Episode_Reward/reaching_object: 1.7650
     Episode_Reward/lifting_object: 148.1553
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0882
          Episode_Reward/joint_vel: -0.0921
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 173703168
                    Iteration time: 2.14s
                      Time elapsed: 01:07:51
                               ETA: 00:08:59

################################################################################
                     [1m Learning iteration 1767/2000 [0m                     

                       Computation: 45972 steps/s (collection: 2.051s, learning 0.088s)
             Mean action noise std: 3.82
          Mean value_function loss: 288.0483
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 84.1500
                       Mean reward: 744.48
               Mean episode length: 210.74
    Episode_Reward/reaching_object: 1.7483
     Episode_Reward/lifting_object: 146.9963
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0894
          Episode_Reward/joint_vel: -0.0940
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 173801472
                    Iteration time: 2.14s
                      Time elapsed: 01:07:53
                               ETA: 00:08:56

################################################################################
                     [1m Learning iteration 1768/2000 [0m                     

                       Computation: 45736 steps/s (collection: 2.042s, learning 0.107s)
             Mean action noise std: 3.82
          Mean value_function loss: 298.4181
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 84.1664
                       Mean reward: 731.09
               Mean episode length: 208.31
    Episode_Reward/reaching_object: 1.7824
     Episode_Reward/lifting_object: 150.0491
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0905
          Episode_Reward/joint_vel: -0.0943
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 173899776
                    Iteration time: 2.15s
                      Time elapsed: 01:07:55
                               ETA: 00:08:54

################################################################################
                     [1m Learning iteration 1769/2000 [0m                     

                       Computation: 44956 steps/s (collection: 2.087s, learning 0.100s)
             Mean action noise std: 3.83
          Mean value_function loss: 268.9160
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 84.1798
                       Mean reward: 719.99
               Mean episode length: 205.85
    Episode_Reward/reaching_object: 1.7739
     Episode_Reward/lifting_object: 149.7610
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0899
          Episode_Reward/joint_vel: -0.0923
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173998080
                    Iteration time: 2.19s
                      Time elapsed: 01:07:57
                               ETA: 00:08:52

################################################################################
                     [1m Learning iteration 1770/2000 [0m                     

                       Computation: 45853 steps/s (collection: 2.027s, learning 0.117s)
             Mean action noise std: 3.83
          Mean value_function loss: 277.0840
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 84.1948
                       Mean reward: 751.80
               Mean episode length: 214.77
    Episode_Reward/reaching_object: 1.7654
     Episode_Reward/lifting_object: 148.3824
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0905
          Episode_Reward/joint_vel: -0.0949
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 174096384
                    Iteration time: 2.14s
                      Time elapsed: 01:07:59
                               ETA: 00:08:49

################################################################################
                     [1m Learning iteration 1771/2000 [0m                     

                       Computation: 45809 steps/s (collection: 2.039s, learning 0.106s)
             Mean action noise std: 3.83
          Mean value_function loss: 306.6778
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 84.2126
                       Mean reward: 665.47
               Mean episode length: 191.08
    Episode_Reward/reaching_object: 1.7302
     Episode_Reward/lifting_object: 145.4921
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0884
          Episode_Reward/joint_vel: -0.0912
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 174194688
                    Iteration time: 2.15s
                      Time elapsed: 01:08:02
                               ETA: 00:08:47

################################################################################
                     [1m Learning iteration 1772/2000 [0m                     

                       Computation: 45275 steps/s (collection: 2.053s, learning 0.118s)
             Mean action noise std: 3.83
          Mean value_function loss: 280.6092
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 84.2316
                       Mean reward: 759.70
               Mean episode length: 211.46
    Episode_Reward/reaching_object: 1.8410
     Episode_Reward/lifting_object: 155.8945
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0924
          Episode_Reward/joint_vel: -0.0938
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 174292992
                    Iteration time: 2.17s
                      Time elapsed: 01:08:04
                               ETA: 00:08:45

################################################################################
                     [1m Learning iteration 1773/2000 [0m                     

                       Computation: 45487 steps/s (collection: 2.052s, learning 0.110s)
             Mean action noise std: 3.83
          Mean value_function loss: 255.4240
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 84.2431
                       Mean reward: 739.60
               Mean episode length: 210.72
    Episode_Reward/reaching_object: 1.7202
     Episode_Reward/lifting_object: 144.5352
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0891
          Episode_Reward/joint_vel: -0.0929
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 174391296
                    Iteration time: 2.16s
                      Time elapsed: 01:08:06
                               ETA: 00:08:42

################################################################################
                     [1m Learning iteration 1774/2000 [0m                     

                       Computation: 45392 steps/s (collection: 2.071s, learning 0.095s)
             Mean action noise std: 3.84
          Mean value_function loss: 225.7343
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 84.2615
                       Mean reward: 814.15
               Mean episode length: 226.33
    Episode_Reward/reaching_object: 1.7979
     Episode_Reward/lifting_object: 151.9416
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0921
          Episode_Reward/joint_vel: -0.0947
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 174489600
                    Iteration time: 2.17s
                      Time elapsed: 01:08:08
                               ETA: 00:08:40

################################################################################
                     [1m Learning iteration 1775/2000 [0m                     

                       Computation: 45455 steps/s (collection: 2.065s, learning 0.098s)
             Mean action noise std: 3.84
          Mean value_function loss: 242.9096
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 84.2785
                       Mean reward: 782.42
               Mean episode length: 222.71
    Episode_Reward/reaching_object: 1.8275
     Episode_Reward/lifting_object: 153.4218
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0937
          Episode_Reward/joint_vel: -0.0971
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 174587904
                    Iteration time: 2.16s
                      Time elapsed: 01:08:10
                               ETA: 00:08:38

################################################################################
                     [1m Learning iteration 1776/2000 [0m                     

                       Computation: 45907 steps/s (collection: 2.046s, learning 0.095s)
             Mean action noise std: 3.84
          Mean value_function loss: 258.0484
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 84.2823
                       Mean reward: 766.12
               Mean episode length: 211.30
    Episode_Reward/reaching_object: 1.7871
     Episode_Reward/lifting_object: 150.8265
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0913
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 174686208
                    Iteration time: 2.14s
                      Time elapsed: 01:08:12
                               ETA: 00:08:35

################################################################################
                     [1m Learning iteration 1777/2000 [0m                     

                       Computation: 41416 steps/s (collection: 2.228s, learning 0.146s)
             Mean action noise std: 3.84
          Mean value_function loss: 257.8791
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 84.2963
                       Mean reward: 741.45
               Mean episode length: 209.04
    Episode_Reward/reaching_object: 1.8001
     Episode_Reward/lifting_object: 151.8902
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0919
          Episode_Reward/joint_vel: -0.0960
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 174784512
                    Iteration time: 2.37s
                      Time elapsed: 01:08:15
                               ETA: 00:08:33

################################################################################
                     [1m Learning iteration 1778/2000 [0m                     

                       Computation: 41110 steps/s (collection: 2.257s, learning 0.134s)
             Mean action noise std: 3.84
          Mean value_function loss: 240.3807
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 84.3183
                       Mean reward: 784.33
               Mean episode length: 216.76
    Episode_Reward/reaching_object: 1.8337
     Episode_Reward/lifting_object: 155.7244
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0929
          Episode_Reward/joint_vel: -0.0932
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 174882816
                    Iteration time: 2.39s
                      Time elapsed: 01:08:17
                               ETA: 00:08:31

################################################################################
                     [1m Learning iteration 1779/2000 [0m                     

                       Computation: 43601 steps/s (collection: 2.157s, learning 0.098s)
             Mean action noise std: 3.85
          Mean value_function loss: 291.4479
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 84.3323
                       Mean reward: 717.18
               Mean episode length: 204.16
    Episode_Reward/reaching_object: 1.7934
     Episode_Reward/lifting_object: 151.5705
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0923
          Episode_Reward/joint_vel: -0.0940
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 174981120
                    Iteration time: 2.25s
                      Time elapsed: 01:08:19
                               ETA: 00:08:29

################################################################################
                     [1m Learning iteration 1780/2000 [0m                     

                       Computation: 42507 steps/s (collection: 2.187s, learning 0.125s)
             Mean action noise std: 3.85
          Mean value_function loss: 287.7784
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 84.3512
                       Mean reward: 762.68
               Mean episode length: 208.81
    Episode_Reward/reaching_object: 1.7767
     Episode_Reward/lifting_object: 150.5430
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0911
          Episode_Reward/joint_vel: -0.0921
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 175079424
                    Iteration time: 2.31s
                      Time elapsed: 01:08:22
                               ETA: 00:08:26

################################################################################
                     [1m Learning iteration 1781/2000 [0m                     

                       Computation: 41700 steps/s (collection: 2.241s, learning 0.116s)
             Mean action noise std: 3.85
          Mean value_function loss: 270.2384
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 84.3735
                       Mean reward: 755.54
               Mean episode length: 212.18
    Episode_Reward/reaching_object: 1.7921
     Episode_Reward/lifting_object: 151.8629
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0919
          Episode_Reward/joint_vel: -0.0932
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 175177728
                    Iteration time: 2.36s
                      Time elapsed: 01:08:24
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 1782/2000 [0m                     

                       Computation: 41408 steps/s (collection: 2.244s, learning 0.130s)
             Mean action noise std: 3.85
          Mean value_function loss: 294.1938
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 84.3908
                       Mean reward: 707.99
               Mean episode length: 199.27
    Episode_Reward/reaching_object: 1.7703
     Episode_Reward/lifting_object: 149.5474
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0910
          Episode_Reward/joint_vel: -0.0924
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 175276032
                    Iteration time: 2.37s
                      Time elapsed: 01:08:26
                               ETA: 00:08:22

################################################################################
                     [1m Learning iteration 1783/2000 [0m                     

                       Computation: 42432 steps/s (collection: 2.208s, learning 0.108s)
             Mean action noise std: 3.85
          Mean value_function loss: 237.8834
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 84.4019
                       Mean reward: 787.64
               Mean episode length: 219.12
    Episode_Reward/reaching_object: 1.8073
     Episode_Reward/lifting_object: 152.7895
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0931
          Episode_Reward/joint_vel: -0.0942
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 175374336
                    Iteration time: 2.32s
                      Time elapsed: 01:08:29
                               ETA: 00:08:19

################################################################################
                     [1m Learning iteration 1784/2000 [0m                     

                       Computation: 40639 steps/s (collection: 2.307s, learning 0.112s)
             Mean action noise std: 3.86
          Mean value_function loss: 220.3630
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 84.4155
                       Mean reward: 768.47
               Mean episode length: 213.00
    Episode_Reward/reaching_object: 1.8295
     Episode_Reward/lifting_object: 154.6415
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0930
          Episode_Reward/joint_vel: -0.0936
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 175472640
                    Iteration time: 2.42s
                      Time elapsed: 01:08:31
                               ETA: 00:08:17

################################################################################
                     [1m Learning iteration 1785/2000 [0m                     

                       Computation: 35057 steps/s (collection: 2.526s, learning 0.278s)
             Mean action noise std: 3.86
          Mean value_function loss: 235.8201
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 84.4340
                       Mean reward: 796.26
               Mean episode length: 219.76
    Episode_Reward/reaching_object: 1.8865
     Episode_Reward/lifting_object: 160.1412
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0962
          Episode_Reward/joint_vel: -0.0948
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 175570944
                    Iteration time: 2.80s
                      Time elapsed: 01:08:34
                               ETA: 00:08:15

################################################################################
                     [1m Learning iteration 1786/2000 [0m                     

                       Computation: 34508 steps/s (collection: 2.663s, learning 0.186s)
             Mean action noise std: 3.86
          Mean value_function loss: 267.1213
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 84.4559
                       Mean reward: 773.57
               Mean episode length: 218.74
    Episode_Reward/reaching_object: 1.7837
     Episode_Reward/lifting_object: 150.3474
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0921
          Episode_Reward/joint_vel: -0.0922
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 175669248
                    Iteration time: 2.85s
                      Time elapsed: 01:08:37
                               ETA: 00:08:13

################################################################################
                     [1m Learning iteration 1787/2000 [0m                     

                       Computation: 38238 steps/s (collection: 2.458s, learning 0.113s)
             Mean action noise std: 3.86
          Mean value_function loss: 318.6429
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 84.4684
                       Mean reward: 752.50
               Mean episode length: 213.16
    Episode_Reward/reaching_object: 1.7745
     Episode_Reward/lifting_object: 149.8216
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0911
          Episode_Reward/joint_vel: -0.0897
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 175767552
                    Iteration time: 2.57s
                      Time elapsed: 01:08:39
                               ETA: 00:08:10

################################################################################
                     [1m Learning iteration 1788/2000 [0m                     

                       Computation: 43863 steps/s (collection: 2.143s, learning 0.098s)
             Mean action noise std: 3.87
          Mean value_function loss: 343.0497
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 84.4811
                       Mean reward: 666.26
               Mean episode length: 190.55
    Episode_Reward/reaching_object: 1.7225
     Episode_Reward/lifting_object: 143.6088
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0894
          Episode_Reward/joint_vel: -0.0900
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 175865856
                    Iteration time: 2.24s
                      Time elapsed: 01:08:42
                               ETA: 00:08:08

################################################################################
                     [1m Learning iteration 1789/2000 [0m                     

                       Computation: 44009 steps/s (collection: 2.126s, learning 0.108s)
             Mean action noise std: 3.87
          Mean value_function loss: 292.6106
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 84.4985
                       Mean reward: 753.91
               Mean episode length: 215.83
    Episode_Reward/reaching_object: 1.7790
     Episode_Reward/lifting_object: 149.8461
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0920
          Episode_Reward/joint_vel: -0.0907
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 175964160
                    Iteration time: 2.23s
                      Time elapsed: 01:08:44
                               ETA: 00:08:06

################################################################################
                     [1m Learning iteration 1790/2000 [0m                     

                       Computation: 43878 steps/s (collection: 2.127s, learning 0.113s)
             Mean action noise std: 3.87
          Mean value_function loss: 309.6243
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 84.5127
                       Mean reward: 757.11
               Mean episode length: 214.53
    Episode_Reward/reaching_object: 1.8035
     Episode_Reward/lifting_object: 152.4188
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0928
          Episode_Reward/joint_vel: -0.0916
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 176062464
                    Iteration time: 2.24s
                      Time elapsed: 01:08:46
                               ETA: 00:08:03

################################################################################
                     [1m Learning iteration 1791/2000 [0m                     

                       Computation: 43851 steps/s (collection: 2.147s, learning 0.095s)
             Mean action noise std: 3.87
          Mean value_function loss: 283.0773
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 84.5200
                       Mean reward: 736.75
               Mean episode length: 208.72
    Episode_Reward/reaching_object: 1.8136
     Episode_Reward/lifting_object: 152.5415
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0934
          Episode_Reward/joint_vel: -0.0919
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 176160768
                    Iteration time: 2.24s
                      Time elapsed: 01:08:48
                               ETA: 00:08:01

################################################################################
                     [1m Learning iteration 1792/2000 [0m                     

                       Computation: 43332 steps/s (collection: 2.138s, learning 0.131s)
             Mean action noise std: 3.87
          Mean value_function loss: 260.5132
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 84.5238
                       Mean reward: 787.13
               Mean episode length: 217.64
    Episode_Reward/reaching_object: 1.8496
     Episode_Reward/lifting_object: 156.0056
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0946
          Episode_Reward/joint_vel: -0.0930
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 176259072
                    Iteration time: 2.27s
                      Time elapsed: 01:08:51
                               ETA: 00:07:59

################################################################################
                     [1m Learning iteration 1793/2000 [0m                     

                       Computation: 41147 steps/s (collection: 2.287s, learning 0.102s)
             Mean action noise std: 3.87
          Mean value_function loss: 262.8298
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 84.5305
                       Mean reward: 784.47
               Mean episode length: 218.60
    Episode_Reward/reaching_object: 1.8386
     Episode_Reward/lifting_object: 154.5813
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0937
          Episode_Reward/joint_vel: -0.0917
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 176357376
                    Iteration time: 2.39s
                      Time elapsed: 01:08:53
                               ETA: 00:07:56

################################################################################
                     [1m Learning iteration 1794/2000 [0m                     

                       Computation: 45073 steps/s (collection: 2.081s, learning 0.100s)
             Mean action noise std: 3.87
          Mean value_function loss: 277.5836
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 84.5397
                       Mean reward: 718.00
               Mean episode length: 206.51
    Episode_Reward/reaching_object: 1.7569
     Episode_Reward/lifting_object: 148.2221
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0920
          Episode_Reward/joint_vel: -0.0904
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 176455680
                    Iteration time: 2.18s
                      Time elapsed: 01:08:55
                               ETA: 00:07:54

################################################################################
                     [1m Learning iteration 1795/2000 [0m                     

                       Computation: 45317 steps/s (collection: 2.077s, learning 0.092s)
             Mean action noise std: 3.88
          Mean value_function loss: 312.5201
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 84.5483
                       Mean reward: 730.30
               Mean episode length: 206.08
    Episode_Reward/reaching_object: 1.7472
     Episode_Reward/lifting_object: 146.5836
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0904
          Episode_Reward/joint_vel: -0.0888
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 176553984
                    Iteration time: 2.17s
                      Time elapsed: 01:08:57
                               ETA: 00:07:52

################################################################################
                     [1m Learning iteration 1796/2000 [0m                     

                       Computation: 43995 steps/s (collection: 2.111s, learning 0.123s)
             Mean action noise std: 3.88
          Mean value_function loss: 337.7727
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 84.5624
                       Mean reward: 755.04
               Mean episode length: 208.18
    Episode_Reward/reaching_object: 1.7323
     Episode_Reward/lifting_object: 145.3481
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0899
          Episode_Reward/joint_vel: -0.0882
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 176652288
                    Iteration time: 2.23s
                      Time elapsed: 01:09:00
                               ETA: 00:07:49

################################################################################
                     [1m Learning iteration 1797/2000 [0m                     

                       Computation: 42401 steps/s (collection: 2.216s, learning 0.103s)
             Mean action noise std: 3.88
          Mean value_function loss: 281.6753
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 84.5797
                       Mean reward: 756.84
               Mean episode length: 218.50
    Episode_Reward/reaching_object: 1.7821
     Episode_Reward/lifting_object: 149.8132
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0936
          Episode_Reward/joint_vel: -0.0919
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 176750592
                    Iteration time: 2.32s
                      Time elapsed: 01:09:02
                               ETA: 00:07:47

################################################################################
                     [1m Learning iteration 1798/2000 [0m                     

                       Computation: 43343 steps/s (collection: 2.156s, learning 0.112s)
             Mean action noise std: 3.88
          Mean value_function loss: 292.6391
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 84.5978
                       Mean reward: 777.16
               Mean episode length: 216.59
    Episode_Reward/reaching_object: 1.7806
     Episode_Reward/lifting_object: 149.7885
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0923
          Episode_Reward/joint_vel: -0.0899
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 176848896
                    Iteration time: 2.27s
                      Time elapsed: 01:09:04
                               ETA: 00:07:45

################################################################################
                     [1m Learning iteration 1799/2000 [0m                     

                       Computation: 44666 steps/s (collection: 2.092s, learning 0.109s)
             Mean action noise std: 3.88
          Mean value_function loss: 358.0401
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 84.6095
                       Mean reward: 774.64
               Mean episode length: 214.40
    Episode_Reward/reaching_object: 1.7802
     Episode_Reward/lifting_object: 149.5235
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0924
          Episode_Reward/joint_vel: -0.0899
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 176947200
                    Iteration time: 2.20s
                      Time elapsed: 01:09:06
                               ETA: 00:07:43

################################################################################
                     [1m Learning iteration 1800/2000 [0m                     

                       Computation: 44718 steps/s (collection: 2.092s, learning 0.106s)
             Mean action noise std: 3.89
          Mean value_function loss: 319.1823
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 84.6216
                       Mean reward: 716.11
               Mean episode length: 200.35
    Episode_Reward/reaching_object: 1.7640
     Episode_Reward/lifting_object: 148.1171
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0921
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 177045504
                    Iteration time: 2.20s
                      Time elapsed: 01:09:09
                               ETA: 00:07:40

################################################################################
                     [1m Learning iteration 1801/2000 [0m                     

                       Computation: 44606 steps/s (collection: 2.111s, learning 0.093s)
             Mean action noise std: 3.89
          Mean value_function loss: 302.9158
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 84.6310
                       Mean reward: 802.95
               Mean episode length: 220.27
    Episode_Reward/reaching_object: 1.7643
     Episode_Reward/lifting_object: 148.6743
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0919
          Episode_Reward/joint_vel: -0.0886
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 177143808
                    Iteration time: 2.20s
                      Time elapsed: 01:09:11
                               ETA: 00:07:38

################################################################################
                     [1m Learning iteration 1802/2000 [0m                     

                       Computation: 45070 steps/s (collection: 2.073s, learning 0.108s)
             Mean action noise std: 3.89
          Mean value_function loss: 272.7334
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 84.6339
                       Mean reward: 743.66
               Mean episode length: 210.86
    Episode_Reward/reaching_object: 1.7385
     Episode_Reward/lifting_object: 146.7374
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0917
          Episode_Reward/joint_vel: -0.0900
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 177242112
                    Iteration time: 2.18s
                      Time elapsed: 01:09:13
                               ETA: 00:07:36

################################################################################
                     [1m Learning iteration 1803/2000 [0m                     

                       Computation: 44898 steps/s (collection: 2.097s, learning 0.093s)
             Mean action noise std: 3.89
          Mean value_function loss: 349.4863
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 84.6439
                       Mean reward: 710.55
               Mean episode length: 202.35
    Episode_Reward/reaching_object: 1.6852
     Episode_Reward/lifting_object: 141.5052
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0891
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 177340416
                    Iteration time: 2.19s
                      Time elapsed: 01:09:15
                               ETA: 00:07:33

################################################################################
                     [1m Learning iteration 1804/2000 [0m                     

                       Computation: 44854 steps/s (collection: 2.105s, learning 0.086s)
             Mean action noise std: 3.89
          Mean value_function loss: 316.6795
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 84.6566
                       Mean reward: 711.99
               Mean episode length: 202.18
    Episode_Reward/reaching_object: 1.6878
     Episode_Reward/lifting_object: 140.8689
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0891
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 177438720
                    Iteration time: 2.19s
                      Time elapsed: 01:09:17
                               ETA: 00:07:31

################################################################################
                     [1m Learning iteration 1805/2000 [0m                     

                       Computation: 45337 steps/s (collection: 2.063s, learning 0.105s)
             Mean action noise std: 3.89
          Mean value_function loss: 258.6468
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 84.6640
                       Mean reward: 782.42
               Mean episode length: 214.42
    Episode_Reward/reaching_object: 1.7845
     Episode_Reward/lifting_object: 150.7640
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0925
          Episode_Reward/joint_vel: -0.0882
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 177537024
                    Iteration time: 2.17s
                      Time elapsed: 01:09:20
                               ETA: 00:07:29

################################################################################
                     [1m Learning iteration 1806/2000 [0m                     

                       Computation: 45260 steps/s (collection: 2.068s, learning 0.104s)
             Mean action noise std: 3.89
          Mean value_function loss: 271.0166
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 84.6688
                       Mean reward: 751.91
               Mean episode length: 212.89
    Episode_Reward/reaching_object: 1.7738
     Episode_Reward/lifting_object: 149.7368
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0936
          Episode_Reward/joint_vel: -0.0911
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 177635328
                    Iteration time: 2.17s
                      Time elapsed: 01:09:22
                               ETA: 00:07:26

################################################################################
                     [1m Learning iteration 1807/2000 [0m                     

                       Computation: 45706 steps/s (collection: 2.061s, learning 0.090s)
             Mean action noise std: 3.89
          Mean value_function loss: 277.1226
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 84.6775
                       Mean reward: 700.62
               Mean episode length: 198.37
    Episode_Reward/reaching_object: 1.7308
     Episode_Reward/lifting_object: 145.3058
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0905
          Episode_Reward/joint_vel: -0.0884
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 177733632
                    Iteration time: 2.15s
                      Time elapsed: 01:09:24
                               ETA: 00:07:24

################################################################################
                     [1m Learning iteration 1808/2000 [0m                     

                       Computation: 45689 steps/s (collection: 2.060s, learning 0.092s)
             Mean action noise std: 3.90
          Mean value_function loss: 259.9817
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 84.6904
                       Mean reward: 767.72
               Mean episode length: 215.43
    Episode_Reward/reaching_object: 1.8285
     Episode_Reward/lifting_object: 154.6271
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0956
          Episode_Reward/joint_vel: -0.0918
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 177831936
                    Iteration time: 2.15s
                      Time elapsed: 01:09:26
                               ETA: 00:07:22

################################################################################
                     [1m Learning iteration 1809/2000 [0m                     

                       Computation: 45748 steps/s (collection: 2.058s, learning 0.091s)
             Mean action noise std: 3.90
          Mean value_function loss: 291.3516
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 84.7000
                       Mean reward: 822.83
               Mean episode length: 224.33
    Episode_Reward/reaching_object: 1.7963
     Episode_Reward/lifting_object: 150.5841
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0938
          Episode_Reward/joint_vel: -0.0920
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 177930240
                    Iteration time: 2.15s
                      Time elapsed: 01:09:28
                               ETA: 00:07:19

################################################################################
                     [1m Learning iteration 1810/2000 [0m                     

                       Computation: 45900 steps/s (collection: 2.048s, learning 0.094s)
             Mean action noise std: 3.90
          Mean value_function loss: 270.0204
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 84.7055
                       Mean reward: 841.04
               Mean episode length: 229.92
    Episode_Reward/reaching_object: 1.8513
     Episode_Reward/lifting_object: 156.6554
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0967
          Episode_Reward/joint_vel: -0.0927
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 178028544
                    Iteration time: 2.14s
                      Time elapsed: 01:09:30
                               ETA: 00:07:17

################################################################################
                     [1m Learning iteration 1811/2000 [0m                     

                       Computation: 45360 steps/s (collection: 2.065s, learning 0.102s)
             Mean action noise std: 3.90
          Mean value_function loss: 247.0322
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 84.7114
                       Mean reward: 784.10
               Mean episode length: 220.70
    Episode_Reward/reaching_object: 1.8615
     Episode_Reward/lifting_object: 156.1104
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0968
          Episode_Reward/joint_vel: -0.0938
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 178126848
                    Iteration time: 2.17s
                      Time elapsed: 01:09:32
                               ETA: 00:07:15

################################################################################
                     [1m Learning iteration 1812/2000 [0m                     

                       Computation: 45515 steps/s (collection: 2.063s, learning 0.097s)
             Mean action noise std: 3.90
          Mean value_function loss: 285.4263
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 84.7183
                       Mean reward: 747.41
               Mean episode length: 209.30
    Episode_Reward/reaching_object: 1.8174
     Episode_Reward/lifting_object: 153.5002
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0943
          Episode_Reward/joint_vel: -0.0898
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 178225152
                    Iteration time: 2.16s
                      Time elapsed: 01:09:35
                               ETA: 00:07:12

################################################################################
                     [1m Learning iteration 1813/2000 [0m                     

                       Computation: 45186 steps/s (collection: 2.085s, learning 0.090s)
             Mean action noise std: 3.90
          Mean value_function loss: 266.6322
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 84.7225
                       Mean reward: 787.45
               Mean episode length: 216.97
    Episode_Reward/reaching_object: 1.8378
     Episode_Reward/lifting_object: 155.5152
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0953
          Episode_Reward/joint_vel: -0.0904
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 178323456
                    Iteration time: 2.18s
                      Time elapsed: 01:09:37
                               ETA: 00:07:10

################################################################################
                     [1m Learning iteration 1814/2000 [0m                     

                       Computation: 45062 steps/s (collection: 2.074s, learning 0.108s)
             Mean action noise std: 3.90
          Mean value_function loss: 307.0971
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 84.7263
                       Mean reward: 776.27
               Mean episode length: 218.26
    Episode_Reward/reaching_object: 1.7791
     Episode_Reward/lifting_object: 149.7290
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0946
          Episode_Reward/joint_vel: -0.0939
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 178421760
                    Iteration time: 2.18s
                      Time elapsed: 01:09:39
                               ETA: 00:07:08

################################################################################
                     [1m Learning iteration 1815/2000 [0m                     

                       Computation: 45019 steps/s (collection: 2.081s, learning 0.102s)
             Mean action noise std: 3.90
          Mean value_function loss: 248.7863
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 84.7366
                       Mean reward: 781.09
               Mean episode length: 218.30
    Episode_Reward/reaching_object: 1.7748
     Episode_Reward/lifting_object: 149.8087
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0929
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 178520064
                    Iteration time: 2.18s
                      Time elapsed: 01:09:41
                               ETA: 00:07:05

################################################################################
                     [1m Learning iteration 1816/2000 [0m                     

                       Computation: 45044 steps/s (collection: 2.077s, learning 0.105s)
             Mean action noise std: 3.91
          Mean value_function loss: 266.5457
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 84.7487
                       Mean reward: 797.54
               Mean episode length: 222.96
    Episode_Reward/reaching_object: 1.7952
     Episode_Reward/lifting_object: 151.0212
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0952
          Episode_Reward/joint_vel: -0.0942
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 178618368
                    Iteration time: 2.18s
                      Time elapsed: 01:09:43
                               ETA: 00:07:03

################################################################################
                     [1m Learning iteration 1817/2000 [0m                     

                       Computation: 45243 steps/s (collection: 2.078s, learning 0.094s)
             Mean action noise std: 3.91
          Mean value_function loss: 244.2289
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 84.7612
                       Mean reward: 666.84
               Mean episode length: 196.58
    Episode_Reward/reaching_object: 1.7931
     Episode_Reward/lifting_object: 151.0397
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0952
          Episode_Reward/joint_vel: -0.0947
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 178716672
                    Iteration time: 2.17s
                      Time elapsed: 01:09:46
                               ETA: 00:07:01

################################################################################
                     [1m Learning iteration 1818/2000 [0m                     

                       Computation: 44812 steps/s (collection: 2.106s, learning 0.088s)
             Mean action noise std: 3.91
          Mean value_function loss: 267.0211
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 84.7708
                       Mean reward: 761.46
               Mean episode length: 216.54
    Episode_Reward/reaching_object: 1.8038
     Episode_Reward/lifting_object: 151.8189
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0955
          Episode_Reward/joint_vel: -0.0952
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 178814976
                    Iteration time: 2.19s
                      Time elapsed: 01:09:48
                               ETA: 00:06:59

################################################################################
                     [1m Learning iteration 1819/2000 [0m                     

                       Computation: 45761 steps/s (collection: 2.047s, learning 0.102s)
             Mean action noise std: 3.91
          Mean value_function loss: 225.4310
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 84.7784
                       Mean reward: 815.98
               Mean episode length: 225.50
    Episode_Reward/reaching_object: 1.9257
     Episode_Reward/lifting_object: 163.5437
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.1001
          Episode_Reward/joint_vel: -0.0973
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 178913280
                    Iteration time: 2.15s
                      Time elapsed: 01:09:50
                               ETA: 00:06:56

################################################################################
                     [1m Learning iteration 1820/2000 [0m                     

                       Computation: 45850 steps/s (collection: 2.036s, learning 0.108s)
             Mean action noise std: 3.91
          Mean value_function loss: 230.8627
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 84.7872
                       Mean reward: 810.68
               Mean episode length: 223.02
    Episode_Reward/reaching_object: 1.8692
     Episode_Reward/lifting_object: 157.6324
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0982
          Episode_Reward/joint_vel: -0.0969
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 179011584
                    Iteration time: 2.14s
                      Time elapsed: 01:09:52
                               ETA: 00:06:54

################################################################################
                     [1m Learning iteration 1821/2000 [0m                     

                       Computation: 43959 steps/s (collection: 2.121s, learning 0.116s)
             Mean action noise std: 3.91
          Mean value_function loss: 247.1854
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 84.7984
                       Mean reward: 789.33
               Mean episode length: 220.92
    Episode_Reward/reaching_object: 1.8538
     Episode_Reward/lifting_object: 156.4725
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0973
          Episode_Reward/joint_vel: -0.0959
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 179109888
                    Iteration time: 2.24s
                      Time elapsed: 01:09:54
                               ETA: 00:06:52

################################################################################
                     [1m Learning iteration 1822/2000 [0m                     

                       Computation: 45563 steps/s (collection: 2.042s, learning 0.115s)
             Mean action noise std: 3.91
          Mean value_function loss: 287.7271
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 84.8097
                       Mean reward: 727.21
               Mean episode length: 208.17
    Episode_Reward/reaching_object: 1.7320
     Episode_Reward/lifting_object: 145.2880
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0932
          Episode_Reward/joint_vel: -0.0942
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 179208192
                    Iteration time: 2.16s
                      Time elapsed: 01:09:56
                               ETA: 00:06:49

################################################################################
                     [1m Learning iteration 1823/2000 [0m                     

                       Computation: 45374 steps/s (collection: 2.059s, learning 0.107s)
             Mean action noise std: 3.92
          Mean value_function loss: 289.8722
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 84.8153
                       Mean reward: 764.16
               Mean episode length: 219.82
    Episode_Reward/reaching_object: 1.8753
     Episode_Reward/lifting_object: 158.3112
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0988
          Episode_Reward/joint_vel: -0.0979
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 179306496
                    Iteration time: 2.17s
                      Time elapsed: 01:09:59
                               ETA: 00:06:47

################################################################################
                     [1m Learning iteration 1824/2000 [0m                     

                       Computation: 45944 steps/s (collection: 2.050s, learning 0.090s)
             Mean action noise std: 3.92
          Mean value_function loss: 261.1155
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 84.8299
                       Mean reward: 685.65
               Mean episode length: 202.53
    Episode_Reward/reaching_object: 1.7613
     Episode_Reward/lifting_object: 146.8826
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0943
          Episode_Reward/joint_vel: -0.0948
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 179404800
                    Iteration time: 2.14s
                      Time elapsed: 01:10:01
                               ETA: 00:06:45

################################################################################
                     [1m Learning iteration 1825/2000 [0m                     

                       Computation: 45399 steps/s (collection: 2.067s, learning 0.099s)
             Mean action noise std: 3.92
          Mean value_function loss: 250.5408
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 84.8488
                       Mean reward: 766.28
               Mean episode length: 212.36
    Episode_Reward/reaching_object: 1.7969
     Episode_Reward/lifting_object: 151.2546
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0956
          Episode_Reward/joint_vel: -0.0939
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 179503104
                    Iteration time: 2.17s
                      Time elapsed: 01:10:03
                               ETA: 00:06:42

################################################################################
                     [1m Learning iteration 1826/2000 [0m                     

                       Computation: 44049 steps/s (collection: 2.098s, learning 0.133s)
             Mean action noise std: 3.92
          Mean value_function loss: 275.7357
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 84.8662
                       Mean reward: 787.98
               Mean episode length: 218.06
    Episode_Reward/reaching_object: 1.7955
     Episode_Reward/lifting_object: 151.5159
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0948
          Episode_Reward/joint_vel: -0.0942
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 179601408
                    Iteration time: 2.23s
                      Time elapsed: 01:10:05
                               ETA: 00:06:40

################################################################################
                     [1m Learning iteration 1827/2000 [0m                     

                       Computation: 43632 steps/s (collection: 2.142s, learning 0.111s)
             Mean action noise std: 3.93
          Mean value_function loss: 238.9581
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 84.8784
                       Mean reward: 766.25
               Mean episode length: 212.83
    Episode_Reward/reaching_object: 1.8309
     Episode_Reward/lifting_object: 154.4716
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0965
          Episode_Reward/joint_vel: -0.0948
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 179699712
                    Iteration time: 2.25s
                      Time elapsed: 01:10:07
                               ETA: 00:06:38

################################################################################
                     [1m Learning iteration 1828/2000 [0m                     

                       Computation: 43871 steps/s (collection: 2.128s, learning 0.113s)
             Mean action noise std: 3.93
          Mean value_function loss: 217.0300
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 84.8878
                       Mean reward: 824.88
               Mean episode length: 226.05
    Episode_Reward/reaching_object: 1.8586
     Episode_Reward/lifting_object: 157.0931
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0993
          Episode_Reward/joint_vel: -0.0983
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 179798016
                    Iteration time: 2.24s
                      Time elapsed: 01:10:10
                               ETA: 00:06:35

################################################################################
                     [1m Learning iteration 1829/2000 [0m                     

                       Computation: 45649 steps/s (collection: 2.059s, learning 0.094s)
             Mean action noise std: 3.93
          Mean value_function loss: 261.4288
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 84.8903
                       Mean reward: 789.22
               Mean episode length: 216.52
    Episode_Reward/reaching_object: 1.7870
     Episode_Reward/lifting_object: 150.2741
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0943
          Episode_Reward/joint_vel: -0.0933
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 179896320
                    Iteration time: 2.15s
                      Time elapsed: 01:10:12
                               ETA: 00:06:33

################################################################################
                     [1m Learning iteration 1830/2000 [0m                     

                       Computation: 45527 steps/s (collection: 2.066s, learning 0.093s)
             Mean action noise std: 3.93
          Mean value_function loss: 241.2662
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 84.9014
                       Mean reward: 813.95
               Mean episode length: 222.76
    Episode_Reward/reaching_object: 1.8173
     Episode_Reward/lifting_object: 153.7327
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0962
          Episode_Reward/joint_vel: -0.0940
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 179994624
                    Iteration time: 2.16s
                      Time elapsed: 01:10:14
                               ETA: 00:06:31

################################################################################
                     [1m Learning iteration 1831/2000 [0m                     

                       Computation: 45220 steps/s (collection: 2.078s, learning 0.096s)
             Mean action noise std: 3.93
          Mean value_function loss: 280.7177
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 84.9212
                       Mean reward: 759.41
               Mean episode length: 215.22
    Episode_Reward/reaching_object: 1.7865
     Episode_Reward/lifting_object: 150.1448
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0948
          Episode_Reward/joint_vel: -0.0947
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 180092928
                    Iteration time: 2.17s
                      Time elapsed: 01:10:16
                               ETA: 00:06:28

################################################################################
                     [1m Learning iteration 1832/2000 [0m                     

                       Computation: 45273 steps/s (collection: 2.079s, learning 0.092s)
             Mean action noise std: 3.93
          Mean value_function loss: 295.0827
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 84.9361
                       Mean reward: 775.91
               Mean episode length: 214.97
    Episode_Reward/reaching_object: 1.7850
     Episode_Reward/lifting_object: 150.2371
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0947
          Episode_Reward/joint_vel: -0.0939
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 180191232
                    Iteration time: 2.17s
                      Time elapsed: 01:10:18
                               ETA: 00:06:26

################################################################################
                     [1m Learning iteration 1833/2000 [0m                     

                       Computation: 45597 steps/s (collection: 2.061s, learning 0.095s)
             Mean action noise std: 3.94
          Mean value_function loss: 225.6749
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 84.9525
                       Mean reward: 788.47
               Mean episode length: 219.35
    Episode_Reward/reaching_object: 1.8243
     Episode_Reward/lifting_object: 153.6053
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0963
          Episode_Reward/joint_vel: -0.0943
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 180289536
                    Iteration time: 2.16s
                      Time elapsed: 01:10:20
                               ETA: 00:06:24

################################################################################
                     [1m Learning iteration 1834/2000 [0m                     

                       Computation: 45852 steps/s (collection: 2.050s, learning 0.094s)
             Mean action noise std: 3.94
          Mean value_function loss: 256.5523
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 84.9640
                       Mean reward: 763.32
               Mean episode length: 216.84
    Episode_Reward/reaching_object: 1.8086
     Episode_Reward/lifting_object: 152.3002
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0963
          Episode_Reward/joint_vel: -0.0956
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 180387840
                    Iteration time: 2.14s
                      Time elapsed: 01:10:23
                               ETA: 00:06:22

################################################################################
                     [1m Learning iteration 1835/2000 [0m                     

                       Computation: 45537 steps/s (collection: 2.067s, learning 0.092s)
             Mean action noise std: 3.94
          Mean value_function loss: 240.4619
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 84.9761
                       Mean reward: 729.49
               Mean episode length: 213.48
    Episode_Reward/reaching_object: 1.7741
     Episode_Reward/lifting_object: 148.9719
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0970
          Episode_Reward/joint_vel: -0.0973
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 180486144
                    Iteration time: 2.16s
                      Time elapsed: 01:10:25
                               ETA: 00:06:19

################################################################################
                     [1m Learning iteration 1836/2000 [0m                     

                       Computation: 45461 steps/s (collection: 2.074s, learning 0.089s)
             Mean action noise std: 3.94
          Mean value_function loss: 221.0817
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 84.9944
                       Mean reward: 733.86
               Mean episode length: 209.06
    Episode_Reward/reaching_object: 1.8050
     Episode_Reward/lifting_object: 151.3423
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0975
          Episode_Reward/joint_vel: -0.0967
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 180584448
                    Iteration time: 2.16s
                      Time elapsed: 01:10:27
                               ETA: 00:06:17

################################################################################
                     [1m Learning iteration 1837/2000 [0m                     

                       Computation: 45556 steps/s (collection: 2.049s, learning 0.109s)
             Mean action noise std: 3.94
          Mean value_function loss: 240.0243
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 85.0088
                       Mean reward: 812.95
               Mean episode length: 223.67
    Episode_Reward/reaching_object: 1.8675
     Episode_Reward/lifting_object: 158.0914
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0991
          Episode_Reward/joint_vel: -0.0955
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 180682752
                    Iteration time: 2.16s
                      Time elapsed: 01:10:29
                               ETA: 00:06:15

################################################################################
                     [1m Learning iteration 1838/2000 [0m                     

                       Computation: 45405 steps/s (collection: 2.068s, learning 0.097s)
             Mean action noise std: 3.94
          Mean value_function loss: 271.2645
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 85.0143
                       Mean reward: 768.85
               Mean episode length: 214.38
    Episode_Reward/reaching_object: 1.7575
     Episode_Reward/lifting_object: 147.8551
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0947
          Episode_Reward/joint_vel: -0.0927
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 180781056
                    Iteration time: 2.17s
                      Time elapsed: 01:10:31
                               ETA: 00:06:12

################################################################################
                     [1m Learning iteration 1839/2000 [0m                     

                       Computation: 45905 steps/s (collection: 2.033s, learning 0.108s)
             Mean action noise std: 3.94
          Mean value_function loss: 326.9254
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 85.0177
                       Mean reward: 759.08
               Mean episode length: 214.72
    Episode_Reward/reaching_object: 1.7651
     Episode_Reward/lifting_object: 148.2223
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0949
          Episode_Reward/joint_vel: -0.0932
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 180879360
                    Iteration time: 2.14s
                      Time elapsed: 01:10:33
                               ETA: 00:06:10

################################################################################
                     [1m Learning iteration 1840/2000 [0m                     

                       Computation: 44938 steps/s (collection: 2.067s, learning 0.120s)
             Mean action noise std: 3.95
          Mean value_function loss: 300.4472
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 85.0222
                       Mean reward: 753.91
               Mean episode length: 212.03
    Episode_Reward/reaching_object: 1.7599
     Episode_Reward/lifting_object: 148.1398
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0946
          Episode_Reward/joint_vel: -0.0924
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 180977664
                    Iteration time: 2.19s
                      Time elapsed: 01:10:36
                               ETA: 00:06:08

################################################################################
                     [1m Learning iteration 1841/2000 [0m                     

                       Computation: 45341 steps/s (collection: 2.058s, learning 0.110s)
             Mean action noise std: 3.95
          Mean value_function loss: 281.0006
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 85.0326
                       Mean reward: 753.74
               Mean episode length: 210.27
    Episode_Reward/reaching_object: 1.7969
     Episode_Reward/lifting_object: 151.0738
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0966
          Episode_Reward/joint_vel: -0.0952
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 181075968
                    Iteration time: 2.17s
                      Time elapsed: 01:10:38
                               ETA: 00:06:05

################################################################################
                     [1m Learning iteration 1842/2000 [0m                     

                       Computation: 45278 steps/s (collection: 2.065s, learning 0.106s)
             Mean action noise std: 3.95
          Mean value_function loss: 304.6533
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 85.0403
                       Mean reward: 747.61
               Mean episode length: 210.22
    Episode_Reward/reaching_object: 1.8182
     Episode_Reward/lifting_object: 153.2438
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0972
          Episode_Reward/joint_vel: -0.0945
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 181174272
                    Iteration time: 2.17s
                      Time elapsed: 01:10:40
                               ETA: 00:06:03

################################################################################
                     [1m Learning iteration 1843/2000 [0m                     

                       Computation: 45882 steps/s (collection: 2.038s, learning 0.105s)
             Mean action noise std: 3.95
          Mean value_function loss: 279.2772
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 85.0434
                       Mean reward: 748.38
               Mean episode length: 211.13
    Episode_Reward/reaching_object: 1.7764
     Episode_Reward/lifting_object: 149.5504
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0960
          Episode_Reward/joint_vel: -0.0939
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 181272576
                    Iteration time: 2.14s
                      Time elapsed: 01:10:42
                               ETA: 00:06:01

################################################################################
                     [1m Learning iteration 1844/2000 [0m                     

                       Computation: 45106 steps/s (collection: 2.066s, learning 0.114s)
             Mean action noise std: 3.95
          Mean value_function loss: 243.0693
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 85.0516
                       Mean reward: 786.84
               Mean episode length: 220.11
    Episode_Reward/reaching_object: 1.8061
     Episode_Reward/lifting_object: 152.3191
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0971
          Episode_Reward/joint_vel: -0.0952
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 181370880
                    Iteration time: 2.18s
                      Time elapsed: 01:10:44
                               ETA: 00:05:58

################################################################################
                     [1m Learning iteration 1845/2000 [0m                     

                       Computation: 45981 steps/s (collection: 2.045s, learning 0.093s)
             Mean action noise std: 3.95
          Mean value_function loss: 257.0684
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 85.0683
                       Mean reward: 749.37
               Mean episode length: 213.73
    Episode_Reward/reaching_object: 1.7638
     Episode_Reward/lifting_object: 148.1797
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0964
          Episode_Reward/joint_vel: -0.0945
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 181469184
                    Iteration time: 2.14s
                      Time elapsed: 01:10:46
                               ETA: 00:05:56

################################################################################
                     [1m Learning iteration 1846/2000 [0m                     

                       Computation: 45104 steps/s (collection: 2.085s, learning 0.095s)
             Mean action noise std: 3.96
          Mean value_function loss: 262.5775
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 85.0876
                       Mean reward: 801.19
               Mean episode length: 221.58
    Episode_Reward/reaching_object: 1.8180
     Episode_Reward/lifting_object: 154.1987
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0990
          Episode_Reward/joint_vel: -0.0969
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 181567488
                    Iteration time: 2.18s
                      Time elapsed: 01:10:48
                               ETA: 00:05:54

################################################################################
                     [1m Learning iteration 1847/2000 [0m                     

                       Computation: 44612 steps/s (collection: 2.102s, learning 0.101s)
             Mean action noise std: 3.96
          Mean value_function loss: 244.5685
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 85.0989
                       Mean reward: 740.70
               Mean episode length: 213.18
    Episode_Reward/reaching_object: 1.7580
     Episode_Reward/lifting_object: 147.6294
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0971
          Episode_Reward/joint_vel: -0.0960
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 181665792
                    Iteration time: 2.20s
                      Time elapsed: 01:10:51
                               ETA: 00:05:51

################################################################################
                     [1m Learning iteration 1848/2000 [0m                     

                       Computation: 43757 steps/s (collection: 2.124s, learning 0.123s)
             Mean action noise std: 3.96
          Mean value_function loss: 267.8694
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 85.1094
                       Mean reward: 720.41
               Mean episode length: 209.98
    Episode_Reward/reaching_object: 1.7703
     Episode_Reward/lifting_object: 148.9480
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0974
          Episode_Reward/joint_vel: -0.0963
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 181764096
                    Iteration time: 2.25s
                      Time elapsed: 01:10:53
                               ETA: 00:05:49

################################################################################
                     [1m Learning iteration 1849/2000 [0m                     

                       Computation: 43826 steps/s (collection: 2.121s, learning 0.122s)
             Mean action noise std: 3.96
          Mean value_function loss: 267.1826
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 85.1265
                       Mean reward: 768.37
               Mean episode length: 217.08
    Episode_Reward/reaching_object: 1.7900
     Episode_Reward/lifting_object: 151.2892
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0985
          Episode_Reward/joint_vel: -0.0971
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 181862400
                    Iteration time: 2.24s
                      Time elapsed: 01:10:55
                               ETA: 00:05:47

################################################################################
                     [1m Learning iteration 1850/2000 [0m                     

                       Computation: 44859 steps/s (collection: 2.085s, learning 0.107s)
             Mean action noise std: 3.96
          Mean value_function loss: 254.9191
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 85.1452
                       Mean reward: 776.58
               Mean episode length: 213.51
    Episode_Reward/reaching_object: 1.8408
     Episode_Reward/lifting_object: 156.9040
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0989
          Episode_Reward/joint_vel: -0.0949
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 181960704
                    Iteration time: 2.19s
                      Time elapsed: 01:10:57
                               ETA: 00:05:45

################################################################################
                     [1m Learning iteration 1851/2000 [0m                     

                       Computation: 44624 steps/s (collection: 2.109s, learning 0.094s)
             Mean action noise std: 3.97
          Mean value_function loss: 255.6215
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 85.1614
                       Mean reward: 791.26
               Mean episode length: 221.95
    Episode_Reward/reaching_object: 1.8397
     Episode_Reward/lifting_object: 156.1907
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0997
          Episode_Reward/joint_vel: -0.0964
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 182059008
                    Iteration time: 2.20s
                      Time elapsed: 01:11:00
                               ETA: 00:05:42

################################################################################
                     [1m Learning iteration 1852/2000 [0m                     

                       Computation: 45537 steps/s (collection: 2.048s, learning 0.111s)
             Mean action noise std: 3.97
          Mean value_function loss: 255.6326
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 85.1797
                       Mean reward: 741.20
               Mean episode length: 210.78
    Episode_Reward/reaching_object: 1.8032
     Episode_Reward/lifting_object: 152.6537
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0991
          Episode_Reward/joint_vel: -0.0960
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 182157312
                    Iteration time: 2.16s
                      Time elapsed: 01:11:02
                               ETA: 00:05:40

################################################################################
                     [1m Learning iteration 1853/2000 [0m                     

                       Computation: 44824 steps/s (collection: 2.080s, learning 0.113s)
             Mean action noise std: 3.97
          Mean value_function loss: 234.1691
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 85.1951
                       Mean reward: 786.17
               Mean episode length: 219.50
    Episode_Reward/reaching_object: 1.8577
     Episode_Reward/lifting_object: 157.3606
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1004
          Episode_Reward/joint_vel: -0.0954
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 182255616
                    Iteration time: 2.19s
                      Time elapsed: 01:11:04
                               ETA: 00:05:38

################################################################################
                     [1m Learning iteration 1854/2000 [0m                     

                       Computation: 45315 steps/s (collection: 2.059s, learning 0.110s)
             Mean action noise std: 3.97
          Mean value_function loss: 243.5533
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 85.2038
                       Mean reward: 802.21
               Mean episode length: 223.08
    Episode_Reward/reaching_object: 1.8563
     Episode_Reward/lifting_object: 156.9835
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.1003
          Episode_Reward/joint_vel: -0.0962
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 182353920
                    Iteration time: 2.17s
                      Time elapsed: 01:11:06
                               ETA: 00:05:35

################################################################################
                     [1m Learning iteration 1855/2000 [0m                     

                       Computation: 46175 steps/s (collection: 2.033s, learning 0.096s)
             Mean action noise std: 3.97
          Mean value_function loss: 253.8715
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 85.2118
                       Mean reward: 757.07
               Mean episode length: 212.57
    Episode_Reward/reaching_object: 1.8266
     Episode_Reward/lifting_object: 154.3272
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0990
          Episode_Reward/joint_vel: -0.0948
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 182452224
                    Iteration time: 2.13s
                      Time elapsed: 01:11:08
                               ETA: 00:05:33

################################################################################
                     [1m Learning iteration 1856/2000 [0m                     

                       Computation: 45969 steps/s (collection: 2.036s, learning 0.102s)
             Mean action noise std: 3.97
          Mean value_function loss: 251.8397
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 85.2152
                       Mean reward: 784.02
               Mean episode length: 221.12
    Episode_Reward/reaching_object: 1.8304
     Episode_Reward/lifting_object: 154.3070
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.1011
          Episode_Reward/joint_vel: -0.0984
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 182550528
                    Iteration time: 2.14s
                      Time elapsed: 01:11:10
                               ETA: 00:05:31

################################################################################
                     [1m Learning iteration 1857/2000 [0m                     

                       Computation: 45916 steps/s (collection: 2.040s, learning 0.101s)
             Mean action noise std: 3.98
          Mean value_function loss: 245.8286
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 85.2251
                       Mean reward: 756.07
               Mean episode length: 215.37
    Episode_Reward/reaching_object: 1.7623
     Episode_Reward/lifting_object: 148.2077
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0977
          Episode_Reward/joint_vel: -0.0969
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 182648832
                    Iteration time: 2.14s
                      Time elapsed: 01:11:13
                               ETA: 00:05:28

################################################################################
                     [1m Learning iteration 1858/2000 [0m                     

                       Computation: 45983 steps/s (collection: 2.046s, learning 0.092s)
             Mean action noise std: 3.98
          Mean value_function loss: 236.0205
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 85.2371
                       Mean reward: 754.17
               Mean episode length: 215.82
    Episode_Reward/reaching_object: 1.7844
     Episode_Reward/lifting_object: 149.8058
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0982
          Episode_Reward/joint_vel: -0.0964
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 182747136
                    Iteration time: 2.14s
                      Time elapsed: 01:11:15
                               ETA: 00:05:26

################################################################################
                     [1m Learning iteration 1859/2000 [0m                     

                       Computation: 45804 steps/s (collection: 2.045s, learning 0.101s)
             Mean action noise std: 3.98
          Mean value_function loss: 209.4449
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 85.2556
                       Mean reward: 790.75
               Mean episode length: 220.30
    Episode_Reward/reaching_object: 1.8839
     Episode_Reward/lifting_object: 158.9049
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.1028
          Episode_Reward/joint_vel: -0.0998
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 182845440
                    Iteration time: 2.15s
                      Time elapsed: 01:11:17
                               ETA: 00:05:24

################################################################################
                     [1m Learning iteration 1860/2000 [0m                     

                       Computation: 45719 steps/s (collection: 2.051s, learning 0.099s)
             Mean action noise std: 3.98
          Mean value_function loss: 251.1946
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 85.2707
                       Mean reward: 771.51
               Mean episode length: 213.52
    Episode_Reward/reaching_object: 1.8297
     Episode_Reward/lifting_object: 154.2916
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0997
          Episode_Reward/joint_vel: -0.0969
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 182943744
                    Iteration time: 2.15s
                      Time elapsed: 01:11:19
                               ETA: 00:05:21

################################################################################
                     [1m Learning iteration 1861/2000 [0m                     

                       Computation: 45924 steps/s (collection: 2.042s, learning 0.098s)
             Mean action noise std: 3.98
          Mean value_function loss: 253.4580
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 85.2832
                       Mean reward: 776.03
               Mean episode length: 221.94
    Episode_Reward/reaching_object: 1.8453
     Episode_Reward/lifting_object: 155.1685
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.1005
          Episode_Reward/joint_vel: -0.0994
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 183042048
                    Iteration time: 2.14s
                      Time elapsed: 01:11:21
                               ETA: 00:05:19

################################################################################
                     [1m Learning iteration 1862/2000 [0m                     

                       Computation: 44621 steps/s (collection: 2.084s, learning 0.120s)
             Mean action noise std: 3.98
          Mean value_function loss: 267.8874
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 85.2894
                       Mean reward: 810.38
               Mean episode length: 224.27
    Episode_Reward/reaching_object: 1.8371
     Episode_Reward/lifting_object: 154.5565
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0994
          Episode_Reward/joint_vel: -0.0971
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 183140352
                    Iteration time: 2.20s
                      Time elapsed: 01:11:23
                               ETA: 00:05:17

################################################################################
                     [1m Learning iteration 1863/2000 [0m                     

                       Computation: 45546 steps/s (collection: 2.051s, learning 0.108s)
             Mean action noise std: 3.99
          Mean value_function loss: 250.6888
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 85.2928
                       Mean reward: 804.94
               Mean episode length: 224.14
    Episode_Reward/reaching_object: 1.8316
     Episode_Reward/lifting_object: 154.2198
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0995
          Episode_Reward/joint_vel: -0.0958
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 183238656
                    Iteration time: 2.16s
                      Time elapsed: 01:11:25
                               ETA: 00:05:15

################################################################################
                     [1m Learning iteration 1864/2000 [0m                     

                       Computation: 45566 steps/s (collection: 2.060s, learning 0.098s)
             Mean action noise std: 3.99
          Mean value_function loss: 273.3952
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 85.3003
                       Mean reward: 769.20
               Mean episode length: 216.41
    Episode_Reward/reaching_object: 1.7899
     Episode_Reward/lifting_object: 150.2249
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0977
          Episode_Reward/joint_vel: -0.0946
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 183336960
                    Iteration time: 2.16s
                      Time elapsed: 01:11:28
                               ETA: 00:05:12

################################################################################
                     [1m Learning iteration 1865/2000 [0m                     

                       Computation: 45452 steps/s (collection: 2.056s, learning 0.106s)
             Mean action noise std: 3.99
          Mean value_function loss: 220.3502
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 85.3082
                       Mean reward: 819.44
               Mean episode length: 228.25
    Episode_Reward/reaching_object: 1.8454
     Episode_Reward/lifting_object: 155.2961
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.1006
          Episode_Reward/joint_vel: -0.0972
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 183435264
                    Iteration time: 2.16s
                      Time elapsed: 01:11:30
                               ETA: 00:05:10

################################################################################
                     [1m Learning iteration 1866/2000 [0m                     

                       Computation: 45290 steps/s (collection: 2.074s, learning 0.096s)
             Mean action noise std: 3.99
          Mean value_function loss: 216.7353
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 85.3174
                       Mean reward: 815.42
               Mean episode length: 226.91
    Episode_Reward/reaching_object: 1.8538
     Episode_Reward/lifting_object: 156.4870
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.1014
          Episode_Reward/joint_vel: -0.0997
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 183533568
                    Iteration time: 2.17s
                      Time elapsed: 01:11:32
                               ETA: 00:05:08

################################################################################
                     [1m Learning iteration 1867/2000 [0m                     

                       Computation: 46110 steps/s (collection: 2.038s, learning 0.094s)
             Mean action noise std: 3.99
          Mean value_function loss: 285.7514
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 85.3234
                       Mean reward: 747.88
               Mean episode length: 212.70
    Episode_Reward/reaching_object: 1.8153
     Episode_Reward/lifting_object: 152.5294
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.1004
          Episode_Reward/joint_vel: -0.0985
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 183631872
                    Iteration time: 2.13s
                      Time elapsed: 01:11:34
                               ETA: 00:05:05

################################################################################
                     [1m Learning iteration 1868/2000 [0m                     

                       Computation: 43690 steps/s (collection: 2.139s, learning 0.111s)
             Mean action noise std: 3.99
          Mean value_function loss: 259.2208
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 85.3281
                       Mean reward: 763.84
               Mean episode length: 216.80
    Episode_Reward/reaching_object: 1.8098
     Episode_Reward/lifting_object: 152.5166
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1011
          Episode_Reward/joint_vel: -0.0990
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 183730176
                    Iteration time: 2.25s
                      Time elapsed: 01:11:36
                               ETA: 00:05:03

################################################################################
                     [1m Learning iteration 1869/2000 [0m                     

                       Computation: 45610 steps/s (collection: 2.053s, learning 0.102s)
             Mean action noise std: 3.99
          Mean value_function loss: 261.2800
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 85.3328
                       Mean reward: 737.76
               Mean episode length: 208.98
    Episode_Reward/reaching_object: 1.7665
     Episode_Reward/lifting_object: 148.5598
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0972
          Episode_Reward/joint_vel: -0.0959
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 183828480
                    Iteration time: 2.16s
                      Time elapsed: 01:11:38
                               ETA: 00:05:01

################################################################################
                     [1m Learning iteration 1870/2000 [0m                     

                       Computation: 45150 steps/s (collection: 2.056s, learning 0.121s)
             Mean action noise std: 3.99
          Mean value_function loss: 290.3416
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 85.3415
                       Mean reward: 741.73
               Mean episode length: 210.78
    Episode_Reward/reaching_object: 1.7477
     Episode_Reward/lifting_object: 146.7473
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0976
          Episode_Reward/joint_vel: -0.0959
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 183926784
                    Iteration time: 2.18s
                      Time elapsed: 01:11:41
                               ETA: 00:04:58

################################################################################
                     [1m Learning iteration 1871/2000 [0m                     

                       Computation: 45264 steps/s (collection: 2.053s, learning 0.119s)
             Mean action noise std: 3.99
          Mean value_function loss: 240.1387
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 85.3545
                       Mean reward: 787.08
               Mean episode length: 221.17
    Episode_Reward/reaching_object: 1.8015
     Episode_Reward/lifting_object: 151.7845
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.1004
          Episode_Reward/joint_vel: -0.0971
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 184025088
                    Iteration time: 2.17s
                      Time elapsed: 01:11:43
                               ETA: 00:04:56

################################################################################
                     [1m Learning iteration 1872/2000 [0m                     

                       Computation: 45783 steps/s (collection: 2.040s, learning 0.107s)
             Mean action noise std: 4.00
          Mean value_function loss: 268.1918
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 85.3664
                       Mean reward: 758.85
               Mean episode length: 216.03
    Episode_Reward/reaching_object: 1.8039
     Episode_Reward/lifting_object: 152.0509
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.1003
          Episode_Reward/joint_vel: -0.0979
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 184123392
                    Iteration time: 2.15s
                      Time elapsed: 01:11:45
                               ETA: 00:04:54

################################################################################
                     [1m Learning iteration 1873/2000 [0m                     

                       Computation: 45881 steps/s (collection: 2.049s, learning 0.094s)
             Mean action noise std: 4.00
          Mean value_function loss: 243.3518
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 85.3784
                       Mean reward: 804.40
               Mean episode length: 221.54
    Episode_Reward/reaching_object: 1.7872
     Episode_Reward/lifting_object: 149.8928
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0984
          Episode_Reward/joint_vel: -0.0950
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 184221696
                    Iteration time: 2.14s
                      Time elapsed: 01:11:47
                               ETA: 00:04:51

################################################################################
                     [1m Learning iteration 1874/2000 [0m                     

                       Computation: 45704 steps/s (collection: 2.056s, learning 0.095s)
             Mean action noise std: 4.00
          Mean value_function loss: 270.1152
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 85.3855
                       Mean reward: 691.45
               Mean episode length: 202.99
    Episode_Reward/reaching_object: 1.7252
     Episode_Reward/lifting_object: 143.6609
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0984
          Episode_Reward/joint_vel: -0.0986
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 184320000
                    Iteration time: 2.15s
                      Time elapsed: 01:11:49
                               ETA: 00:04:49

################################################################################
                     [1m Learning iteration 1875/2000 [0m                     

                       Computation: 45233 steps/s (collection: 2.078s, learning 0.095s)
             Mean action noise std: 4.00
          Mean value_function loss: 222.9568
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 85.3944
                       Mean reward: 760.91
               Mean episode length: 214.25
    Episode_Reward/reaching_object: 1.8398
     Episode_Reward/lifting_object: 155.3754
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1014
          Episode_Reward/joint_vel: -0.0972
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 184418304
                    Iteration time: 2.17s
                      Time elapsed: 01:11:51
                               ETA: 00:04:47

################################################################################
                     [1m Learning iteration 1876/2000 [0m                     

                       Computation: 45530 steps/s (collection: 2.057s, learning 0.102s)
             Mean action noise std: 4.00
          Mean value_function loss: 271.5305
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 85.4002
                       Mean reward: 765.77
               Mean episode length: 216.41
    Episode_Reward/reaching_object: 1.8154
     Episode_Reward/lifting_object: 152.5214
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.1014
          Episode_Reward/joint_vel: -0.0986
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 184516608
                    Iteration time: 2.16s
                      Time elapsed: 01:11:54
                               ETA: 00:04:45

################################################################################
                     [1m Learning iteration 1877/2000 [0m                     

                       Computation: 44642 steps/s (collection: 2.105s, learning 0.097s)
             Mean action noise std: 4.00
          Mean value_function loss: 268.7545
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 85.4107
                       Mean reward: 755.93
               Mean episode length: 216.41
    Episode_Reward/reaching_object: 1.7960
     Episode_Reward/lifting_object: 150.9053
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0995
          Episode_Reward/joint_vel: -0.0955
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 184614912
                    Iteration time: 2.20s
                      Time elapsed: 01:11:56
                               ETA: 00:04:42

################################################################################
                     [1m Learning iteration 1878/2000 [0m                     

                       Computation: 44695 steps/s (collection: 2.099s, learning 0.100s)
             Mean action noise std: 4.00
          Mean value_function loss: 316.7261
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 85.4219
                       Mean reward: 691.95
               Mean episode length: 195.90
    Episode_Reward/reaching_object: 1.7050
     Episode_Reward/lifting_object: 142.6603
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0953
          Episode_Reward/joint_vel: -0.0922
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 184713216
                    Iteration time: 2.20s
                      Time elapsed: 01:11:58
                               ETA: 00:04:40

################################################################################
                     [1m Learning iteration 1879/2000 [0m                     

                       Computation: 45246 steps/s (collection: 2.060s, learning 0.113s)
             Mean action noise std: 4.01
          Mean value_function loss: 269.0621
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 85.4287
                       Mean reward: 759.61
               Mean episode length: 212.10
    Episode_Reward/reaching_object: 1.7964
     Episode_Reward/lifting_object: 151.7219
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0999
          Episode_Reward/joint_vel: -0.0963
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 184811520
                    Iteration time: 2.17s
                      Time elapsed: 01:12:00
                               ETA: 00:04:38

################################################################################
                     [1m Learning iteration 1880/2000 [0m                     

                       Computation: 44946 steps/s (collection: 2.077s, learning 0.110s)
             Mean action noise std: 4.01
          Mean value_function loss: 295.2758
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 85.4429
                       Mean reward: 777.54
               Mean episode length: 221.93
    Episode_Reward/reaching_object: 1.8375
     Episode_Reward/lifting_object: 155.9916
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.1013
          Episode_Reward/joint_vel: -0.0951
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 184909824
                    Iteration time: 2.19s
                      Time elapsed: 01:12:02
                               ETA: 00:04:35

################################################################################
                     [1m Learning iteration 1881/2000 [0m                     

                       Computation: 44361 steps/s (collection: 2.118s, learning 0.098s)
             Mean action noise std: 4.01
          Mean value_function loss: 276.7135
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 85.4530
                       Mean reward: 774.40
               Mean episode length: 219.94
    Episode_Reward/reaching_object: 1.7956
     Episode_Reward/lifting_object: 151.2805
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.1013
          Episode_Reward/joint_vel: -0.0978
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 185008128
                    Iteration time: 2.22s
                      Time elapsed: 01:12:05
                               ETA: 00:04:33

################################################################################
                     [1m Learning iteration 1882/2000 [0m                     

                       Computation: 44767 steps/s (collection: 2.079s, learning 0.117s)
             Mean action noise std: 4.01
          Mean value_function loss: 255.7870
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 85.4619
                       Mean reward: 709.43
               Mean episode length: 201.65
    Episode_Reward/reaching_object: 1.7630
     Episode_Reward/lifting_object: 147.7509
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0993
          Episode_Reward/joint_vel: -0.0946
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 185106432
                    Iteration time: 2.20s
                      Time elapsed: 01:12:07
                               ETA: 00:04:31

################################################################################
                     [1m Learning iteration 1883/2000 [0m                     

                       Computation: 43145 steps/s (collection: 2.162s, learning 0.117s)
             Mean action noise std: 4.01
          Mean value_function loss: 275.7046
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 85.4687
                       Mean reward: 757.07
               Mean episode length: 209.99
    Episode_Reward/reaching_object: 1.7739
     Episode_Reward/lifting_object: 150.0468
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0987
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 185204736
                    Iteration time: 2.28s
                      Time elapsed: 01:12:09
                               ETA: 00:04:28

################################################################################
                     [1m Learning iteration 1884/2000 [0m                     

                       Computation: 43205 steps/s (collection: 2.140s, learning 0.135s)
             Mean action noise std: 4.01
          Mean value_function loss: 277.7513
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 85.4708
                       Mean reward: 764.09
               Mean episode length: 211.39
    Episode_Reward/reaching_object: 1.7960
     Episode_Reward/lifting_object: 151.7920
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1000
          Episode_Reward/joint_vel: -0.0944
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 185303040
                    Iteration time: 2.28s
                      Time elapsed: 01:12:11
                               ETA: 00:04:26

################################################################################
                     [1m Learning iteration 1885/2000 [0m                     

                       Computation: 40170 steps/s (collection: 2.308s, learning 0.139s)
             Mean action noise std: 4.01
          Mean value_function loss: 263.0608
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 85.4733
                       Mean reward: 731.57
               Mean episode length: 210.06
    Episode_Reward/reaching_object: 1.7968
     Episode_Reward/lifting_object: 151.1219
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1006
          Episode_Reward/joint_vel: -0.0970
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 185401344
                    Iteration time: 2.45s
                      Time elapsed: 01:12:14
                               ETA: 00:04:24

################################################################################
                     [1m Learning iteration 1886/2000 [0m                     

                       Computation: 41758 steps/s (collection: 2.248s, learning 0.106s)
             Mean action noise std: 4.01
          Mean value_function loss: 256.2936
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 85.4809
                       Mean reward: 819.97
               Mean episode length: 224.76
    Episode_Reward/reaching_object: 1.7531
     Episode_Reward/lifting_object: 148.2865
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0988
          Episode_Reward/joint_vel: -0.0955
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 185499648
                    Iteration time: 2.35s
                      Time elapsed: 01:12:16
                               ETA: 00:04:21

################################################################################
                     [1m Learning iteration 1887/2000 [0m                     

                       Computation: 40346 steps/s (collection: 2.310s, learning 0.126s)
             Mean action noise std: 4.02
          Mean value_function loss: 235.2774
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 85.4946
                       Mean reward: 792.02
               Mean episode length: 226.14
    Episode_Reward/reaching_object: 1.8897
     Episode_Reward/lifting_object: 161.0645
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.1052
          Episode_Reward/joint_vel: -0.0993
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 185597952
                    Iteration time: 2.44s
                      Time elapsed: 01:12:19
                               ETA: 00:04:19

################################################################################
                     [1m Learning iteration 1888/2000 [0m                     

                       Computation: 41978 steps/s (collection: 2.217s, learning 0.125s)
             Mean action noise std: 4.02
          Mean value_function loss: 234.9860
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 85.5110
                       Mean reward: 761.41
               Mean episode length: 210.47
    Episode_Reward/reaching_object: 1.7973
     Episode_Reward/lifting_object: 151.6188
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1003
          Episode_Reward/joint_vel: -0.0961
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 185696256
                    Iteration time: 2.34s
                      Time elapsed: 01:12:21
                               ETA: 00:04:17

################################################################################
                     [1m Learning iteration 1889/2000 [0m                     

                       Computation: 43345 steps/s (collection: 2.171s, learning 0.096s)
             Mean action noise std: 4.02
          Mean value_function loss: 277.5424
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 85.5198
                       Mean reward: 733.67
               Mean episode length: 212.46
    Episode_Reward/reaching_object: 1.7618
     Episode_Reward/lifting_object: 148.5527
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0995
          Episode_Reward/joint_vel: -0.0963
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 185794560
                    Iteration time: 2.27s
                      Time elapsed: 01:12:23
                               ETA: 00:04:15

################################################################################
                     [1m Learning iteration 1890/2000 [0m                     

                       Computation: 43171 steps/s (collection: 2.157s, learning 0.120s)
             Mean action noise std: 4.02
          Mean value_function loss: 249.9333
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 85.5302
                       Mean reward: 765.06
               Mean episode length: 211.45
    Episode_Reward/reaching_object: 1.8171
     Episode_Reward/lifting_object: 153.6800
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1021
          Episode_Reward/joint_vel: -0.0987
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 185892864
                    Iteration time: 2.28s
                      Time elapsed: 01:12:25
                               ETA: 00:04:12

################################################################################
                     [1m Learning iteration 1891/2000 [0m                     

                       Computation: 42912 steps/s (collection: 2.184s, learning 0.107s)
             Mean action noise std: 4.02
          Mean value_function loss: 241.8006
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 85.5417
                       Mean reward: 762.45
               Mean episode length: 213.28
    Episode_Reward/reaching_object: 1.8300
     Episode_Reward/lifting_object: 154.2329
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.1023
          Episode_Reward/joint_vel: -0.0978
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 185991168
                    Iteration time: 2.29s
                      Time elapsed: 01:12:28
                               ETA: 00:04:10

################################################################################
                     [1m Learning iteration 1892/2000 [0m                     

                       Computation: 44468 steps/s (collection: 2.106s, learning 0.105s)
             Mean action noise std: 4.03
          Mean value_function loss: 240.8899
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 85.5517
                       Mean reward: 809.70
               Mean episode length: 222.89
    Episode_Reward/reaching_object: 1.8465
     Episode_Reward/lifting_object: 156.7887
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.1032
          Episode_Reward/joint_vel: -0.0981
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 186089472
                    Iteration time: 2.21s
                      Time elapsed: 01:12:30
                               ETA: 00:04:08

################################################################################
                     [1m Learning iteration 1893/2000 [0m                     

                       Computation: 44287 steps/s (collection: 2.117s, learning 0.103s)
             Mean action noise std: 4.03
          Mean value_function loss: 232.1194
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 85.5621
                       Mean reward: 800.15
               Mean episode length: 222.17
    Episode_Reward/reaching_object: 1.8584
     Episode_Reward/lifting_object: 157.6308
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.1035
          Episode_Reward/joint_vel: -0.0982
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 186187776
                    Iteration time: 2.22s
                      Time elapsed: 01:12:32
                               ETA: 00:04:05

################################################################################
                     [1m Learning iteration 1894/2000 [0m                     

                       Computation: 44222 steps/s (collection: 2.111s, learning 0.112s)
             Mean action noise std: 4.03
          Mean value_function loss: 286.0214
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 85.5676
                       Mean reward: 780.74
               Mean episode length: 219.82
    Episode_Reward/reaching_object: 1.7899
     Episode_Reward/lifting_object: 151.0539
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1018
          Episode_Reward/joint_vel: -0.0985
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 186286080
                    Iteration time: 2.22s
                      Time elapsed: 01:12:34
                               ETA: 00:04:03

################################################################################
                     [1m Learning iteration 1895/2000 [0m                     

                       Computation: 43717 steps/s (collection: 2.136s, learning 0.113s)
             Mean action noise std: 4.03
          Mean value_function loss: 241.0819
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 85.5765
                       Mean reward: 774.69
               Mean episode length: 216.71
    Episode_Reward/reaching_object: 1.8157
     Episode_Reward/lifting_object: 152.9855
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.1023
          Episode_Reward/joint_vel: -0.0976
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 186384384
                    Iteration time: 2.25s
                      Time elapsed: 01:12:37
                               ETA: 00:04:01

################################################################################
                     [1m Learning iteration 1896/2000 [0m                     

                       Computation: 43947 steps/s (collection: 2.111s, learning 0.126s)
             Mean action noise std: 4.03
          Mean value_function loss: 226.8166
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 85.5818
                       Mean reward: 751.50
               Mean episode length: 211.40
    Episode_Reward/reaching_object: 1.8078
     Episode_Reward/lifting_object: 152.5190
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1021
          Episode_Reward/joint_vel: -0.0977
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 186482688
                    Iteration time: 2.24s
                      Time elapsed: 01:12:39
                               ETA: 00:03:58

################################################################################
                     [1m Learning iteration 1897/2000 [0m                     

                       Computation: 44204 steps/s (collection: 2.092s, learning 0.132s)
             Mean action noise std: 4.03
          Mean value_function loss: 240.2358
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 85.5860
                       Mean reward: 762.34
               Mean episode length: 213.56
    Episode_Reward/reaching_object: 1.7782
     Episode_Reward/lifting_object: 149.5332
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1019
          Episode_Reward/joint_vel: -0.0986
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 186580992
                    Iteration time: 2.22s
                      Time elapsed: 01:12:41
                               ETA: 00:03:56

################################################################################
                     [1m Learning iteration 1898/2000 [0m                     

                       Computation: 44286 steps/s (collection: 2.098s, learning 0.122s)
             Mean action noise std: 4.03
          Mean value_function loss: 297.0067
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 85.5938
                       Mean reward: 686.43
               Mean episode length: 201.29
    Episode_Reward/reaching_object: 1.7397
     Episode_Reward/lifting_object: 145.9441
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0993
          Episode_Reward/joint_vel: -0.0952
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 186679296
                    Iteration time: 2.22s
                      Time elapsed: 01:12:43
                               ETA: 00:03:54

################################################################################
                     [1m Learning iteration 1899/2000 [0m                     

                       Computation: 43712 steps/s (collection: 2.128s, learning 0.121s)
             Mean action noise std: 4.03
          Mean value_function loss: 236.0888
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 85.6087
                       Mean reward: 746.17
               Mean episode length: 208.91
    Episode_Reward/reaching_object: 1.8219
     Episode_Reward/lifting_object: 153.8841
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1035
          Episode_Reward/joint_vel: -0.0983
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 186777600
                    Iteration time: 2.25s
                      Time elapsed: 01:12:46
                               ETA: 00:03:52

################################################################################
                     [1m Learning iteration 1900/2000 [0m                     

                       Computation: 44284 steps/s (collection: 2.116s, learning 0.104s)
             Mean action noise std: 4.04
          Mean value_function loss: 240.8175
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 85.6237
                       Mean reward: 748.14
               Mean episode length: 212.90
    Episode_Reward/reaching_object: 1.8024
     Episode_Reward/lifting_object: 152.1075
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.1025
          Episode_Reward/joint_vel: -0.0972
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 186875904
                    Iteration time: 2.22s
                      Time elapsed: 01:12:48
                               ETA: 00:03:49

################################################################################
                     [1m Learning iteration 1901/2000 [0m                     

                       Computation: 43860 steps/s (collection: 2.140s, learning 0.102s)
             Mean action noise std: 4.04
          Mean value_function loss: 241.5184
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 85.6333
                       Mean reward: 772.56
               Mean episode length: 217.41
    Episode_Reward/reaching_object: 1.8134
     Episode_Reward/lifting_object: 153.2105
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1035
          Episode_Reward/joint_vel: -0.0981
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 186974208
                    Iteration time: 2.24s
                      Time elapsed: 01:12:50
                               ETA: 00:03:47

################################################################################
                     [1m Learning iteration 1902/2000 [0m                     

                       Computation: 45048 steps/s (collection: 2.086s, learning 0.097s)
             Mean action noise std: 4.04
          Mean value_function loss: 226.2375
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 85.6404
                       Mean reward: 770.31
               Mean episode length: 217.71
    Episode_Reward/reaching_object: 1.8351
     Episode_Reward/lifting_object: 155.6122
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.1049
          Episode_Reward/joint_vel: -0.0987
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 187072512
                    Iteration time: 2.18s
                      Time elapsed: 01:12:52
                               ETA: 00:03:45

################################################################################
                     [1m Learning iteration 1903/2000 [0m                     

                       Computation: 44311 steps/s (collection: 2.101s, learning 0.117s)
             Mean action noise std: 4.04
          Mean value_function loss: 208.0089
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 85.6440
                       Mean reward: 741.84
               Mean episode length: 217.67
    Episode_Reward/reaching_object: 1.8182
     Episode_Reward/lifting_object: 154.0000
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1049
          Episode_Reward/joint_vel: -0.0997
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 187170816
                    Iteration time: 2.22s
                      Time elapsed: 01:12:54
                               ETA: 00:03:42

################################################################################
                     [1m Learning iteration 1904/2000 [0m                     

                       Computation: 43883 steps/s (collection: 2.122s, learning 0.118s)
             Mean action noise std: 4.04
          Mean value_function loss: 196.9256
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 85.6465
                       Mean reward: 812.96
               Mean episode length: 231.13
    Episode_Reward/reaching_object: 1.8322
     Episode_Reward/lifting_object: 154.6643
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1052
          Episode_Reward/joint_vel: -0.0992
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 187269120
                    Iteration time: 2.24s
                      Time elapsed: 01:12:57
                               ETA: 00:03:40

################################################################################
                     [1m Learning iteration 1905/2000 [0m                     

                       Computation: 43986 steps/s (collection: 2.128s, learning 0.107s)
             Mean action noise std: 4.04
          Mean value_function loss: 217.1932
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 85.6523
                       Mean reward: 817.69
               Mean episode length: 227.39
    Episode_Reward/reaching_object: 1.8442
     Episode_Reward/lifting_object: 155.8214
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.1047
          Episode_Reward/joint_vel: -0.0986
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 187367424
                    Iteration time: 2.23s
                      Time elapsed: 01:12:59
                               ETA: 00:03:38

################################################################################
                     [1m Learning iteration 1906/2000 [0m                     

                       Computation: 44296 steps/s (collection: 2.116s, learning 0.103s)
             Mean action noise std: 4.04
          Mean value_function loss: 224.4808
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 85.6614
                       Mean reward: 786.25
               Mean episode length: 226.18
    Episode_Reward/reaching_object: 1.8341
     Episode_Reward/lifting_object: 155.8298
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1047
          Episode_Reward/joint_vel: -0.0962
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 187465728
                    Iteration time: 2.22s
                      Time elapsed: 01:13:01
                               ETA: 00:03:35

################################################################################
                     [1m Learning iteration 1907/2000 [0m                     

                       Computation: 44183 steps/s (collection: 2.118s, learning 0.107s)
             Mean action noise std: 4.04
          Mean value_function loss: 222.8199
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 85.6753
                       Mean reward: 800.41
               Mean episode length: 223.80
    Episode_Reward/reaching_object: 1.8057
     Episode_Reward/lifting_object: 152.1723
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1025
          Episode_Reward/joint_vel: -0.0958
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 187564032
                    Iteration time: 2.22s
                      Time elapsed: 01:13:03
                               ETA: 00:03:33

################################################################################
                     [1m Learning iteration 1908/2000 [0m                     

                       Computation: 44693 steps/s (collection: 2.100s, learning 0.100s)
             Mean action noise std: 4.05
          Mean value_function loss: 247.9080
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 85.6895
                       Mean reward: 768.83
               Mean episode length: 216.60
    Episode_Reward/reaching_object: 1.7978
     Episode_Reward/lifting_object: 152.2191
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1028
          Episode_Reward/joint_vel: -0.0946
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 187662336
                    Iteration time: 2.20s
                      Time elapsed: 01:13:06
                               ETA: 00:03:31

################################################################################
                     [1m Learning iteration 1909/2000 [0m                     

                       Computation: 44062 steps/s (collection: 2.115s, learning 0.116s)
             Mean action noise std: 4.05
          Mean value_function loss: 226.9932
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 85.7038
                       Mean reward: 762.89
               Mean episode length: 213.63
    Episode_Reward/reaching_object: 1.7929
     Episode_Reward/lifting_object: 151.2454
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.1032
          Episode_Reward/joint_vel: -0.0966
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 187760640
                    Iteration time: 2.23s
                      Time elapsed: 01:13:08
                               ETA: 00:03:29

################################################################################
                     [1m Learning iteration 1910/2000 [0m                     

                       Computation: 43514 steps/s (collection: 2.136s, learning 0.123s)
             Mean action noise std: 4.05
          Mean value_function loss: 297.7340
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 85.7136
                       Mean reward: 752.38
               Mean episode length: 210.72
    Episode_Reward/reaching_object: 1.7654
     Episode_Reward/lifting_object: 148.2348
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.1017
          Episode_Reward/joint_vel: -0.0959
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 187858944
                    Iteration time: 2.26s
                      Time elapsed: 01:13:10
                               ETA: 00:03:26

################################################################################
                     [1m Learning iteration 1911/2000 [0m                     

                       Computation: 43555 steps/s (collection: 2.133s, learning 0.124s)
             Mean action noise std: 4.05
          Mean value_function loss: 289.8506
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 85.7264
                       Mean reward: 778.80
               Mean episode length: 217.08
    Episode_Reward/reaching_object: 1.7693
     Episode_Reward/lifting_object: 148.8325
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.1021
          Episode_Reward/joint_vel: -0.0960
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 187957248
                    Iteration time: 2.26s
                      Time elapsed: 01:13:12
                               ETA: 00:03:24

################################################################################
                     [1m Learning iteration 1912/2000 [0m                     

                       Computation: 44910 steps/s (collection: 2.089s, learning 0.100s)
             Mean action noise std: 4.05
          Mean value_function loss: 299.3839
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 85.7357
                       Mean reward: 753.77
               Mean episode length: 213.48
    Episode_Reward/reaching_object: 1.7766
     Episode_Reward/lifting_object: 149.8785
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1017
          Episode_Reward/joint_vel: -0.0935
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 188055552
                    Iteration time: 2.19s
                      Time elapsed: 01:13:14
                               ETA: 00:03:22

################################################################################
                     [1m Learning iteration 1913/2000 [0m                     

                       Computation: 44555 steps/s (collection: 2.109s, learning 0.097s)
             Mean action noise std: 4.06
          Mean value_function loss: 319.5035
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 85.7484
                       Mean reward: 729.66
               Mean episode length: 207.83
    Episode_Reward/reaching_object: 1.6956
     Episode_Reward/lifting_object: 143.2665
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0976
          Episode_Reward/joint_vel: -0.0904
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 188153856
                    Iteration time: 2.21s
                      Time elapsed: 01:13:17
                               ETA: 00:03:19

################################################################################
                     [1m Learning iteration 1914/2000 [0m                     

                       Computation: 44346 steps/s (collection: 2.109s, learning 0.108s)
             Mean action noise std: 4.06
          Mean value_function loss: 336.3297
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 85.7675
                       Mean reward: 729.70
               Mean episode length: 204.85
    Episode_Reward/reaching_object: 1.7156
     Episode_Reward/lifting_object: 145.4183
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0978
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 188252160
                    Iteration time: 2.22s
                      Time elapsed: 01:13:19
                               ETA: 00:03:17

################################################################################
                     [1m Learning iteration 1915/2000 [0m                     

                       Computation: 44532 steps/s (collection: 2.099s, learning 0.108s)
             Mean action noise std: 4.06
          Mean value_function loss: 269.3393
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 85.7825
                       Mean reward: 762.65
               Mean episode length: 210.79
    Episode_Reward/reaching_object: 1.7314
     Episode_Reward/lifting_object: 146.3306
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.1001
          Episode_Reward/joint_vel: -0.0923
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 188350464
                    Iteration time: 2.21s
                      Time elapsed: 01:13:21
                               ETA: 00:03:15

################################################################################
                     [1m Learning iteration 1916/2000 [0m                     

                       Computation: 44313 steps/s (collection: 2.114s, learning 0.104s)
             Mean action noise std: 4.06
          Mean value_function loss: 249.3315
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 85.7934
                       Mean reward: 740.35
               Mean episode length: 207.50
    Episode_Reward/reaching_object: 1.7530
     Episode_Reward/lifting_object: 148.8142
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.1011
          Episode_Reward/joint_vel: -0.0911
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 188448768
                    Iteration time: 2.22s
                      Time elapsed: 01:13:23
                               ETA: 00:03:12

################################################################################
                     [1m Learning iteration 1917/2000 [0m                     

                       Computation: 43658 steps/s (collection: 2.142s, learning 0.110s)
             Mean action noise std: 4.06
          Mean value_function loss: 230.3147
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 85.8139
                       Mean reward: 763.66
               Mean episode length: 209.70
    Episode_Reward/reaching_object: 1.8139
     Episode_Reward/lifting_object: 155.1759
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.1040
          Episode_Reward/joint_vel: -0.0943
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 188547072
                    Iteration time: 2.25s
                      Time elapsed: 01:13:26
                               ETA: 00:03:10

################################################################################
                     [1m Learning iteration 1918/2000 [0m                     

                       Computation: 44909 steps/s (collection: 2.089s, learning 0.100s)
             Mean action noise std: 4.07
          Mean value_function loss: 246.3700
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 85.8304
                       Mean reward: 803.43
               Mean episode length: 223.09
    Episode_Reward/reaching_object: 1.8167
     Episode_Reward/lifting_object: 155.0011
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.1039
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 188645376
                    Iteration time: 2.19s
                      Time elapsed: 01:13:28
                               ETA: 00:03:08

################################################################################
                     [1m Learning iteration 1919/2000 [0m                     

                       Computation: 44462 steps/s (collection: 2.111s, learning 0.100s)
             Mean action noise std: 4.07
          Mean value_function loss: 269.5548
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 85.8390
                       Mean reward: 768.45
               Mean episode length: 215.76
    Episode_Reward/reaching_object: 1.7734
     Episode_Reward/lifting_object: 151.1890
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.1028
          Episode_Reward/joint_vel: -0.0936
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 188743680
                    Iteration time: 2.21s
                      Time elapsed: 01:13:30
                               ETA: 00:03:06

################################################################################
                     [1m Learning iteration 1920/2000 [0m                     

                       Computation: 44615 steps/s (collection: 2.098s, learning 0.105s)
             Mean action noise std: 4.07
          Mean value_function loss: 247.3257
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 85.8502
                       Mean reward: 770.34
               Mean episode length: 219.75
    Episode_Reward/reaching_object: 1.7746
     Episode_Reward/lifting_object: 150.5368
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.1032
          Episode_Reward/joint_vel: -0.0953
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 188841984
                    Iteration time: 2.20s
                      Time elapsed: 01:13:32
                               ETA: 00:03:03

################################################################################
                     [1m Learning iteration 1921/2000 [0m                     

                       Computation: 44209 steps/s (collection: 2.123s, learning 0.101s)
             Mean action noise std: 4.07
          Mean value_function loss: 273.1625
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 85.8563
                       Mean reward: 748.41
               Mean episode length: 217.55
    Episode_Reward/reaching_object: 1.7981
     Episode_Reward/lifting_object: 153.3141
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.1048
          Episode_Reward/joint_vel: -0.0973
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 188940288
                    Iteration time: 2.22s
                      Time elapsed: 01:13:34
                               ETA: 00:03:01

################################################################################
                     [1m Learning iteration 1922/2000 [0m                     

                       Computation: 43732 steps/s (collection: 2.115s, learning 0.133s)
             Mean action noise std: 4.07
          Mean value_function loss: 251.7466
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 85.8639
                       Mean reward: 819.53
               Mean episode length: 223.42
    Episode_Reward/reaching_object: 1.8231
     Episode_Reward/lifting_object: 155.4980
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1049
          Episode_Reward/joint_vel: -0.0965
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 189038592
                    Iteration time: 2.25s
                      Time elapsed: 01:13:37
                               ETA: 00:02:59

################################################################################
                     [1m Learning iteration 1923/2000 [0m                     

                       Computation: 44089 steps/s (collection: 2.103s, learning 0.127s)
             Mean action noise std: 4.07
          Mean value_function loss: 253.4957
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 85.8811
                       Mean reward: 781.48
               Mean episode length: 215.90
    Episode_Reward/reaching_object: 1.7953
     Episode_Reward/lifting_object: 153.5728
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1035
          Episode_Reward/joint_vel: -0.0938
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 189136896
                    Iteration time: 2.23s
                      Time elapsed: 01:13:39
                               ETA: 00:02:56

################################################################################
                     [1m Learning iteration 1924/2000 [0m                     

                       Computation: 44267 steps/s (collection: 2.112s, learning 0.109s)
             Mean action noise std: 4.08
          Mean value_function loss: 233.6468
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 85.9031
                       Mean reward: 762.19
               Mean episode length: 209.34
    Episode_Reward/reaching_object: 1.8342
     Episode_Reward/lifting_object: 156.8984
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1060
          Episode_Reward/joint_vel: -0.0972
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 189235200
                    Iteration time: 2.22s
                      Time elapsed: 01:13:41
                               ETA: 00:02:54

################################################################################
                     [1m Learning iteration 1925/2000 [0m                     

                       Computation: 44439 steps/s (collection: 2.099s, learning 0.113s)
             Mean action noise std: 4.08
          Mean value_function loss: 229.7219
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 85.9118
                       Mean reward: 793.75
               Mean episode length: 216.93
    Episode_Reward/reaching_object: 1.8118
     Episode_Reward/lifting_object: 154.9062
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1045
          Episode_Reward/joint_vel: -0.0959
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 189333504
                    Iteration time: 2.21s
                      Time elapsed: 01:13:43
                               ETA: 00:02:52

################################################################################
                     [1m Learning iteration 1926/2000 [0m                     

                       Computation: 43589 steps/s (collection: 2.135s, learning 0.121s)
             Mean action noise std: 4.08
          Mean value_function loss: 236.7829
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 85.9208
                       Mean reward: 774.48
               Mean episode length: 217.91
    Episode_Reward/reaching_object: 1.7866
     Episode_Reward/lifting_object: 151.6651
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1043
          Episode_Reward/joint_vel: -0.0970
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 189431808
                    Iteration time: 2.26s
                      Time elapsed: 01:13:46
                               ETA: 00:02:49

################################################################################
                     [1m Learning iteration 1927/2000 [0m                     

                       Computation: 43815 steps/s (collection: 2.113s, learning 0.131s)
             Mean action noise std: 4.08
          Mean value_function loss: 213.2879
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 85.9289
                       Mean reward: 788.67
               Mean episode length: 219.17
    Episode_Reward/reaching_object: 1.7828
     Episode_Reward/lifting_object: 151.7015
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1033
          Episode_Reward/joint_vel: -0.0933
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 189530112
                    Iteration time: 2.24s
                      Time elapsed: 01:13:48
                               ETA: 00:02:47

################################################################################
                     [1m Learning iteration 1928/2000 [0m                     

                       Computation: 44051 steps/s (collection: 2.125s, learning 0.107s)
             Mean action noise std: 4.08
          Mean value_function loss: 219.9960
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 85.9405
                       Mean reward: 801.96
               Mean episode length: 226.32
    Episode_Reward/reaching_object: 1.8191
     Episode_Reward/lifting_object: 155.2675
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.1057
          Episode_Reward/joint_vel: -0.0974
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 189628416
                    Iteration time: 2.23s
                      Time elapsed: 01:13:50
                               ETA: 00:02:45

################################################################################
                     [1m Learning iteration 1929/2000 [0m                     

                       Computation: 44379 steps/s (collection: 2.120s, learning 0.095s)
             Mean action noise std: 4.08
          Mean value_function loss: 201.2528
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 85.9518
                       Mean reward: 769.40
               Mean episode length: 220.03
    Episode_Reward/reaching_object: 1.8506
     Episode_Reward/lifting_object: 156.9094
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.1080
          Episode_Reward/joint_vel: -0.0991
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 189726720
                    Iteration time: 2.22s
                      Time elapsed: 01:13:52
                               ETA: 00:02:43

################################################################################
                     [1m Learning iteration 1930/2000 [0m                     

                       Computation: 44115 steps/s (collection: 2.125s, learning 0.103s)
             Mean action noise std: 4.08
          Mean value_function loss: 223.0613
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 85.9580
                       Mean reward: 798.11
               Mean episode length: 221.73
    Episode_Reward/reaching_object: 1.8276
     Episode_Reward/lifting_object: 156.0678
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.1057
          Episode_Reward/joint_vel: -0.0965
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 189825024
                    Iteration time: 2.23s
                      Time elapsed: 01:13:54
                               ETA: 00:02:40

################################################################################
                     [1m Learning iteration 1931/2000 [0m                     

                       Computation: 43740 steps/s (collection: 2.140s, learning 0.107s)
             Mean action noise std: 4.09
          Mean value_function loss: 233.3804
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 85.9666
                       Mean reward: 790.08
               Mean episode length: 219.96
    Episode_Reward/reaching_object: 1.8205
     Episode_Reward/lifting_object: 155.0610
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1056
          Episode_Reward/joint_vel: -0.0965
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 189923328
                    Iteration time: 2.25s
                      Time elapsed: 01:13:57
                               ETA: 00:02:38

################################################################################
                     [1m Learning iteration 1932/2000 [0m                     

                       Computation: 43995 steps/s (collection: 2.126s, learning 0.108s)
             Mean action noise std: 4.09
          Mean value_function loss: 274.7713
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 85.9813
                       Mean reward: 774.27
               Mean episode length: 218.02
    Episode_Reward/reaching_object: 1.7869
     Episode_Reward/lifting_object: 151.7468
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1045
          Episode_Reward/joint_vel: -0.0960
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 190021632
                    Iteration time: 2.23s
                      Time elapsed: 01:13:59
                               ETA: 00:02:36

################################################################################
                     [1m Learning iteration 1933/2000 [0m                     

                       Computation: 43847 steps/s (collection: 2.142s, learning 0.100s)
             Mean action noise std: 4.09
          Mean value_function loss: 266.8820
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 86.0018
                       Mean reward: 793.09
               Mean episode length: 218.17
    Episode_Reward/reaching_object: 1.8358
     Episode_Reward/lifting_object: 155.3304
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1052
          Episode_Reward/joint_vel: -0.0963
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 190119936
                    Iteration time: 2.24s
                      Time elapsed: 01:14:01
                               ETA: 00:02:33

################################################################################
                     [1m Learning iteration 1934/2000 [0m                     

                       Computation: 44181 steps/s (collection: 2.125s, learning 0.100s)
             Mean action noise std: 4.09
          Mean value_function loss: 290.8278
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 86.0100
                       Mean reward: 750.03
               Mean episode length: 209.23
    Episode_Reward/reaching_object: 1.7823
     Episode_Reward/lifting_object: 150.3679
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1035
          Episode_Reward/joint_vel: -0.0961
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 190218240
                    Iteration time: 2.22s
                      Time elapsed: 01:14:03
                               ETA: 00:02:31

################################################################################
                     [1m Learning iteration 1935/2000 [0m                     

                       Computation: 44505 steps/s (collection: 2.100s, learning 0.109s)
             Mean action noise std: 4.09
          Mean value_function loss: 255.9080
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 86.0164
                       Mean reward: 790.70
               Mean episode length: 217.58
    Episode_Reward/reaching_object: 1.7881
     Episode_Reward/lifting_object: 151.4439
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1037
          Episode_Reward/joint_vel: -0.0951
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 190316544
                    Iteration time: 2.21s
                      Time elapsed: 01:14:06
                               ETA: 00:02:29

################################################################################
                     [1m Learning iteration 1936/2000 [0m                     

                       Computation: 43378 steps/s (collection: 2.142s, learning 0.124s)
             Mean action noise std: 4.09
          Mean value_function loss: 291.9664
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 86.0263
                       Mean reward: 791.25
               Mean episode length: 220.95
    Episode_Reward/reaching_object: 1.7897
     Episode_Reward/lifting_object: 152.1837
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.1044
          Episode_Reward/joint_vel: -0.0969
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 190414848
                    Iteration time: 2.27s
                      Time elapsed: 01:14:08
                               ETA: 00:02:26

################################################################################
                     [1m Learning iteration 1937/2000 [0m                     

                       Computation: 42704 steps/s (collection: 2.175s, learning 0.127s)
             Mean action noise std: 4.10
          Mean value_function loss: 288.6986
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 86.0407
                       Mean reward: 777.93
               Mean episode length: 214.41
    Episode_Reward/reaching_object: 1.8081
     Episode_Reward/lifting_object: 152.5409
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.1046
          Episode_Reward/joint_vel: -0.0980
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 190513152
                    Iteration time: 2.30s
                      Time elapsed: 01:14:10
                               ETA: 00:02:24

################################################################################
                     [1m Learning iteration 1938/2000 [0m                     

                       Computation: 42818 steps/s (collection: 2.171s, learning 0.125s)
             Mean action noise std: 4.10
          Mean value_function loss: 264.6188
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 86.0506
                       Mean reward: 758.40
               Mean episode length: 212.28
    Episode_Reward/reaching_object: 1.7856
     Episode_Reward/lifting_object: 151.3616
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.1042
          Episode_Reward/joint_vel: -0.0969
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 190611456
                    Iteration time: 2.30s
                      Time elapsed: 01:14:13
                               ETA: 00:02:22

################################################################################
                     [1m Learning iteration 1939/2000 [0m                     

                       Computation: 42972 steps/s (collection: 2.160s, learning 0.128s)
             Mean action noise std: 4.10
          Mean value_function loss: 227.8968
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 86.0644
                       Mean reward: 740.83
               Mean episode length: 213.81
    Episode_Reward/reaching_object: 1.7711
     Episode_Reward/lifting_object: 150.2612
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1058
          Episode_Reward/joint_vel: -0.0997
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 190709760
                    Iteration time: 2.29s
                      Time elapsed: 01:14:15
                               ETA: 00:02:20

################################################################################
                     [1m Learning iteration 1940/2000 [0m                     

                       Computation: 42468 steps/s (collection: 2.190s, learning 0.125s)
             Mean action noise std: 4.10
          Mean value_function loss: 227.3420
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 86.0775
                       Mean reward: 794.91
               Mean episode length: 217.87
    Episode_Reward/reaching_object: 1.8304
     Episode_Reward/lifting_object: 154.9227
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.1071
          Episode_Reward/joint_vel: -0.1004
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 190808064
                    Iteration time: 2.31s
                      Time elapsed: 01:14:17
                               ETA: 00:02:17

################################################################################
                     [1m Learning iteration 1941/2000 [0m                     

                       Computation: 42890 steps/s (collection: 2.161s, learning 0.131s)
             Mean action noise std: 4.10
          Mean value_function loss: 212.5620
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 86.0902
                       Mean reward: 742.91
               Mean episode length: 211.90
    Episode_Reward/reaching_object: 1.8108
     Episode_Reward/lifting_object: 153.3987
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1058
          Episode_Reward/joint_vel: -0.1004
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 190906368
                    Iteration time: 2.29s
                      Time elapsed: 01:14:19
                               ETA: 00:02:15

################################################################################
                     [1m Learning iteration 1942/2000 [0m                     

                       Computation: 42047 steps/s (collection: 2.216s, learning 0.122s)
             Mean action noise std: 4.11
          Mean value_function loss: 218.8451
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 86.1052
                       Mean reward: 781.88
               Mean episode length: 216.01
    Episode_Reward/reaching_object: 1.8609
     Episode_Reward/lifting_object: 158.1686
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.1078
          Episode_Reward/joint_vel: -0.0997
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 191004672
                    Iteration time: 2.34s
                      Time elapsed: 01:14:22
                               ETA: 00:02:13

################################################################################
                     [1m Learning iteration 1943/2000 [0m                     

                       Computation: 43008 steps/s (collection: 2.160s, learning 0.126s)
             Mean action noise std: 4.11
          Mean value_function loss: 204.7944
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 86.1172
                       Mean reward: 785.71
               Mean episode length: 220.78
    Episode_Reward/reaching_object: 1.8128
     Episode_Reward/lifting_object: 154.5186
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.1070
          Episode_Reward/joint_vel: -0.0987
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 191102976
                    Iteration time: 2.29s
                      Time elapsed: 01:14:24
                               ETA: 00:02:10

################################################################################
                     [1m Learning iteration 1944/2000 [0m                     

                       Computation: 43248 steps/s (collection: 2.136s, learning 0.137s)
             Mean action noise std: 4.11
          Mean value_function loss: 234.9384
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 86.1213
                       Mean reward: 794.24
               Mean episode length: 220.45
    Episode_Reward/reaching_object: 1.7983
     Episode_Reward/lifting_object: 151.7595
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.1059
          Episode_Reward/joint_vel: -0.0992
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 191201280
                    Iteration time: 2.27s
                      Time elapsed: 01:14:26
                               ETA: 00:02:08

################################################################################
                     [1m Learning iteration 1945/2000 [0m                     

                       Computation: 43271 steps/s (collection: 2.137s, learning 0.135s)
             Mean action noise std: 4.11
          Mean value_function loss: 218.4826
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 86.1294
                       Mean reward: 834.53
               Mean episode length: 229.73
    Episode_Reward/reaching_object: 1.8715
     Episode_Reward/lifting_object: 158.6951
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.1084
          Episode_Reward/joint_vel: -0.1002
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 191299584
                    Iteration time: 2.27s
                      Time elapsed: 01:14:29
                               ETA: 00:02:06

################################################################################
                     [1m Learning iteration 1946/2000 [0m                     

                       Computation: 41502 steps/s (collection: 2.247s, learning 0.122s)
             Mean action noise std: 4.11
          Mean value_function loss: 223.7974
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 86.1362
                       Mean reward: 805.37
               Mean episode length: 226.99
    Episode_Reward/reaching_object: 1.8493
     Episode_Reward/lifting_object: 156.5306
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.1091
          Episode_Reward/joint_vel: -0.0998
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 191397888
                    Iteration time: 2.37s
                      Time elapsed: 01:14:31
                               ETA: 00:02:04

################################################################################
                     [1m Learning iteration 1947/2000 [0m                     

                       Computation: 41645 steps/s (collection: 2.228s, learning 0.133s)
             Mean action noise std: 4.11
          Mean value_function loss: 243.4545
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 86.1437
                       Mean reward: 758.35
               Mean episode length: 217.68
    Episode_Reward/reaching_object: 1.7873
     Episode_Reward/lifting_object: 150.6133
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.1058
          Episode_Reward/joint_vel: -0.0985
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 191496192
                    Iteration time: 2.36s
                      Time elapsed: 01:14:33
                               ETA: 00:02:01

################################################################################
                     [1m Learning iteration 1948/2000 [0m                     

                       Computation: 43232 steps/s (collection: 2.150s, learning 0.124s)
             Mean action noise std: 4.11
          Mean value_function loss: 217.1548
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 86.1566
                       Mean reward: 784.73
               Mean episode length: 219.80
    Episode_Reward/reaching_object: 1.8376
     Episode_Reward/lifting_object: 155.4684
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.1078
          Episode_Reward/joint_vel: -0.0990
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 191594496
                    Iteration time: 2.27s
                      Time elapsed: 01:14:36
                               ETA: 00:01:59

################################################################################
                     [1m Learning iteration 1949/2000 [0m                     

                       Computation: 43554 steps/s (collection: 2.141s, learning 0.116s)
             Mean action noise std: 4.12
          Mean value_function loss: 309.7793
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 86.1673
                       Mean reward: 742.94
               Mean episode length: 208.67
    Episode_Reward/reaching_object: 1.7548
     Episode_Reward/lifting_object: 147.7144
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.1036
          Episode_Reward/joint_vel: -0.0947
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 191692800
                    Iteration time: 2.26s
                      Time elapsed: 01:14:38
                               ETA: 00:01:57

################################################################################
                     [1m Learning iteration 1950/2000 [0m                     

                       Computation: 43959 steps/s (collection: 2.125s, learning 0.112s)
             Mean action noise std: 4.12
          Mean value_function loss: 254.0071
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 86.1804
                       Mean reward: 759.69
               Mean episode length: 211.02
    Episode_Reward/reaching_object: 1.8106
     Episode_Reward/lifting_object: 153.3925
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.1055
          Episode_Reward/joint_vel: -0.0954
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 191791104
                    Iteration time: 2.24s
                      Time elapsed: 01:14:40
                               ETA: 00:01:54

################################################################################
                     [1m Learning iteration 1951/2000 [0m                     

                       Computation: 43224 steps/s (collection: 2.155s, learning 0.119s)
             Mean action noise std: 4.12
          Mean value_function loss: 196.6522
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 86.1948
                       Mean reward: 798.55
               Mean episode length: 225.22
    Episode_Reward/reaching_object: 1.8795
     Episode_Reward/lifting_object: 159.7508
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1100
          Episode_Reward/joint_vel: -0.0995
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 191889408
                    Iteration time: 2.27s
                      Time elapsed: 01:14:42
                               ETA: 00:01:52

################################################################################
                     [1m Learning iteration 1952/2000 [0m                     

                       Computation: 44357 steps/s (collection: 2.099s, learning 0.117s)
             Mean action noise std: 4.12
          Mean value_function loss: 262.3545
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 86.2019
                       Mean reward: 773.33
               Mean episode length: 215.34
    Episode_Reward/reaching_object: 1.8147
     Episode_Reward/lifting_object: 153.3375
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.1072
          Episode_Reward/joint_vel: -0.0980
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 191987712
                    Iteration time: 2.22s
                      Time elapsed: 01:14:45
                               ETA: 00:01:50

################################################################################
                     [1m Learning iteration 1953/2000 [0m                     

                       Computation: 44123 steps/s (collection: 2.122s, learning 0.106s)
             Mean action noise std: 4.12
          Mean value_function loss: 196.2834
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 86.2184
                       Mean reward: 842.31
               Mean episode length: 234.00
    Episode_Reward/reaching_object: 1.8632
     Episode_Reward/lifting_object: 158.3712
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.1100
          Episode_Reward/joint_vel: -0.0992
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 192086016
                    Iteration time: 2.23s
                      Time elapsed: 01:14:47
                               ETA: 00:01:47

################################################################################
                     [1m Learning iteration 1954/2000 [0m                     

                       Computation: 44055 steps/s (collection: 2.126s, learning 0.105s)
             Mean action noise std: 4.13
          Mean value_function loss: 218.6657
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 86.2350
                       Mean reward: 846.48
               Mean episode length: 233.20
    Episode_Reward/reaching_object: 1.8381
     Episode_Reward/lifting_object: 156.0847
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1076
          Episode_Reward/joint_vel: -0.0970
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 192184320
                    Iteration time: 2.23s
                      Time elapsed: 01:14:49
                               ETA: 00:01:45

################################################################################
                     [1m Learning iteration 1955/2000 [0m                     

                       Computation: 43937 steps/s (collection: 2.114s, learning 0.123s)
             Mean action noise std: 4.13
          Mean value_function loss: 228.4710
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 86.2444
                       Mean reward: 796.37
               Mean episode length: 227.17
    Episode_Reward/reaching_object: 1.8689
     Episode_Reward/lifting_object: 159.1062
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.1093
          Episode_Reward/joint_vel: -0.0980
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 192282624
                    Iteration time: 2.24s
                      Time elapsed: 01:14:51
                               ETA: 00:01:43

################################################################################
                     [1m Learning iteration 1956/2000 [0m                     

                       Computation: 44038 steps/s (collection: 2.126s, learning 0.106s)
             Mean action noise std: 4.13
          Mean value_function loss: 226.8368
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 86.2497
                       Mean reward: 788.21
               Mean episode length: 227.01
    Episode_Reward/reaching_object: 1.8175
     Episode_Reward/lifting_object: 153.4490
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1077
          Episode_Reward/joint_vel: -0.0980
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 192380928
                    Iteration time: 2.23s
                      Time elapsed: 01:14:53
                               ETA: 00:01:41

################################################################################
                     [1m Learning iteration 1957/2000 [0m                     

                       Computation: 44272 steps/s (collection: 2.122s, learning 0.099s)
             Mean action noise std: 4.13
          Mean value_function loss: 230.6446
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 86.2569
                       Mean reward: 755.10
               Mean episode length: 212.17
    Episode_Reward/reaching_object: 1.8342
     Episode_Reward/lifting_object: 155.8256
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1075
          Episode_Reward/joint_vel: -0.0968
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 192479232
                    Iteration time: 2.22s
                      Time elapsed: 01:14:56
                               ETA: 00:01:38

################################################################################
                     [1m Learning iteration 1958/2000 [0m                     

                       Computation: 44343 steps/s (collection: 2.112s, learning 0.105s)
             Mean action noise std: 4.13
          Mean value_function loss: 218.6909
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 86.2655
                       Mean reward: 782.71
               Mean episode length: 217.42
    Episode_Reward/reaching_object: 1.8494
     Episode_Reward/lifting_object: 156.9452
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.1086
          Episode_Reward/joint_vel: -0.0978
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 192577536
                    Iteration time: 2.22s
                      Time elapsed: 01:14:58
                               ETA: 00:01:36

################################################################################
                     [1m Learning iteration 1959/2000 [0m                     

                       Computation: 44002 steps/s (collection: 2.127s, learning 0.107s)
             Mean action noise std: 4.13
          Mean value_function loss: 220.7519
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 86.2729
                       Mean reward: 813.25
               Mean episode length: 225.04
    Episode_Reward/reaching_object: 1.8620
     Episode_Reward/lifting_object: 158.3701
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.1104
          Episode_Reward/joint_vel: -0.0994
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 192675840
                    Iteration time: 2.23s
                      Time elapsed: 01:15:00
                               ETA: 00:01:34

################################################################################
                     [1m Learning iteration 1960/2000 [0m                     

                       Computation: 43731 steps/s (collection: 2.133s, learning 0.115s)
             Mean action noise std: 4.13
          Mean value_function loss: 216.1661
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 86.2845
                       Mean reward: 795.27
               Mean episode length: 217.08
    Episode_Reward/reaching_object: 1.8316
     Episode_Reward/lifting_object: 155.9333
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1076
          Episode_Reward/joint_vel: -0.0958
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 192774144
                    Iteration time: 2.25s
                      Time elapsed: 01:15:02
                               ETA: 00:01:31

################################################################################
                     [1m Learning iteration 1961/2000 [0m                     

                       Computation: 43649 steps/s (collection: 2.138s, learning 0.114s)
             Mean action noise std: 4.14
          Mean value_function loss: 246.9886
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 86.3027
                       Mean reward: 794.44
               Mean episode length: 220.79
    Episode_Reward/reaching_object: 1.8473
     Episode_Reward/lifting_object: 156.9508
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1085
          Episode_Reward/joint_vel: -0.0975
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 192872448
                    Iteration time: 2.25s
                      Time elapsed: 01:15:05
                               ETA: 00:01:29

################################################################################
                     [1m Learning iteration 1962/2000 [0m                     

                       Computation: 44577 steps/s (collection: 2.109s, learning 0.096s)
             Mean action noise std: 4.14
          Mean value_function loss: 246.5631
               Mean surrogate loss: 0.0150
                 Mean entropy loss: 86.3201
                       Mean reward: 743.25
               Mean episode length: 212.32
    Episode_Reward/reaching_object: 1.7658
     Episode_Reward/lifting_object: 149.3216
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.1062
          Episode_Reward/joint_vel: -0.0967
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 192970752
                    Iteration time: 2.21s
                      Time elapsed: 01:15:07
                               ETA: 00:01:27

################################################################################
                     [1m Learning iteration 1963/2000 [0m                     

                       Computation: 44035 steps/s (collection: 2.110s, learning 0.122s)
             Mean action noise std: 4.14
          Mean value_function loss: 246.9066
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 86.3235
                       Mean reward: 748.99
               Mean episode length: 213.96
    Episode_Reward/reaching_object: 1.8055
     Episode_Reward/lifting_object: 153.3847
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1083
          Episode_Reward/joint_vel: -0.0981
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 193069056
                    Iteration time: 2.23s
                      Time elapsed: 01:15:09
                               ETA: 00:01:24

################################################################################
                     [1m Learning iteration 1964/2000 [0m                     

                       Computation: 43771 steps/s (collection: 2.130s, learning 0.116s)
             Mean action noise std: 4.14
          Mean value_function loss: 201.2441
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 86.3313
                       Mean reward: 810.32
               Mean episode length: 220.72
    Episode_Reward/reaching_object: 1.8561
     Episode_Reward/lifting_object: 157.9339
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1092
          Episode_Reward/joint_vel: -0.0968
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 193167360
                    Iteration time: 2.25s
                      Time elapsed: 01:15:11
                               ETA: 00:01:22

################################################################################
                     [1m Learning iteration 1965/2000 [0m                     

                       Computation: 44100 steps/s (collection: 2.106s, learning 0.123s)
             Mean action noise std: 4.14
          Mean value_function loss: 222.2839
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 86.3434
                       Mean reward: 803.68
               Mean episode length: 222.48
    Episode_Reward/reaching_object: 1.8159
     Episode_Reward/lifting_object: 154.6943
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1073
          Episode_Reward/joint_vel: -0.0954
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 193265664
                    Iteration time: 2.23s
                      Time elapsed: 01:15:14
                               ETA: 00:01:20

################################################################################
                     [1m Learning iteration 1966/2000 [0m                     

                       Computation: 43319 steps/s (collection: 2.158s, learning 0.112s)
             Mean action noise std: 4.14
          Mean value_function loss: 211.9227
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 86.3529
                       Mean reward: 824.50
               Mean episode length: 225.45
    Episode_Reward/reaching_object: 1.9129
     Episode_Reward/lifting_object: 163.1655
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.1131
          Episode_Reward/joint_vel: -0.1000
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 193363968
                    Iteration time: 2.27s
                      Time elapsed: 01:15:16
                               ETA: 00:01:18

################################################################################
                     [1m Learning iteration 1967/2000 [0m                     

                       Computation: 43927 steps/s (collection: 2.133s, learning 0.105s)
             Mean action noise std: 4.14
          Mean value_function loss: 216.8867
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 86.3655
                       Mean reward: 827.48
               Mean episode length: 225.23
    Episode_Reward/reaching_object: 1.8385
     Episode_Reward/lifting_object: 156.0684
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.1090
          Episode_Reward/joint_vel: -0.0965
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 193462272
                    Iteration time: 2.24s
                      Time elapsed: 01:15:18
                               ETA: 00:01:15

################################################################################
                     [1m Learning iteration 1968/2000 [0m                     

                       Computation: 44671 steps/s (collection: 2.106s, learning 0.095s)
             Mean action noise std: 4.14
          Mean value_function loss: 263.2757
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 86.3740
                       Mean reward: 758.87
               Mean episode length: 217.20
    Episode_Reward/reaching_object: 1.8056
     Episode_Reward/lifting_object: 152.8693
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.1073
          Episode_Reward/joint_vel: -0.0950
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 193560576
                    Iteration time: 2.20s
                      Time elapsed: 01:15:20
                               ETA: 00:01:13

################################################################################
                     [1m Learning iteration 1969/2000 [0m                     

                       Computation: 44331 steps/s (collection: 2.109s, learning 0.109s)
             Mean action noise std: 4.15
          Mean value_function loss: 270.6346
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 86.3774
                       Mean reward: 762.06
               Mean episode length: 211.46
    Episode_Reward/reaching_object: 1.7929
     Episode_Reward/lifting_object: 151.6705
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.1063
          Episode_Reward/joint_vel: -0.0945
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 193658880
                    Iteration time: 2.22s
                      Time elapsed: 01:15:23
                               ETA: 00:01:11

################################################################################
                     [1m Learning iteration 1970/2000 [0m                     

                       Computation: 44094 steps/s (collection: 2.122s, learning 0.107s)
             Mean action noise std: 4.15
          Mean value_function loss: 248.1214
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 86.3830
                       Mean reward: 759.52
               Mean episode length: 218.14
    Episode_Reward/reaching_object: 1.8248
     Episode_Reward/lifting_object: 155.2422
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.1094
          Episode_Reward/joint_vel: -0.0970
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 193757184
                    Iteration time: 2.23s
                      Time elapsed: 01:15:25
                               ETA: 00:01:08

################################################################################
                     [1m Learning iteration 1971/2000 [0m                     

                       Computation: 44509 steps/s (collection: 2.102s, learning 0.107s)
             Mean action noise std: 4.15
          Mean value_function loss: 264.3169
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 86.3930
                       Mean reward: 781.97
               Mean episode length: 215.02
    Episode_Reward/reaching_object: 1.8008
     Episode_Reward/lifting_object: 152.3567
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.1077
          Episode_Reward/joint_vel: -0.0957
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 193855488
                    Iteration time: 2.21s
                      Time elapsed: 01:15:27
                               ETA: 00:01:06

################################################################################
                     [1m Learning iteration 1972/2000 [0m                     

                       Computation: 44596 steps/s (collection: 2.098s, learning 0.106s)
             Mean action noise std: 4.15
          Mean value_function loss: 203.4940
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 86.4046
                       Mean reward: 773.63
               Mean episode length: 216.77
    Episode_Reward/reaching_object: 1.8310
     Episode_Reward/lifting_object: 155.6839
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1102
          Episode_Reward/joint_vel: -0.0974
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 193953792
                    Iteration time: 2.20s
                      Time elapsed: 01:15:29
                               ETA: 00:01:04

################################################################################
                     [1m Learning iteration 1973/2000 [0m                     

                       Computation: 44747 steps/s (collection: 2.099s, learning 0.098s)
             Mean action noise std: 4.15
          Mean value_function loss: 260.5442
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 86.4180
                       Mean reward: 793.89
               Mean episode length: 219.98
    Episode_Reward/reaching_object: 1.8329
     Episode_Reward/lifting_object: 155.5203
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.1084
          Episode_Reward/joint_vel: -0.0966
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 194052096
                    Iteration time: 2.20s
                      Time elapsed: 01:15:31
                               ETA: 00:01:01

################################################################################
                     [1m Learning iteration 1974/2000 [0m                     

                       Computation: 44101 steps/s (collection: 2.122s, learning 0.107s)
             Mean action noise std: 4.15
          Mean value_function loss: 246.7214
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 86.4311
                       Mean reward: 760.44
               Mean episode length: 213.41
    Episode_Reward/reaching_object: 1.8347
     Episode_Reward/lifting_object: 156.3269
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1090
          Episode_Reward/joint_vel: -0.0948
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 194150400
                    Iteration time: 2.23s
                      Time elapsed: 01:15:34
                               ETA: 00:00:59

################################################################################
                     [1m Learning iteration 1975/2000 [0m                     

                       Computation: 44663 steps/s (collection: 2.106s, learning 0.095s)
             Mean action noise std: 4.16
          Mean value_function loss: 218.4084
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 86.4537
                       Mean reward: 813.13
               Mean episode length: 229.32
    Episode_Reward/reaching_object: 1.8298
     Episode_Reward/lifting_object: 155.5549
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.1110
          Episode_Reward/joint_vel: -0.0988
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 194248704
                    Iteration time: 2.20s
                      Time elapsed: 01:15:36
                               ETA: 00:00:57

################################################################################
                     [1m Learning iteration 1976/2000 [0m                     

                       Computation: 44007 steps/s (collection: 2.127s, learning 0.107s)
             Mean action noise std: 4.16
          Mean value_function loss: 242.7975
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 86.4697
                       Mean reward: 786.89
               Mean episode length: 220.92
    Episode_Reward/reaching_object: 1.8030
     Episode_Reward/lifting_object: 152.6176
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1086
          Episode_Reward/joint_vel: -0.0957
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 194347008
                    Iteration time: 2.23s
                      Time elapsed: 01:15:38
                               ETA: 00:00:55

################################################################################
                     [1m Learning iteration 1977/2000 [0m                     

                       Computation: 43808 steps/s (collection: 2.124s, learning 0.120s)
             Mean action noise std: 4.16
          Mean value_function loss: 219.5667
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 86.4813
                       Mean reward: 823.58
               Mean episode length: 227.01
    Episode_Reward/reaching_object: 1.8204
     Episode_Reward/lifting_object: 153.7859
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.1098
          Episode_Reward/joint_vel: -0.0984
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 194445312
                    Iteration time: 2.24s
                      Time elapsed: 01:15:40
                               ETA: 00:00:52

################################################################################
                     [1m Learning iteration 1978/2000 [0m                     

                       Computation: 44497 steps/s (collection: 2.094s, learning 0.116s)
             Mean action noise std: 4.16
          Mean value_function loss: 269.5879
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 86.4926
                       Mean reward: 780.37
               Mean episode length: 219.16
    Episode_Reward/reaching_object: 1.8273
     Episode_Reward/lifting_object: 154.2402
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.1097
          Episode_Reward/joint_vel: -0.0978
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 194543616
                    Iteration time: 2.21s
                      Time elapsed: 01:15:42
                               ETA: 00:00:50

################################################################################
                     [1m Learning iteration 1979/2000 [0m                     

                       Computation: 43930 steps/s (collection: 2.121s, learning 0.117s)
             Mean action noise std: 4.16
          Mean value_function loss: 235.2985
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 86.4975
                       Mean reward: 821.56
               Mean episode length: 224.19
    Episode_Reward/reaching_object: 1.8382
     Episode_Reward/lifting_object: 155.9174
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1105
          Episode_Reward/joint_vel: -0.0972
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 194641920
                    Iteration time: 2.24s
                      Time elapsed: 01:15:45
                               ETA: 00:00:48

################################################################################
                     [1m Learning iteration 1980/2000 [0m                     

                       Computation: 43138 steps/s (collection: 2.170s, learning 0.109s)
             Mean action noise std: 4.17
          Mean value_function loss: 222.5270
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 86.5083
                       Mean reward: 746.31
               Mean episode length: 212.63
    Episode_Reward/reaching_object: 1.8086
     Episode_Reward/lifting_object: 153.5268
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1083
          Episode_Reward/joint_vel: -0.0950
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 194740224
                    Iteration time: 2.28s
                      Time elapsed: 01:15:47
                               ETA: 00:00:45

################################################################################
                     [1m Learning iteration 1981/2000 [0m                     

                       Computation: 43985 steps/s (collection: 2.131s, learning 0.104s)
             Mean action noise std: 4.17
          Mean value_function loss: 223.8416
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 86.5148
                       Mean reward: 781.41
               Mean episode length: 218.60
    Episode_Reward/reaching_object: 1.8546
     Episode_Reward/lifting_object: 157.5060
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.1108
          Episode_Reward/joint_vel: -0.0950
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 194838528
                    Iteration time: 2.23s
                      Time elapsed: 01:15:49
                               ETA: 00:00:43

################################################################################
                     [1m Learning iteration 1982/2000 [0m                     

                       Computation: 43930 steps/s (collection: 2.131s, learning 0.107s)
             Mean action noise std: 4.17
          Mean value_function loss: 234.5604
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 86.5239
                       Mean reward: 811.00
               Mean episode length: 225.39
    Episode_Reward/reaching_object: 1.8007
     Episode_Reward/lifting_object: 151.9159
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.1086
          Episode_Reward/joint_vel: -0.0971
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 194936832
                    Iteration time: 2.24s
                      Time elapsed: 01:15:51
                               ETA: 00:00:41

################################################################################
                     [1m Learning iteration 1983/2000 [0m                     

                       Computation: 44296 steps/s (collection: 2.113s, learning 0.106s)
             Mean action noise std: 4.17
          Mean value_function loss: 212.2774
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 86.5366
                       Mean reward: 794.63
               Mean episode length: 220.93
    Episode_Reward/reaching_object: 1.8655
     Episode_Reward/lifting_object: 158.5057
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.1125
          Episode_Reward/joint_vel: -0.0984
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 195035136
                    Iteration time: 2.22s
                      Time elapsed: 01:15:54
                               ETA: 00:00:39

################################################################################
                     [1m Learning iteration 1984/2000 [0m                     

                       Computation: 44164 steps/s (collection: 2.128s, learning 0.098s)
             Mean action noise std: 4.17
          Mean value_function loss: 224.9900
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 86.5475
                       Mean reward: 769.70
               Mean episode length: 221.73
    Episode_Reward/reaching_object: 1.8421
     Episode_Reward/lifting_object: 155.4960
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1108
          Episode_Reward/joint_vel: -0.0974
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 195133440
                    Iteration time: 2.23s
                      Time elapsed: 01:15:56
                               ETA: 00:00:36

################################################################################
                     [1m Learning iteration 1985/2000 [0m                     

                       Computation: 44259 steps/s (collection: 2.117s, learning 0.105s)
             Mean action noise std: 4.17
          Mean value_function loss: 248.2407
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 86.5622
                       Mean reward: 747.45
               Mean episode length: 213.29
    Episode_Reward/reaching_object: 1.7755
     Episode_Reward/lifting_object: 150.2388
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1076
          Episode_Reward/joint_vel: -0.0946
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 195231744
                    Iteration time: 2.22s
                      Time elapsed: 01:15:58
                               ETA: 00:00:34

################################################################################
                     [1m Learning iteration 1986/2000 [0m                     

                       Computation: 43809 steps/s (collection: 2.129s, learning 0.115s)
             Mean action noise std: 4.18
          Mean value_function loss: 269.8767
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 86.5734
                       Mean reward: 791.98
               Mean episode length: 221.23
    Episode_Reward/reaching_object: 1.8069
     Episode_Reward/lifting_object: 152.6004
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1091
          Episode_Reward/joint_vel: -0.0962
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 195330048
                    Iteration time: 2.24s
                      Time elapsed: 01:16:00
                               ETA: 00:00:32

################################################################################
                     [1m Learning iteration 1987/2000 [0m                     

                       Computation: 44522 steps/s (collection: 2.109s, learning 0.099s)
             Mean action noise std: 4.18
          Mean value_function loss: 252.6546
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 86.5830
                       Mean reward: 779.80
               Mean episode length: 222.14
    Episode_Reward/reaching_object: 1.7822
     Episode_Reward/lifting_object: 150.9855
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1083
          Episode_Reward/joint_vel: -0.0946
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 195428352
                    Iteration time: 2.21s
                      Time elapsed: 01:16:03
                               ETA: 00:00:29

################################################################################
                     [1m Learning iteration 1988/2000 [0m                     

                       Computation: 44246 steps/s (collection: 2.117s, learning 0.105s)
             Mean action noise std: 4.18
          Mean value_function loss: 232.6919
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 86.5906
                       Mean reward: 763.77
               Mean episode length: 220.17
    Episode_Reward/reaching_object: 1.8272
     Episode_Reward/lifting_object: 155.3596
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.1101
          Episode_Reward/joint_vel: -0.0947
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 195526656
                    Iteration time: 2.22s
                      Time elapsed: 01:16:05
                               ETA: 00:00:27

################################################################################
                     [1m Learning iteration 1989/2000 [0m                     

                       Computation: 43661 steps/s (collection: 2.149s, learning 0.103s)
             Mean action noise std: 4.18
          Mean value_function loss: 240.2583
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 86.6032
                       Mean reward: 805.58
               Mean episode length: 222.16
    Episode_Reward/reaching_object: 1.8242
     Episode_Reward/lifting_object: 154.6161
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.1108
          Episode_Reward/joint_vel: -0.0970
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 195624960
                    Iteration time: 2.25s
                      Time elapsed: 01:16:07
                               ETA: 00:00:25

################################################################################
                     [1m Learning iteration 1990/2000 [0m                     

                       Computation: 44270 steps/s (collection: 2.116s, learning 0.104s)
             Mean action noise std: 4.18
          Mean value_function loss: 256.9972
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 86.6171
                       Mean reward: 814.75
               Mean episode length: 222.82
    Episode_Reward/reaching_object: 1.8535
     Episode_Reward/lifting_object: 157.7329
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.1105
          Episode_Reward/joint_vel: -0.0947
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 195723264
                    Iteration time: 2.22s
                      Time elapsed: 01:16:09
                               ETA: 00:00:22

################################################################################
                     [1m Learning iteration 1991/2000 [0m                     

                       Computation: 43317 steps/s (collection: 2.150s, learning 0.119s)
             Mean action noise std: 4.18
          Mean value_function loss: 260.0873
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 86.6293
                       Mean reward: 756.09
               Mean episode length: 211.76
    Episode_Reward/reaching_object: 1.8264
     Episode_Reward/lifting_object: 154.8920
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.1100
          Episode_Reward/joint_vel: -0.0950
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 195821568
                    Iteration time: 2.27s
                      Time elapsed: 01:16:12
                               ETA: 00:00:20

################################################################################
                     [1m Learning iteration 1992/2000 [0m                     

                       Computation: 43691 steps/s (collection: 2.129s, learning 0.121s)
             Mean action noise std: 4.18
          Mean value_function loss: 303.2125
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 86.6400
                       Mean reward: 744.77
               Mean episode length: 207.46
    Episode_Reward/reaching_object: 1.7852
     Episode_Reward/lifting_object: 151.3458
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.1093
          Episode_Reward/joint_vel: -0.0960
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 195919872
                    Iteration time: 2.25s
                      Time elapsed: 01:16:14
                               ETA: 00:00:18

################################################################################
                     [1m Learning iteration 1993/2000 [0m                     

                       Computation: 44204 steps/s (collection: 2.120s, learning 0.104s)
             Mean action noise std: 4.19
          Mean value_function loss: 313.4088
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 86.6454
                       Mean reward: 758.88
               Mean episode length: 211.12
    Episode_Reward/reaching_object: 1.7589
     Episode_Reward/lifting_object: 149.1053
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.1083
          Episode_Reward/joint_vel: -0.0967
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 196018176
                    Iteration time: 2.22s
                      Time elapsed: 01:16:16
                               ETA: 00:00:16

################################################################################
                     [1m Learning iteration 1994/2000 [0m                     

                       Computation: 43921 steps/s (collection: 2.141s, learning 0.097s)
             Mean action noise std: 4.19
          Mean value_function loss: 238.5975
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 86.6532
                       Mean reward: 758.14
               Mean episode length: 208.87
    Episode_Reward/reaching_object: 1.7928
     Episode_Reward/lifting_object: 152.5950
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.1091
          Episode_Reward/joint_vel: -0.0945
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 196116480
                    Iteration time: 2.24s
                      Time elapsed: 01:16:18
                               ETA: 00:00:13

################################################################################
                     [1m Learning iteration 1995/2000 [0m                     

                       Computation: 42945 steps/s (collection: 2.191s, learning 0.098s)
             Mean action noise std: 4.19
          Mean value_function loss: 231.8575
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 86.6670
                       Mean reward: 806.61
               Mean episode length: 222.13
    Episode_Reward/reaching_object: 1.7904
     Episode_Reward/lifting_object: 151.6219
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.1108
          Episode_Reward/joint_vel: -0.0980
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 196214784
                    Iteration time: 2.29s
                      Time elapsed: 01:16:21
                               ETA: 00:00:11

################################################################################
                     [1m Learning iteration 1996/2000 [0m                     

                       Computation: 42194 steps/s (collection: 2.213s, learning 0.117s)
             Mean action noise std: 4.19
          Mean value_function loss: 217.7031
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 86.6746
                       Mean reward: 833.52
               Mean episode length: 227.24
    Episode_Reward/reaching_object: 1.8120
     Episode_Reward/lifting_object: 153.9506
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1107
          Episode_Reward/joint_vel: -0.0966
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 196313088
                    Iteration time: 2.33s
                      Time elapsed: 01:16:23
                               ETA: 00:00:09

################################################################################
                     [1m Learning iteration 1997/2000 [0m                     

                       Computation: 41797 steps/s (collection: 2.233s, learning 0.119s)
             Mean action noise std: 4.19
          Mean value_function loss: 244.6586
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 86.6810
                       Mean reward: 759.13
               Mean episode length: 211.11
    Episode_Reward/reaching_object: 1.8218
     Episode_Reward/lifting_object: 154.9504
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1102
          Episode_Reward/joint_vel: -0.0946
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 196411392
                    Iteration time: 2.35s
                      Time elapsed: 01:16:25
                               ETA: 00:00:06

################################################################################
                     [1m Learning iteration 1998/2000 [0m                     

                       Computation: 42046 steps/s (collection: 2.232s, learning 0.106s)
             Mean action noise std: 4.19
          Mean value_function loss: 245.8331
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 86.6904
                       Mean reward: 757.07
               Mean episode length: 210.15
    Episode_Reward/reaching_object: 1.7571
     Episode_Reward/lifting_object: 148.6271
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1093
          Episode_Reward/joint_vel: -0.0947
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 196509696
                    Iteration time: 2.34s
                      Time elapsed: 01:16:28
                               ETA: 00:00:04

################################################################################
                     [1m Learning iteration 1999/2000 [0m                     

                       Computation: 42606 steps/s (collection: 2.214s, learning 0.094s)
             Mean action noise std: 4.19
          Mean value_function loss: 264.2503
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 86.6969
                       Mean reward: 707.15
               Mean episode length: 203.64
    Episode_Reward/reaching_object: 1.7531
     Episode_Reward/lifting_object: 148.5097
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.1081
          Episode_Reward/joint_vel: -0.0943
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 196608000
                    Iteration time: 2.31s
                      Time elapsed: 01:16:30
                               ETA: 00:00:02

