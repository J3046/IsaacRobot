################################################################################
                      [1m Learning iteration 0/2000 [0m                       

                       Computation: 10848 steps/s (collection: 8.795s, learning 0.267s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 31.2578
                       Mean reward: 0.00
               Mean episode length: 21.31
    Episode_Reward/reaching_object: 0.0006
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0001
        Episode_Reward/action_rate: -0.0002
          Episode_Reward/joint_vel: -0.0003
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 9.06s
                      Time elapsed: 00:00:09
                               ETA: 05:02:02

################################################################################
                      [1m Learning iteration 1/2000 [0m                       

                       Computation: 14849 steps/s (collection: 6.487s, learning 0.133s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 31.3733
                       Mean reward: 0.00
               Mean episode length: 45.35
    Episode_Reward/reaching_object: 0.0018
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0003
        Episode_Reward/action_rate: -0.0006
          Episode_Reward/joint_vel: -0.0008
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 6.62s
                      Time elapsed: 00:00:15
                               ETA: 04:21:13

################################################################################
                      [1m Learning iteration 2/2000 [0m                       

                       Computation: 14753 steps/s (collection: 6.515s, learning 0.149s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 31.4645
                       Mean reward: 0.01
               Mean episode length: 69.71
    Episode_Reward/reaching_object: 0.0031
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0005
        Episode_Reward/action_rate: -0.0011
          Episode_Reward/joint_vel: -0.0014
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 6.66s
                      Time elapsed: 00:00:22
                               ETA: 04:08:01

################################################################################
                      [1m Learning iteration 3/2000 [0m                       

                       Computation: 14665 steps/s (collection: 6.564s, learning 0.139s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 31.4772
                       Mean reward: 0.01
               Mean episode length: 93.33
    Episode_Reward/reaching_object: 0.0044
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0007
        Episode_Reward/action_rate: -0.0015
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 6.70s
                      Time elapsed: 00:00:29
                               ETA: 04:01:41

################################################################################
                      [1m Learning iteration 4/2000 [0m                       

                       Computation: 15014 steps/s (collection: 6.401s, learning 0.147s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 31.5040
                       Mean reward: 0.02
               Mean episode length: 117.98
    Episode_Reward/reaching_object: 0.0061
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 6.55s
                      Time elapsed: 00:00:35
                               ETA: 03:56:49

################################################################################
                      [1m Learning iteration 5/2000 [0m                       

                       Computation: 15509 steps/s (collection: 6.175s, learning 0.163s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 31.5573
                       Mean reward: 0.01
               Mean episode length: 141.18
    Episode_Reward/reaching_object: 0.0076
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0031
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 6.34s
                      Time elapsed: 00:00:41
                               ETA: 03:52:22

################################################################################
                      [1m Learning iteration 6/2000 [0m                       

                       Computation: 14574 steps/s (collection: 6.595s, learning 0.150s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 31.5791
                       Mean reward: 0.02
               Mean episode length: 165.30
    Episode_Reward/reaching_object: 0.0096
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0036
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 6.74s
                      Time elapsed: 00:00:48
                               ETA: 03:51:06

################################################################################
                      [1m Learning iteration 7/2000 [0m                       

                       Computation: 14878 steps/s (collection: 6.470s, learning 0.137s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 31.5628
                       Mean reward: 0.04
               Mean episode length: 189.28
    Episode_Reward/reaching_object: 0.0120
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 6.61s
                      Time elapsed: 00:00:55
                               ETA: 03:49:32

################################################################################
                      [1m Learning iteration 8/2000 [0m                       

                       Computation: 17995 steps/s (collection: 5.344s, learning 0.119s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 31.5647
                       Mean reward: 0.04
               Mean episode length: 213.34
    Episode_Reward/reaching_object: 0.0139
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0048
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 5.46s
                      Time elapsed: 00:01:00
                               ETA: 03:44:05

################################################################################
                      [1m Learning iteration 9/2000 [0m                       

                       Computation: 59625 steps/s (collection: 1.539s, learning 0.110s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 31.5981
                       Mean reward: 0.06
               Mean episode length: 237.08
    Episode_Reward/reaching_object: 0.0191
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0053
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 1.65s
                      Time elapsed: 00:01:02
                               ETA: 03:27:03

################################################################################
                      [1m Learning iteration 10/2000 [0m                      

                       Computation: 61255 steps/s (collection: 1.510s, learning 0.095s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 31.6122
                       Mean reward: 0.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0221
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0058
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 1.60s
                      Time elapsed: 00:01:04
                               ETA: 03:12:58

################################################################################
                      [1m Learning iteration 11/2000 [0m                      

                       Computation: 58746 steps/s (collection: 1.560s, learning 0.114s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 31.6161
                       Mean reward: 0.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0252
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0058
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 1.67s
                      Time elapsed: 00:01:05
                               ETA: 03:01:25

################################################################################
                      [1m Learning iteration 12/2000 [0m                      

                       Computation: 59841 steps/s (collection: 1.526s, learning 0.117s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 31.6353
                       Mean reward: 0.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0299
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0058
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 1.64s
                      Time elapsed: 00:01:07
                               ETA: 02:51:34

################################################################################
                      [1m Learning iteration 13/2000 [0m                      

                       Computation: 61311 steps/s (collection: 1.492s, learning 0.111s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 31.6504
                       Mean reward: 0.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0383
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0058
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 1.60s
                      Time elapsed: 00:01:08
                               ETA: 02:43:01

################################################################################
                      [1m Learning iteration 14/2000 [0m                      

                       Computation: 58784 steps/s (collection: 1.561s, learning 0.111s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 31.6843
                       Mean reward: 0.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0474
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0058
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 1.67s
                      Time elapsed: 00:01:10
                               ETA: 02:35:46

################################################################################
                      [1m Learning iteration 15/2000 [0m                      

                       Computation: 58730 steps/s (collection: 1.572s, learning 0.102s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.6123
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 31.7320
                       Mean reward: 0.29
               Mean episode length: 249.45
    Episode_Reward/reaching_object: 0.0634
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0058
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 1.67s
                      Time elapsed: 00:01:12
                               ETA: 02:29:25

################################################################################
                      [1m Learning iteration 16/2000 [0m                      

                       Computation: 56276 steps/s (collection: 1.622s, learning 0.125s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.1111
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 31.8149
                       Mean reward: -0.04
               Mean episode length: 249.52
    Episode_Reward/reaching_object: 0.0870
     Episode_Reward/lifting_object: -0.0876
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0058
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 1.75s
                      Time elapsed: 00:01:14
                               ETA: 02:23:57

################################################################################
                      [1m Learning iteration 17/2000 [0m                      

                       Computation: 54633 steps/s (collection: 1.689s, learning 0.110s)
             Mean action noise std: 1.04
          Mean value_function loss: 2.9921
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 32.0458
                       Mean reward: -1.06
               Mean episode length: 248.66
    Episode_Reward/reaching_object: 0.1070
     Episode_Reward/lifting_object: -0.2073
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0058
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 1.80s
                      Time elapsed: 00:01:15
                               ETA: 02:19:12

################################################################################
                      [1m Learning iteration 18/2000 [0m                      

                       Computation: 54115 steps/s (collection: 1.710s, learning 0.107s)
             Mean action noise std: 1.05
          Mean value_function loss: 1.9367
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 32.1754
                       Mean reward: 0.44
               Mean episode length: 247.36
    Episode_Reward/reaching_object: 0.1326
     Episode_Reward/lifting_object: -0.2810
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0058
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 1.82s
                      Time elapsed: 00:01:17
                               ETA: 02:14:57

################################################################################
                      [1m Learning iteration 19/2000 [0m                      

                       Computation: 54918 steps/s (collection: 1.701s, learning 0.089s)
             Mean action noise std: 1.05
          Mean value_function loss: 1.3361
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 32.3000
                       Mean reward: -0.45
               Mean episode length: 247.38
    Episode_Reward/reaching_object: 0.1464
     Episode_Reward/lifting_object: -0.4305
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0059
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 1.79s
                      Time elapsed: 00:01:19
                               ETA: 02:11:06

################################################################################
                      [1m Learning iteration 20/2000 [0m                      

                       Computation: 55215 steps/s (collection: 1.682s, learning 0.098s)
             Mean action noise std: 1.06
          Mean value_function loss: 1.7428
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 32.4287
                       Mean reward: -2.49
               Mean episode length: 247.35
    Episode_Reward/reaching_object: 0.1853
     Episode_Reward/lifting_object: -0.1992
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0059
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 1.78s
                      Time elapsed: 00:01:21
                               ETA: 02:07:35

################################################################################
                      [1m Learning iteration 21/2000 [0m                      

                       Computation: 54288 steps/s (collection: 1.702s, learning 0.109s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.5350
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 32.5683
                       Mean reward: -0.15
               Mean episode length: 247.60
    Episode_Reward/reaching_object: 0.2112
     Episode_Reward/lifting_object: -0.1477
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0060
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 1.81s
                      Time elapsed: 00:01:23
                               ETA: 02:04:27

################################################################################
                      [1m Learning iteration 22/2000 [0m                      

                       Computation: 54666 steps/s (collection: 1.690s, learning 0.109s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.0589
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 32.6754
                       Mean reward: 0.76
               Mean episode length: 247.73
    Episode_Reward/reaching_object: 0.2041
     Episode_Reward/lifting_object: -0.0420
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0060
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 1.80s
                      Time elapsed: 00:01:24
                               ETA: 02:01:33

################################################################################
                      [1m Learning iteration 23/2000 [0m                      

                       Computation: 55354 steps/s (collection: 1.692s, learning 0.084s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.2314
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 32.8976
                       Mean reward: 1.05
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 0.2225
     Episode_Reward/lifting_object: -0.0446
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0061
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 1.78s
                      Time elapsed: 00:01:26
                               ETA: 01:58:52

################################################################################
                      [1m Learning iteration 24/2000 [0m                      

                       Computation: 54587 steps/s (collection: 1.697s, learning 0.104s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 32.9703
                       Mean reward: 0.94
               Mean episode length: 248.81
    Episode_Reward/reaching_object: 0.2120
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0062
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 1.80s
                      Time elapsed: 00:01:28
                               ETA: 01:56:25

################################################################################
                      [1m Learning iteration 25/2000 [0m                      

                       Computation: 55118 steps/s (collection: 1.674s, learning 0.109s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0010
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 33.1039
                       Mean reward: 0.90
               Mean episode length: 249.34
    Episode_Reward/reaching_object: 0.1999
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0063
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 1.78s
                      Time elapsed: 00:01:30
                               ETA: 01:54:09

################################################################################
                      [1m Learning iteration 26/2000 [0m                      

                       Computation: 54004 steps/s (collection: 1.709s, learning 0.112s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 33.1592
                       Mean reward: 0.88
               Mean episode length: 249.31
    Episode_Reward/reaching_object: 0.1866
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0064
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 1.82s
                      Time elapsed: 00:01:31
                               ETA: 01:52:05

################################################################################
                      [1m Learning iteration 27/2000 [0m                      

                       Computation: 49154 steps/s (collection: 1.844s, learning 0.156s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.2339
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 33.1987
                       Mean reward: 0.36
               Mean episode length: 249.24
    Episode_Reward/reaching_object: 0.1640
     Episode_Reward/lifting_object: -0.0431
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0065
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 2.00s
                      Time elapsed: 00:01:33
                               ETA: 01:50:22

################################################################################
                      [1m Learning iteration 28/2000 [0m                      

                       Computation: 51119 steps/s (collection: 1.785s, learning 0.138s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0009
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 33.2383
                       Mean reward: 0.65
               Mean episode length: 249.72
    Episode_Reward/reaching_object: 0.1475
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0066
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 1.92s
                      Time elapsed: 00:01:35
                               ETA: 01:48:42

################################################################################
                      [1m Learning iteration 29/2000 [0m                      

                       Computation: 51894 steps/s (collection: 1.802s, learning 0.093s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.6796
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 33.3552
                       Mean reward: 0.65
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.1410
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0066
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 1.89s
                      Time elapsed: 00:01:37
                               ETA: 01:47:05

################################################################################
                      [1m Learning iteration 30/2000 [0m                      

                       Computation: 52201 steps/s (collection: 1.748s, learning 0.136s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0009
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 33.4075
                       Mean reward: 0.45
               Mean episode length: 249.25
    Episode_Reward/reaching_object: 0.1299
     Episode_Reward/lifting_object: -0.0566
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0067
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 1.88s
                      Time elapsed: 00:01:39
                               ETA: 01:45:35

################################################################################
                      [1m Learning iteration 31/2000 [0m                      

                       Computation: 50513 steps/s (collection: 1.828s, learning 0.118s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 33.4939
                       Mean reward: 0.66
               Mean episode length: 249.16
    Episode_Reward/reaching_object: 0.1459
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0068
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 1.95s
                      Time elapsed: 00:01:41
                               ETA: 01:44:13

################################################################################
                      [1m Learning iteration 32/2000 [0m                      

                       Computation: 52185 steps/s (collection: 1.739s, learning 0.145s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0204
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 33.5037
                       Mean reward: 0.71
               Mean episode length: 249.57
    Episode_Reward/reaching_object: 0.1382
     Episode_Reward/lifting_object: -0.0079
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 1.88s
                      Time elapsed: 00:01:43
                               ETA: 01:42:53

################################################################################
                      [1m Learning iteration 33/2000 [0m                      

                       Computation: 52689 steps/s (collection: 1.775s, learning 0.091s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 33.5237
                       Mean reward: 0.78
               Mean episode length: 249.71
    Episode_Reward/reaching_object: 0.1543
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0070
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 1.87s
                      Time elapsed: 00:01:45
                               ETA: 01:41:36

################################################################################
                      [1m Learning iteration 34/2000 [0m                      

                       Computation: 51523 steps/s (collection: 1.799s, learning 0.109s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.8535
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 33.6020
                       Mean reward: -0.14
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.1674
     Episode_Reward/lifting_object: -0.0742
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0070
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 1.91s
                      Time elapsed: 00:01:47
                               ETA: 01:40:26

################################################################################
                      [1m Learning iteration 35/2000 [0m                      

                       Computation: 51601 steps/s (collection: 1.771s, learning 0.134s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.2528
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 33.6644
                       Mean reward: 0.92
               Mean episode length: 247.09
    Episode_Reward/reaching_object: 0.1845
     Episode_Reward/lifting_object: -0.0634
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0071
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 1.91s
                      Time elapsed: 00:01:49
                               ETA: 01:39:20

################################################################################
                      [1m Learning iteration 36/2000 [0m                      

                       Computation: 51672 steps/s (collection: 1.771s, learning 0.132s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.1888
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 33.8157
                       Mean reward: 0.61
               Mean episode length: 246.77
    Episode_Reward/reaching_object: 0.2060
     Episode_Reward/lifting_object: -0.0602
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0072
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 1.90s
                      Time elapsed: 00:01:51
                               ETA: 01:38:17

################################################################################
                      [1m Learning iteration 37/2000 [0m                      

                       Computation: 52920 steps/s (collection: 1.753s, learning 0.105s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.7984
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 33.9039
                       Mean reward: 1.00
               Mean episode length: 244.24
    Episode_Reward/reaching_object: 0.2235
     Episode_Reward/lifting_object: -0.0542
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0073
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 1.86s
                      Time elapsed: 00:01:52
                               ETA: 01:37:15

################################################################################
                      [1m Learning iteration 38/2000 [0m                      

                       Computation: 53254 steps/s (collection: 1.758s, learning 0.088s)
             Mean action noise std: 1.14
          Mean value_function loss: 1.0046
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 33.9990
                       Mean reward: 0.68
               Mean episode length: 246.43
    Episode_Reward/reaching_object: 0.2296
     Episode_Reward/lifting_object: -0.1685
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0073
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 1.85s
                      Time elapsed: 00:01:54
                               ETA: 01:36:15

################################################################################
                      [1m Learning iteration 39/2000 [0m                      

                       Computation: 51709 steps/s (collection: 1.757s, learning 0.145s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.8235
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 34.0893
                       Mean reward: 0.65
               Mean episode length: 243.91
    Episode_Reward/reaching_object: 0.2485
     Episode_Reward/lifting_object: -0.1934
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0073
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 1.90s
                      Time elapsed: 00:01:56
                               ETA: 01:35:21

################################################################################
                      [1m Learning iteration 40/2000 [0m                      

                       Computation: 51985 steps/s (collection: 1.793s, learning 0.098s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.2683
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 34.1713
                       Mean reward: 1.09
               Mean episode length: 241.69
    Episode_Reward/reaching_object: 0.2488
     Episode_Reward/lifting_object: -0.1222
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0074
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 1.89s
                      Time elapsed: 00:01:58
                               ETA: 01:34:29

################################################################################
                      [1m Learning iteration 41/2000 [0m                      

                       Computation: 52623 steps/s (collection: 1.759s, learning 0.110s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.6431
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 34.2290
                       Mean reward: 0.42
               Mean episode length: 246.19
    Episode_Reward/reaching_object: 0.2545
     Episode_Reward/lifting_object: -0.0843
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0074
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 1.87s
                      Time elapsed: 00:02:00
                               ETA: 01:33:38

################################################################################
                      [1m Learning iteration 42/2000 [0m                      

                       Computation: 53023 steps/s (collection: 1.737s, learning 0.117s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.1404
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 34.3028
                       Mean reward: 1.34
               Mean episode length: 246.07
    Episode_Reward/reaching_object: 0.2704
     Episode_Reward/lifting_object: -0.0208
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 1.85s
                      Time elapsed: 00:02:02
                               ETA: 01:32:49

################################################################################
                      [1m Learning iteration 43/2000 [0m                      

                       Computation: 51062 steps/s (collection: 1.740s, learning 0.186s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 34.3665
                       Mean reward: 1.30
               Mean episode length: 244.19
    Episode_Reward/reaching_object: 0.2717
     Episode_Reward/lifting_object: -0.0628
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 1.93s
                      Time elapsed: 00:02:04
                               ETA: 01:32:06

################################################################################
                      [1m Learning iteration 44/2000 [0m                      

                       Computation: 52430 steps/s (collection: 1.731s, learning 0.144s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.3670
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 34.4425
                       Mean reward: 0.28
               Mean episode length: 247.89
    Episode_Reward/reaching_object: 0.2692
     Episode_Reward/lifting_object: -0.0731
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 1.87s
                      Time elapsed: 00:02:06
                               ETA: 01:31:21

################################################################################
                      [1m Learning iteration 45/2000 [0m                      

                       Computation: 49972 steps/s (collection: 1.797s, learning 0.171s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.0595
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 34.4852
                       Mean reward: 0.92
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.2455
     Episode_Reward/lifting_object: -0.0207
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 1.97s
                      Time elapsed: 00:02:08
                               ETA: 01:30:43

################################################################################
                      [1m Learning iteration 46/2000 [0m                      

                       Computation: 53396 steps/s (collection: 1.734s, learning 0.107s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0120
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 34.6697
                       Mean reward: 1.17
               Mean episode length: 246.84
    Episode_Reward/reaching_object: 0.2356
     Episode_Reward/lifting_object: -0.0111
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 1.84s
                      Time elapsed: 00:02:09
                               ETA: 01:30:01

################################################################################
                      [1m Learning iteration 47/2000 [0m                      

                       Computation: 52481 steps/s (collection: 1.780s, learning 0.093s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 34.8288
                       Mean reward: 1.08
               Mean episode length: 246.48
    Episode_Reward/reaching_object: 0.2227
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 1.87s
                      Time elapsed: 00:02:11
                               ETA: 01:29:22

################################################################################
                      [1m Learning iteration 48/2000 [0m                      

                       Computation: 52876 steps/s (collection: 1.748s, learning 0.111s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.2791
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 34.8169
                       Mean reward: 1.06
               Mean episode length: 247.44
    Episode_Reward/reaching_object: 0.2149
     Episode_Reward/lifting_object: -0.0076
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 1.86s
                      Time elapsed: 00:02:13
                               ETA: 01:28:44

################################################################################
                      [1m Learning iteration 49/2000 [0m                      

                       Computation: 52877 steps/s (collection: 1.759s, learning 0.100s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.3738
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 34.8464
                       Mean reward: -0.10
               Mean episode length: 244.22
    Episode_Reward/reaching_object: 0.2231
     Episode_Reward/lifting_object: -0.1203
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 1.86s
                      Time elapsed: 00:02:15
                               ETA: 01:28:07

################################################################################
                      [1m Learning iteration 50/2000 [0m                      

                       Computation: 52778 steps/s (collection: 1.759s, learning 0.104s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0060
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 34.9194
                       Mean reward: 1.02
               Mean episode length: 239.28
    Episode_Reward/reaching_object: 0.2222
     Episode_Reward/lifting_object: -0.0128
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 1.86s
                      Time elapsed: 00:02:17
                               ETA: 01:27:32

################################################################################
                      [1m Learning iteration 51/2000 [0m                      

                       Computation: 51751 steps/s (collection: 1.757s, learning 0.142s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.1353
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.0530
                       Mean reward: 0.76
               Mean episode length: 237.51
    Episode_Reward/reaching_object: 0.2238
     Episode_Reward/lifting_object: -0.0185
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 1.90s
                      Time elapsed: 00:02:19
                               ETA: 01:27:00

################################################################################
                      [1m Learning iteration 52/2000 [0m                      

                       Computation: 53787 steps/s (collection: 1.722s, learning 0.106s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.1336
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 35.1034
                       Mean reward: 1.08
               Mean episode length: 236.61
    Episode_Reward/reaching_object: 0.2327
     Episode_Reward/lifting_object: -0.0518
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 1.83s
                      Time elapsed: 00:02:21
                               ETA: 01:26:26

################################################################################
                      [1m Learning iteration 53/2000 [0m                      

                       Computation: 54186 steps/s (collection: 1.719s, learning 0.096s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.7818
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 35.1919
                       Mean reward: 0.56
               Mean episode length: 229.55
    Episode_Reward/reaching_object: 0.2473
     Episode_Reward/lifting_object: -0.1052
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 1.81s
                      Time elapsed: 00:02:22
                               ETA: 01:25:53

################################################################################
                      [1m Learning iteration 54/2000 [0m                      

                       Computation: 53480 steps/s (collection: 1.750s, learning 0.089s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.0585
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.2596
                       Mean reward: 1.21
               Mean episode length: 226.23
    Episode_Reward/reaching_object: 0.2505
     Episode_Reward/lifting_object: -0.0213
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 1.84s
                      Time elapsed: 00:02:24
                               ETA: 01:25:21

################################################################################
                      [1m Learning iteration 55/2000 [0m                      

                       Computation: 49346 steps/s (collection: 1.891s, learning 0.102s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.3672
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 35.3695
                       Mean reward: 1.01
               Mean episode length: 226.87
    Episode_Reward/reaching_object: 0.2565
     Episode_Reward/lifting_object: -0.0846
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 1.99s
                      Time elapsed: 00:02:26
                               ETA: 01:24:57

################################################################################
                      [1m Learning iteration 56/2000 [0m                      

                       Computation: 51204 steps/s (collection: 1.807s, learning 0.113s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.4584
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 35.4262
                       Mean reward: 0.10
               Mean episode length: 226.66
    Episode_Reward/reaching_object: 0.2626
     Episode_Reward/lifting_object: -0.0958
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.1667
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 1.92s
                      Time elapsed: 00:02:28
                               ETA: 01:24:30

################################################################################
                      [1m Learning iteration 57/2000 [0m                      

                       Computation: 51176 steps/s (collection: 1.828s, learning 0.093s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0604
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.5035
                       Mean reward: 0.96
               Mean episode length: 226.55
    Episode_Reward/reaching_object: 0.2721
     Episode_Reward/lifting_object: -0.0237
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 5.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 1.92s
                      Time elapsed: 00:02:30
                               ETA: 01:24:04

################################################################################
                      [1m Learning iteration 58/2000 [0m                      

                       Computation: 52782 steps/s (collection: 1.760s, learning 0.103s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0381
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.6182
                       Mean reward: 1.28
               Mean episode length: 229.35
    Episode_Reward/reaching_object: 0.2638
     Episode_Reward/lifting_object: -0.0184
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 1.86s
                      Time elapsed: 00:02:32
                               ETA: 01:23:38

################################################################################
                      [1m Learning iteration 59/2000 [0m                      

                       Computation: 51149 steps/s (collection: 1.815s, learning 0.107s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 35.7000
                       Mean reward: 1.33
               Mean episode length: 228.10
    Episode_Reward/reaching_object: 0.2684
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0081
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 1.92s
                      Time elapsed: 00:02:34
                               ETA: 01:23:14

################################################################################
                      [1m Learning iteration 60/2000 [0m                      

                       Computation: 51450 steps/s (collection: 1.778s, learning 0.133s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 35.7849
                       Mean reward: 1.26
               Mean episode length: 223.16
    Episode_Reward/reaching_object: 0.2682
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0081
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 1.91s
                      Time elapsed: 00:02:36
                               ETA: 01:22:50

################################################################################
                      [1m Learning iteration 61/2000 [0m                      

                       Computation: 53923 steps/s (collection: 1.737s, learning 0.086s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0265
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.8268
                       Mean reward: 1.22
               Mean episode length: 219.39
    Episode_Reward/reaching_object: 0.2699
     Episode_Reward/lifting_object: -0.0076
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0081
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 1.82s
                      Time elapsed: 00:02:38
                               ETA: 01:22:24

################################################################################
                      [1m Learning iteration 62/2000 [0m                      

                       Computation: 51999 steps/s (collection: 1.806s, learning 0.084s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.1548
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 35.8666
                       Mean reward: 1.14
               Mean episode length: 223.06
    Episode_Reward/reaching_object: 0.2639
     Episode_Reward/lifting_object: -0.0351
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0082
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 1.89s
                      Time elapsed: 00:02:40
                               ETA: 01:22:01

################################################################################
                      [1m Learning iteration 63/2000 [0m                      

                       Computation: 49224 steps/s (collection: 1.908s, learning 0.090s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.4450
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 35.9088
                       Mean reward: 1.32
               Mean episode length: 219.70
    Episode_Reward/reaching_object: 0.2656
     Episode_Reward/lifting_object: -0.0595
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0082
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.5000
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 2.00s
                      Time elapsed: 00:02:41
                               ETA: 01:21:42

################################################################################
                      [1m Learning iteration 64/2000 [0m                      

                       Computation: 50373 steps/s (collection: 1.835s, learning 0.116s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.0533
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.9689
                       Mean reward: 1.03
               Mean episode length: 210.00
    Episode_Reward/reaching_object: 0.2567
     Episode_Reward/lifting_object: -0.0264
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 1.95s
                      Time elapsed: 00:02:43
                               ETA: 01:21:23

################################################################################
                      [1m Learning iteration 65/2000 [0m                      

                       Computation: 48031 steps/s (collection: 1.887s, learning 0.160s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.2753
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 36.0373
                       Mean reward: 0.56
               Mean episode length: 199.46
    Episode_Reward/reaching_object: 0.2514
     Episode_Reward/lifting_object: -0.0706
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.8333
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 2.05s
                      Time elapsed: 00:02:45
                               ETA: 01:21:06

################################################################################
                      [1m Learning iteration 66/2000 [0m                      

                       Computation: 52589 steps/s (collection: 1.784s, learning 0.085s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0609
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.0881
                       Mean reward: 1.04
               Mean episode length: 200.24
    Episode_Reward/reaching_object: 0.2555
     Episode_Reward/lifting_object: -0.0197
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 18.8750
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 1.87s
                      Time elapsed: 00:02:47
                               ETA: 01:20:45

################################################################################
                      [1m Learning iteration 67/2000 [0m                      

                       Computation: 52511 steps/s (collection: 1.770s, learning 0.102s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0318
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 36.2428
                       Mean reward: 1.31
               Mean episode length: 196.81
    Episode_Reward/reaching_object: 0.2687
     Episode_Reward/lifting_object: -0.0185
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.6667
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 1.87s
                      Time elapsed: 00:02:49
                               ETA: 01:20:25

################################################################################
                      [1m Learning iteration 68/2000 [0m                      

                       Computation: 51344 steps/s (collection: 1.823s, learning 0.092s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.4709
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.3427
                       Mean reward: 0.25
               Mean episode length: 192.21
    Episode_Reward/reaching_object: 0.2631
     Episode_Reward/lifting_object: -0.0466
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 4.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.7083
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 1.91s
                      Time elapsed: 00:02:51
                               ETA: 01:20:06

################################################################################
                      [1m Learning iteration 69/2000 [0m                      

                       Computation: 50118 steps/s (collection: 1.872s, learning 0.090s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0527
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 36.3859
                       Mean reward: 1.46
               Mean episode length: 204.06
    Episode_Reward/reaching_object: 0.2780
     Episode_Reward/lifting_object: -0.0327
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 3.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.2500
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 1.96s
                      Time elapsed: 00:02:53
                               ETA: 01:19:49

################################################################################
                      [1m Learning iteration 70/2000 [0m                      

                       Computation: 51081 steps/s (collection: 1.818s, learning 0.107s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.0544
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 36.5294
                       Mean reward: 1.37
               Mean episode length: 197.89
    Episode_Reward/reaching_object: 0.2829
     Episode_Reward/lifting_object: -0.0142
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 3.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.7083
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 1.92s
                      Time elapsed: 00:02:55
                               ETA: 01:19:31

################################################################################
                      [1m Learning iteration 71/2000 [0m                      

                       Computation: 51386 steps/s (collection: 1.812s, learning 0.101s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.2538
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.6733
                       Mean reward: 1.23
               Mean episode length: 201.10
    Episode_Reward/reaching_object: 0.2984
     Episode_Reward/lifting_object: -0.0867
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 1.91s
                      Time elapsed: 00:02:57
                               ETA: 01:19:14

################################################################################
                      [1m Learning iteration 72/2000 [0m                      

                       Computation: 52011 steps/s (collection: 1.756s, learning 0.134s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.0042
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 36.7046
                       Mean reward: 1.71
               Mean episode length: 210.96
    Episode_Reward/reaching_object: 0.3230
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0081
      Episode_Termination/time_out: 3.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 1.89s
                      Time elapsed: 00:02:59
                               ETA: 01:18:56

################################################################################
                      [1m Learning iteration 73/2000 [0m                      

                       Computation: 52899 steps/s (collection: 1.762s, learning 0.097s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.2822
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.7571
                       Mean reward: 0.94
               Mean episode length: 210.92
    Episode_Reward/reaching_object: 0.3235
     Episode_Reward/lifting_object: -0.0744
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0085
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 1.86s
                      Time elapsed: 00:03:01
                               ETA: 01:18:38

################################################################################
                      [1m Learning iteration 74/2000 [0m                      

                       Computation: 52247 steps/s (collection: 1.779s, learning 0.103s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.0457
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.7819
                       Mean reward: 1.64
               Mean episode length: 218.88
    Episode_Reward/reaching_object: 0.3433
     Episode_Reward/lifting_object: -0.0115
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 1.88s
                      Time elapsed: 00:03:03
                               ETA: 01:18:21

################################################################################
                      [1m Learning iteration 75/2000 [0m                      

                       Computation: 50110 steps/s (collection: 1.873s, learning 0.089s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.1459
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.8590
                       Mean reward: 1.09
               Mean episode length: 217.27
    Episode_Reward/reaching_object: 0.3570
     Episode_Reward/lifting_object: -0.0252
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 1.96s
                      Time elapsed: 00:03:05
                               ETA: 01:18:06

################################################################################
                      [1m Learning iteration 76/2000 [0m                      

                       Computation: 48246 steps/s (collection: 1.925s, learning 0.113s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.5904
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 36.8881
                       Mean reward: 0.72
               Mean episode length: 225.31
    Episode_Reward/reaching_object: 0.3569
     Episode_Reward/lifting_object: -0.0788
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 2.04s
                      Time elapsed: 00:03:07
                               ETA: 01:17:54

################################################################################
                      [1m Learning iteration 77/2000 [0m                      

                       Computation: 51165 steps/s (collection: 1.825s, learning 0.096s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.0045
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 36.9184
                       Mean reward: 1.62
               Mean episode length: 221.18
    Episode_Reward/reaching_object: 0.3648
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 1.92s
                      Time elapsed: 00:03:09
                               ETA: 01:17:39

################################################################################
                      [1m Learning iteration 78/2000 [0m                      

                       Computation: 51613 steps/s (collection: 1.812s, learning 0.093s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.0874
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 36.9589
                       Mean reward: 1.75
               Mean episode length: 221.72
    Episode_Reward/reaching_object: 0.3587
     Episode_Reward/lifting_object: -0.0243
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 1.90s
                      Time elapsed: 00:03:10
                               ETA: 01:17:24

################################################################################
                      [1m Learning iteration 79/2000 [0m                      

                       Computation: 49249 steps/s (collection: 1.889s, learning 0.107s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.1927
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.9873
                       Mean reward: 1.84
               Mean episode length: 226.17
    Episode_Reward/reaching_object: 0.3721
     Episode_Reward/lifting_object: -0.0473
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 2.00s
                      Time elapsed: 00:03:12
                               ETA: 01:17:12

################################################################################
                      [1m Learning iteration 80/2000 [0m                      

                       Computation: 52599 steps/s (collection: 1.770s, learning 0.099s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.2391
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 37.0459
                       Mean reward: 1.60
               Mean episode length: 227.96
    Episode_Reward/reaching_object: 0.3875
     Episode_Reward/lifting_object: -0.0635
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 1.87s
                      Time elapsed: 00:03:14
                               ETA: 01:16:56

################################################################################
                      [1m Learning iteration 81/2000 [0m                      

                       Computation: 51996 steps/s (collection: 1.804s, learning 0.086s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.1261
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.1568
                       Mean reward: 1.84
               Mean episode length: 217.37
    Episode_Reward/reaching_object: 0.3822
     Episode_Reward/lifting_object: -0.0209
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 1.89s
                      Time elapsed: 00:03:16
                               ETA: 01:16:42

################################################################################
                      [1m Learning iteration 82/2000 [0m                      

                       Computation: 51226 steps/s (collection: 1.813s, learning 0.106s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.1852
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.2095
                       Mean reward: 1.73
               Mean episode length: 212.61
    Episode_Reward/reaching_object: 0.3925
     Episode_Reward/lifting_object: -0.0184
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 1.92s
                      Time elapsed: 00:03:18
                               ETA: 01:16:28

################################################################################
                      [1m Learning iteration 83/2000 [0m                      

                       Computation: 50931 steps/s (collection: 1.810s, learning 0.121s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.3201
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.2645
                       Mean reward: 1.96
               Mean episode length: 209.35
    Episode_Reward/reaching_object: 0.4024
     Episode_Reward/lifting_object: -0.1013
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.1667
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 1.93s
                      Time elapsed: 00:03:20
                               ETA: 01:16:15

################################################################################
                      [1m Learning iteration 84/2000 [0m                      

                       Computation: 50334 steps/s (collection: 1.821s, learning 0.132s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.2216
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.3127
                       Mean reward: 1.90
               Mean episode length: 217.24
    Episode_Reward/reaching_object: 0.4501
     Episode_Reward/lifting_object: -0.0365
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 1.95s
                      Time elapsed: 00:03:22
                               ETA: 01:16:03

################################################################################
                      [1m Learning iteration 85/2000 [0m                      

                       Computation: 51121 steps/s (collection: 1.800s, learning 0.123s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.0323
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 37.4092
                       Mean reward: 2.27
               Mean episode length: 222.74
    Episode_Reward/reaching_object: 0.4520
     Episode_Reward/lifting_object: -0.0350
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 1.92s
                      Time elapsed: 00:03:24
                               ETA: 01:15:51

################################################################################
                      [1m Learning iteration 86/2000 [0m                      

                       Computation: 51061 steps/s (collection: 1.828s, learning 0.098s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.2140
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 37.4709
                       Mean reward: 2.22
               Mean episode length: 228.06
    Episode_Reward/reaching_object: 0.4726
     Episode_Reward/lifting_object: -0.0164
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 1.93s
                      Time elapsed: 00:03:26
                               ETA: 01:15:38

################################################################################
                      [1m Learning iteration 87/2000 [0m                      

                       Computation: 52479 steps/s (collection: 1.789s, learning 0.084s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0334
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 37.5492
                       Mean reward: 2.20
               Mean episode length: 223.40
    Episode_Reward/reaching_object: 0.4821
     Episode_Reward/lifting_object: -0.0063
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 1.87s
                      Time elapsed: 00:03:28
                               ETA: 01:15:25

################################################################################
                      [1m Learning iteration 88/2000 [0m                      

                       Computation: 49700 steps/s (collection: 1.883s, learning 0.095s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.3107
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 37.6400
                       Mean reward: 1.37
               Mean episode length: 223.45
    Episode_Reward/reaching_object: 0.5155
     Episode_Reward/lifting_object: -0.0663
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 1.98s
                      Time elapsed: 00:03:30
                               ETA: 01:15:14

################################################################################
                      [1m Learning iteration 89/2000 [0m                      

                       Computation: 48628 steps/s (collection: 1.907s, learning 0.114s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.1958
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.6553
                       Mean reward: 2.22
               Mean episode length: 225.57
    Episode_Reward/reaching_object: 0.5372
     Episode_Reward/lifting_object: -0.0177
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 2.02s
                      Time elapsed: 00:03:32
                               ETA: 01:15:05

################################################################################
                      [1m Learning iteration 90/2000 [0m                      

                       Computation: 50869 steps/s (collection: 1.810s, learning 0.122s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.2275
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.6902
                       Mean reward: 2.23
               Mean episode length: 231.64
    Episode_Reward/reaching_object: 0.5357
     Episode_Reward/lifting_object: -0.0108
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 1.93s
                      Time elapsed: 00:03:34
                               ETA: 01:14:54

################################################################################
                      [1m Learning iteration 91/2000 [0m                      

                       Computation: 48530 steps/s (collection: 1.902s, learning 0.124s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.1107
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 37.7083
                       Mean reward: 2.36
               Mean episode length: 224.94
    Episode_Reward/reaching_object: 0.5353
     Episode_Reward/lifting_object: -0.0298
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 2.03s
                      Time elapsed: 00:03:36
                               ETA: 01:14:44

################################################################################
                      [1m Learning iteration 92/2000 [0m                      

                       Computation: 50023 steps/s (collection: 1.877s, learning 0.089s)
             Mean action noise std: 1.35
          Mean value_function loss: 1.0638
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.7675
                       Mean reward: 2.59
               Mean episode length: 226.82
    Episode_Reward/reaching_object: 0.5517
     Episode_Reward/lifting_object: -0.0452
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 1.97s
                      Time elapsed: 00:03:38
                               ETA: 01:14:34

################################################################################
                      [1m Learning iteration 93/2000 [0m                      

                       Computation: 51270 steps/s (collection: 1.811s, learning 0.106s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.3353
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.7934
                       Mean reward: 2.85
               Mean episode length: 234.14
    Episode_Reward/reaching_object: 0.5722
     Episode_Reward/lifting_object: -0.0985
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 1.92s
                      Time elapsed: 00:03:40
                               ETA: 01:14:23

################################################################################
                      [1m Learning iteration 94/2000 [0m                      

                       Computation: 48413 steps/s (collection: 1.909s, learning 0.121s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.5414
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.8471
                       Mean reward: 2.31
               Mean episode length: 231.81
    Episode_Reward/reaching_object: 0.5786
     Episode_Reward/lifting_object: -0.0674
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 2.03s
                      Time elapsed: 00:03:42
                               ETA: 01:14:15

################################################################################
                      [1m Learning iteration 95/2000 [0m                      

                       Computation: 48529 steps/s (collection: 1.908s, learning 0.118s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.1655
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.8678
                       Mean reward: 3.06
               Mean episode length: 237.60
    Episode_Reward/reaching_object: 0.6052
     Episode_Reward/lifting_object: -0.0184
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 2.03s
                      Time elapsed: 00:03:44
                               ETA: 01:14:06

################################################################################
                      [1m Learning iteration 96/2000 [0m                      

                       Computation: 51813 steps/s (collection: 1.795s, learning 0.102s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0135
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 37.9238
                       Mean reward: 3.05
               Mean episode length: 234.85
    Episode_Reward/reaching_object: 0.6042
     Episode_Reward/lifting_object: -0.0283
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 1.90s
                      Time elapsed: 00:03:45
                               ETA: 01:13:55

################################################################################
                      [1m Learning iteration 97/2000 [0m                      

                       Computation: 51183 steps/s (collection: 1.812s, learning 0.109s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0081
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 37.9715
                       Mean reward: 3.23
               Mean episode length: 237.22
    Episode_Reward/reaching_object: 0.6208
     Episode_Reward/lifting_object: 0.0056
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 1.92s
                      Time elapsed: 00:03:47
                               ETA: 01:13:45

################################################################################
                      [1m Learning iteration 98/2000 [0m                      

                       Computation: 51076 steps/s (collection: 1.828s, learning 0.096s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.2326
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.9931
                       Mean reward: 3.03
               Mean episode length: 228.21
    Episode_Reward/reaching_object: 0.6234
     Episode_Reward/lifting_object: -0.1462
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 1.92s
                      Time elapsed: 00:03:49
                               ETA: 01:13:35

################################################################################
                      [1m Learning iteration 99/2000 [0m                      

                       Computation: 50215 steps/s (collection: 1.861s, learning 0.097s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.7133
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.0093
                       Mean reward: 1.73
               Mean episode length: 228.36
    Episode_Reward/reaching_object: 0.6187
     Episode_Reward/lifting_object: -0.0741
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 1.96s
                      Time elapsed: 00:03:51
                               ETA: 01:13:26

################################################################################
                     [1m Learning iteration 100/2000 [0m                      

                       Computation: 51012 steps/s (collection: 1.835s, learning 0.093s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.2854
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.0302
                       Mean reward: 3.12
               Mean episode length: 226.06
    Episode_Reward/reaching_object: 0.6173
     Episode_Reward/lifting_object: -0.0751
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 1.93s
                      Time elapsed: 00:03:53
                               ETA: 01:13:16

################################################################################
                     [1m Learning iteration 101/2000 [0m                      

                       Computation: 49596 steps/s (collection: 1.868s, learning 0.114s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.3216
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.0909
                       Mean reward: 2.96
               Mean episode length: 225.01
    Episode_Reward/reaching_object: 0.6152
     Episode_Reward/lifting_object: -0.0536
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 1.98s
                      Time elapsed: 00:03:55
                               ETA: 01:13:08

################################################################################
                     [1m Learning iteration 102/2000 [0m                      

                       Computation: 50876 steps/s (collection: 1.799s, learning 0.134s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.0547
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 38.1688
                       Mean reward: 3.10
               Mean episode length: 225.01
    Episode_Reward/reaching_object: 0.6125
     Episode_Reward/lifting_object: -0.0046
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 1.93s
                      Time elapsed: 00:03:57
                               ETA: 01:12:58

################################################################################
                     [1m Learning iteration 103/2000 [0m                      

                       Computation: 49831 steps/s (collection: 1.820s, learning 0.153s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.1956
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.2572
                       Mean reward: 3.21
               Mean episode length: 227.26
    Episode_Reward/reaching_object: 0.6142
     Episode_Reward/lifting_object: -0.0401
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 1.97s
                      Time elapsed: 00:03:59
                               ETA: 01:12:50

################################################################################
                     [1m Learning iteration 104/2000 [0m                      

                       Computation: 51410 steps/s (collection: 1.799s, learning 0.113s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.0699
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.3313
                       Mean reward: 2.76
               Mean episode length: 233.67
    Episode_Reward/reaching_object: 0.6477
     Episode_Reward/lifting_object: -0.0369
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 1.91s
                      Time elapsed: 00:04:01
                               ETA: 01:12:40

################################################################################
                     [1m Learning iteration 105/2000 [0m                      

                       Computation: 50211 steps/s (collection: 1.857s, learning 0.101s)
             Mean action noise std: 1.39
          Mean value_function loss: 1.2912
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.3796
                       Mean reward: 3.23
               Mean episode length: 222.75
    Episode_Reward/reaching_object: 0.6471
     Episode_Reward/lifting_object: -0.0074
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 1.96s
                      Time elapsed: 00:04:03
                               ETA: 01:12:32

################################################################################
                     [1m Learning iteration 106/2000 [0m                      

                       Computation: 50385 steps/s (collection: 1.847s, learning 0.104s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.6583
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.3944
                       Mean reward: 2.99
               Mean episode length: 228.41
    Episode_Reward/reaching_object: 0.6369
     Episode_Reward/lifting_object: -0.0598
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 1.95s
                      Time elapsed: 00:04:05
                               ETA: 01:12:24

################################################################################
                     [1m Learning iteration 107/2000 [0m                      

                       Computation: 50179 steps/s (collection: 1.835s, learning 0.124s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.5843
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.4253
                       Mean reward: 2.63
               Mean episode length: 231.79
    Episode_Reward/reaching_object: 0.6757
     Episode_Reward/lifting_object: -0.0833
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 1.96s
                      Time elapsed: 00:04:07
                               ETA: 01:12:15

################################################################################
                     [1m Learning iteration 108/2000 [0m                      

                       Computation: 51217 steps/s (collection: 1.820s, learning 0.100s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.0609
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 38.4466
                       Mean reward: 3.40
               Mean episode length: 233.84
    Episode_Reward/reaching_object: 0.6921
     Episode_Reward/lifting_object: -0.0212
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0122
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 1.92s
                      Time elapsed: 00:04:09
                               ETA: 01:12:07

################################################################################
                     [1m Learning iteration 109/2000 [0m                      

                       Computation: 49292 steps/s (collection: 1.894s, learning 0.101s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.2842
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.4874
                       Mean reward: 3.76
               Mean episode length: 242.34
    Episode_Reward/reaching_object: 0.7347
     Episode_Reward/lifting_object: -0.0224
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 1.99s
                      Time elapsed: 00:04:11
                               ETA: 01:11:59

################################################################################
                     [1m Learning iteration 110/2000 [0m                      

                       Computation: 52105 steps/s (collection: 1.799s, learning 0.088s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0909
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 38.5273
                       Mean reward: 2.70
               Mean episode length: 238.09
    Episode_Reward/reaching_object: 0.7253
     Episode_Reward/lifting_object: -0.0350
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0128
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 1.89s
                      Time elapsed: 00:04:13
                               ETA: 01:11:50

################################################################################
                     [1m Learning iteration 111/2000 [0m                      

                       Computation: 49989 steps/s (collection: 1.839s, learning 0.128s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.2504
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 38.5934
                       Mean reward: 2.95
               Mean episode length: 237.27
    Episode_Reward/reaching_object: 0.7137
     Episode_Reward/lifting_object: -0.0769
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 1.97s
                      Time elapsed: 00:04:15
                               ETA: 01:11:43

################################################################################
                     [1m Learning iteration 112/2000 [0m                      

                       Computation: 49306 steps/s (collection: 1.889s, learning 0.105s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.1219
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.6146
                       Mean reward: 3.04
               Mean episode length: 235.37
    Episode_Reward/reaching_object: 0.6974
     Episode_Reward/lifting_object: -0.0759
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 1.99s
                      Time elapsed: 00:04:17
                               ETA: 01:11:36

################################################################################
                     [1m Learning iteration 113/2000 [0m                      

                       Computation: 51480 steps/s (collection: 1.804s, learning 0.106s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.3956
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 38.6513
                       Mean reward: 3.77
               Mean episode length: 237.24
    Episode_Reward/reaching_object: 0.7065
     Episode_Reward/lifting_object: -0.0511
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 1.91s
                      Time elapsed: 00:04:19
                               ETA: 01:11:27

################################################################################
                     [1m Learning iteration 114/2000 [0m                      

                       Computation: 50885 steps/s (collection: 1.831s, learning 0.101s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.9460
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.6662
                       Mean reward: 2.39
               Mean episode length: 229.25
    Episode_Reward/reaching_object: 0.7101
     Episode_Reward/lifting_object: -0.0600
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 1.93s
                      Time elapsed: 00:04:20
                               ETA: 01:11:20

################################################################################
                     [1m Learning iteration 115/2000 [0m                      

                       Computation: 50647 steps/s (collection: 1.837s, learning 0.104s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.0534
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.6874
                       Mean reward: 3.50
               Mean episode length: 234.35
    Episode_Reward/reaching_object: 0.6955
     Episode_Reward/lifting_object: 0.0101
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 1.94s
                      Time elapsed: 00:04:22
                               ETA: 01:11:12

################################################################################
                     [1m Learning iteration 116/2000 [0m                      

                       Computation: 51635 steps/s (collection: 1.814s, learning 0.090s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.1931
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.7362
                       Mean reward: 3.11
               Mean episode length: 227.93
    Episode_Reward/reaching_object: 0.6981
     Episode_Reward/lifting_object: -0.0381
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 1.90s
                      Time elapsed: 00:04:24
                               ETA: 01:11:04

################################################################################
                     [1m Learning iteration 117/2000 [0m                      

                       Computation: 51354 steps/s (collection: 1.825s, learning 0.090s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.7881
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 38.7732
                       Mean reward: 3.46
               Mean episode length: 224.21
    Episode_Reward/reaching_object: 0.7004
     Episode_Reward/lifting_object: 0.0039
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 1.91s
                      Time elapsed: 00:04:26
                               ETA: 01:10:56

################################################################################
                     [1m Learning iteration 118/2000 [0m                      

                       Computation: 50803 steps/s (collection: 1.818s, learning 0.117s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.1364
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.7902
                       Mean reward: 3.43
               Mean episode length: 230.39
    Episode_Reward/reaching_object: 0.6923
     Episode_Reward/lifting_object: -0.0944
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 1.93s
                      Time elapsed: 00:04:28
                               ETA: 01:10:49

################################################################################
                     [1m Learning iteration 119/2000 [0m                      

                       Computation: 50141 steps/s (collection: 1.810s, learning 0.151s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0475
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.8390
                       Mean reward: 3.29
               Mean episode length: 225.38
    Episode_Reward/reaching_object: 0.6923
     Episode_Reward/lifting_object: -0.0047
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0128
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 1.96s
                      Time elapsed: 00:04:30
                               ETA: 01:10:42

################################################################################
                     [1m Learning iteration 120/2000 [0m                      

                       Computation: 48711 steps/s (collection: 1.909s, learning 0.110s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.4768
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.9037
                       Mean reward: 2.48
               Mean episode length: 229.31
    Episode_Reward/reaching_object: 0.7301
     Episode_Reward/lifting_object: -0.0747
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0131
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 2.02s
                      Time elapsed: 00:04:32
                               ETA: 01:10:36

################################################################################
                     [1m Learning iteration 121/2000 [0m                      

                       Computation: 49667 steps/s (collection: 1.844s, learning 0.135s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0379
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.9204
                       Mean reward: 3.44
               Mean episode length: 230.03
    Episode_Reward/reaching_object: 0.6952
     Episode_Reward/lifting_object: 0.0123
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 1.98s
                      Time elapsed: 00:04:34
                               ETA: 01:10:29

################################################################################
                     [1m Learning iteration 122/2000 [0m                      

                       Computation: 48962 steps/s (collection: 1.833s, learning 0.175s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.1431
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.9470
                       Mean reward: 3.07
               Mean episode length: 230.53
    Episode_Reward/reaching_object: 0.7300
     Episode_Reward/lifting_object: -0.0032
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 2.01s
                      Time elapsed: 00:04:36
                               ETA: 01:10:23

################################################################################
                     [1m Learning iteration 123/2000 [0m                      

                       Computation: 49283 steps/s (collection: 1.829s, learning 0.166s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.4412
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 38.9659
                       Mean reward: 3.31
               Mean episode length: 229.64
    Episode_Reward/reaching_object: 0.7182
     Episode_Reward/lifting_object: -0.0186
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0132
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 1.99s
                      Time elapsed: 00:04:38
                               ETA: 01:10:17

################################################################################
                     [1m Learning iteration 124/2000 [0m                      

                       Computation: 49570 steps/s (collection: 1.858s, learning 0.125s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.1745
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 39.0205
                       Mean reward: 3.33
               Mean episode length: 225.34
    Episode_Reward/reaching_object: 0.7347
     Episode_Reward/lifting_object: -0.0597
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0133
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 1.98s
                      Time elapsed: 00:04:40
                               ETA: 01:10:11

################################################################################
                     [1m Learning iteration 125/2000 [0m                      

                       Computation: 49709 steps/s (collection: 1.841s, learning 0.137s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.1228
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 39.0896
                       Mean reward: 3.07
               Mean episode length: 222.15
    Episode_Reward/reaching_object: 0.6897
     Episode_Reward/lifting_object: 0.0123
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 1.98s
                      Time elapsed: 00:04:42
                               ETA: 01:10:05

################################################################################
                     [1m Learning iteration 126/2000 [0m                      

                       Computation: 49991 steps/s (collection: 1.842s, learning 0.124s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.1060
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 39.1081
                       Mean reward: 3.32
               Mean episode length: 211.52
    Episode_Reward/reaching_object: 0.7052
     Episode_Reward/lifting_object: -0.0148
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 1.97s
                      Time elapsed: 00:04:44
                               ETA: 01:09:58

################################################################################
                     [1m Learning iteration 127/2000 [0m                      

                       Computation: 50645 steps/s (collection: 1.847s, learning 0.094s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.1533
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.1259
                       Mean reward: 3.69
               Mean episode length: 228.13
    Episode_Reward/reaching_object: 0.6829
     Episode_Reward/lifting_object: -0.0073
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 1.94s
                      Time elapsed: 00:04:46
                               ETA: 01:09:52

################################################################################
                     [1m Learning iteration 128/2000 [0m                      

                       Computation: 50270 steps/s (collection: 1.869s, learning 0.086s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.4672
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.1472
                       Mean reward: 2.49
               Mean episode length: 217.19
    Episode_Reward/reaching_object: 0.7252
     Episode_Reward/lifting_object: -0.0319
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0132
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 1.96s
                      Time elapsed: 00:04:48
                               ETA: 01:09:45

################################################################################
                     [1m Learning iteration 129/2000 [0m                      

                       Computation: 50511 steps/s (collection: 1.857s, learning 0.090s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.3874
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 39.1594
                       Mean reward: 3.89
               Mean episode length: 228.30
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: -0.0751
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 1.95s
                      Time elapsed: 00:04:50
                               ETA: 01:09:39

################################################################################
                     [1m Learning iteration 130/2000 [0m                      

                       Computation: 50889 steps/s (collection: 1.846s, learning 0.086s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.0888
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.2011
                       Mean reward: 3.66
               Mean episode length: 229.32
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: -0.0179
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 1.93s
                      Time elapsed: 00:04:52
                               ETA: 01:09:33

################################################################################
                     [1m Learning iteration 131/2000 [0m                      

                       Computation: 51422 steps/s (collection: 1.818s, learning 0.094s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.0598
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.2725
                       Mean reward: 3.64
               Mean episode length: 233.40
    Episode_Reward/reaching_object: 0.7844
     Episode_Reward/lifting_object: 0.0017
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 1.91s
                      Time elapsed: 00:04:54
                               ETA: 01:09:26

################################################################################
                     [1m Learning iteration 132/2000 [0m                      

                       Computation: 48367 steps/s (collection: 1.918s, learning 0.114s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.4499
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.3122
                       Mean reward: 3.26
               Mean episode length: 232.67
    Episode_Reward/reaching_object: 0.8063
     Episode_Reward/lifting_object: -0.0936
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 2.03s
                      Time elapsed: 00:04:56
                               ETA: 01:09:21

################################################################################
                     [1m Learning iteration 133/2000 [0m                      

                       Computation: 46751 steps/s (collection: 1.965s, learning 0.138s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.1162
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.3267
                       Mean reward: 3.99
               Mean episode length: 235.08
    Episode_Reward/reaching_object: 0.7992
     Episode_Reward/lifting_object: 0.0381
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 2.10s
                      Time elapsed: 00:04:58
                               ETA: 01:09:17

################################################################################
                     [1m Learning iteration 134/2000 [0m                      

                       Computation: 44385 steps/s (collection: 2.089s, learning 0.126s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.7249
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.3635
                       Mean reward: 3.97
               Mean episode length: 235.36
    Episode_Reward/reaching_object: 0.8162
     Episode_Reward/lifting_object: -0.0031
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 2.21s
                      Time elapsed: 00:05:00
                               ETA: 01:09:14

################################################################################
                     [1m Learning iteration 135/2000 [0m                      

                       Computation: 49577 steps/s (collection: 1.863s, learning 0.120s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.2505
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.3776
                       Mean reward: 3.69
               Mean episode length: 231.58
    Episode_Reward/reaching_object: 0.7797
     Episode_Reward/lifting_object: -0.0020
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 1.98s
                      Time elapsed: 00:05:02
                               ETA: 01:09:09

################################################################################
                     [1m Learning iteration 136/2000 [0m                      

                       Computation: 49769 steps/s (collection: 1.883s, learning 0.093s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.1301
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.3969
                       Mean reward: 3.72
               Mean episode length: 241.04
    Episode_Reward/reaching_object: 0.7923
     Episode_Reward/lifting_object: -0.0151
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 1.98s
                      Time elapsed: 00:05:04
                               ETA: 01:09:03

################################################################################
                     [1m Learning iteration 137/2000 [0m                      

                       Computation: 49959 steps/s (collection: 1.879s, learning 0.089s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.2253
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 39.4300
                       Mean reward: 3.14
               Mean episode length: 233.94
    Episode_Reward/reaching_object: 0.7799
     Episode_Reward/lifting_object: -0.0255
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 1.97s
                      Time elapsed: 00:05:06
                               ETA: 01:08:58

################################################################################
                     [1m Learning iteration 138/2000 [0m                      

                       Computation: 50015 steps/s (collection: 1.875s, learning 0.091s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.1876
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 39.4602
                       Mean reward: 3.97
               Mean episode length: 229.84
    Episode_Reward/reaching_object: 0.8086
     Episode_Reward/lifting_object: -0.0379
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 1.97s
                      Time elapsed: 00:05:08
                               ETA: 01:08:52

################################################################################
                     [1m Learning iteration 139/2000 [0m                      

                       Computation: 49386 steps/s (collection: 1.906s, learning 0.085s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.3312
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.5001
                       Mean reward: 3.48
               Mean episode length: 230.36
    Episode_Reward/reaching_object: 0.7774
     Episode_Reward/lifting_object: -0.0115
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 1.99s
                      Time elapsed: 00:05:10
                               ETA: 01:08:47

################################################################################
                     [1m Learning iteration 140/2000 [0m                      

                       Computation: 50712 steps/s (collection: 1.834s, learning 0.104s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.0420
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.5106
                       Mean reward: 4.09
               Mean episode length: 239.15
    Episode_Reward/reaching_object: 0.8140
     Episode_Reward/lifting_object: 0.0151
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 1.94s
                      Time elapsed: 00:05:12
                               ETA: 01:08:41

################################################################################
                     [1m Learning iteration 141/2000 [0m                      

                       Computation: 48633 steps/s (collection: 1.912s, learning 0.110s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.4428
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.5315
                       Mean reward: 2.28
               Mean episode length: 239.46
    Episode_Reward/reaching_object: 0.8314
     Episode_Reward/lifting_object: -0.1051
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 2.02s
                      Time elapsed: 00:05:14
                               ETA: 01:08:36

################################################################################
                     [1m Learning iteration 142/2000 [0m                      

                       Computation: 48877 steps/s (collection: 1.915s, learning 0.096s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.0408
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 39.5502
                       Mean reward: 4.01
               Mean episode length: 238.48
    Episode_Reward/reaching_object: 0.8187
     Episode_Reward/lifting_object: -0.0151
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 2.01s
                      Time elapsed: 00:05:16
                               ETA: 01:08:31

################################################################################
                     [1m Learning iteration 143/2000 [0m                      

                       Computation: 49759 steps/s (collection: 1.887s, learning 0.089s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.1468
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.5865
                       Mean reward: 4.47
               Mean episode length: 245.52
    Episode_Reward/reaching_object: 0.8229
     Episode_Reward/lifting_object: 0.0373
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 1.98s
                      Time elapsed: 00:05:18
                               ETA: 01:08:26

################################################################################
                     [1m Learning iteration 144/2000 [0m                      

                       Computation: 50222 steps/s (collection: 1.861s, learning 0.096s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.1112
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 39.6137
                       Mean reward: 4.10
               Mean episode length: 240.15
    Episode_Reward/reaching_object: 0.8261
     Episode_Reward/lifting_object: -0.0367
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 1.96s
                      Time elapsed: 00:05:20
                               ETA: 01:08:20

################################################################################
                     [1m Learning iteration 145/2000 [0m                      

                       Computation: 49511 steps/s (collection: 1.882s, learning 0.103s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.2732
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.6529
                       Mean reward: 3.84
               Mean episode length: 231.51
    Episode_Reward/reaching_object: 0.8179
     Episode_Reward/lifting_object: 0.0009
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 1.99s
                      Time elapsed: 00:05:22
                               ETA: 01:08:15

################################################################################
                     [1m Learning iteration 146/2000 [0m                      

                       Computation: 49393 steps/s (collection: 1.903s, learning 0.087s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.3136
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.6675
                       Mean reward: 3.60
               Mean episode length: 236.72
    Episode_Reward/reaching_object: 0.8022
     Episode_Reward/lifting_object: -0.0087
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 1.99s
                      Time elapsed: 00:05:24
                               ETA: 01:08:10

################################################################################
                     [1m Learning iteration 147/2000 [0m                      

                       Computation: 50386 steps/s (collection: 1.864s, learning 0.087s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.0334
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 39.6974
                       Mean reward: 4.11
               Mean episode length: 228.61
    Episode_Reward/reaching_object: 0.8085
     Episode_Reward/lifting_object: -0.0117
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 18.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 1.95s
                      Time elapsed: 00:05:26
                               ETA: 01:08:05

################################################################################
                     [1m Learning iteration 148/2000 [0m                      

                       Computation: 50118 steps/s (collection: 1.868s, learning 0.094s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.0729
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 39.7556
                       Mean reward: 3.08
               Mean episode length: 234.17
    Episode_Reward/reaching_object: 0.8157
     Episode_Reward/lifting_object: -0.0460
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 1.96s
                      Time elapsed: 00:05:28
                               ETA: 01:08:00

################################################################################
                     [1m Learning iteration 149/2000 [0m                      

                       Computation: 50348 steps/s (collection: 1.865s, learning 0.088s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.3359
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 39.8106
                       Mean reward: 3.79
               Mean episode length: 223.07
    Episode_Reward/reaching_object: 0.7827
     Episode_Reward/lifting_object: -0.0236
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 1.95s
                      Time elapsed: 00:05:30
                               ETA: 01:07:54

################################################################################
                     [1m Learning iteration 150/2000 [0m                      

                       Computation: 47880 steps/s (collection: 1.965s, learning 0.089s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.1135
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 39.8392
                       Mean reward: 3.53
               Mean episode length: 226.62
    Episode_Reward/reaching_object: 0.7716
     Episode_Reward/lifting_object: -0.0199
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 2.05s
                      Time elapsed: 00:05:32
                               ETA: 01:07:50

################################################################################
                     [1m Learning iteration 151/2000 [0m                      

                       Computation: 48452 steps/s (collection: 1.944s, learning 0.085s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.2529
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 39.8872
                       Mean reward: 3.60
               Mean episode length: 231.73
    Episode_Reward/reaching_object: 0.7997
     Episode_Reward/lifting_object: -0.0278
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 2.03s
                      Time elapsed: 00:05:34
                               ETA: 01:07:46

################################################################################
                     [1m Learning iteration 152/2000 [0m                      

                       Computation: 50280 steps/s (collection: 1.866s, learning 0.089s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.1877
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 39.9532
                       Mean reward: 3.77
               Mean episode length: 226.72
    Episode_Reward/reaching_object: 0.7823
     Episode_Reward/lifting_object: 0.0029
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 1.96s
                      Time elapsed: 00:05:36
                               ETA: 01:07:41

################################################################################
                     [1m Learning iteration 153/2000 [0m                      

                       Computation: 49921 steps/s (collection: 1.884s, learning 0.086s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.2461
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 39.9995
                       Mean reward: 3.72
               Mean episode length: 240.58
    Episode_Reward/reaching_object: 0.8216
     Episode_Reward/lifting_object: -0.0136
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 1.97s
                      Time elapsed: 00:05:38
                               ETA: 01:07:36

################################################################################
                     [1m Learning iteration 154/2000 [0m                      

                       Computation: 49417 steps/s (collection: 1.874s, learning 0.115s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.1976
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 40.0447
                       Mean reward: 4.04
               Mean episode length: 228.49
    Episode_Reward/reaching_object: 0.8222
     Episode_Reward/lifting_object: -0.0254
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 1.99s
                      Time elapsed: 00:05:40
                               ETA: 01:07:31

################################################################################
                     [1m Learning iteration 155/2000 [0m                      

                       Computation: 49958 steps/s (collection: 1.857s, learning 0.111s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.8063
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 40.1005
                       Mean reward: 4.10
               Mean episode length: 239.61
    Episode_Reward/reaching_object: 0.8178
     Episode_Reward/lifting_object: 0.0280
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 1.97s
                      Time elapsed: 00:05:42
                               ETA: 01:07:26

################################################################################
                     [1m Learning iteration 156/2000 [0m                      

                       Computation: 49097 steps/s (collection: 1.898s, learning 0.104s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.1912
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 40.1139
                       Mean reward: 4.21
               Mean episode length: 235.93
    Episode_Reward/reaching_object: 0.7825
     Episode_Reward/lifting_object: -0.0330
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 2.00s
                      Time elapsed: 00:05:44
                               ETA: 01:07:22

################################################################################
                     [1m Learning iteration 157/2000 [0m                      

                       Computation: 47253 steps/s (collection: 1.991s, learning 0.090s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.1467
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 40.1553
                       Mean reward: 4.08
               Mean episode length: 232.01
    Episode_Reward/reaching_object: 0.8100
     Episode_Reward/lifting_object: 0.0527
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 2.08s
                      Time elapsed: 00:05:46
                               ETA: 01:07:19

################################################################################
                     [1m Learning iteration 158/2000 [0m                      

                       Computation: 49925 steps/s (collection: 1.864s, learning 0.105s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.0837
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.2217
                       Mean reward: 3.81
               Mean episode length: 234.39
    Episode_Reward/reaching_object: 0.8241
     Episode_Reward/lifting_object: -0.0022
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 1.97s
                      Time elapsed: 00:05:48
                               ETA: 01:07:14

################################################################################
                     [1m Learning iteration 159/2000 [0m                      

                       Computation: 49685 steps/s (collection: 1.873s, learning 0.105s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.2099
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 40.2444
                       Mean reward: 4.39
               Mean episode length: 232.14
    Episode_Reward/reaching_object: 0.8018
     Episode_Reward/lifting_object: 0.0364
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 1.98s
                      Time elapsed: 00:05:50
                               ETA: 01:07:09

################################################################################
                     [1m Learning iteration 160/2000 [0m                      

                       Computation: 50038 steps/s (collection: 1.844s, learning 0.121s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.3290
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 40.2866
                       Mean reward: 3.99
               Mean episode length: 223.07
    Episode_Reward/reaching_object: 0.7842
     Episode_Reward/lifting_object: -0.0632
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 1.96s
                      Time elapsed: 00:05:52
                               ETA: 01:07:04

################################################################################
                     [1m Learning iteration 161/2000 [0m                      

                       Computation: 51197 steps/s (collection: 1.816s, learning 0.104s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.3000
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 40.3319
                       Mean reward: 3.83
               Mean episode length: 220.59
    Episode_Reward/reaching_object: 0.7751
     Episode_Reward/lifting_object: 0.0177
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 1.92s
                      Time elapsed: 00:05:54
                               ETA: 01:06:59

################################################################################
                     [1m Learning iteration 162/2000 [0m                      

                       Computation: 49299 steps/s (collection: 1.886s, learning 0.108s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.1384
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 40.3550
                       Mean reward: 3.99
               Mean episode length: 221.42
    Episode_Reward/reaching_object: 0.7945
     Episode_Reward/lifting_object: 0.0376
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 1.99s
                      Time elapsed: 00:05:56
                               ETA: 01:06:55

################################################################################
                     [1m Learning iteration 163/2000 [0m                      

                       Computation: 49410 steps/s (collection: 1.871s, learning 0.119s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.1277
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 40.4013
                       Mean reward: 3.18
               Mean episode length: 220.61
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: -0.0203
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 1.99s
                      Time elapsed: 00:05:58
                               ETA: 01:06:50

################################################################################
                     [1m Learning iteration 164/2000 [0m                      

                       Computation: 50003 steps/s (collection: 1.844s, learning 0.122s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.0779
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.4463
                       Mean reward: 4.14
               Mean episode length: 218.85
    Episode_Reward/reaching_object: 0.7896
     Episode_Reward/lifting_object: 0.0373
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 1.97s
                      Time elapsed: 00:06:00
                               ETA: 01:06:46

################################################################################
                     [1m Learning iteration 165/2000 [0m                      

                       Computation: 50344 steps/s (collection: 1.835s, learning 0.118s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.1895
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.4807
                       Mean reward: 3.57
               Mean episode length: 222.00
    Episode_Reward/reaching_object: 0.7989
     Episode_Reward/lifting_object: 0.0247
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 1.95s
                      Time elapsed: 00:06:02
                               ETA: 01:06:41

################################################################################
                     [1m Learning iteration 166/2000 [0m                      

                       Computation: 50351 steps/s (collection: 1.844s, learning 0.109s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.2619
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.4970
                       Mean reward: 4.27
               Mean episode length: 233.51
    Episode_Reward/reaching_object: 0.8187
     Episode_Reward/lifting_object: 0.0172
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 1.95s
                      Time elapsed: 00:06:03
                               ETA: 01:06:36

################################################################################
                     [1m Learning iteration 167/2000 [0m                      

                       Computation: 49573 steps/s (collection: 1.875s, learning 0.108s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.5809
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.5124
                       Mean reward: 2.28
               Mean episode length: 220.73
    Episode_Reward/reaching_object: 0.8206
     Episode_Reward/lifting_object: -0.0395
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 1.98s
                      Time elapsed: 00:06:05
                               ETA: 01:06:32

################################################################################
                     [1m Learning iteration 168/2000 [0m                      

                       Computation: 50658 steps/s (collection: 1.822s, learning 0.118s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.0767
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 40.5373
                       Mean reward: 4.22
               Mean episode length: 231.46
    Episode_Reward/reaching_object: 0.8390
     Episode_Reward/lifting_object: -0.0072
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 1.94s
                      Time elapsed: 00:06:07
                               ETA: 01:06:27

################################################################################
                     [1m Learning iteration 169/2000 [0m                      

                       Computation: 51226 steps/s (collection: 1.818s, learning 0.101s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.3682
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 40.5774
                       Mean reward: 4.06
               Mean episode length: 226.63
    Episode_Reward/reaching_object: 0.8349
     Episode_Reward/lifting_object: -0.0147
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 1.92s
                      Time elapsed: 00:06:09
                               ETA: 01:06:22

################################################################################
                     [1m Learning iteration 170/2000 [0m                      

                       Computation: 49937 steps/s (collection: 1.868s, learning 0.101s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.1374
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 40.5987
                       Mean reward: 4.97
               Mean episode length: 236.84
    Episode_Reward/reaching_object: 0.8582
     Episode_Reward/lifting_object: 0.0651
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 1.97s
                      Time elapsed: 00:06:11
                               ETA: 01:06:18

################################################################################
                     [1m Learning iteration 171/2000 [0m                      

                       Computation: 50064 steps/s (collection: 1.845s, learning 0.118s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.1627
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 40.6435
                       Mean reward: 4.69
               Mean episode length: 229.72
    Episode_Reward/reaching_object: 0.8438
     Episode_Reward/lifting_object: 0.0510
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 1.96s
                      Time elapsed: 00:06:13
                               ETA: 01:06:14

################################################################################
                     [1m Learning iteration 172/2000 [0m                      

                       Computation: 49948 steps/s (collection: 1.848s, learning 0.121s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.1981
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 40.6837
                       Mean reward: 4.32
               Mean episode length: 231.03
    Episode_Reward/reaching_object: 0.8137
     Episode_Reward/lifting_object: 0.0319
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 1.97s
                      Time elapsed: 00:06:15
                               ETA: 01:06:09

################################################################################
                     [1m Learning iteration 173/2000 [0m                      

                       Computation: 49297 steps/s (collection: 1.879s, learning 0.115s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.3917
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 40.7160
                       Mean reward: 4.19
               Mean episode length: 230.73
    Episode_Reward/reaching_object: 0.8438
     Episode_Reward/lifting_object: 0.0107
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 1.99s
                      Time elapsed: 00:06:17
                               ETA: 01:06:05

################################################################################
                     [1m Learning iteration 174/2000 [0m                      

                       Computation: 49919 steps/s (collection: 1.856s, learning 0.113s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.2011
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 40.7588
                       Mean reward: 4.21
               Mean episode length: 235.71
    Episode_Reward/reaching_object: 0.8435
     Episode_Reward/lifting_object: -0.0106
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 1.97s
                      Time elapsed: 00:06:19
                               ETA: 01:06:01

################################################################################
                     [1m Learning iteration 175/2000 [0m                      

                       Computation: 49596 steps/s (collection: 1.873s, learning 0.109s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.0713
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 40.8217
                       Mean reward: 4.35
               Mean episode length: 235.84
    Episode_Reward/reaching_object: 0.8571
     Episode_Reward/lifting_object: 0.0484
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 1.98s
                      Time elapsed: 00:06:21
                               ETA: 01:05:57

################################################################################
                     [1m Learning iteration 176/2000 [0m                      

                       Computation: 49973 steps/s (collection: 1.853s, learning 0.114s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.1242
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.8742
                       Mean reward: 4.50
               Mean episode length: 231.95
    Episode_Reward/reaching_object: 0.8458
     Episode_Reward/lifting_object: 0.0134
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 1.97s
                      Time elapsed: 00:06:23
                               ETA: 01:05:53

################################################################################
                     [1m Learning iteration 177/2000 [0m                      

                       Computation: 50127 steps/s (collection: 1.849s, learning 0.112s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.1110
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.9122
                       Mean reward: 3.46
               Mean episode length: 230.53
    Episode_Reward/reaching_object: 0.8338
     Episode_Reward/lifting_object: -0.0393
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 1.96s
                      Time elapsed: 00:06:25
                               ETA: 01:05:48

################################################################################
                     [1m Learning iteration 178/2000 [0m                      

                       Computation: 50209 steps/s (collection: 1.847s, learning 0.111s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.1706
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 40.9592
                       Mean reward: 4.14
               Mean episode length: 229.45
    Episode_Reward/reaching_object: 0.8333
     Episode_Reward/lifting_object: 0.0336
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 1.96s
                      Time elapsed: 00:06:27
                               ETA: 01:05:44

################################################################################
                     [1m Learning iteration 179/2000 [0m                      

                       Computation: 50373 steps/s (collection: 1.843s, learning 0.109s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.3002
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.9990
                       Mean reward: 3.61
               Mean episode length: 233.50
    Episode_Reward/reaching_object: 0.8471
     Episode_Reward/lifting_object: 0.0026
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 1.95s
                      Time elapsed: 00:06:29
                               ETA: 01:05:40

################################################################################
                     [1m Learning iteration 180/2000 [0m                      

                       Computation: 49850 steps/s (collection: 1.858s, learning 0.114s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.9765
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 41.0250
                       Mean reward: 4.13
               Mean episode length: 224.67
    Episode_Reward/reaching_object: 0.8361
     Episode_Reward/lifting_object: 0.0265
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 1.97s
                      Time elapsed: 00:06:31
                               ETA: 01:05:36

################################################################################
                     [1m Learning iteration 181/2000 [0m                      

                       Computation: 50099 steps/s (collection: 1.853s, learning 0.109s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.2821
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 41.0385
                       Mean reward: 4.22
               Mean episode length: 232.43
    Episode_Reward/reaching_object: 0.8416
     Episode_Reward/lifting_object: 0.0223
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 1.96s
                      Time elapsed: 00:06:33
                               ETA: 01:05:31

################################################################################
                     [1m Learning iteration 182/2000 [0m                      

                       Computation: 50807 steps/s (collection: 1.845s, learning 0.090s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.4967
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 41.0704
                       Mean reward: 4.59
               Mean episode length: 230.18
    Episode_Reward/reaching_object: 0.8481
     Episode_Reward/lifting_object: 0.0168
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 1.93s
                      Time elapsed: 00:06:35
                               ETA: 01:05:27

################################################################################
                     [1m Learning iteration 183/2000 [0m                      

                       Computation: 51310 steps/s (collection: 1.807s, learning 0.109s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.3556
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 41.0962
                       Mean reward: 3.91
               Mean episode length: 224.70
    Episode_Reward/reaching_object: 0.8026
     Episode_Reward/lifting_object: -0.0944
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 1.92s
                      Time elapsed: 00:06:37
                               ETA: 01:05:22

################################################################################
                     [1m Learning iteration 184/2000 [0m                      

                       Computation: 50766 steps/s (collection: 1.817s, learning 0.119s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.3636
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 41.1426
                       Mean reward: 4.09
               Mean episode length: 223.40
    Episode_Reward/reaching_object: 0.8094
     Episode_Reward/lifting_object: -0.0502
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 1.94s
                      Time elapsed: 00:06:39
                               ETA: 01:05:18

################################################################################
                     [1m Learning iteration 185/2000 [0m                      

                       Computation: 51305 steps/s (collection: 1.802s, learning 0.114s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.1908
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 41.1980
                       Mean reward: 4.32
               Mean episode length: 207.70
    Episode_Reward/reaching_object: 0.7850
     Episode_Reward/lifting_object: -0.0274
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 1.92s
                      Time elapsed: 00:06:41
                               ETA: 01:05:14

################################################################################
                     [1m Learning iteration 186/2000 [0m                      

                       Computation: 50241 steps/s (collection: 1.840s, learning 0.117s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.1665
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 41.2603
                       Mean reward: 4.10
               Mean episode length: 223.25
    Episode_Reward/reaching_object: 0.8205
     Episode_Reward/lifting_object: -0.0160
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 1.96s
                      Time elapsed: 00:06:43
                               ETA: 01:05:10

################################################################################
                     [1m Learning iteration 187/2000 [0m                      

                       Computation: 49874 steps/s (collection: 1.857s, learning 0.114s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.5851
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 41.3008
                       Mean reward: 4.19
               Mean episode length: 225.69
    Episode_Reward/reaching_object: 0.7948
     Episode_Reward/lifting_object: 0.0707
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 1.97s
                      Time elapsed: 00:06:45
                               ETA: 01:05:06

################################################################################
                     [1m Learning iteration 188/2000 [0m                      

                       Computation: 50028 steps/s (collection: 1.841s, learning 0.124s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.4997
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 41.3179
                       Mean reward: 3.95
               Mean episode length: 225.44
    Episode_Reward/reaching_object: 0.7995
     Episode_Reward/lifting_object: 0.0253
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 1.96s
                      Time elapsed: 00:06:47
                               ETA: 01:05:02

################################################################################
                     [1m Learning iteration 189/2000 [0m                      

                       Computation: 50674 steps/s (collection: 1.825s, learning 0.115s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.0715
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 41.3636
                       Mean reward: 4.92
               Mean episode length: 232.17
    Episode_Reward/reaching_object: 0.8323
     Episode_Reward/lifting_object: -0.0226
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 1.94s
                      Time elapsed: 00:06:48
                               ETA: 01:04:57

################################################################################
                     [1m Learning iteration 190/2000 [0m                      

                       Computation: 50721 steps/s (collection: 1.824s, learning 0.114s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.1956
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 41.4272
                       Mean reward: 3.24
               Mean episode length: 230.20
    Episode_Reward/reaching_object: 0.8230
     Episode_Reward/lifting_object: -0.1520
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 1.94s
                      Time elapsed: 00:06:50
                               ETA: 01:04:53

################################################################################
                     [1m Learning iteration 191/2000 [0m                      

                       Computation: 51001 steps/s (collection: 1.819s, learning 0.108s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.1772
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 41.4624
                       Mean reward: 4.57
               Mean episode length: 225.67
    Episode_Reward/reaching_object: 0.8312
     Episode_Reward/lifting_object: 0.0941
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 1.93s
                      Time elapsed: 00:06:52
                               ETA: 01:04:49

################################################################################
                     [1m Learning iteration 192/2000 [0m                      

                       Computation: 51562 steps/s (collection: 1.812s, learning 0.095s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.3477
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 41.5053
                       Mean reward: 3.71
               Mean episode length: 227.25
    Episode_Reward/reaching_object: 0.8392
     Episode_Reward/lifting_object: 0.0914
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 1.91s
                      Time elapsed: 00:06:54
                               ETA: 01:04:45

################################################################################
                     [1m Learning iteration 193/2000 [0m                      

                       Computation: 50630 steps/s (collection: 1.819s, learning 0.123s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.2744
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 41.5504
                       Mean reward: 4.08
               Mean episode length: 229.47
    Episode_Reward/reaching_object: 0.8177
     Episode_Reward/lifting_object: 0.1345
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 1.94s
                      Time elapsed: 00:06:56
                               ETA: 01:04:40

################################################################################
                     [1m Learning iteration 194/2000 [0m                      

                       Computation: 50447 steps/s (collection: 1.835s, learning 0.114s)
             Mean action noise std: 1.61
          Mean value_function loss: 1.0559
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 41.5838
                       Mean reward: 4.62
               Mean episode length: 230.73
    Episode_Reward/reaching_object: 0.8481
     Episode_Reward/lifting_object: 0.0474
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 1.95s
                      Time elapsed: 00:06:58
                               ETA: 01:04:36

################################################################################
                     [1m Learning iteration 195/2000 [0m                      

                       Computation: 49962 steps/s (collection: 1.855s, learning 0.113s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.9844
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.5950
                       Mean reward: 4.78
               Mean episode length: 233.73
    Episode_Reward/reaching_object: 0.8654
     Episode_Reward/lifting_object: -0.0337
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 1.97s
                      Time elapsed: 00:07:00
                               ETA: 01:04:33

################################################################################
                     [1m Learning iteration 196/2000 [0m                      

                       Computation: 49756 steps/s (collection: 1.865s, learning 0.111s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.3858
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 41.6290
                       Mean reward: 4.87
               Mean episode length: 240.98
    Episode_Reward/reaching_object: 0.8766
     Episode_Reward/lifting_object: 0.0901
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 1.98s
                      Time elapsed: 00:07:02
                               ETA: 01:04:29

################################################################################
                     [1m Learning iteration 197/2000 [0m                      

                       Computation: 49900 steps/s (collection: 1.850s, learning 0.120s)
             Mean action noise std: 1.61
          Mean value_function loss: 1.4056
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 41.6646
                       Mean reward: 4.32
               Mean episode length: 236.20
    Episode_Reward/reaching_object: 0.8346
     Episode_Reward/lifting_object: 0.0326
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 1.97s
                      Time elapsed: 00:07:04
                               ETA: 01:04:25

################################################################################
                     [1m Learning iteration 198/2000 [0m                      

                       Computation: 48810 steps/s (collection: 1.922s, learning 0.092s)
             Mean action noise std: 1.61
          Mean value_function loss: 1.2112
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 41.6892
                       Mean reward: 5.20
               Mean episode length: 225.55
    Episode_Reward/reaching_object: 0.8627
     Episode_Reward/lifting_object: 0.1653
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 2.01s
                      Time elapsed: 00:07:06
                               ETA: 01:04:22

################################################################################
                     [1m Learning iteration 199/2000 [0m                      

                       Computation: 50140 steps/s (collection: 1.854s, learning 0.106s)
             Mean action noise std: 1.62
          Mean value_function loss: 6.7896
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 41.7307
                       Mean reward: 0.78
               Mean episode length: 237.71
    Episode_Reward/reaching_object: 0.8700
     Episode_Reward/lifting_object: -0.2958
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 1.96s
                      Time elapsed: 00:07:08
                               ETA: 01:04:18

################################################################################
                     [1m Learning iteration 200/2000 [0m                      

                       Computation: 49535 steps/s (collection: 1.872s, learning 0.113s)
             Mean action noise std: 1.62
          Mean value_function loss: 2.8610
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.7452
                       Mean reward: 4.80
               Mean episode length: 228.04
    Episode_Reward/reaching_object: 0.8418
     Episode_Reward/lifting_object: 0.1436
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 1.98s
                      Time elapsed: 00:07:10
                               ETA: 01:04:15

################################################################################
                     [1m Learning iteration 201/2000 [0m                      

                       Computation: 50113 steps/s (collection: 1.872s, learning 0.090s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.4258
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 41.7910
                       Mean reward: 4.46
               Mean episode length: 233.87
    Episode_Reward/reaching_object: 0.8414
     Episode_Reward/lifting_object: 0.1305
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 1.96s
                      Time elapsed: 00:07:12
                               ETA: 01:04:11

################################################################################
                     [1m Learning iteration 202/2000 [0m                      

                       Computation: 50499 steps/s (collection: 1.833s, learning 0.114s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.3234
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 41.8427
                       Mean reward: 5.36
               Mean episode length: 224.83
    Episode_Reward/reaching_object: 0.8296
     Episode_Reward/lifting_object: 0.2079
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 1.95s
                      Time elapsed: 00:07:14
                               ETA: 01:04:07

################################################################################
                     [1m Learning iteration 203/2000 [0m                      

                       Computation: 49602 steps/s (collection: 1.842s, learning 0.140s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.7279
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 41.8769
                       Mean reward: 3.23
               Mean episode length: 219.89
    Episode_Reward/reaching_object: 0.8244
     Episode_Reward/lifting_object: 0.0353
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 1.98s
                      Time elapsed: 00:07:16
                               ETA: 01:04:03

################################################################################
                     [1m Learning iteration 204/2000 [0m                      

                       Computation: 49409 steps/s (collection: 1.869s, learning 0.121s)
             Mean action noise std: 1.63
          Mean value_function loss: 4.8169
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.9059
                       Mean reward: 1.27
               Mean episode length: 234.74
    Episode_Reward/reaching_object: 0.8051
     Episode_Reward/lifting_object: 0.0735
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 1.99s
                      Time elapsed: 00:07:18
                               ETA: 01:04:00

################################################################################
                     [1m Learning iteration 205/2000 [0m                      

                       Computation: 49547 steps/s (collection: 1.868s, learning 0.116s)
             Mean action noise std: 1.63
          Mean value_function loss: 1.3633
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 41.9184
                       Mean reward: 3.16
               Mean episode length: 228.85
    Episode_Reward/reaching_object: 0.8042
     Episode_Reward/lifting_object: 0.0653
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 1.98s
                      Time elapsed: 00:07:20
                               ETA: 01:03:57

################################################################################
                     [1m Learning iteration 206/2000 [0m                      

                       Computation: 50522 steps/s (collection: 1.830s, learning 0.116s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.6614
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 41.9506
                       Mean reward: 5.31
               Mean episode length: 229.09
    Episode_Reward/reaching_object: 0.8372
     Episode_Reward/lifting_object: 0.1139
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 1.95s
                      Time elapsed: 00:07:22
                               ETA: 01:03:53

################################################################################
                     [1m Learning iteration 207/2000 [0m                      

                       Computation: 50529 steps/s (collection: 1.822s, learning 0.123s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.9890
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 41.9748
                       Mean reward: 4.36
               Mean episode length: 223.51
    Episode_Reward/reaching_object: 0.8378
     Episode_Reward/lifting_object: 0.0964
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 1.95s
                      Time elapsed: 00:07:24
                               ETA: 01:03:49

################################################################################
                     [1m Learning iteration 208/2000 [0m                      

                       Computation: 51449 steps/s (collection: 1.794s, learning 0.117s)
             Mean action noise std: 1.64
          Mean value_function loss: 1.6129
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 42.0072
                       Mean reward: 5.11
               Mean episode length: 225.21
    Episode_Reward/reaching_object: 0.7939
     Episode_Reward/lifting_object: 0.0764
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 1.91s
                      Time elapsed: 00:07:26
                               ETA: 01:03:45

################################################################################
                     [1m Learning iteration 209/2000 [0m                      

                       Computation: 50922 steps/s (collection: 1.818s, learning 0.113s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.4980
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 42.0317
                       Mean reward: 2.38
               Mean episode length: 216.83
    Episode_Reward/reaching_object: 0.7671
     Episode_Reward/lifting_object: 0.0678
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 1.93s
                      Time elapsed: 00:07:28
                               ETA: 01:03:41

################################################################################
                     [1m Learning iteration 210/2000 [0m                      

                       Computation: 50364 steps/s (collection: 1.846s, learning 0.106s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.4686
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 42.0815
                       Mean reward: 3.72
               Mean episode length: 231.04
    Episode_Reward/reaching_object: 0.8008
     Episode_Reward/lifting_object: 0.1771
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 1.95s
                      Time elapsed: 00:07:30
                               ETA: 01:03:37

################################################################################
                     [1m Learning iteration 211/2000 [0m                      

                       Computation: 49904 steps/s (collection: 1.855s, learning 0.115s)
             Mean action noise std: 1.65
          Mean value_function loss: 1.2058
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 42.1226
                       Mean reward: 4.82
               Mean episode length: 226.92
    Episode_Reward/reaching_object: 0.7871
     Episode_Reward/lifting_object: 0.1468
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 1.97s
                      Time elapsed: 00:07:32
                               ETA: 01:03:34

################################################################################
                     [1m Learning iteration 212/2000 [0m                      

                       Computation: 50640 steps/s (collection: 1.833s, learning 0.108s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.7190
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 42.1536
                       Mean reward: 5.43
               Mean episode length: 226.47
    Episode_Reward/reaching_object: 0.8048
     Episode_Reward/lifting_object: 0.2360
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 1.94s
                      Time elapsed: 00:07:33
                               ETA: 01:03:30

################################################################################
                     [1m Learning iteration 213/2000 [0m                      

                       Computation: 50936 steps/s (collection: 1.822s, learning 0.108s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.6049
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 42.2087
                       Mean reward: 5.45
               Mean episode length: 230.49
    Episode_Reward/reaching_object: 0.8135
     Episode_Reward/lifting_object: 0.3434
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 1.93s
                      Time elapsed: 00:07:35
                               ETA: 01:03:26

################################################################################
                     [1m Learning iteration 214/2000 [0m                      

                       Computation: 51768 steps/s (collection: 1.804s, learning 0.095s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.7156
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 42.2617
                       Mean reward: 5.25
               Mean episode length: 224.00
    Episode_Reward/reaching_object: 0.8130
     Episode_Reward/lifting_object: 0.2297
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 1.90s
                      Time elapsed: 00:07:37
                               ETA: 01:03:22

################################################################################
                     [1m Learning iteration 215/2000 [0m                      

                       Computation: 50900 steps/s (collection: 1.833s, learning 0.099s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.9132
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 42.2930
                       Mean reward: 5.17
               Mean episode length: 223.16
    Episode_Reward/reaching_object: 0.7812
     Episode_Reward/lifting_object: 0.2623
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 1.93s
                      Time elapsed: 00:07:39
                               ETA: 01:03:18

################################################################################
                     [1m Learning iteration 216/2000 [0m                      

                       Computation: 51268 steps/s (collection: 1.795s, learning 0.123s)
             Mean action noise std: 1.66
          Mean value_function loss: 1.2512
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 42.3271
                       Mean reward: 4.47
               Mean episode length: 228.06
    Episode_Reward/reaching_object: 0.8030
     Episode_Reward/lifting_object: 0.2491
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 1.92s
                      Time elapsed: 00:07:41
                               ETA: 01:03:15

################################################################################
                     [1m Learning iteration 217/2000 [0m                      

                       Computation: 50931 steps/s (collection: 1.815s, learning 0.115s)
             Mean action noise std: 1.67
          Mean value_function loss: 1.0708
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 42.3687
                       Mean reward: 4.91
               Mean episode length: 224.00
    Episode_Reward/reaching_object: 0.7962
     Episode_Reward/lifting_object: 0.1818
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 1.93s
                      Time elapsed: 00:07:43
                               ETA: 01:03:11

################################################################################
                     [1m Learning iteration 218/2000 [0m                      

                       Computation: 51645 steps/s (collection: 1.782s, learning 0.122s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.9944
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 42.4109
                       Mean reward: 5.04
               Mean episode length: 220.13
    Episode_Reward/reaching_object: 0.7786
     Episode_Reward/lifting_object: 0.2680
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 1.90s
                      Time elapsed: 00:07:45
                               ETA: 01:03:07

################################################################################
                     [1m Learning iteration 219/2000 [0m                      

                       Computation: 51215 steps/s (collection: 1.802s, learning 0.118s)
             Mean action noise std: 1.67
          Mean value_function loss: 1.2667
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 42.4629
                       Mean reward: 5.09
               Mean episode length: 225.77
    Episode_Reward/reaching_object: 0.7983
     Episode_Reward/lifting_object: 0.3705
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 1.92s
                      Time elapsed: 00:07:47
                               ETA: 01:03:03

################################################################################
                     [1m Learning iteration 220/2000 [0m                      

                       Computation: 50599 steps/s (collection: 1.825s, learning 0.118s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.7362
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 42.4977
                       Mean reward: 5.17
               Mean episode length: 225.28
    Episode_Reward/reaching_object: 0.7489
     Episode_Reward/lifting_object: 0.3098
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 1.94s
                      Time elapsed: 00:07:49
                               ETA: 01:03:00

################################################################################
                     [1m Learning iteration 221/2000 [0m                      

                       Computation: 50546 steps/s (collection: 1.827s, learning 0.118s)
             Mean action noise std: 1.68
          Mean value_function loss: 3.3955
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 42.5342
                       Mean reward: 5.22
               Mean episode length: 209.51
    Episode_Reward/reaching_object: 0.7618
     Episode_Reward/lifting_object: 0.3709
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 1.94s
                      Time elapsed: 00:07:51
                               ETA: 01:02:56

################################################################################
                     [1m Learning iteration 222/2000 [0m                      

                       Computation: 51520 steps/s (collection: 1.796s, learning 0.112s)
             Mean action noise std: 1.68
          Mean value_function loss: 1.0863
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 42.5729
                       Mean reward: 5.18
               Mean episode length: 224.61
    Episode_Reward/reaching_object: 0.7483
     Episode_Reward/lifting_object: 0.2099
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 1.91s
                      Time elapsed: 00:07:53
                               ETA: 01:02:52

################################################################################
                     [1m Learning iteration 223/2000 [0m                      

                       Computation: 51595 steps/s (collection: 1.802s, learning 0.104s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.8807
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 42.6257
                       Mean reward: 3.66
               Mean episode length: 219.30
    Episode_Reward/reaching_object: 0.7300
     Episode_Reward/lifting_object: 0.3165
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 1.91s
                      Time elapsed: 00:07:55
                               ETA: 01:02:48

################################################################################
                     [1m Learning iteration 224/2000 [0m                      

                       Computation: 51686 steps/s (collection: 1.780s, learning 0.122s)
             Mean action noise std: 1.69
          Mean value_function loss: 1.5881
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 42.6651
                       Mean reward: 6.02
               Mean episode length: 212.47
    Episode_Reward/reaching_object: 0.7160
     Episode_Reward/lifting_object: 0.3042
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 1.90s
                      Time elapsed: 00:07:56
                               ETA: 01:02:44

################################################################################
                     [1m Learning iteration 225/2000 [0m                      

                       Computation: 51756 steps/s (collection: 1.789s, learning 0.111s)
             Mean action noise std: 1.69
          Mean value_function loss: 1.0825
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 42.7202
                       Mean reward: 6.25
               Mean episode length: 221.75
    Episode_Reward/reaching_object: 0.7155
     Episode_Reward/lifting_object: 0.3808
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 1.90s
                      Time elapsed: 00:07:58
                               ETA: 01:02:41

################################################################################
                     [1m Learning iteration 226/2000 [0m                      

                       Computation: 51256 steps/s (collection: 1.808s, learning 0.110s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.9535
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 42.7680
                       Mean reward: 4.56
               Mean episode length: 207.80
    Episode_Reward/reaching_object: 0.6942
     Episode_Reward/lifting_object: 0.3445
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 1.92s
                      Time elapsed: 00:08:00
                               ETA: 01:02:37

################################################################################
                     [1m Learning iteration 227/2000 [0m                      

                       Computation: 52361 steps/s (collection: 1.781s, learning 0.096s)
             Mean action noise std: 1.70
          Mean value_function loss: 1.6553
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 42.7947
                       Mean reward: 5.21
               Mean episode length: 212.25
    Episode_Reward/reaching_object: 0.7178
     Episode_Reward/lifting_object: 0.3711
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 1.88s
                      Time elapsed: 00:08:02
                               ETA: 01:02:33

################################################################################
                     [1m Learning iteration 228/2000 [0m                      

                       Computation: 51148 steps/s (collection: 1.811s, learning 0.111s)
             Mean action noise std: 1.70
          Mean value_function loss: 1.0719
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 42.8339
                       Mean reward: 4.67
               Mean episode length: 206.96
    Episode_Reward/reaching_object: 0.6983
     Episode_Reward/lifting_object: 0.3731
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 1.92s
                      Time elapsed: 00:08:04
                               ETA: 01:02:29

################################################################################
                     [1m Learning iteration 229/2000 [0m                      

                       Computation: 52125 steps/s (collection: 1.794s, learning 0.092s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.8079
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 42.8775
                       Mean reward: 5.42
               Mean episode length: 217.53
    Episode_Reward/reaching_object: 0.7104
     Episode_Reward/lifting_object: 0.4195
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 1.89s
                      Time elapsed: 00:08:06
                               ETA: 01:02:25

################################################################################
                     [1m Learning iteration 230/2000 [0m                      

                       Computation: 51376 steps/s (collection: 1.807s, learning 0.106s)
             Mean action noise std: 1.71
          Mean value_function loss: 1.0408
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 42.9215
                       Mean reward: 5.10
               Mean episode length: 219.73
    Episode_Reward/reaching_object: 0.7102
     Episode_Reward/lifting_object: 0.4124
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 1.91s
                      Time elapsed: 00:08:08
                               ETA: 01:02:22

################################################################################
                     [1m Learning iteration 231/2000 [0m                      

                       Computation: 50451 steps/s (collection: 1.837s, learning 0.112s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.9408
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 42.9590
                       Mean reward: 5.04
               Mean episode length: 223.31
    Episode_Reward/reaching_object: 0.6833
     Episode_Reward/lifting_object: 0.4174
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 1.95s
                      Time elapsed: 00:08:10
                               ETA: 01:02:18

################################################################################
                     [1m Learning iteration 232/2000 [0m                      

                       Computation: 52046 steps/s (collection: 1.781s, learning 0.108s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.8462
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 43.0219
                       Mean reward: 7.24
               Mean episode length: 212.84
    Episode_Reward/reaching_object: 0.7247
     Episode_Reward/lifting_object: 0.4850
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 1.89s
                      Time elapsed: 00:08:12
                               ETA: 01:02:15

################################################################################
                     [1m Learning iteration 233/2000 [0m                      

                       Computation: 49947 steps/s (collection: 1.851s, learning 0.118s)
             Mean action noise std: 1.72
          Mean value_function loss: 1.3655
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 43.0893
                       Mean reward: 5.37
               Mean episode length: 216.15
    Episode_Reward/reaching_object: 0.6614
     Episode_Reward/lifting_object: 0.4352
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 1.97s
                      Time elapsed: 00:08:14
                               ETA: 01:02:11

################################################################################
                     [1m Learning iteration 234/2000 [0m                      

                       Computation: 52226 steps/s (collection: 1.763s, learning 0.119s)
             Mean action noise std: 1.73
          Mean value_function loss: 1.0014
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 43.1469
                       Mean reward: 4.56
               Mean episode length: 208.93
    Episode_Reward/reaching_object: 0.6699
     Episode_Reward/lifting_object: 0.4511
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 1.88s
                      Time elapsed: 00:08:16
                               ETA: 01:02:07

################################################################################
                     [1m Learning iteration 235/2000 [0m                      

                       Computation: 52035 steps/s (collection: 1.776s, learning 0.113s)
             Mean action noise std: 1.73
          Mean value_function loss: 1.0721
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 43.2207
                       Mean reward: 5.49
               Mean episode length: 230.97
    Episode_Reward/reaching_object: 0.6943
     Episode_Reward/lifting_object: 0.3674
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 1.89s
                      Time elapsed: 00:08:17
                               ETA: 01:02:04

################################################################################
                     [1m Learning iteration 236/2000 [0m                      

                       Computation: 51320 steps/s (collection: 1.804s, learning 0.111s)
             Mean action noise std: 1.74
          Mean value_function loss: 1.4367
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 43.2825
                       Mean reward: 5.66
               Mean episode length: 216.64
    Episode_Reward/reaching_object: 0.6941
     Episode_Reward/lifting_object: 0.3146
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 1.92s
                      Time elapsed: 00:08:19
                               ETA: 01:02:00

################################################################################
                     [1m Learning iteration 237/2000 [0m                      

                       Computation: 51375 steps/s (collection: 1.807s, learning 0.106s)
             Mean action noise std: 1.74
          Mean value_function loss: 1.1041
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 43.3307
                       Mean reward: 5.02
               Mean episode length: 235.48
    Episode_Reward/reaching_object: 0.7244
     Episode_Reward/lifting_object: 0.4199
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 1.91s
                      Time elapsed: 00:08:21
                               ETA: 01:01:57

################################################################################
                     [1m Learning iteration 238/2000 [0m                      

                       Computation: 51360 steps/s (collection: 1.822s, learning 0.092s)
             Mean action noise std: 1.74
          Mean value_function loss: 1.3017
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 43.3748
                       Mean reward: 6.19
               Mean episode length: 216.55
    Episode_Reward/reaching_object: 0.7093
     Episode_Reward/lifting_object: 0.4844
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 1.91s
                      Time elapsed: 00:08:23
                               ETA: 01:01:53

################################################################################
                     [1m Learning iteration 239/2000 [0m                      

                       Computation: 50432 steps/s (collection: 1.844s, learning 0.106s)
             Mean action noise std: 1.75
          Mean value_function loss: 1.2210
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 43.4171
                       Mean reward: 5.47
               Mean episode length: 222.84
    Episode_Reward/reaching_object: 0.7132
     Episode_Reward/lifting_object: 0.4094
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 1.95s
                      Time elapsed: 00:08:25
                               ETA: 01:01:50

################################################################################
                     [1m Learning iteration 240/2000 [0m                      

                       Computation: 49614 steps/s (collection: 1.882s, learning 0.099s)
             Mean action noise std: 1.75
          Mean value_function loss: 1.6064
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 43.4634
                       Mean reward: 4.85
               Mean episode length: 218.70
    Episode_Reward/reaching_object: 0.6966
     Episode_Reward/lifting_object: 0.4440
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 1.98s
                      Time elapsed: 00:08:27
                               ETA: 01:01:47

################################################################################
                     [1m Learning iteration 241/2000 [0m                      

                       Computation: 45733 steps/s (collection: 2.031s, learning 0.118s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.9858
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 43.5319
                       Mean reward: 5.57
               Mean episode length: 226.67
    Episode_Reward/reaching_object: 0.6959
     Episode_Reward/lifting_object: 0.5182
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 2.15s
                      Time elapsed: 00:08:29
                               ETA: 01:01:45

################################################################################
                     [1m Learning iteration 242/2000 [0m                      

                       Computation: 47492 steps/s (collection: 1.968s, learning 0.102s)
             Mean action noise std: 1.76
          Mean value_function loss: 1.1216
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 43.5795
                       Mean reward: 6.49
               Mean episode length: 205.60
    Episode_Reward/reaching_object: 0.6932
     Episode_Reward/lifting_object: 0.5254
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 2.07s
                      Time elapsed: 00:08:31
                               ETA: 01:01:43

################################################################################
                     [1m Learning iteration 243/2000 [0m                      

                       Computation: 48157 steps/s (collection: 1.912s, learning 0.130s)
             Mean action noise std: 1.76
          Mean value_function loss: 1.4726
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 43.6184
                       Mean reward: 6.75
               Mean episode length: 223.71
    Episode_Reward/reaching_object: 0.6962
     Episode_Reward/lifting_object: 0.4761
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 2.04s
                      Time elapsed: 00:08:33
                               ETA: 01:01:40

################################################################################
                     [1m Learning iteration 244/2000 [0m                      

                       Computation: 47881 steps/s (collection: 1.957s, learning 0.096s)
             Mean action noise std: 1.77
          Mean value_function loss: 1.4542
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 43.6614
                       Mean reward: 5.40
               Mean episode length: 207.63
    Episode_Reward/reaching_object: 0.6873
     Episode_Reward/lifting_object: 0.4418
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 2.05s
                      Time elapsed: 00:08:35
                               ETA: 01:01:38

################################################################################
                     [1m Learning iteration 245/2000 [0m                      

                       Computation: 51018 steps/s (collection: 1.823s, learning 0.104s)
             Mean action noise std: 1.77
          Mean value_function loss: 1.0244
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 43.7179
                       Mean reward: 6.24
               Mean episode length: 230.88
    Episode_Reward/reaching_object: 0.7140
     Episode_Reward/lifting_object: 0.5084
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 1.93s
                      Time elapsed: 00:08:37
                               ETA: 01:01:34

################################################################################
                     [1m Learning iteration 246/2000 [0m                      

                       Computation: 49336 steps/s (collection: 1.890s, learning 0.102s)
             Mean action noise std: 1.78
          Mean value_function loss: 2.0170
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 43.7822
                       Mean reward: 5.77
               Mean episode length: 210.93
    Episode_Reward/reaching_object: 0.7012
     Episode_Reward/lifting_object: 0.3619
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 1.99s
                      Time elapsed: 00:08:39
                               ETA: 01:01:31

################################################################################
                     [1m Learning iteration 247/2000 [0m                      

                       Computation: 50660 steps/s (collection: 1.840s, learning 0.100s)
             Mean action noise std: 1.78
          Mean value_function loss: 1.2885
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 43.8466
                       Mean reward: 6.18
               Mean episode length: 219.48
    Episode_Reward/reaching_object: 0.6987
     Episode_Reward/lifting_object: 0.5897
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 1.94s
                      Time elapsed: 00:08:41
                               ETA: 01:01:28

################################################################################
                     [1m Learning iteration 248/2000 [0m                      

                       Computation: 50556 steps/s (collection: 1.840s, learning 0.105s)
             Mean action noise std: 1.79
          Mean value_function loss: 1.3689
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 43.9041
                       Mean reward: 5.88
               Mean episode length: 217.70
    Episode_Reward/reaching_object: 0.6985
     Episode_Reward/lifting_object: 0.4843
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 1.94s
                      Time elapsed: 00:08:43
                               ETA: 01:01:25

################################################################################
                     [1m Learning iteration 249/2000 [0m                      

                       Computation: 49890 steps/s (collection: 1.877s, learning 0.094s)
             Mean action noise std: 1.79
          Mean value_function loss: 2.2233
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 43.9382
                       Mean reward: 6.61
               Mean episode length: 211.60
    Episode_Reward/reaching_object: 0.6569
     Episode_Reward/lifting_object: 0.5256
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 1.97s
                      Time elapsed: 00:08:45
                               ETA: 01:01:22

################################################################################
                     [1m Learning iteration 250/2000 [0m                      

                       Computation: 50224 steps/s (collection: 1.865s, learning 0.092s)
             Mean action noise std: 1.79
          Mean value_function loss: 1.6439
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 43.9860
                       Mean reward: 5.79
               Mean episode length: 185.19
    Episode_Reward/reaching_object: 0.6274
     Episode_Reward/lifting_object: 0.5118
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 1.96s
                      Time elapsed: 00:08:47
                               ETA: 01:01:19

################################################################################
                     [1m Learning iteration 251/2000 [0m                      

                       Computation: 51019 steps/s (collection: 1.822s, learning 0.105s)
             Mean action noise std: 1.80
          Mean value_function loss: 1.7312
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 44.0518
                       Mean reward: 4.99
               Mean episode length: 189.96
    Episode_Reward/reaching_object: 0.6229
     Episode_Reward/lifting_object: 0.4757
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 1.93s
                      Time elapsed: 00:08:49
                               ETA: 01:01:15

################################################################################
                     [1m Learning iteration 252/2000 [0m                      

                       Computation: 50650 steps/s (collection: 1.834s, learning 0.107s)
             Mean action noise std: 1.81
          Mean value_function loss: 1.7252
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 44.1219
                       Mean reward: 5.90
               Mean episode length: 212.61
    Episode_Reward/reaching_object: 0.6521
     Episode_Reward/lifting_object: 0.5199
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 1.94s
                      Time elapsed: 00:08:51
                               ETA: 01:01:12

################################################################################
                     [1m Learning iteration 253/2000 [0m                      

                       Computation: 50512 steps/s (collection: 1.842s, learning 0.105s)
             Mean action noise std: 1.81
          Mean value_function loss: 1.4010
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 44.1713
                       Mean reward: 5.61
               Mean episode length: 193.55
    Episode_Reward/reaching_object: 0.6256
     Episode_Reward/lifting_object: 0.4247
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 1.95s
                      Time elapsed: 00:08:53
                               ETA: 01:01:09

################################################################################
                     [1m Learning iteration 254/2000 [0m                      

                       Computation: 51375 steps/s (collection: 1.812s, learning 0.101s)
             Mean action noise std: 1.81
          Mean value_function loss: 1.9582
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 44.2172
                       Mean reward: 5.23
               Mean episode length: 195.19
    Episode_Reward/reaching_object: 0.6237
     Episode_Reward/lifting_object: 0.4195
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 1.91s
                      Time elapsed: 00:08:55
                               ETA: 01:01:06

################################################################################
                     [1m Learning iteration 255/2000 [0m                      

                       Computation: 50304 steps/s (collection: 1.843s, learning 0.111s)
             Mean action noise std: 1.82
          Mean value_function loss: 1.9874
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 44.2755
                       Mean reward: 4.70
               Mean episode length: 188.32
    Episode_Reward/reaching_object: 0.6305
     Episode_Reward/lifting_object: 0.4800
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 1.95s
                      Time elapsed: 00:08:57
                               ETA: 01:01:02

################################################################################
                     [1m Learning iteration 256/2000 [0m                      

                       Computation: 50158 steps/s (collection: 1.850s, learning 0.109s)
             Mean action noise std: 1.82
          Mean value_function loss: 2.1293
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 44.3295
                       Mean reward: 5.81
               Mean episode length: 205.15
    Episode_Reward/reaching_object: 0.6011
     Episode_Reward/lifting_object: 0.5145
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 1.96s
                      Time elapsed: 00:08:59
                               ETA: 01:00:59

################################################################################
                     [1m Learning iteration 257/2000 [0m                      

                       Computation: 50057 steps/s (collection: 1.848s, learning 0.115s)
             Mean action noise std: 1.83
          Mean value_function loss: 1.6852
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 44.3778
                       Mean reward: 6.26
               Mean episode length: 210.41
    Episode_Reward/reaching_object: 0.6288
     Episode_Reward/lifting_object: 0.4563
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 1.96s
                      Time elapsed: 00:09:01
                               ETA: 01:00:56

################################################################################
                     [1m Learning iteration 258/2000 [0m                      

                       Computation: 49119 steps/s (collection: 1.892s, learning 0.109s)
             Mean action noise std: 1.83
          Mean value_function loss: 2.8337
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 44.4384
                       Mean reward: 3.02
               Mean episode length: 206.56
    Episode_Reward/reaching_object: 0.6113
     Episode_Reward/lifting_object: 0.5145
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 2.00s
                      Time elapsed: 00:09:03
                               ETA: 01:00:54

################################################################################
                     [1m Learning iteration 259/2000 [0m                      

                       Computation: 48670 steps/s (collection: 1.901s, learning 0.119s)
             Mean action noise std: 1.84
          Mean value_function loss: 1.8301
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 44.4963
                       Mean reward: 5.86
               Mean episode length: 207.46
    Episode_Reward/reaching_object: 0.6156
     Episode_Reward/lifting_object: 0.6796
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 2.02s
                      Time elapsed: 00:09:05
                               ETA: 01:00:51

################################################################################
                     [1m Learning iteration 260/2000 [0m                      

                       Computation: 48372 steps/s (collection: 1.872s, learning 0.160s)
             Mean action noise std: 1.84
          Mean value_function loss: 2.0856
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 44.5625
                       Mean reward: 5.90
               Mean episode length: 221.13
    Episode_Reward/reaching_object: 0.5955
     Episode_Reward/lifting_object: 0.6159
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 2.03s
                      Time elapsed: 00:09:07
                               ETA: 01:00:48

################################################################################
                     [1m Learning iteration 261/2000 [0m                      

                       Computation: 47888 steps/s (collection: 1.932s, learning 0.121s)
             Mean action noise std: 1.85
          Mean value_function loss: 2.7393
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 44.6233
                       Mean reward: 6.38
               Mean episode length: 201.95
    Episode_Reward/reaching_object: 0.5705
     Episode_Reward/lifting_object: 0.5949
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 2.05s
                      Time elapsed: 00:09:09
                               ETA: 01:00:46

################################################################################
                     [1m Learning iteration 262/2000 [0m                      

                       Computation: 47039 steps/s (collection: 1.928s, learning 0.162s)
             Mean action noise std: 1.85
          Mean value_function loss: 1.4232
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 44.6713
                       Mean reward: 4.36
               Mean episode length: 194.28
    Episode_Reward/reaching_object: 0.5557
     Episode_Reward/lifting_object: 0.5125
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 2.09s
                      Time elapsed: 00:09:11
                               ETA: 01:00:44

################################################################################
                     [1m Learning iteration 263/2000 [0m                      

                       Computation: 44042 steps/s (collection: 1.981s, learning 0.251s)
             Mean action noise std: 1.86
          Mean value_function loss: 1.3022
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 44.7230
                       Mean reward: 3.45
               Mean episode length: 200.18
    Episode_Reward/reaching_object: 0.5432
     Episode_Reward/lifting_object: 0.6004
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 2.23s
                      Time elapsed: 00:09:13
                               ETA: 01:00:43

################################################################################
                     [1m Learning iteration 264/2000 [0m                      

                       Computation: 43125 steps/s (collection: 2.177s, learning 0.103s)
             Mean action noise std: 1.86
          Mean value_function loss: 1.9433
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 44.7619
                       Mean reward: 3.88
               Mean episode length: 193.73
    Episode_Reward/reaching_object: 0.4816
     Episode_Reward/lifting_object: 0.4655
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 2.28s
                      Time elapsed: 00:09:16
                               ETA: 01:00:42

################################################################################
                     [1m Learning iteration 265/2000 [0m                      

                       Computation: 37149 steps/s (collection: 2.465s, learning 0.182s)
             Mean action noise std: 1.86
          Mean value_function loss: 1.1689
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 44.7924
                       Mean reward: 5.00
               Mean episode length: 197.90
    Episode_Reward/reaching_object: 0.4908
     Episode_Reward/lifting_object: 0.6277
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 2.65s
                      Time elapsed: 00:09:18
                               ETA: 01:00:43

################################################################################
                     [1m Learning iteration 266/2000 [0m                      

                       Computation: 47358 steps/s (collection: 1.948s, learning 0.128s)
             Mean action noise std: 1.87
          Mean value_function loss: 1.5886
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 44.8417
                       Mean reward: 5.03
               Mean episode length: 192.52
    Episode_Reward/reaching_object: 0.4736
     Episode_Reward/lifting_object: 0.4820
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 2.08s
                      Time elapsed: 00:09:20
                               ETA: 01:00:41

################################################################################
                     [1m Learning iteration 267/2000 [0m                      

                       Computation: 48501 steps/s (collection: 1.841s, learning 0.186s)
             Mean action noise std: 1.87
          Mean value_function loss: 1.2147
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 44.8658
                       Mean reward: 4.65
               Mean episode length: 190.92
    Episode_Reward/reaching_object: 0.4705
     Episode_Reward/lifting_object: 0.4352
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 2.03s
                      Time elapsed: 00:09:22
                               ETA: 01:00:38

################################################################################
                     [1m Learning iteration 268/2000 [0m                      

                       Computation: 49585 steps/s (collection: 1.852s, learning 0.130s)
             Mean action noise std: 1.87
          Mean value_function loss: 1.2548
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 44.8913
                       Mean reward: 5.81
               Mean episode length: 192.01
    Episode_Reward/reaching_object: 0.4472
     Episode_Reward/lifting_object: 0.6435
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 1.98s
                      Time elapsed: 00:09:24
                               ETA: 01:00:36

################################################################################
                     [1m Learning iteration 269/2000 [0m                      

                       Computation: 46601 steps/s (collection: 1.980s, learning 0.130s)
             Mean action noise std: 1.87
          Mean value_function loss: 1.2111
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 44.9307
                       Mean reward: 4.03
               Mean episode length: 212.56
    Episode_Reward/reaching_object: 0.4845
     Episode_Reward/lifting_object: 0.5990
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 2.11s
                      Time elapsed: 00:09:26
                               ETA: 01:00:34

################################################################################
                     [1m Learning iteration 270/2000 [0m                      

                       Computation: 50677 steps/s (collection: 1.841s, learning 0.099s)
             Mean action noise std: 1.88
          Mean value_function loss: 2.3437
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 44.9755
                       Mean reward: 3.73
               Mean episode length: 202.05
    Episode_Reward/reaching_object: 0.5011
     Episode_Reward/lifting_object: 0.5674
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 1.94s
                      Time elapsed: 00:09:28
                               ETA: 01:00:30

################################################################################
                     [1m Learning iteration 271/2000 [0m                      

                       Computation: 49358 steps/s (collection: 1.899s, learning 0.093s)
             Mean action noise std: 1.88
          Mean value_function loss: 1.9468
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 45.0345
                       Mean reward: 5.80
               Mean episode length: 216.01
    Episode_Reward/reaching_object: 0.5196
     Episode_Reward/lifting_object: 0.6092
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 1.99s
                      Time elapsed: 00:09:30
                               ETA: 01:00:28

################################################################################
                     [1m Learning iteration 272/2000 [0m                      

                       Computation: 49064 steps/s (collection: 1.902s, learning 0.101s)
             Mean action noise std: 1.89
          Mean value_function loss: 1.2402
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 45.0842
                       Mean reward: 6.06
               Mean episode length: 211.51
    Episode_Reward/reaching_object: 0.5124
     Episode_Reward/lifting_object: 0.7460
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 2.00s
                      Time elapsed: 00:09:32
                               ETA: 01:00:25

################################################################################
                     [1m Learning iteration 273/2000 [0m                      

                       Computation: 47821 steps/s (collection: 1.934s, learning 0.122s)
             Mean action noise std: 1.89
          Mean value_function loss: 1.6322
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 45.1217
                       Mean reward: 6.83
               Mean episode length: 213.17
    Episode_Reward/reaching_object: 0.5292
     Episode_Reward/lifting_object: 0.8190
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 2.06s
                      Time elapsed: 00:09:34
                               ETA: 01:00:23

################################################################################
                     [1m Learning iteration 274/2000 [0m                      

                       Computation: 50477 steps/s (collection: 1.840s, learning 0.107s)
             Mean action noise std: 1.89
          Mean value_function loss: 1.4078
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 45.1604
                       Mean reward: 6.90
               Mean episode length: 209.17
    Episode_Reward/reaching_object: 0.5602
     Episode_Reward/lifting_object: 0.6577
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 1.95s
                      Time elapsed: 00:09:36
                               ETA: 01:00:20

################################################################################
                     [1m Learning iteration 275/2000 [0m                      

                       Computation: 48624 steps/s (collection: 1.930s, learning 0.092s)
             Mean action noise std: 1.90
          Mean value_function loss: 1.3746
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 45.2070
                       Mean reward: 6.03
               Mean episode length: 218.96
    Episode_Reward/reaching_object: 0.5683
     Episode_Reward/lifting_object: 0.7115
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 2.02s
                      Time elapsed: 00:09:38
                               ETA: 01:00:17

################################################################################
                     [1m Learning iteration 276/2000 [0m                      

                       Computation: 47726 steps/s (collection: 1.932s, learning 0.128s)
             Mean action noise std: 1.90
          Mean value_function loss: 1.6448
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 45.2485
                       Mean reward: 4.82
               Mean episode length: 209.75
    Episode_Reward/reaching_object: 0.5331
     Episode_Reward/lifting_object: 0.5048
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 2.06s
                      Time elapsed: 00:09:40
                               ETA: 01:00:15

################################################################################
                     [1m Learning iteration 277/2000 [0m                      

                       Computation: 50602 steps/s (collection: 1.803s, learning 0.140s)
             Mean action noise std: 1.91
          Mean value_function loss: 1.3939
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 45.2932
                       Mean reward: 5.20
               Mean episode length: 195.28
    Episode_Reward/reaching_object: 0.5645
     Episode_Reward/lifting_object: 0.5603
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 1.94s
                      Time elapsed: 00:09:42
                               ETA: 01:00:12

################################################################################
                     [1m Learning iteration 278/2000 [0m                      

                       Computation: 49851 steps/s (collection: 1.858s, learning 0.114s)
             Mean action noise std: 1.91
          Mean value_function loss: 1.3965
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 45.3282
                       Mean reward: 6.05
               Mean episode length: 210.67
    Episode_Reward/reaching_object: 0.6120
     Episode_Reward/lifting_object: 0.7302
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 1.97s
                      Time elapsed: 00:09:44
                               ETA: 01:00:09

################################################################################
                     [1m Learning iteration 279/2000 [0m                      

                       Computation: 49709 steps/s (collection: 1.852s, learning 0.126s)
             Mean action noise std: 1.91
          Mean value_function loss: 2.3745
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 45.3656
                       Mean reward: 5.52
               Mean episode length: 207.41
    Episode_Reward/reaching_object: 0.6154
     Episode_Reward/lifting_object: 0.6289
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 1.98s
                      Time elapsed: 00:09:46
                               ETA: 01:00:06

################################################################################
                     [1m Learning iteration 280/2000 [0m                      

                       Computation: 50663 steps/s (collection: 1.822s, learning 0.119s)
             Mean action noise std: 1.92
          Mean value_function loss: 1.8637
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 45.4167
                       Mean reward: 6.45
               Mean episode length: 198.37
    Episode_Reward/reaching_object: 0.6110
     Episode_Reward/lifting_object: 0.6462
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 1.94s
                      Time elapsed: 00:09:48
                               ETA: 01:00:03

################################################################################
                     [1m Learning iteration 281/2000 [0m                      

                       Computation: 49227 steps/s (collection: 1.878s, learning 0.119s)
             Mean action noise std: 1.92
          Mean value_function loss: 1.6556
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 45.4627
                       Mean reward: 8.60
               Mean episode length: 214.22
    Episode_Reward/reaching_object: 0.6613
     Episode_Reward/lifting_object: 0.8598
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 2.00s
                      Time elapsed: 00:09:50
                               ETA: 01:00:00

################################################################################
                     [1m Learning iteration 282/2000 [0m                      

                       Computation: 49954 steps/s (collection: 1.851s, learning 0.117s)
             Mean action noise std: 1.92
          Mean value_function loss: 4.8581
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 45.4880
                       Mean reward: 4.02
               Mean episode length: 200.92
    Episode_Reward/reaching_object: 0.6021
     Episode_Reward/lifting_object: 0.5895
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 1.97s
                      Time elapsed: 00:09:52
                               ETA: 00:59:57

################################################################################
                     [1m Learning iteration 283/2000 [0m                      

                       Computation: 49792 steps/s (collection: 1.861s, learning 0.114s)
             Mean action noise std: 1.93
          Mean value_function loss: 2.2319
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 45.5437
                       Mean reward: 6.12
               Mean episode length: 191.92
    Episode_Reward/reaching_object: 0.6103
     Episode_Reward/lifting_object: 0.5895
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 1.97s
                      Time elapsed: 00:09:54
                               ETA: 00:59:55

################################################################################
                     [1m Learning iteration 284/2000 [0m                      

                       Computation: 50058 steps/s (collection: 1.855s, learning 0.109s)
             Mean action noise std: 1.93
          Mean value_function loss: 2.9331
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 45.5892
                       Mean reward: 6.77
               Mean episode length: 197.12
    Episode_Reward/reaching_object: 0.6038
     Episode_Reward/lifting_object: 0.8494
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 1.96s
                      Time elapsed: 00:09:56
                               ETA: 00:59:52

################################################################################
                     [1m Learning iteration 285/2000 [0m                      

                       Computation: 51415 steps/s (collection: 1.818s, learning 0.094s)
             Mean action noise std: 1.94
          Mean value_function loss: 2.1337
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 45.6250
                       Mean reward: 7.04
               Mean episode length: 218.15
    Episode_Reward/reaching_object: 0.6669
     Episode_Reward/lifting_object: 0.7146
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 1.91s
                      Time elapsed: 00:09:58
                               ETA: 00:59:48

################################################################################
                     [1m Learning iteration 286/2000 [0m                      

                       Computation: 50449 steps/s (collection: 1.834s, learning 0.115s)
             Mean action noise std: 1.94
          Mean value_function loss: 1.9742
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 45.6663
                       Mean reward: 5.94
               Mean episode length: 196.49
    Episode_Reward/reaching_object: 0.6318
     Episode_Reward/lifting_object: 0.6858
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 1.95s
                      Time elapsed: 00:10:00
                               ETA: 00:59:46

################################################################################
                     [1m Learning iteration 287/2000 [0m                      

                       Computation: 50819 steps/s (collection: 1.841s, learning 0.094s)
             Mean action noise std: 1.94
          Mean value_function loss: 1.8949
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 45.7047
                       Mean reward: 4.47
               Mean episode length: 208.26
    Episode_Reward/reaching_object: 0.6432
     Episode_Reward/lifting_object: 0.6487
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 1.93s
                      Time elapsed: 00:10:02
                               ETA: 00:59:42

################################################################################
                     [1m Learning iteration 288/2000 [0m                      

                       Computation: 50386 steps/s (collection: 1.834s, learning 0.117s)
             Mean action noise std: 1.95
          Mean value_function loss: 2.0529
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 45.7454
                       Mean reward: 6.74
               Mean episode length: 201.61
    Episode_Reward/reaching_object: 0.6454
     Episode_Reward/lifting_object: 0.7981
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 1.95s
                      Time elapsed: 00:10:04
                               ETA: 00:59:40

################################################################################
                     [1m Learning iteration 289/2000 [0m                      

                       Computation: 49478 steps/s (collection: 1.866s, learning 0.121s)
             Mean action noise std: 1.95
          Mean value_function loss: 2.3074
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 45.7863
                       Mean reward: 5.63
               Mean episode length: 202.10
    Episode_Reward/reaching_object: 0.6026
     Episode_Reward/lifting_object: 0.7960
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 1.99s
                      Time elapsed: 00:10:06
                               ETA: 00:59:37

################################################################################
                     [1m Learning iteration 290/2000 [0m                      

                       Computation: 49819 steps/s (collection: 1.863s, learning 0.111s)
             Mean action noise std: 1.96
          Mean value_function loss: 2.1351
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 45.8292
                       Mean reward: 7.70
               Mean episode length: 195.22
    Episode_Reward/reaching_object: 0.6060
     Episode_Reward/lifting_object: 0.7749
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 1.97s
                      Time elapsed: 00:10:08
                               ETA: 00:59:34

################################################################################
                     [1m Learning iteration 291/2000 [0m                      

                       Computation: 49640 steps/s (collection: 1.883s, learning 0.098s)
             Mean action noise std: 1.96
          Mean value_function loss: 2.1961
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 45.8743
                       Mean reward: 7.83
               Mean episode length: 201.76
    Episode_Reward/reaching_object: 0.5828
     Episode_Reward/lifting_object: 0.6807
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 1.98s
                      Time elapsed: 00:10:10
                               ETA: 00:59:31

################################################################################
                     [1m Learning iteration 292/2000 [0m                      

                       Computation: 50209 steps/s (collection: 1.862s, learning 0.096s)
             Mean action noise std: 1.96
          Mean value_function loss: 1.9550
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 45.8994
                       Mean reward: 5.56
               Mean episode length: 196.11
    Episode_Reward/reaching_object: 0.5642
     Episode_Reward/lifting_object: 0.8312
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 1.96s
                      Time elapsed: 00:10:12
                               ETA: 00:59:28

################################################################################
                     [1m Learning iteration 293/2000 [0m                      

                       Computation: 50353 steps/s (collection: 1.854s, learning 0.098s)
             Mean action noise std: 1.97
          Mean value_function loss: 1.9377
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 45.9373
                       Mean reward: 6.77
               Mean episode length: 206.30
    Episode_Reward/reaching_object: 0.5764
     Episode_Reward/lifting_object: 0.7645
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 1.95s
                      Time elapsed: 00:10:14
                               ETA: 00:59:26

################################################################################
                     [1m Learning iteration 294/2000 [0m                      

                       Computation: 49526 steps/s (collection: 1.854s, learning 0.131s)
             Mean action noise std: 1.97
          Mean value_function loss: 2.6222
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 45.9812
                       Mean reward: 6.45
               Mean episode length: 190.59
    Episode_Reward/reaching_object: 0.5296
     Episode_Reward/lifting_object: 0.7766
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 1.98s
                      Time elapsed: 00:10:16
                               ETA: 00:59:23

################################################################################
                     [1m Learning iteration 295/2000 [0m                      

                       Computation: 50103 steps/s (collection: 1.844s, learning 0.118s)
             Mean action noise std: 1.97
          Mean value_function loss: 1.9958
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 46.0272
                       Mean reward: 6.84
               Mean episode length: 191.40
    Episode_Reward/reaching_object: 0.5195
     Episode_Reward/lifting_object: 0.7896
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 1.96s
                      Time elapsed: 00:10:18
                               ETA: 00:59:20

################################################################################
                     [1m Learning iteration 296/2000 [0m                      

                       Computation: 50223 steps/s (collection: 1.853s, learning 0.105s)
             Mean action noise std: 1.98
          Mean value_function loss: 2.1559
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 46.0675
                       Mean reward: 7.13
               Mean episode length: 201.81
    Episode_Reward/reaching_object: 0.4987
     Episode_Reward/lifting_object: 0.7938
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 1.96s
                      Time elapsed: 00:10:20
                               ETA: 00:59:17

################################################################################
                     [1m Learning iteration 297/2000 [0m                      

                       Computation: 50531 steps/s (collection: 1.853s, learning 0.092s)
             Mean action noise std: 1.98
          Mean value_function loss: 3.2295
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 46.1055
                       Mean reward: 5.79
               Mean episode length: 210.41
    Episode_Reward/reaching_object: 0.4964
     Episode_Reward/lifting_object: 0.8038
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 1.95s
                      Time elapsed: 00:10:22
                               ETA: 00:59:14

################################################################################
                     [1m Learning iteration 298/2000 [0m                      

                       Computation: 49311 steps/s (collection: 1.880s, learning 0.114s)
             Mean action noise std: 1.99
          Mean value_function loss: 2.1131
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 46.1608
                       Mean reward: 6.40
               Mean episode length: 196.11
    Episode_Reward/reaching_object: 0.4942
     Episode_Reward/lifting_object: 0.8601
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 1.99s
                      Time elapsed: 00:10:24
                               ETA: 00:59:12

################################################################################
                     [1m Learning iteration 299/2000 [0m                      

                       Computation: 50041 steps/s (collection: 1.867s, learning 0.097s)
             Mean action noise std: 1.99
          Mean value_function loss: 3.9283
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 46.2089
                       Mean reward: 7.48
               Mean episode length: 204.47
    Episode_Reward/reaching_object: 0.4938
     Episode_Reward/lifting_object: 1.0168
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 1.96s
                      Time elapsed: 00:10:26
                               ETA: 00:59:09

################################################################################
                     [1m Learning iteration 300/2000 [0m                      

                       Computation: 49504 steps/s (collection: 1.864s, learning 0.122s)
             Mean action noise std: 1.99
          Mean value_function loss: 2.3665
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 46.2512
                       Mean reward: 6.23
               Mean episode length: 209.33
    Episode_Reward/reaching_object: 0.5021
     Episode_Reward/lifting_object: 0.7724
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 1.99s
                      Time elapsed: 00:10:27
                               ETA: 00:59:06

################################################################################
                     [1m Learning iteration 301/2000 [0m                      

                       Computation: 48528 steps/s (collection: 1.901s, learning 0.125s)
             Mean action noise std: 2.00
          Mean value_function loss: 2.4838
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 46.2846
                       Mean reward: 6.09
               Mean episode length: 202.60
    Episode_Reward/reaching_object: 0.5274
     Episode_Reward/lifting_object: 0.9997
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 2.03s
                      Time elapsed: 00:10:30
                               ETA: 00:59:04

################################################################################
                     [1m Learning iteration 302/2000 [0m                      

                       Computation: 50256 steps/s (collection: 1.842s, learning 0.114s)
             Mean action noise std: 2.00
          Mean value_function loss: 2.2734
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 46.3163
                       Mean reward: 5.43
               Mean episode length: 193.21
    Episode_Reward/reaching_object: 0.5431
     Episode_Reward/lifting_object: 1.1312
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 1.96s
                      Time elapsed: 00:10:31
                               ETA: 00:59:01

################################################################################
                     [1m Learning iteration 303/2000 [0m                      

                       Computation: 49117 steps/s (collection: 1.878s, learning 0.123s)
             Mean action noise std: 2.00
          Mean value_function loss: 1.9376
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 46.3401
                       Mean reward: 7.50
               Mean episode length: 184.22
    Episode_Reward/reaching_object: 0.5464
     Episode_Reward/lifting_object: 1.0305
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 2.00s
                      Time elapsed: 00:10:33
                               ETA: 00:58:58

################################################################################
                     [1m Learning iteration 304/2000 [0m                      

                       Computation: 49710 steps/s (collection: 1.852s, learning 0.125s)
             Mean action noise std: 2.00
          Mean value_function loss: 2.3317
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 46.3571
                       Mean reward: 7.89
               Mean episode length: 194.82
    Episode_Reward/reaching_object: 0.5490
     Episode_Reward/lifting_object: 0.9360
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 1.98s
                      Time elapsed: 00:10:35
                               ETA: 00:58:56

################################################################################
                     [1m Learning iteration 305/2000 [0m                      

                       Computation: 49853 steps/s (collection: 1.845s, learning 0.127s)
             Mean action noise std: 2.00
          Mean value_function loss: 2.8201
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 46.3673
                       Mean reward: 7.29
               Mean episode length: 200.88
    Episode_Reward/reaching_object: 0.5666
     Episode_Reward/lifting_object: 0.9656
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 1.97s
                      Time elapsed: 00:10:37
                               ETA: 00:58:53

################################################################################
                     [1m Learning iteration 306/2000 [0m                      

                       Computation: 49179 steps/s (collection: 1.878s, learning 0.121s)
             Mean action noise std: 2.01
          Mean value_function loss: 3.3791
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 46.3779
                       Mean reward: 9.34
               Mean episode length: 195.46
    Episode_Reward/reaching_object: 0.6019
     Episode_Reward/lifting_object: 1.2415
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 2.00s
                      Time elapsed: 00:10:39
                               ETA: 00:58:51

################################################################################
                     [1m Learning iteration 307/2000 [0m                      

                       Computation: 48778 steps/s (collection: 1.882s, learning 0.133s)
             Mean action noise std: 2.01
          Mean value_function loss: 3.7329
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 46.4013
                       Mean reward: 7.43
               Mean episode length: 181.22
    Episode_Reward/reaching_object: 0.5728
     Episode_Reward/lifting_object: 1.0395
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 2.02s
                      Time elapsed: 00:10:41
                               ETA: 00:58:48

################################################################################
                     [1m Learning iteration 308/2000 [0m                      

                       Computation: 49819 steps/s (collection: 1.872s, learning 0.102s)
             Mean action noise std: 2.01
          Mean value_function loss: 2.7561
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 46.4331
                       Mean reward: 8.36
               Mean episode length: 190.44
    Episode_Reward/reaching_object: 0.5601
     Episode_Reward/lifting_object: 0.9016
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 1.97s
                      Time elapsed: 00:10:43
                               ETA: 00:58:45

################################################################################
                     [1m Learning iteration 309/2000 [0m                      

                       Computation: 49478 steps/s (collection: 1.886s, learning 0.101s)
             Mean action noise std: 2.02
          Mean value_function loss: 2.9528
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 46.4728
                       Mean reward: 6.52
               Mean episode length: 194.28
    Episode_Reward/reaching_object: 0.5690
     Episode_Reward/lifting_object: 0.9061
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 1.99s
                      Time elapsed: 00:10:45
                               ETA: 00:58:43

################################################################################
                     [1m Learning iteration 310/2000 [0m                      

                       Computation: 48358 steps/s (collection: 1.923s, learning 0.110s)
             Mean action noise std: 2.02
          Mean value_function loss: 5.7856
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 46.5183
                       Mean reward: 8.09
               Mean episode length: 188.88
    Episode_Reward/reaching_object: 0.5514
     Episode_Reward/lifting_object: 1.0250
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 2.03s
                      Time elapsed: 00:10:47
                               ETA: 00:58:40

################################################################################
                     [1m Learning iteration 311/2000 [0m                      

                       Computation: 46255 steps/s (collection: 1.979s, learning 0.147s)
             Mean action noise std: 2.02
          Mean value_function loss: 3.3414
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 46.5537
                       Mean reward: 11.17
               Mean episode length: 185.78
    Episode_Reward/reaching_object: 0.5468
     Episode_Reward/lifting_object: 1.0892
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 2.13s
                      Time elapsed: 00:10:50
                               ETA: 00:58:39

################################################################################
                     [1m Learning iteration 312/2000 [0m                      

                       Computation: 47232 steps/s (collection: 1.977s, learning 0.104s)
             Mean action noise std: 2.02
          Mean value_function loss: 2.6404
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 46.5714
                       Mean reward: 9.80
               Mean episode length: 193.72
    Episode_Reward/reaching_object: 0.5718
     Episode_Reward/lifting_object: 1.3119
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 2.08s
                      Time elapsed: 00:10:52
                               ETA: 00:58:36

################################################################################
                     [1m Learning iteration 313/2000 [0m                      

                       Computation: 47105 steps/s (collection: 1.950s, learning 0.137s)
             Mean action noise std: 2.03
          Mean value_function loss: 3.1276
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 46.6012
                       Mean reward: 8.32
               Mean episode length: 185.40
    Episode_Reward/reaching_object: 0.5551
     Episode_Reward/lifting_object: 1.0658
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 2.09s
                      Time elapsed: 00:10:54
                               ETA: 00:58:34

################################################################################
                     [1m Learning iteration 314/2000 [0m                      

                       Computation: 40267 steps/s (collection: 2.302s, learning 0.139s)
             Mean action noise std: 2.03
          Mean value_function loss: 4.2284
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 46.6442
                       Mean reward: 8.00
               Mean episode length: 178.76
    Episode_Reward/reaching_object: 0.5479
     Episode_Reward/lifting_object: 1.0304
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 2.44s
                      Time elapsed: 00:10:56
                               ETA: 00:58:34

################################################################################
                     [1m Learning iteration 315/2000 [0m                      

                       Computation: 38108 steps/s (collection: 2.446s, learning 0.134s)
             Mean action noise std: 2.03
          Mean value_function loss: 4.7505
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 46.6865
                       Mean reward: 8.43
               Mean episode length: 178.43
    Episode_Reward/reaching_object: 0.5329
     Episode_Reward/lifting_object: 0.9560
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 2.58s
                      Time elapsed: 00:10:59
                               ETA: 00:58:35

################################################################################
                     [1m Learning iteration 316/2000 [0m                      

                       Computation: 43523 steps/s (collection: 2.150s, learning 0.109s)
             Mean action noise std: 2.04
          Mean value_function loss: 4.4692
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 46.7276
                       Mean reward: 7.44
               Mean episode length: 185.86
    Episode_Reward/reaching_object: 0.5369
     Episode_Reward/lifting_object: 1.1285
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.6667
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 2.26s
                      Time elapsed: 00:11:01
                               ETA: 00:58:34

################################################################################
                     [1m Learning iteration 317/2000 [0m                      

                       Computation: 47273 steps/s (collection: 1.958s, learning 0.122s)
             Mean action noise std: 2.04
          Mean value_function loss: 3.1977
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 46.7653
                       Mean reward: 9.91
               Mean episode length: 188.37
    Episode_Reward/reaching_object: 0.5324
     Episode_Reward/lifting_object: 1.1758
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 2.08s
                      Time elapsed: 00:11:03
                               ETA: 00:58:31

################################################################################
                     [1m Learning iteration 318/2000 [0m                      

                       Computation: 46493 steps/s (collection: 1.988s, learning 0.126s)
             Mean action noise std: 2.04
          Mean value_function loss: 3.6543
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 46.7963
                       Mean reward: 6.11
               Mean episode length: 167.04
    Episode_Reward/reaching_object: 0.5171
     Episode_Reward/lifting_object: 1.0035
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.9583
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 2.11s
                      Time elapsed: 00:11:05
                               ETA: 00:58:30

################################################################################
                     [1m Learning iteration 319/2000 [0m                      

                       Computation: 48501 steps/s (collection: 1.905s, learning 0.122s)
             Mean action noise std: 2.05
          Mean value_function loss: 3.1782
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 46.8263
                       Mean reward: 8.84
               Mean episode length: 178.99
    Episode_Reward/reaching_object: 0.5366
     Episode_Reward/lifting_object: 1.1240
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 14.2917
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 2.03s
                      Time elapsed: 00:11:07
                               ETA: 00:58:27

################################################################################
                     [1m Learning iteration 320/2000 [0m                      

                       Computation: 47391 steps/s (collection: 1.946s, learning 0.128s)
             Mean action noise std: 2.05
          Mean value_function loss: 2.7650
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 46.8687
                       Mean reward: 7.75
               Mean episode length: 194.54
    Episode_Reward/reaching_object: 0.5720
     Episode_Reward/lifting_object: 1.0718
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 2.07s
                      Time elapsed: 00:11:09
                               ETA: 00:58:25

################################################################################
                     [1m Learning iteration 321/2000 [0m                      

                       Computation: 48284 steps/s (collection: 1.913s, learning 0.123s)
             Mean action noise std: 2.05
          Mean value_function loss: 3.5313
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 46.9003
                       Mean reward: 10.39
               Mean episode length: 198.74
    Episode_Reward/reaching_object: 0.5494
     Episode_Reward/lifting_object: 1.3661
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 2.04s
                      Time elapsed: 00:11:11
                               ETA: 00:58:23

################################################################################
                     [1m Learning iteration 322/2000 [0m                      

                       Computation: 48072 steps/s (collection: 1.914s, learning 0.131s)
             Mean action noise std: 2.06
          Mean value_function loss: 3.5975
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 46.9270
                       Mean reward: 9.96
               Mean episode length: 183.32
    Episode_Reward/reaching_object: 0.5582
     Episode_Reward/lifting_object: 1.1252
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 2.04s
                      Time elapsed: 00:11:13
                               ETA: 00:58:20

################################################################################
                     [1m Learning iteration 323/2000 [0m                      

                       Computation: 48118 steps/s (collection: 1.911s, learning 0.132s)
             Mean action noise std: 2.06
          Mean value_function loss: 4.2721
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 46.9693
                       Mean reward: 9.45
               Mean episode length: 187.84
    Episode_Reward/reaching_object: 0.5823
     Episode_Reward/lifting_object: 1.5608
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 2.04s
                      Time elapsed: 00:11:15
                               ETA: 00:58:18

################################################################################
                     [1m Learning iteration 324/2000 [0m                      

                       Computation: 48921 steps/s (collection: 1.880s, learning 0.129s)
             Mean action noise std: 2.06
          Mean value_function loss: 5.7030
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 47.0038
                       Mean reward: 8.85
               Mean episode length: 180.60
    Episode_Reward/reaching_object: 0.5724
     Episode_Reward/lifting_object: 1.3341
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 2.01s
                      Time elapsed: 00:11:17
                               ETA: 00:58:16

################################################################################
                     [1m Learning iteration 325/2000 [0m                      

                       Computation: 48945 steps/s (collection: 1.882s, learning 0.126s)
             Mean action noise std: 2.07
          Mean value_function loss: 5.2087
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 47.0319
                       Mean reward: 9.13
               Mean episode length: 195.03
    Episode_Reward/reaching_object: 0.5935
     Episode_Reward/lifting_object: 1.3518
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 2.01s
                      Time elapsed: 00:11:19
                               ETA: 00:58:13

################################################################################
                     [1m Learning iteration 326/2000 [0m                      

                       Computation: 48982 steps/s (collection: 1.878s, learning 0.129s)
             Mean action noise std: 2.07
          Mean value_function loss: 3.5995
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 47.0915
                       Mean reward: 10.57
               Mean episode length: 201.66
    Episode_Reward/reaching_object: 0.6021
     Episode_Reward/lifting_object: 1.5769
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 2.01s
                      Time elapsed: 00:11:21
                               ETA: 00:58:11

################################################################################
                     [1m Learning iteration 327/2000 [0m                      

                       Computation: 48190 steps/s (collection: 1.916s, learning 0.124s)
             Mean action noise std: 2.08
          Mean value_function loss: 3.7556
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 47.1416
                       Mean reward: 9.68
               Mean episode length: 180.97
    Episode_Reward/reaching_object: 0.5843
     Episode_Reward/lifting_object: 1.4544
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.9583
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 2.04s
                      Time elapsed: 00:11:23
                               ETA: 00:58:08

################################################################################
                     [1m Learning iteration 328/2000 [0m                      

                       Computation: 48792 steps/s (collection: 1.890s, learning 0.125s)
             Mean action noise std: 2.08
          Mean value_function loss: 5.3843
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 47.1725
                       Mean reward: 12.08
               Mean episode length: 191.14
    Episode_Reward/reaching_object: 0.5869
     Episode_Reward/lifting_object: 1.5696
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.7917
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 2.01s
                      Time elapsed: 00:11:25
                               ETA: 00:58:06

################################################################################
                     [1m Learning iteration 329/2000 [0m                      

                       Computation: 49054 steps/s (collection: 1.893s, learning 0.111s)
             Mean action noise std: 2.08
          Mean value_function loss: 3.8219
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 47.2090
                       Mean reward: 10.69
               Mean episode length: 205.10
    Episode_Reward/reaching_object: 0.6030
     Episode_Reward/lifting_object: 1.5028
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 2.00s
                      Time elapsed: 00:11:28
                               ETA: 00:58:03

################################################################################
                     [1m Learning iteration 330/2000 [0m                      

                       Computation: 49237 steps/s (collection: 1.887s, learning 0.109s)
             Mean action noise std: 2.09
          Mean value_function loss: 4.3627
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 47.2483
                       Mean reward: 8.61
               Mean episode length: 188.47
    Episode_Reward/reaching_object: 0.6076
     Episode_Reward/lifting_object: 1.5429
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 11.1667
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 2.00s
                      Time elapsed: 00:11:29
                               ETA: 00:58:01

################################################################################
                     [1m Learning iteration 331/2000 [0m                      

                       Computation: 49375 steps/s (collection: 1.882s, learning 0.109s)
             Mean action noise std: 2.09
          Mean value_function loss: 4.3689
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 47.2783
                       Mean reward: 11.03
               Mean episode length: 201.65
    Episode_Reward/reaching_object: 0.5977
     Episode_Reward/lifting_object: 1.5365
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 1.99s
                      Time elapsed: 00:11:31
                               ETA: 00:57:58

################################################################################
                     [1m Learning iteration 332/2000 [0m                      

                       Computation: 49203 steps/s (collection: 1.890s, learning 0.108s)
             Mean action noise std: 2.09
          Mean value_function loss: 4.6194
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 47.3153
                       Mean reward: 10.88
               Mean episode length: 194.70
    Episode_Reward/reaching_object: 0.6202
     Episode_Reward/lifting_object: 1.5639
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 2.00s
                      Time elapsed: 00:11:33
                               ETA: 00:57:56

################################################################################
                     [1m Learning iteration 333/2000 [0m                      

                       Computation: 19630 steps/s (collection: 4.880s, learning 0.128s)
             Mean action noise std: 2.10
          Mean value_function loss: 4.4357
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 47.3393
                       Mean reward: 9.51
               Mean episode length: 192.22
    Episode_Reward/reaching_object: 0.6250
     Episode_Reward/lifting_object: 1.5586
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 5.01s
                      Time elapsed: 00:11:38
                               ETA: 00:58:08

################################################################################
                     [1m Learning iteration 334/2000 [0m                      

                       Computation: 14679 steps/s (collection: 6.566s, learning 0.131s)
             Mean action noise std: 2.10
          Mean value_function loss: 4.0874
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 47.3745
                       Mean reward: 12.43
               Mean episode length: 183.52
    Episode_Reward/reaching_object: 0.6020
     Episode_Reward/lifting_object: 1.7061
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.3750
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 6.70s
                      Time elapsed: 00:11:45
                               ETA: 00:58:29

################################################################################
                     [1m Learning iteration 335/2000 [0m                      

                       Computation: 14701 steps/s (collection: 6.558s, learning 0.129s)
             Mean action noise std: 2.10
          Mean value_function loss: 4.7992
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 47.4129
                       Mean reward: 11.95
               Mean episode length: 185.08
    Episode_Reward/reaching_object: 0.6046
     Episode_Reward/lifting_object: 1.7655
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 6.69s
                      Time elapsed: 00:11:52
                               ETA: 00:58:50

################################################################################
                     [1m Learning iteration 336/2000 [0m                      

                       Computation: 14181 steps/s (collection: 6.780s, learning 0.152s)
             Mean action noise std: 2.11
          Mean value_function loss: 5.4971
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 47.4438
                       Mean reward: 9.24
               Mean episode length: 197.03
    Episode_Reward/reaching_object: 0.6015
     Episode_Reward/lifting_object: 1.6448
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 6.93s
                      Time elapsed: 00:11:59
                               ETA: 00:59:11

################################################################################
                     [1m Learning iteration 337/2000 [0m                      

                       Computation: 14679 steps/s (collection: 6.579s, learning 0.118s)
             Mean action noise std: 2.11
          Mean value_function loss: 5.2489
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.4665
                       Mean reward: 14.04
               Mean episode length: 175.88
    Episode_Reward/reaching_object: 0.5744
     Episode_Reward/lifting_object: 1.7688
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.9167
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 6.70s
                      Time elapsed: 00:12:06
                               ETA: 00:59:32

################################################################################
                     [1m Learning iteration 338/2000 [0m                      

                       Computation: 14695 steps/s (collection: 6.543s, learning 0.146s)
             Mean action noise std: 2.11
          Mean value_function loss: 6.3929
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 47.4880
                       Mean reward: 12.49
               Mean episode length: 193.69
    Episode_Reward/reaching_object: 0.6188
     Episode_Reward/lifting_object: 2.0205
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 6.69s
                      Time elapsed: 00:12:12
                               ETA: 00:59:52

################################################################################
                     [1m Learning iteration 339/2000 [0m                      

                       Computation: 14386 steps/s (collection: 6.726s, learning 0.107s)
             Mean action noise std: 2.11
          Mean value_function loss: 4.6485
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 47.5134
                       Mean reward: 12.60
               Mean episode length: 185.48
    Episode_Reward/reaching_object: 0.5887
     Episode_Reward/lifting_object: 1.7667
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.0417
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 6.83s
                      Time elapsed: 00:12:19
                               ETA: 01:00:12

################################################################################
                     [1m Learning iteration 340/2000 [0m                      

                       Computation: 14822 steps/s (collection: 6.503s, learning 0.130s)
             Mean action noise std: 2.12
          Mean value_function loss: 5.4721
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 47.5399
                       Mean reward: 13.02
               Mean episode length: 181.07
    Episode_Reward/reaching_object: 0.5764
     Episode_Reward/lifting_object: 1.9944
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 6.63s
                      Time elapsed: 00:12:26
                               ETA: 01:00:32

################################################################################
                     [1m Learning iteration 341/2000 [0m                      

                       Computation: 13795 steps/s (collection: 7.007s, learning 0.119s)
             Mean action noise std: 2.12
          Mean value_function loss: 6.1899
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 47.5592
                       Mean reward: 11.73
               Mean episode length: 163.71
    Episode_Reward/reaching_object: 0.5746
     Episode_Reward/lifting_object: 1.9901
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 7.13s
                      Time elapsed: 00:12:33
                               ETA: 01:00:54

################################################################################
                     [1m Learning iteration 342/2000 [0m                      

                       Computation: 48125 steps/s (collection: 1.927s, learning 0.116s)
             Mean action noise std: 2.12
          Mean value_function loss: 5.6130
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 47.5887
                       Mean reward: 10.81
               Mean episode length: 166.21
    Episode_Reward/reaching_object: 0.5584
     Episode_Reward/lifting_object: 1.6594
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 16.5000
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 2.04s
                      Time elapsed: 00:12:35
                               ETA: 01:00:51

################################################################################
                     [1m Learning iteration 343/2000 [0m                      

                       Computation: 51085 steps/s (collection: 1.817s, learning 0.107s)
             Mean action noise std: 2.12
          Mean value_function loss: 4.9745
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 47.6124
                       Mean reward: 13.71
               Mean episode length: 183.91
    Episode_Reward/reaching_object: 0.5771
     Episode_Reward/lifting_object: 1.8158
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 1.92s
                      Time elapsed: 00:12:37
                               ETA: 01:00:47

################################################################################
                     [1m Learning iteration 344/2000 [0m                      

                       Computation: 50304 steps/s (collection: 1.845s, learning 0.110s)
             Mean action noise std: 2.13
          Mean value_function loss: 5.5653
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 47.6467
                       Mean reward: 11.65
               Mean episode length: 181.04
    Episode_Reward/reaching_object: 0.5832
     Episode_Reward/lifting_object: 1.8889
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 1.95s
                      Time elapsed: 00:12:39
                               ETA: 01:00:44

################################################################################
                     [1m Learning iteration 345/2000 [0m                      

                       Computation: 48440 steps/s (collection: 1.920s, learning 0.110s)
             Mean action noise std: 2.13
          Mean value_function loss: 5.4693
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 47.6793
                       Mean reward: 14.73
               Mean episode length: 171.83
    Episode_Reward/reaching_object: 0.5820
     Episode_Reward/lifting_object: 2.2636
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.7083
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 2.03s
                      Time elapsed: 00:12:41
                               ETA: 01:00:41

################################################################################
                     [1m Learning iteration 346/2000 [0m                      

                       Computation: 49252 steps/s (collection: 1.897s, learning 0.099s)
             Mean action noise std: 2.13
          Mean value_function loss: 6.7843
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 47.6993
                       Mean reward: 11.24
               Mean episode length: 186.81
    Episode_Reward/reaching_object: 0.5932
     Episode_Reward/lifting_object: 2.0981
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.4583
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 2.00s
                      Time elapsed: 00:12:43
                               ETA: 01:00:38

################################################################################
                     [1m Learning iteration 347/2000 [0m                      

                       Computation: 49726 steps/s (collection: 1.865s, learning 0.112s)
             Mean action noise std: 2.13
          Mean value_function loss: 6.2252
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 47.7182
                       Mean reward: 14.20
               Mean episode length: 183.30
    Episode_Reward/reaching_object: 0.5802
     Episode_Reward/lifting_object: 2.1288
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 1.98s
                      Time elapsed: 00:12:45
                               ETA: 01:00:34

################################################################################
                     [1m Learning iteration 348/2000 [0m                      

                       Computation: 50349 steps/s (collection: 1.828s, learning 0.125s)
             Mean action noise std: 2.14
          Mean value_function loss: 7.0208
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 47.7337
                       Mean reward: 12.24
               Mean episode length: 175.91
    Episode_Reward/reaching_object: 0.6097
     Episode_Reward/lifting_object: 2.2947
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.4167
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 1.95s
                      Time elapsed: 00:12:47
                               ETA: 01:00:31

################################################################################
                     [1m Learning iteration 349/2000 [0m                      

                       Computation: 49979 steps/s (collection: 1.853s, learning 0.114s)
             Mean action noise std: 2.14
          Mean value_function loss: 6.7532
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 47.7468
                       Mean reward: 14.50
               Mean episode length: 185.00
    Episode_Reward/reaching_object: 0.5786
     Episode_Reward/lifting_object: 2.1774
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.8333
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 1.97s
                      Time elapsed: 00:12:49
                               ETA: 01:00:28

################################################################################
                     [1m Learning iteration 350/2000 [0m                      

                       Computation: 50377 steps/s (collection: 1.830s, learning 0.122s)
             Mean action noise std: 2.14
          Mean value_function loss: 6.9495
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 47.7718
                       Mean reward: 14.37
               Mean episode length: 167.03
    Episode_Reward/reaching_object: 0.5909
     Episode_Reward/lifting_object: 2.2396
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 16.6250
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 1.95s
                      Time elapsed: 00:12:51
                               ETA: 01:00:24

################################################################################
                     [1m Learning iteration 351/2000 [0m                      

                       Computation: 48740 steps/s (collection: 1.899s, learning 0.118s)
             Mean action noise std: 2.14
          Mean value_function loss: 8.2320
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 47.7996
                       Mean reward: 11.02
               Mean episode length: 173.66
    Episode_Reward/reaching_object: 0.5994
     Episode_Reward/lifting_object: 2.0377
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.9167
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 2.02s
                      Time elapsed: 00:12:53
                               ETA: 01:00:21

################################################################################
                     [1m Learning iteration 352/2000 [0m                      

                       Computation: 49952 steps/s (collection: 1.857s, learning 0.111s)
             Mean action noise std: 2.14
          Mean value_function loss: 7.3968
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 47.8131
                       Mean reward: 17.63
               Mean episode length: 173.31
    Episode_Reward/reaching_object: 0.5727
     Episode_Reward/lifting_object: 2.3601
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.0417
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 1.97s
                      Time elapsed: 00:12:55
                               ETA: 01:00:18

################################################################################
                     [1m Learning iteration 353/2000 [0m                      

                       Computation: 50369 steps/s (collection: 1.839s, learning 0.113s)
             Mean action noise std: 2.15
          Mean value_function loss: 7.0440
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 47.8376
                       Mean reward: 17.05
               Mean episode length: 163.65
    Episode_Reward/reaching_object: 0.5850
     Episode_Reward/lifting_object: 2.4731
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 16.8750
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 1.95s
                      Time elapsed: 00:12:57
                               ETA: 01:00:15

################################################################################
                     [1m Learning iteration 354/2000 [0m                      

                       Computation: 50682 steps/s (collection: 1.827s, learning 0.113s)
             Mean action noise std: 2.15
          Mean value_function loss: 7.1553
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 47.8583
                       Mean reward: 13.99
               Mean episode length: 159.06
    Episode_Reward/reaching_object: 0.5742
     Episode_Reward/lifting_object: 2.3635
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.0417
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 1.94s
                      Time elapsed: 00:12:58
                               ETA: 01:00:11

################################################################################
                     [1m Learning iteration 355/2000 [0m                      

                       Computation: 48568 steps/s (collection: 1.906s, learning 0.118s)
             Mean action noise std: 2.15
          Mean value_function loss: 13.4517
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.8696
                       Mean reward: 12.91
               Mean episode length: 148.80
    Episode_Reward/reaching_object: 0.5575
     Episode_Reward/lifting_object: 2.3182
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.1250
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 2.02s
                      Time elapsed: 00:13:00
                               ETA: 01:00:08

################################################################################
                     [1m Learning iteration 356/2000 [0m                      

                       Computation: 47662 steps/s (collection: 1.938s, learning 0.125s)
             Mean action noise std: 2.15
          Mean value_function loss: 8.3974
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 47.8824
                       Mean reward: 14.08
               Mean episode length: 160.54
    Episode_Reward/reaching_object: 0.5729
     Episode_Reward/lifting_object: 2.4294
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 19.1667
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 2.06s
                      Time elapsed: 00:13:03
                               ETA: 01:00:05

################################################################################
                     [1m Learning iteration 357/2000 [0m                      

                       Computation: 48345 steps/s (collection: 1.922s, learning 0.111s)
             Mean action noise std: 2.15
          Mean value_function loss: 7.6349
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 47.8942
                       Mean reward: 17.61
               Mean episode length: 163.71
    Episode_Reward/reaching_object: 0.5556
     Episode_Reward/lifting_object: 2.4259
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 21.3333
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 2.03s
                      Time elapsed: 00:13:05
                               ETA: 01:00:03

################################################################################
                     [1m Learning iteration 358/2000 [0m                      

                       Computation: 50286 steps/s (collection: 1.830s, learning 0.125s)
             Mean action noise std: 2.16
          Mean value_function loss: 9.6450
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 47.9185
                       Mean reward: 16.11
               Mean episode length: 162.50
    Episode_Reward/reaching_object: 0.5594
     Episode_Reward/lifting_object: 2.2577
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.8333
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 1.95s
                      Time elapsed: 00:13:07
                               ETA: 00:59:59

################################################################################
                     [1m Learning iteration 359/2000 [0m                      

                       Computation: 49257 steps/s (collection: 1.877s, learning 0.119s)
             Mean action noise std: 2.16
          Mean value_function loss: 10.2957
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 47.9399
                       Mean reward: 15.64
               Mean episode length: 167.79
    Episode_Reward/reaching_object: 0.5598
     Episode_Reward/lifting_object: 2.3392
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 20.0417
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 2.00s
                      Time elapsed: 00:13:09
                               ETA: 00:59:56

################################################################################
                     [1m Learning iteration 360/2000 [0m                      

                       Computation: 50522 steps/s (collection: 1.842s, learning 0.104s)
             Mean action noise std: 2.16
          Mean value_function loss: 11.8353
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 47.9573
                       Mean reward: 16.05
               Mean episode length: 159.84
    Episode_Reward/reaching_object: 0.5641
     Episode_Reward/lifting_object: 2.7426
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 19.3750
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 1.95s
                      Time elapsed: 00:13:10
                               ETA: 00:59:53

################################################################################
                     [1m Learning iteration 361/2000 [0m                      

                       Computation: 50688 steps/s (collection: 1.831s, learning 0.109s)
             Mean action noise std: 2.16
          Mean value_function loss: 10.8815
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 47.9859
                       Mean reward: 13.41
               Mean episode length: 146.90
    Episode_Reward/reaching_object: 0.5383
     Episode_Reward/lifting_object: 2.3174
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 20.2083
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 1.94s
                      Time elapsed: 00:13:12
                               ETA: 00:59:50

################################################################################
                     [1m Learning iteration 362/2000 [0m                      

                       Computation: 50064 steps/s (collection: 1.848s, learning 0.116s)
             Mean action noise std: 2.17
          Mean value_function loss: 8.6965
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 48.0142
                       Mean reward: 19.35
               Mean episode length: 161.24
    Episode_Reward/reaching_object: 0.5632
     Episode_Reward/lifting_object: 2.6996
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 20.7083
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 1.96s
                      Time elapsed: 00:13:14
                               ETA: 00:59:46

################################################################################
                     [1m Learning iteration 363/2000 [0m                      

                       Computation: 50606 steps/s (collection: 1.847s, learning 0.095s)
             Mean action noise std: 2.17
          Mean value_function loss: 12.6350
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 48.0272
                       Mean reward: 18.62
               Mean episode length: 151.58
    Episode_Reward/reaching_object: 0.5647
     Episode_Reward/lifting_object: 2.9572
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 19.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 1.94s
                      Time elapsed: 00:13:16
                               ETA: 00:59:43

################################################################################
                     [1m Learning iteration 364/2000 [0m                      

                       Computation: 49968 steps/s (collection: 1.857s, learning 0.110s)
             Mean action noise std: 2.17
          Mean value_function loss: 10.1665
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 48.0436
                       Mean reward: 14.91
               Mean episode length: 141.24
    Episode_Reward/reaching_object: 0.5415
     Episode_Reward/lifting_object: 2.3890
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 23.8750
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 1.97s
                      Time elapsed: 00:13:18
                               ETA: 00:59:40

################################################################################
                     [1m Learning iteration 365/2000 [0m                      

                       Computation: 49998 steps/s (collection: 1.852s, learning 0.114s)
             Mean action noise std: 2.17
          Mean value_function loss: 9.8477
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 48.0653
                       Mean reward: 16.81
               Mean episode length: 136.93
    Episode_Reward/reaching_object: 0.5387
     Episode_Reward/lifting_object: 2.8316
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 22.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 1.97s
                      Time elapsed: 00:13:20
                               ETA: 00:59:37

################################################################################
                     [1m Learning iteration 366/2000 [0m                      

                       Computation: 50071 steps/s (collection: 1.852s, learning 0.111s)
             Mean action noise std: 2.17
          Mean value_function loss: 9.7994
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 48.0820
                       Mean reward: 21.85
               Mean episode length: 154.82
    Episode_Reward/reaching_object: 0.5388
     Episode_Reward/lifting_object: 2.9627
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 21.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 1.96s
                      Time elapsed: 00:13:22
                               ETA: 00:59:33

################################################################################
                     [1m Learning iteration 367/2000 [0m                      

                       Computation: 50557 steps/s (collection: 1.832s, learning 0.112s)
             Mean action noise std: 2.18
          Mean value_function loss: 10.9858
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 48.1065
                       Mean reward: 19.45
               Mean episode length: 153.41
    Episode_Reward/reaching_object: 0.5527
     Episode_Reward/lifting_object: 2.6490
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 21.4167
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 1.94s
                      Time elapsed: 00:13:24
                               ETA: 00:59:30

################################################################################
                     [1m Learning iteration 368/2000 [0m                      

                       Computation: 49572 steps/s (collection: 1.871s, learning 0.112s)
             Mean action noise std: 2.18
          Mean value_function loss: 12.5315
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 48.1275
                       Mean reward: 18.91
               Mean episode length: 149.82
    Episode_Reward/reaching_object: 0.5583
     Episode_Reward/lifting_object: 3.0529
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 19.8750
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 1.98s
                      Time elapsed: 00:13:26
                               ETA: 00:59:27

################################################################################
                     [1m Learning iteration 369/2000 [0m                      

                       Computation: 50074 steps/s (collection: 1.855s, learning 0.109s)
             Mean action noise std: 2.18
          Mean value_function loss: 10.3777
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 48.1373
                       Mean reward: 18.35
               Mean episode length: 144.67
    Episode_Reward/reaching_object: 0.5507
     Episode_Reward/lifting_object: 3.1765
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 19.5833
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 1.96s
                      Time elapsed: 00:13:28
                               ETA: 00:59:24

################################################################################
                     [1m Learning iteration 370/2000 [0m                      

                       Computation: 50164 steps/s (collection: 1.862s, learning 0.098s)
             Mean action noise std: 2.18
          Mean value_function loss: 13.4398
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 48.1455
                       Mean reward: 19.14
               Mean episode length: 139.63
    Episode_Reward/reaching_object: 0.5680
     Episode_Reward/lifting_object: 3.1532
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 18.7083
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 1.96s
                      Time elapsed: 00:13:30
                               ETA: 00:59:21

################################################################################
                     [1m Learning iteration 371/2000 [0m                      

                       Computation: 50394 steps/s (collection: 1.864s, learning 0.087s)
             Mean action noise std: 2.18
          Mean value_function loss: 11.5647
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 48.1625
                       Mean reward: 19.13
               Mean episode length: 155.49
    Episode_Reward/reaching_object: 0.5808
     Episode_Reward/lifting_object: 3.4656
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.7917
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 1.95s
                      Time elapsed: 00:13:32
                               ETA: 00:59:18

################################################################################
                     [1m Learning iteration 372/2000 [0m                      

                       Computation: 50684 steps/s (collection: 1.827s, learning 0.113s)
             Mean action noise std: 2.18
          Mean value_function loss: 14.9453
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 48.1841
                       Mean reward: 20.59
               Mean episode length: 168.46
    Episode_Reward/reaching_object: 0.5643
     Episode_Reward/lifting_object: 3.3603
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 18.1667
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 1.94s
                      Time elapsed: 00:13:34
                               ETA: 00:59:14

################################################################################
                     [1m Learning iteration 373/2000 [0m                      

                       Computation: 49761 steps/s (collection: 1.854s, learning 0.122s)
             Mean action noise std: 2.18
          Mean value_function loss: 11.1235
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 48.1990
                       Mean reward: 19.41
               Mean episode length: 151.52
    Episode_Reward/reaching_object: 0.5815
     Episode_Reward/lifting_object: 3.3385
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.3750
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 1.98s
                      Time elapsed: 00:13:36
                               ETA: 00:59:11

################################################################################
                     [1m Learning iteration 374/2000 [0m                      

                       Computation: 50296 steps/s (collection: 1.843s, learning 0.112s)
             Mean action noise std: 2.19
          Mean value_function loss: 15.5398
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 48.2073
                       Mean reward: 24.85
               Mean episode length: 166.40
    Episode_Reward/reaching_object: 0.5917
     Episode_Reward/lifting_object: 3.6549
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 20.2917
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 1.95s
                      Time elapsed: 00:13:38
                               ETA: 00:59:08

################################################################################
                     [1m Learning iteration 375/2000 [0m                      

                       Computation: 49595 steps/s (collection: 1.868s, learning 0.114s)
             Mean action noise std: 2.19
          Mean value_function loss: 10.2249
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 48.2097
                       Mean reward: 26.89
               Mean episode length: 172.62
    Episode_Reward/reaching_object: 0.6227
     Episode_Reward/lifting_object: 4.1448
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 16.7500
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 1.98s
                      Time elapsed: 00:13:40
                               ETA: 00:59:05

################################################################################
                     [1m Learning iteration 376/2000 [0m                      

                       Computation: 49409 steps/s (collection: 1.883s, learning 0.107s)
             Mean action noise std: 2.19
          Mean value_function loss: 16.7469
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 48.2141
                       Mean reward: 19.57
               Mean episode length: 165.26
    Episode_Reward/reaching_object: 0.5880
     Episode_Reward/lifting_object: 3.6452
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 20.0417
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 1.99s
                      Time elapsed: 00:13:42
                               ETA: 00:59:02

################################################################################
                     [1m Learning iteration 377/2000 [0m                      

                       Computation: 48108 steps/s (collection: 1.935s, learning 0.109s)
             Mean action noise std: 2.19
          Mean value_function loss: 16.0600
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.2285
                       Mean reward: 18.98
               Mean episode length: 158.14
    Episode_Reward/reaching_object: 0.6204
     Episode_Reward/lifting_object: 3.6448
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.2083
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 2.04s
                      Time elapsed: 00:13:44
                               ETA: 00:58:59

################################################################################
                     [1m Learning iteration 378/2000 [0m                      

                       Computation: 48607 steps/s (collection: 1.908s, learning 0.114s)
             Mean action noise std: 2.19
          Mean value_function loss: 11.1677
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 48.2440
                       Mean reward: 19.73
               Mean episode length: 142.33
    Episode_Reward/reaching_object: 0.5930
     Episode_Reward/lifting_object: 3.7283
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 20.0417
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 2.02s
                      Time elapsed: 00:13:46
                               ETA: 00:58:56

################################################################################
                     [1m Learning iteration 379/2000 [0m                      

                       Computation: 48471 steps/s (collection: 1.917s, learning 0.112s)
             Mean action noise std: 2.19
          Mean value_function loss: 11.7723
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 48.2524
                       Mean reward: 22.73
               Mean episode length: 157.75
    Episode_Reward/reaching_object: 0.5975
     Episode_Reward/lifting_object: 4.1080
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 18.8750
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 2.03s
                      Time elapsed: 00:13:48
                               ETA: 00:58:53

################################################################################
                     [1m Learning iteration 380/2000 [0m                      

                       Computation: 47174 steps/s (collection: 1.969s, learning 0.115s)
             Mean action noise std: 2.19
          Mean value_function loss: 11.8377
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 48.2594
                       Mean reward: 26.19
               Mean episode length: 156.49
    Episode_Reward/reaching_object: 0.6183
     Episode_Reward/lifting_object: 4.2059
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.9583
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 2.08s
                      Time elapsed: 00:13:50
                               ETA: 00:58:51

################################################################################
                     [1m Learning iteration 381/2000 [0m                      

                       Computation: 48019 steps/s (collection: 1.931s, learning 0.117s)
             Mean action noise std: 2.19
          Mean value_function loss: 11.8714
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 48.2711
                       Mean reward: 23.07
               Mean episode length: 156.53
    Episode_Reward/reaching_object: 0.5902
     Episode_Reward/lifting_object: 3.6124
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.3750
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 2.05s
                      Time elapsed: 00:13:52
                               ETA: 00:58:48

################################################################################
                     [1m Learning iteration 382/2000 [0m                      

                       Computation: 49041 steps/s (collection: 1.898s, learning 0.107s)
             Mean action noise std: 2.19
          Mean value_function loss: 14.0157
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 48.2794
                       Mean reward: 17.51
               Mean episode length: 166.23
    Episode_Reward/reaching_object: 0.6045
     Episode_Reward/lifting_object: 4.0459
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.4167
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 2.00s
                      Time elapsed: 00:13:54
                               ETA: 00:58:45

################################################################################
                     [1m Learning iteration 383/2000 [0m                      

                       Computation: 49946 steps/s (collection: 1.858s, learning 0.111s)
             Mean action noise std: 2.19
          Mean value_function loss: 14.5974
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 48.2860
                       Mean reward: 23.74
               Mean episode length: 152.79
    Episode_Reward/reaching_object: 0.6114
     Episode_Reward/lifting_object: 4.2940
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 17.6250
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 1.97s
                      Time elapsed: 00:13:56
                               ETA: 00:58:42

################################################################################
                     [1m Learning iteration 384/2000 [0m                      

                       Computation: 50103 steps/s (collection: 1.853s, learning 0.109s)
             Mean action noise std: 2.20
          Mean value_function loss: 16.1963
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 48.2998
                       Mean reward: 22.69
               Mean episode length: 142.84
    Episode_Reward/reaching_object: 0.6011
     Episode_Reward/lifting_object: 4.7860
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 20.7083
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 1.96s
                      Time elapsed: 00:13:58
                               ETA: 00:58:39

################################################################################
                     [1m Learning iteration 385/2000 [0m                      

                       Computation: 49555 steps/s (collection: 1.870s, learning 0.114s)
             Mean action noise std: 2.20
          Mean value_function loss: 13.7831
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 48.3199
                       Mean reward: 25.53
               Mean episode length: 160.39
    Episode_Reward/reaching_object: 0.6211
     Episode_Reward/lifting_object: 4.6501
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 1.98s
                      Time elapsed: 00:14:00
                               ETA: 00:58:36

################################################################################
                     [1m Learning iteration 386/2000 [0m                      

                       Computation: 49299 steps/s (collection: 1.878s, learning 0.116s)
             Mean action noise std: 2.20
          Mean value_function loss: 14.1384
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 48.3313
                       Mean reward: 24.94
               Mean episode length: 159.92
    Episode_Reward/reaching_object: 0.6115
     Episode_Reward/lifting_object: 4.4367
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 18.4167
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 1.99s
                      Time elapsed: 00:14:02
                               ETA: 00:58:33

################################################################################
                     [1m Learning iteration 387/2000 [0m                      

                       Computation: 49004 steps/s (collection: 1.897s, learning 0.109s)
             Mean action noise std: 2.20
          Mean value_function loss: 14.6767
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 48.3372
                       Mean reward: 33.77
               Mean episode length: 168.24
    Episode_Reward/reaching_object: 0.6229
     Episode_Reward/lifting_object: 5.3875
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.7083
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 2.01s
                      Time elapsed: 00:14:04
                               ETA: 00:58:30

################################################################################
                     [1m Learning iteration 388/2000 [0m                      

                       Computation: 49064 steps/s (collection: 1.884s, learning 0.120s)
             Mean action noise std: 2.20
          Mean value_function loss: 17.2541
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 48.3507
                       Mean reward: 28.84
               Mean episode length: 178.78
    Episode_Reward/reaching_object: 0.6409
     Episode_Reward/lifting_object: 4.9148
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.7500
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 2.00s
                      Time elapsed: 00:14:06
                               ETA: 00:58:27

################################################################################
                     [1m Learning iteration 389/2000 [0m                      

                       Computation: 48011 steps/s (collection: 1.929s, learning 0.119s)
             Mean action noise std: 2.20
          Mean value_function loss: 15.8248
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 48.3635
                       Mean reward: 23.53
               Mean episode length: 150.66
    Episode_Reward/reaching_object: 0.6252
     Episode_Reward/lifting_object: 4.5127
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.9583
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 2.05s
                      Time elapsed: 00:14:08
                               ETA: 00:58:25

################################################################################
                     [1m Learning iteration 390/2000 [0m                      

                       Computation: 49209 steps/s (collection: 1.884s, learning 0.114s)
             Mean action noise std: 2.20
          Mean value_function loss: 16.2539
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 48.3673
                       Mean reward: 27.76
               Mean episode length: 160.59
    Episode_Reward/reaching_object: 0.6430
     Episode_Reward/lifting_object: 5.0307
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 20.0417
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 2.00s
                      Time elapsed: 00:14:10
                               ETA: 00:58:22

################################################################################
                     [1m Learning iteration 391/2000 [0m                      

                       Computation: 49034 steps/s (collection: 1.895s, learning 0.110s)
             Mean action noise std: 2.21
          Mean value_function loss: 14.3409
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 48.3751
                       Mean reward: 28.77
               Mean episode length: 156.40
    Episode_Reward/reaching_object: 0.6430
     Episode_Reward/lifting_object: 4.9642
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 16.6667
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 2.00s
                      Time elapsed: 00:14:12
                               ETA: 00:58:19

################################################################################
                     [1m Learning iteration 392/2000 [0m                      

                       Computation: 48932 steps/s (collection: 1.900s, learning 0.109s)
             Mean action noise std: 2.21
          Mean value_function loss: 15.4023
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 48.3875
                       Mean reward: 30.60
               Mean episode length: 160.88
    Episode_Reward/reaching_object: 0.6624
     Episode_Reward/lifting_object: 5.3898
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.8333
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 2.01s
                      Time elapsed: 00:14:14
                               ETA: 00:58:16

################################################################################
                     [1m Learning iteration 393/2000 [0m                      

                       Computation: 50295 steps/s (collection: 1.847s, learning 0.107s)
             Mean action noise std: 2.21
          Mean value_function loss: 16.4227
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 48.3982
                       Mean reward: 29.91
               Mean episode length: 168.25
    Episode_Reward/reaching_object: 0.6497
     Episode_Reward/lifting_object: 5.6270
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 1.95s
                      Time elapsed: 00:14:16
                               ETA: 00:58:13

################################################################################
                     [1m Learning iteration 394/2000 [0m                      

                       Computation: 50014 steps/s (collection: 1.864s, learning 0.101s)
             Mean action noise std: 2.21
          Mean value_function loss: 18.9689
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 48.4046
                       Mean reward: 29.41
               Mean episode length: 161.76
    Episode_Reward/reaching_object: 0.6716
     Episode_Reward/lifting_object: 5.5520
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.3750
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 1.97s
                      Time elapsed: 00:14:18
                               ETA: 00:58:10

################################################################################
                     [1m Learning iteration 395/2000 [0m                      

                       Computation: 50256 steps/s (collection: 1.863s, learning 0.093s)
             Mean action noise std: 2.21
          Mean value_function loss: 15.6197
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 48.4159
                       Mean reward: 31.44
               Mean episode length: 159.33
    Episode_Reward/reaching_object: 0.6608
     Episode_Reward/lifting_object: 5.5057
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 18.9583
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 1.96s
                      Time elapsed: 00:14:20
                               ETA: 00:58:07

################################################################################
                     [1m Learning iteration 396/2000 [0m                      

                       Computation: 48280 steps/s (collection: 1.943s, learning 0.094s)
             Mean action noise std: 2.21
          Mean value_function loss: 16.3677
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 48.4308
                       Mean reward: 34.98
               Mean episode length: 178.05
    Episode_Reward/reaching_object: 0.6621
     Episode_Reward/lifting_object: 5.8858
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 18.2083
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 2.04s
                      Time elapsed: 00:14:22
                               ETA: 00:58:04

################################################################################
                     [1m Learning iteration 397/2000 [0m                      

                       Computation: 48955 steps/s (collection: 1.921s, learning 0.087s)
             Mean action noise std: 2.21
          Mean value_function loss: 16.5560
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 48.4434
                       Mean reward: 36.88
               Mean episode length: 164.09
    Episode_Reward/reaching_object: 0.6460
     Episode_Reward/lifting_object: 6.0848
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 18.6250
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 2.01s
                      Time elapsed: 00:14:24
                               ETA: 00:58:01

################################################################################
                     [1m Learning iteration 398/2000 [0m                      

                       Computation: 49271 steps/s (collection: 1.901s, learning 0.094s)
             Mean action noise std: 2.21
          Mean value_function loss: 19.6571
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 48.4512
                       Mean reward: 34.20
               Mean episode length: 157.09
    Episode_Reward/reaching_object: 0.6327
     Episode_Reward/lifting_object: 5.9688
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 18.8750
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 2.00s
                      Time elapsed: 00:14:26
                               ETA: 00:57:58

################################################################################
                     [1m Learning iteration 399/2000 [0m                      

                       Computation: 47426 steps/s (collection: 1.979s, learning 0.094s)
             Mean action noise std: 2.21
          Mean value_function loss: 16.8281
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 48.4538
                       Mean reward: 32.01
               Mean episode length: 161.80
    Episode_Reward/reaching_object: 0.6682
     Episode_Reward/lifting_object: 6.1533
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.9167
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 2.07s
                      Time elapsed: 00:14:28
                               ETA: 00:57:56

################################################################################
                     [1m Learning iteration 400/2000 [0m                      

                       Computation: 49548 steps/s (collection: 1.896s, learning 0.088s)
             Mean action noise std: 2.21
          Mean value_function loss: 17.2735
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 48.4550
                       Mean reward: 37.72
               Mean episode length: 169.30
    Episode_Reward/reaching_object: 0.6360
     Episode_Reward/lifting_object: 5.8569
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.7083
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 1.98s
                      Time elapsed: 00:14:30
                               ETA: 00:57:53

################################################################################
                     [1m Learning iteration 401/2000 [0m                      

                       Computation: 49127 steps/s (collection: 1.910s, learning 0.091s)
             Mean action noise std: 2.21
          Mean value_function loss: 17.0684
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 48.4624
                       Mean reward: 30.74
               Mean episode length: 143.69
    Episode_Reward/reaching_object: 0.6389
     Episode_Reward/lifting_object: 6.0581
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.4583
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 2.00s
                      Time elapsed: 00:14:32
                               ETA: 00:57:50

################################################################################
                     [1m Learning iteration 402/2000 [0m                      

                       Computation: 49564 steps/s (collection: 1.896s, learning 0.087s)
             Mean action noise std: 2.22
          Mean value_function loss: 24.6097
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.4650
                       Mean reward: 35.96
               Mean episode length: 170.48
    Episode_Reward/reaching_object: 0.6235
     Episode_Reward/lifting_object: 6.2340
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 17.9167
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 1.98s
                      Time elapsed: 00:14:34
                               ETA: 00:57:47

################################################################################
                     [1m Learning iteration 403/2000 [0m                      

                       Computation: 49271 steps/s (collection: 1.900s, learning 0.095s)
             Mean action noise std: 2.22
          Mean value_function loss: 20.5249
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 48.4740
                       Mean reward: 34.86
               Mean episode length: 146.64
    Episode_Reward/reaching_object: 0.6019
     Episode_Reward/lifting_object: 6.2625
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 19.9167
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 2.00s
                      Time elapsed: 00:14:36
                               ETA: 00:57:44

################################################################################
                     [1m Learning iteration 404/2000 [0m                      

                       Computation: 45652 steps/s (collection: 2.050s, learning 0.104s)
             Mean action noise std: 2.22
          Mean value_function loss: 21.3360
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 48.4866
                       Mean reward: 31.68
               Mean episode length: 161.10
    Episode_Reward/reaching_object: 0.6127
     Episode_Reward/lifting_object: 6.1963
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 20.5833
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 2.15s
                      Time elapsed: 00:14:38
                               ETA: 00:57:42

################################################################################
                     [1m Learning iteration 405/2000 [0m                      

                       Computation: 48570 steps/s (collection: 1.929s, learning 0.095s)
             Mean action noise std: 2.22
          Mean value_function loss: 30.6476
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 48.4989
                       Mean reward: 38.01
               Mean episode length: 167.07
    Episode_Reward/reaching_object: 0.6249
     Episode_Reward/lifting_object: 6.3832
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 21.7083
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 2.02s
                      Time elapsed: 00:14:40
                               ETA: 00:57:39

################################################################################
                     [1m Learning iteration 406/2000 [0m                      

                       Computation: 48286 steps/s (collection: 1.942s, learning 0.094s)
             Mean action noise std: 2.22
          Mean value_function loss: 24.8939
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 48.5140
                       Mean reward: 36.36
               Mean episode length: 163.70
    Episode_Reward/reaching_object: 0.6209
     Episode_Reward/lifting_object: 6.1101
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 19.6667
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 2.04s
                      Time elapsed: 00:14:42
                               ETA: 00:57:37

################################################################################
                     [1m Learning iteration 407/2000 [0m                      

                       Computation: 48292 steps/s (collection: 1.936s, learning 0.100s)
             Mean action noise std: 2.22
          Mean value_function loss: 20.3608
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 48.5257
                       Mean reward: 32.60
               Mean episode length: 141.97
    Episode_Reward/reaching_object: 0.6341
     Episode_Reward/lifting_object: 6.3646
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 21.0833
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 2.04s
                      Time elapsed: 00:14:44
                               ETA: 00:57:34

################################################################################
                     [1m Learning iteration 408/2000 [0m                      

                       Computation: 48830 steps/s (collection: 1.905s, learning 0.108s)
             Mean action noise std: 2.22
          Mean value_function loss: 22.5293
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 48.5327
                       Mean reward: 38.50
               Mean episode length: 156.83
    Episode_Reward/reaching_object: 0.6069
     Episode_Reward/lifting_object: 6.3302
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 20.5000
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 2.01s
                      Time elapsed: 00:14:46
                               ETA: 00:57:31

################################################################################
                     [1m Learning iteration 409/2000 [0m                      

                       Computation: 49350 steps/s (collection: 1.902s, learning 0.090s)
             Mean action noise std: 2.22
          Mean value_function loss: 30.0925
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 48.5404
                       Mean reward: 35.47
               Mean episode length: 140.67
    Episode_Reward/reaching_object: 0.5871
     Episode_Reward/lifting_object: 6.4856
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 22.6250
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 1.99s
                      Time elapsed: 00:14:48
                               ETA: 00:57:28

################################################################################
                     [1m Learning iteration 410/2000 [0m                      

                       Computation: 48580 steps/s (collection: 1.920s, learning 0.104s)
             Mean action noise std: 2.22
          Mean value_function loss: 19.8211
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 48.5466
                       Mean reward: 34.56
               Mean episode length: 148.88
    Episode_Reward/reaching_object: 0.6329
     Episode_Reward/lifting_object: 7.0976
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 20.6250
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 2.02s
                      Time elapsed: 00:14:50
                               ETA: 00:57:26

################################################################################
                     [1m Learning iteration 411/2000 [0m                      

                       Computation: 49546 steps/s (collection: 1.876s, learning 0.109s)
             Mean action noise std: 2.23
          Mean value_function loss: 31.1522
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 48.5498
                       Mean reward: 43.88
               Mean episode length: 152.22
    Episode_Reward/reaching_object: 0.6035
     Episode_Reward/lifting_object: 7.1992
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.2500
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 1.98s
                      Time elapsed: 00:14:52
                               ETA: 00:57:23

################################################################################
                     [1m Learning iteration 412/2000 [0m                      

                       Computation: 47860 steps/s (collection: 1.941s, learning 0.113s)
             Mean action noise std: 2.23
          Mean value_function loss: 26.8861
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 48.5539
                       Mean reward: 36.10
               Mean episode length: 137.75
    Episode_Reward/reaching_object: 0.6017
     Episode_Reward/lifting_object: 7.0180
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 21.2917
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 2.05s
                      Time elapsed: 00:14:54
                               ETA: 00:57:20

################################################################################
                     [1m Learning iteration 413/2000 [0m                      

                       Computation: 48978 steps/s (collection: 1.892s, learning 0.116s)
             Mean action noise std: 2.23
          Mean value_function loss: 24.2458
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 48.5610
                       Mean reward: 34.96
               Mean episode length: 154.29
    Episode_Reward/reaching_object: 0.6054
     Episode_Reward/lifting_object: 6.8679
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 21.1250
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 2.01s
                      Time elapsed: 00:14:56
                               ETA: 00:57:17

################################################################################
                     [1m Learning iteration 414/2000 [0m                      

                       Computation: 47520 steps/s (collection: 1.961s, learning 0.108s)
             Mean action noise std: 2.23
          Mean value_function loss: 24.9404
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.5669
                       Mean reward: 39.29
               Mean episode length: 150.05
    Episode_Reward/reaching_object: 0.5989
     Episode_Reward/lifting_object: 6.8064
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 23.1250
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 2.07s
                      Time elapsed: 00:14:58
                               ETA: 00:57:15

################################################################################
                     [1m Learning iteration 415/2000 [0m                      

                       Computation: 45039 steps/s (collection: 2.021s, learning 0.161s)
             Mean action noise std: 2.23
          Mean value_function loss: 30.2371
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 48.5712
                       Mean reward: 36.79
               Mean episode length: 139.55
    Episode_Reward/reaching_object: 0.6289
     Episode_Reward/lifting_object: 7.1202
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 20.6250
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 2.18s
                      Time elapsed: 00:15:01
                               ETA: 00:57:13

################################################################################
                     [1m Learning iteration 416/2000 [0m                      

                       Computation: 39692 steps/s (collection: 2.308s, learning 0.169s)
             Mean action noise std: 2.23
          Mean value_function loss: 28.0389
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 48.5773
                       Mean reward: 37.36
               Mean episode length: 152.20
    Episode_Reward/reaching_object: 0.6504
     Episode_Reward/lifting_object: 7.4276
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 20.7500
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 2.48s
                      Time elapsed: 00:15:03
                               ETA: 00:57:12

################################################################################
                     [1m Learning iteration 417/2000 [0m                      

                       Computation: 48497 steps/s (collection: 1.932s, learning 0.095s)
             Mean action noise std: 2.23
          Mean value_function loss: 29.7326
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.5851
                       Mean reward: 39.89
               Mean episode length: 155.44
    Episode_Reward/reaching_object: 0.6287
     Episode_Reward/lifting_object: 7.4231
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 22.7917
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 2.03s
                      Time elapsed: 00:15:05
                               ETA: 00:57:09

################################################################################
                     [1m Learning iteration 418/2000 [0m                      

                       Computation: 45132 steps/s (collection: 2.031s, learning 0.148s)
             Mean action noise std: 2.23
          Mean value_function loss: 32.9982
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 48.5915
                       Mean reward: 47.56
               Mean episode length: 152.04
    Episode_Reward/reaching_object: 0.6251
     Episode_Reward/lifting_object: 7.3664
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 21.6250
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 2.18s
                      Time elapsed: 00:15:07
                               ETA: 00:57:07

################################################################################
                     [1m Learning iteration 419/2000 [0m                      

                       Computation: 46803 steps/s (collection: 2.000s, learning 0.100s)
             Mean action noise std: 2.23
          Mean value_function loss: 31.6692
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 48.5983
                       Mean reward: 30.10
               Mean episode length: 134.10
    Episode_Reward/reaching_object: 0.6261
     Episode_Reward/lifting_object: 7.1116
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 22.3333
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 2.10s
                      Time elapsed: 00:15:09
                               ETA: 00:57:05

################################################################################
                     [1m Learning iteration 420/2000 [0m                      

                       Computation: 47853 steps/s (collection: 1.956s, learning 0.098s)
             Mean action noise std: 2.23
          Mean value_function loss: 27.0377
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 48.6033
                       Mean reward: 41.66
               Mean episode length: 153.54
    Episode_Reward/reaching_object: 0.6483
     Episode_Reward/lifting_object: 7.4856
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0365
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 21.6250
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 2.05s
                      Time elapsed: 00:15:11
                               ETA: 00:57:02

################################################################################
                     [1m Learning iteration 421/2000 [0m                      

                       Computation: 48636 steps/s (collection: 1.910s, learning 0.111s)
             Mean action noise std: 2.23
          Mean value_function loss: 27.2379
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 48.6062
                       Mean reward: 41.53
               Mean episode length: 147.20
    Episode_Reward/reaching_object: 0.6380
     Episode_Reward/lifting_object: 8.0303
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 21.7083
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 2.02s
                      Time elapsed: 00:15:13
                               ETA: 00:56:59

################################################################################
                     [1m Learning iteration 422/2000 [0m                      

                       Computation: 48172 steps/s (collection: 1.945s, learning 0.096s)
             Mean action noise std: 2.23
          Mean value_function loss: 37.4015
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 48.6105
                       Mean reward: 42.70
               Mean episode length: 152.24
    Episode_Reward/reaching_object: 0.6400
     Episode_Reward/lifting_object: 7.8802
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.0833
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 2.04s
                      Time elapsed: 00:15:15
                               ETA: 00:56:57

################################################################################
                     [1m Learning iteration 423/2000 [0m                      

                       Computation: 47202 steps/s (collection: 1.971s, learning 0.112s)
             Mean action noise std: 2.23
          Mean value_function loss: 30.7601
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 48.6160
                       Mean reward: 40.75
               Mean episode length: 146.14
    Episode_Reward/reaching_object: 0.6385
     Episode_Reward/lifting_object: 6.9872
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 22.0833
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 2.08s
                      Time elapsed: 00:15:18
                               ETA: 00:56:54

################################################################################
                     [1m Learning iteration 424/2000 [0m                      

                       Computation: 45513 steps/s (collection: 2.046s, learning 0.114s)
             Mean action noise std: 2.23
          Mean value_function loss: 31.0093
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 48.6238
                       Mean reward: 41.43
               Mean episode length: 144.74
    Episode_Reward/reaching_object: 0.6478
     Episode_Reward/lifting_object: 8.1458
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 21.2917
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 2.16s
                      Time elapsed: 00:15:20
                               ETA: 00:56:52

################################################################################
                     [1m Learning iteration 425/2000 [0m                      

                       Computation: 48478 steps/s (collection: 1.922s, learning 0.106s)
             Mean action noise std: 2.24
          Mean value_function loss: 33.8115
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.6309
                       Mean reward: 46.55
               Mean episode length: 143.14
    Episode_Reward/reaching_object: 0.6700
     Episode_Reward/lifting_object: 8.0913
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 19.3750
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 2.03s
                      Time elapsed: 00:15:22
                               ETA: 00:56:49

################################################################################
                     [1m Learning iteration 426/2000 [0m                      

                       Computation: 47548 steps/s (collection: 1.941s, learning 0.127s)
             Mean action noise std: 2.24
          Mean value_function loss: 39.4464
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 48.6406
                       Mean reward: 47.42
               Mean episode length: 147.38
    Episode_Reward/reaching_object: 0.6637
     Episode_Reward/lifting_object: 8.3327
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 21.5833
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 2.07s
                      Time elapsed: 00:15:24
                               ETA: 00:56:47

################################################################################
                     [1m Learning iteration 427/2000 [0m                      

                       Computation: 48204 steps/s (collection: 1.946s, learning 0.094s)
             Mean action noise std: 2.24
          Mean value_function loss: 30.8541
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 48.6505
                       Mean reward: 45.17
               Mean episode length: 151.61
    Episode_Reward/reaching_object: 0.6680
     Episode_Reward/lifting_object: 8.3043
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 20.9167
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 2.04s
                      Time elapsed: 00:15:26
                               ETA: 00:56:44

################################################################################
                     [1m Learning iteration 428/2000 [0m                      

                       Computation: 46929 steps/s (collection: 1.938s, learning 0.157s)
             Mean action noise std: 2.24
          Mean value_function loss: 33.5910
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 48.6575
                       Mean reward: 47.95
               Mean episode length: 154.70
    Episode_Reward/reaching_object: 0.6729
     Episode_Reward/lifting_object: 8.5088
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 22.3333
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 2.09s
                      Time elapsed: 00:15:28
                               ETA: 00:56:42

################################################################################
                     [1m Learning iteration 429/2000 [0m                      

                       Computation: 46610 steps/s (collection: 1.984s, learning 0.125s)
             Mean action noise std: 2.24
          Mean value_function loss: 26.9847
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 48.6647
                       Mean reward: 44.65
               Mean episode length: 146.79
    Episode_Reward/reaching_object: 0.6535
     Episode_Reward/lifting_object: 8.1684
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 20.6667
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 2.11s
                      Time elapsed: 00:15:30
                               ETA: 00:56:39

################################################################################
                     [1m Learning iteration 430/2000 [0m                      

                       Computation: 46656 steps/s (collection: 1.958s, learning 0.149s)
             Mean action noise std: 2.24
          Mean value_function loss: 45.1844
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 48.6691
                       Mean reward: 43.46
               Mean episode length: 146.74
    Episode_Reward/reaching_object: 0.6574
     Episode_Reward/lifting_object: 8.2244
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 21.2917
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 2.11s
                      Time elapsed: 00:15:32
                               ETA: 00:56:37

################################################################################
                     [1m Learning iteration 431/2000 [0m                      

                       Computation: 47784 steps/s (collection: 1.926s, learning 0.132s)
             Mean action noise std: 2.24
          Mean value_function loss: 28.0892
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 48.6751
                       Mean reward: 52.01
               Mean episode length: 161.09
    Episode_Reward/reaching_object: 0.6614
     Episode_Reward/lifting_object: 8.0762
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 18.7917
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 2.06s
                      Time elapsed: 00:15:34
                               ETA: 00:56:34

################################################################################
                     [1m Learning iteration 432/2000 [0m                      

                       Computation: 48897 steps/s (collection: 1.914s, learning 0.097s)
             Mean action noise std: 2.24
          Mean value_function loss: 44.0559
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 48.6773
                       Mean reward: 49.23
               Mean episode length: 151.36
    Episode_Reward/reaching_object: 0.6441
     Episode_Reward/lifting_object: 8.9956
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 20.8750
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 2.01s
                      Time elapsed: 00:15:36
                               ETA: 00:56:32

################################################################################
                     [1m Learning iteration 433/2000 [0m                      

                       Computation: 48149 steps/s (collection: 1.930s, learning 0.112s)
             Mean action noise std: 2.24
          Mean value_function loss: 34.3846
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 48.6795
                       Mean reward: 37.00
               Mean episode length: 135.78
    Episode_Reward/reaching_object: 0.6656
     Episode_Reward/lifting_object: 8.7477
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 22.5417
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 2.04s
                      Time elapsed: 00:15:38
                               ETA: 00:56:29

################################################################################
                     [1m Learning iteration 434/2000 [0m                      

                       Computation: 48876 steps/s (collection: 1.921s, learning 0.090s)
             Mean action noise std: 2.24
          Mean value_function loss: 37.9619
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.6836
                       Mean reward: 50.81
               Mean episode length: 160.97
    Episode_Reward/reaching_object: 0.6776
     Episode_Reward/lifting_object: 9.2213
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.2083
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 2.01s
                      Time elapsed: 00:15:40
                               ETA: 00:56:26

################################################################################
                     [1m Learning iteration 435/2000 [0m                      

                       Computation: 47290 steps/s (collection: 1.965s, learning 0.114s)
             Mean action noise std: 2.24
          Mean value_function loss: 39.6956
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 48.6900
                       Mean reward: 46.54
               Mean episode length: 146.54
    Episode_Reward/reaching_object: 0.6649
     Episode_Reward/lifting_object: 8.9387
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 21.6667
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 2.08s
                      Time elapsed: 00:15:42
                               ETA: 00:56:24

################################################################################
                     [1m Learning iteration 436/2000 [0m                      

                       Computation: 47765 steps/s (collection: 1.945s, learning 0.113s)
             Mean action noise std: 2.24
          Mean value_function loss: 39.4942
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 48.6983
                       Mean reward: 47.33
               Mean episode length: 161.13
    Episode_Reward/reaching_object: 0.6548
     Episode_Reward/lifting_object: 8.3216
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 21.6667
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 2.06s
                      Time elapsed: 00:15:44
                               ETA: 00:56:21

################################################################################
                     [1m Learning iteration 437/2000 [0m                      

                       Computation: 47622 steps/s (collection: 1.963s, learning 0.101s)
             Mean action noise std: 2.24
          Mean value_function loss: 38.2534
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 48.7032
                       Mean reward: 48.88
               Mean episode length: 147.08
    Episode_Reward/reaching_object: 0.6353
     Episode_Reward/lifting_object: 8.3819
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 23.3333
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 2.06s
                      Time elapsed: 00:15:46
                               ETA: 00:56:19

################################################################################
                     [1m Learning iteration 438/2000 [0m                      

                       Computation: 47959 steps/s (collection: 1.961s, learning 0.089s)
             Mean action noise std: 2.25
          Mean value_function loss: 39.3190
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.7117
                       Mean reward: 47.18
               Mean episode length: 141.01
    Episode_Reward/reaching_object: 0.6525
     Episode_Reward/lifting_object: 8.7049
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 6.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 21.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 2.05s
                      Time elapsed: 00:15:49
                               ETA: 00:56:16

################################################################################
                     [1m Learning iteration 439/2000 [0m                      

                       Computation: 47734 steps/s (collection: 1.973s, learning 0.087s)
             Mean action noise std: 2.25
          Mean value_function loss: 40.1071
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 48.7205
                       Mean reward: 45.39
               Mean episode length: 141.23
    Episode_Reward/reaching_object: 0.6424
     Episode_Reward/lifting_object: 9.5716
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 23.2083
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 2.06s
                      Time elapsed: 00:15:51
                               ETA: 00:56:14

################################################################################
                     [1m Learning iteration 440/2000 [0m                      

                       Computation: 45776 steps/s (collection: 2.033s, learning 0.115s)
             Mean action noise std: 2.25
          Mean value_function loss: 46.4233
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 48.7234
                       Mean reward: 51.29
               Mean episode length: 148.80
    Episode_Reward/reaching_object: 0.6400
     Episode_Reward/lifting_object: 9.3194
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 23.4583
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 2.15s
                      Time elapsed: 00:15:53
                               ETA: 00:56:12

################################################################################
                     [1m Learning iteration 441/2000 [0m                      

                       Computation: 47167 steps/s (collection: 1.977s, learning 0.107s)
             Mean action noise std: 2.25
          Mean value_function loss: 34.1192
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 48.7270
                       Mean reward: 42.52
               Mean episode length: 133.70
    Episode_Reward/reaching_object: 0.6138
     Episode_Reward/lifting_object: 8.3062
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 25.0417
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 2.08s
                      Time elapsed: 00:15:55
                               ETA: 00:56:09

################################################################################
                     [1m Learning iteration 442/2000 [0m                      

                       Computation: 48904 steps/s (collection: 1.910s, learning 0.100s)
             Mean action noise std: 2.25
          Mean value_function loss: 34.1899
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 48.7303
                       Mean reward: 54.37
               Mean episode length: 145.24
    Episode_Reward/reaching_object: 0.6252
     Episode_Reward/lifting_object: 9.0561
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 21.7500
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 2.01s
                      Time elapsed: 00:15:57
                               ETA: 00:56:06

################################################################################
                     [1m Learning iteration 443/2000 [0m                      

                       Computation: 47270 steps/s (collection: 1.963s, learning 0.117s)
             Mean action noise std: 2.25
          Mean value_function loss: 42.0625
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 48.7317
                       Mean reward: 40.72
               Mean episode length: 130.03
    Episode_Reward/reaching_object: 0.6331
     Episode_Reward/lifting_object: 8.9995
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.3333
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 2.08s
                      Time elapsed: 00:15:59
                               ETA: 00:56:04

################################################################################
                     [1m Learning iteration 444/2000 [0m                      

                       Computation: 47301 steps/s (collection: 1.966s, learning 0.112s)
             Mean action noise std: 2.25
          Mean value_function loss: 41.7014
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 48.7325
                       Mean reward: 50.78
               Mean episode length: 147.04
    Episode_Reward/reaching_object: 0.6502
     Episode_Reward/lifting_object: 9.6522
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 23.6250
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 2.08s
                      Time elapsed: 00:16:01
                               ETA: 00:56:02

################################################################################
                     [1m Learning iteration 445/2000 [0m                      

                       Computation: 47529 steps/s (collection: 1.938s, learning 0.130s)
             Mean action noise std: 2.25
          Mean value_function loss: 43.6049
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 48.7335
                       Mean reward: 49.02
               Mean episode length: 147.61
    Episode_Reward/reaching_object: 0.6420
     Episode_Reward/lifting_object: 9.5738
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 24.2083
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 2.07s
                      Time elapsed: 00:16:03
                               ETA: 00:55:59

################################################################################
                     [1m Learning iteration 446/2000 [0m                      

                       Computation: 46841 steps/s (collection: 1.964s, learning 0.135s)
             Mean action noise std: 2.25
          Mean value_function loss: 31.2370
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 48.7372
                       Mean reward: 46.82
               Mean episode length: 140.94
    Episode_Reward/reaching_object: 0.6319
     Episode_Reward/lifting_object: 9.1867
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 5.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 25.0833
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 2.10s
                      Time elapsed: 00:16:05
                               ETA: 00:55:57

################################################################################
                     [1m Learning iteration 447/2000 [0m                      

                       Computation: 43722 steps/s (collection: 2.136s, learning 0.112s)
             Mean action noise std: 2.25
          Mean value_function loss: 39.6547
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 48.7434
                       Mean reward: 52.82
               Mean episode length: 145.56
    Episode_Reward/reaching_object: 0.6212
     Episode_Reward/lifting_object: 9.5063
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 24.2083
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 2.25s
                      Time elapsed: 00:16:07
                               ETA: 00:55:55

################################################################################
                     [1m Learning iteration 448/2000 [0m                      

                       Computation: 45309 steps/s (collection: 2.077s, learning 0.092s)
             Mean action noise std: 2.25
          Mean value_function loss: 48.1523
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 48.7548
                       Mean reward: 48.16
               Mean episode length: 135.61
    Episode_Reward/reaching_object: 0.6197
     Episode_Reward/lifting_object: 9.4773
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 25.6667
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 2.17s
                      Time elapsed: 00:16:10
                               ETA: 00:55:53

################################################################################
                     [1m Learning iteration 449/2000 [0m                      

                       Computation: 47478 steps/s (collection: 1.968s, learning 0.103s)
             Mean action noise std: 2.25
          Mean value_function loss: 37.1622
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 48.7567
                       Mean reward: 44.07
               Mean episode length: 145.18
    Episode_Reward/reaching_object: 0.6132
     Episode_Reward/lifting_object: 8.7938
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 26.3750
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 2.07s
                      Time elapsed: 00:16:12
                               ETA: 00:55:50

################################################################################
                     [1m Learning iteration 450/2000 [0m                      

                       Computation: 45150 steps/s (collection: 2.067s, learning 0.110s)
             Mean action noise std: 2.25
          Mean value_function loss: 40.3698
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 48.7583
                       Mean reward: 43.40
               Mean episode length: 132.46
    Episode_Reward/reaching_object: 0.6195
     Episode_Reward/lifting_object: 9.2543
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0360
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 26.1250
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 2.18s
                      Time elapsed: 00:16:14
                               ETA: 00:55:48

################################################################################
                     [1m Learning iteration 451/2000 [0m                      

                       Computation: 46730 steps/s (collection: 1.987s, learning 0.117s)
             Mean action noise std: 2.25
          Mean value_function loss: 39.3825
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 48.7625
                       Mean reward: 50.03
               Mean episode length: 135.78
    Episode_Reward/reaching_object: 0.5990
     Episode_Reward/lifting_object: 8.9074
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 3.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 25.7083
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 2.10s
                      Time elapsed: 00:16:16
                               ETA: 00:55:46

################################################################################
                     [1m Learning iteration 452/2000 [0m                      

                       Computation: 48846 steps/s (collection: 1.924s, learning 0.088s)
             Mean action noise std: 2.25
          Mean value_function loss: 38.9798
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 48.7641
                       Mean reward: 45.47
               Mean episode length: 138.97
    Episode_Reward/reaching_object: 0.6212
     Episode_Reward/lifting_object: 9.1589
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 25.4583
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 2.01s
                      Time elapsed: 00:16:18
                               ETA: 00:55:43

################################################################################
                     [1m Learning iteration 453/2000 [0m                      

                       Computation: 47999 steps/s (collection: 1.932s, learning 0.116s)
             Mean action noise std: 2.25
          Mean value_function loss: 39.4441
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 48.7658
                       Mean reward: 50.66
               Mean episode length: 137.14
    Episode_Reward/reaching_object: 0.6261
     Episode_Reward/lifting_object: 9.8297
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 23.1667
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 2.05s
                      Time elapsed: 00:16:20
                               ETA: 00:55:41

################################################################################
                     [1m Learning iteration 454/2000 [0m                      

                       Computation: 46259 steps/s (collection: 2.001s, learning 0.124s)
             Mean action noise std: 2.25
          Mean value_function loss: 60.4394
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 48.7670
                       Mean reward: 57.55
               Mean episode length: 156.91
    Episode_Reward/reaching_object: 0.6450
     Episode_Reward/lifting_object: 9.9394
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0365
      Episode_Termination/time_out: 4.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 20.6250
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 2.13s
                      Time elapsed: 00:16:22
                               ETA: 00:55:38

################################################################################
                     [1m Learning iteration 455/2000 [0m                      

                       Computation: 46890 steps/s (collection: 1.955s, learning 0.141s)
             Mean action noise std: 2.25
          Mean value_function loss: 33.3714
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 48.7688
                       Mean reward: 52.24
               Mean episode length: 143.70
    Episode_Reward/reaching_object: 0.6441
     Episode_Reward/lifting_object: 10.4480
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 23.5833
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 2.10s
                      Time elapsed: 00:16:24
                               ETA: 00:55:36

################################################################################
                     [1m Learning iteration 456/2000 [0m                      

                       Computation: 48094 steps/s (collection: 1.945s, learning 0.099s)
             Mean action noise std: 2.25
          Mean value_function loss: 37.0668
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 48.7728
                       Mean reward: 50.33
               Mean episode length: 134.54
    Episode_Reward/reaching_object: 0.6453
     Episode_Reward/lifting_object: 10.4564
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0365
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 24.4583
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 2.04s
                      Time elapsed: 00:16:26
                               ETA: 00:55:33

################################################################################
                     [1m Learning iteration 457/2000 [0m                      

                       Computation: 45542 steps/s (collection: 2.013s, learning 0.146s)
             Mean action noise std: 2.25
          Mean value_function loss: 43.7050
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 48.7774
                       Mean reward: 61.23
               Mean episode length: 148.84
    Episode_Reward/reaching_object: 0.6611
     Episode_Reward/lifting_object: 10.9192
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 23.8333
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 2.16s
                      Time elapsed: 00:16:28
                               ETA: 00:55:31

################################################################################
                     [1m Learning iteration 458/2000 [0m                      

                       Computation: 47868 steps/s (collection: 1.939s, learning 0.115s)
             Mean action noise std: 2.25
          Mean value_function loss: 39.4843
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.7793
                       Mean reward: 60.78
               Mean episode length: 146.46
    Episode_Reward/reaching_object: 0.6258
     Episode_Reward/lifting_object: 10.2539
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 22.5833
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 2.05s
                      Time elapsed: 00:16:30
                               ETA: 00:55:29

################################################################################
                     [1m Learning iteration 459/2000 [0m                      

                       Computation: 48013 steps/s (collection: 1.950s, learning 0.097s)
             Mean action noise std: 2.25
          Mean value_function loss: 36.8238
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 48.7807
                       Mean reward: 62.15
               Mean episode length: 148.53
    Episode_Reward/reaching_object: 0.6626
     Episode_Reward/lifting_object: 11.2317
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 25.6667
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 2.05s
                      Time elapsed: 00:16:33
                               ETA: 00:55:26

################################################################################
                     [1m Learning iteration 460/2000 [0m                      

                       Computation: 47488 steps/s (collection: 1.975s, learning 0.095s)
             Mean action noise std: 2.25
          Mean value_function loss: 34.6899
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 48.7811
                       Mean reward: 50.61
               Mean episode length: 138.33
    Episode_Reward/reaching_object: 0.6453
     Episode_Reward/lifting_object: 10.4597
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0365
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 22.5417
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 2.07s
                      Time elapsed: 00:16:35
                               ETA: 00:55:24

################################################################################
                     [1m Learning iteration 461/2000 [0m                      

                       Computation: 47338 steps/s (collection: 1.969s, learning 0.108s)
             Mean action noise std: 2.25
          Mean value_function loss: 38.4070
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 48.7818
                       Mean reward: 53.40
               Mean episode length: 141.46
    Episode_Reward/reaching_object: 0.6586
     Episode_Reward/lifting_object: 10.9728
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 23.4167
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 2.08s
                      Time elapsed: 00:16:37
                               ETA: 00:55:21

################################################################################
                     [1m Learning iteration 462/2000 [0m                      

                       Computation: 47569 steps/s (collection: 1.953s, learning 0.113s)
             Mean action noise std: 2.25
          Mean value_function loss: 38.1024
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 48.7826
                       Mean reward: 53.78
               Mean episode length: 141.85
    Episode_Reward/reaching_object: 0.6380
     Episode_Reward/lifting_object: 10.6255
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 5.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 22.8333
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 2.07s
                      Time elapsed: 00:16:39
                               ETA: 00:55:19

################################################################################
                     [1m Learning iteration 463/2000 [0m                      

                       Computation: 47006 steps/s (collection: 2.004s, learning 0.088s)
             Mean action noise std: 2.25
          Mean value_function loss: 40.2992
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 48.7819
                       Mean reward: 62.02
               Mean episode length: 146.60
    Episode_Reward/reaching_object: 0.6538
     Episode_Reward/lifting_object: 11.8129
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.1250
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 2.09s
                      Time elapsed: 00:16:41
                               ETA: 00:55:16

################################################################################
                     [1m Learning iteration 464/2000 [0m                      

                       Computation: 46666 steps/s (collection: 1.993s, learning 0.114s)
             Mean action noise std: 2.26
          Mean value_function loss: 46.1191
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 48.7828
                       Mean reward: 58.49
               Mean episode length: 134.60
    Episode_Reward/reaching_object: 0.6521
     Episode_Reward/lifting_object: 11.6301
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 21.5833
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 2.11s
                      Time elapsed: 00:16:43
                               ETA: 00:55:14

################################################################################
                     [1m Learning iteration 465/2000 [0m                      

                       Computation: 47231 steps/s (collection: 1.976s, learning 0.105s)
             Mean action noise std: 2.26
          Mean value_function loss: 52.3207
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 48.7876
                       Mean reward: 57.97
               Mean episode length: 143.98
    Episode_Reward/reaching_object: 0.6813
     Episode_Reward/lifting_object: 11.9069
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 5.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 21.7500
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 2.08s
                      Time elapsed: 00:16:45
                               ETA: 00:55:12

################################################################################
                     [1m Learning iteration 466/2000 [0m                      

                       Computation: 46583 steps/s (collection: 1.995s, learning 0.115s)
             Mean action noise std: 2.26
          Mean value_function loss: 44.3610
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 48.7916
                       Mean reward: 66.00
               Mean episode length: 153.85
    Episode_Reward/reaching_object: 0.6653
     Episode_Reward/lifting_object: 11.8223
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 21.4167
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 2.11s
                      Time elapsed: 00:16:47
                               ETA: 00:55:09

################################################################################
                     [1m Learning iteration 467/2000 [0m                      

                       Computation: 46042 steps/s (collection: 1.973s, learning 0.163s)
             Mean action noise std: 2.26
          Mean value_function loss: 36.9183
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 48.7917
                       Mean reward: 58.15
               Mean episode length: 148.25
    Episode_Reward/reaching_object: 0.6753
     Episode_Reward/lifting_object: 11.8732
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 22.6667
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 2.14s
                      Time elapsed: 00:16:49
                               ETA: 00:55:07

################################################################################
                     [1m Learning iteration 468/2000 [0m                      

                       Computation: 47245 steps/s (collection: 1.969s, learning 0.112s)
             Mean action noise std: 2.26
          Mean value_function loss: 58.5710
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 48.7920
                       Mean reward: 57.65
               Mean episode length: 140.64
    Episode_Reward/reaching_object: 0.6598
     Episode_Reward/lifting_object: 11.7595
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 23.4583
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 2.08s
                      Time elapsed: 00:16:51
                               ETA: 00:55:05

################################################################################
                     [1m Learning iteration 469/2000 [0m                      

                       Computation: 47405 steps/s (collection: 1.926s, learning 0.147s)
             Mean action noise std: 2.26
          Mean value_function loss: 53.9601
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 48.7979
                       Mean reward: 66.65
               Mean episode length: 154.95
    Episode_Reward/reaching_object: 0.6750
     Episode_Reward/lifting_object: 11.6785
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 23.4167
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 2.07s
                      Time elapsed: 00:16:53
                               ETA: 00:55:02

################################################################################
                     [1m Learning iteration 470/2000 [0m                      

                       Computation: 47297 steps/s (collection: 1.967s, learning 0.111s)
             Mean action noise std: 2.26
          Mean value_function loss: 47.6753
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 48.8034
                       Mean reward: 65.03
               Mean episode length: 146.41
    Episode_Reward/reaching_object: 0.6784
     Episode_Reward/lifting_object: 12.9886
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 22.2500
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 2.08s
                      Time elapsed: 00:16:55
                               ETA: 00:55:00

################################################################################
                     [1m Learning iteration 471/2000 [0m                      

                       Computation: 47559 steps/s (collection: 1.958s, learning 0.109s)
             Mean action noise std: 2.26
          Mean value_function loss: 54.5050
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 48.8062
                       Mean reward: 67.50
               Mean episode length: 150.10
    Episode_Reward/reaching_object: 0.6713
     Episode_Reward/lifting_object: 11.6163
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 22.9583
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 2.07s
                      Time elapsed: 00:16:58
                               ETA: 00:54:57

################################################################################
                     [1m Learning iteration 472/2000 [0m                      

                       Computation: 47070 steps/s (collection: 1.976s, learning 0.113s)
             Mean action noise std: 2.26
          Mean value_function loss: 53.8897
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 48.8079
                       Mean reward: 63.70
               Mean episode length: 140.98
    Episode_Reward/reaching_object: 0.6405
     Episode_Reward/lifting_object: 11.7795
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 24.2083
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 2.09s
                      Time elapsed: 00:17:00
                               ETA: 00:54:55

################################################################################
                     [1m Learning iteration 473/2000 [0m                      

                       Computation: 43687 steps/s (collection: 2.056s, learning 0.195s)
             Mean action noise std: 2.26
          Mean value_function loss: 43.0537
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.8090
                       Mean reward: 54.10
               Mean episode length: 126.19
    Episode_Reward/reaching_object: 0.6533
     Episode_Reward/lifting_object: 12.3931
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 22.5417
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 2.25s
                      Time elapsed: 00:17:02
                               ETA: 00:54:53

################################################################################
                     [1m Learning iteration 474/2000 [0m                      

                       Computation: 46007 steps/s (collection: 2.037s, learning 0.100s)
             Mean action noise std: 2.26
          Mean value_function loss: 51.0057
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 48.8095
                       Mean reward: 59.64
               Mean episode length: 134.09
    Episode_Reward/reaching_object: 0.6478
     Episode_Reward/lifting_object: 12.5275
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 5.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 24.7500
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 2.14s
                      Time elapsed: 00:17:04
                               ETA: 00:54:51

################################################################################
                     [1m Learning iteration 475/2000 [0m                      

                       Computation: 46375 steps/s (collection: 1.984s, learning 0.136s)
             Mean action noise std: 2.26
          Mean value_function loss: 53.0730
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 48.8096
                       Mean reward: 61.92
               Mean episode length: 137.57
    Episode_Reward/reaching_object: 0.6176
     Episode_Reward/lifting_object: 12.1131
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 24.7083
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 2.12s
                      Time elapsed: 00:17:06
                               ETA: 00:54:49

################################################################################
                     [1m Learning iteration 476/2000 [0m                      

                       Computation: 45582 steps/s (collection: 2.012s, learning 0.145s)
             Mean action noise std: 2.26
          Mean value_function loss: 58.1127
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 48.8106
                       Mean reward: 71.91
               Mean episode length: 147.04
    Episode_Reward/reaching_object: 0.6310
     Episode_Reward/lifting_object: 12.4158
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.7083
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 2.16s
                      Time elapsed: 00:17:08
                               ETA: 00:54:47

################################################################################
                     [1m Learning iteration 477/2000 [0m                      

                       Computation: 45882 steps/s (collection: 2.045s, learning 0.097s)
             Mean action noise std: 2.26
          Mean value_function loss: 49.0118
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 48.8117
                       Mean reward: 68.66
               Mean episode length: 140.51
    Episode_Reward/reaching_object: 0.6473
     Episode_Reward/lifting_object: 12.9096
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 24.7500
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 2.14s
                      Time elapsed: 00:17:10
                               ETA: 00:54:44

################################################################################
                     [1m Learning iteration 478/2000 [0m                      

                       Computation: 47401 steps/s (collection: 1.964s, learning 0.110s)
             Mean action noise std: 2.26
          Mean value_function loss: 49.7828
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 48.8130
                       Mean reward: 58.81
               Mean episode length: 128.30
    Episode_Reward/reaching_object: 0.6350
     Episode_Reward/lifting_object: 13.3402
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 23.6667
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 2.07s
                      Time elapsed: 00:17:13
                               ETA: 00:54:42

################################################################################
                     [1m Learning iteration 479/2000 [0m                      

                       Computation: 47944 steps/s (collection: 1.962s, learning 0.089s)
             Mean action noise std: 2.26
          Mean value_function loss: 52.5623
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 48.8153
                       Mean reward: 64.37
               Mean episode length: 134.38
    Episode_Reward/reaching_object: 0.6396
     Episode_Reward/lifting_object: 12.5950
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 5.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 21.9583
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 2.05s
                      Time elapsed: 00:17:15
                               ETA: 00:54:39

################################################################################
                     [1m Learning iteration 480/2000 [0m                      

                       Computation: 47307 steps/s (collection: 1.963s, learning 0.115s)
             Mean action noise std: 2.26
          Mean value_function loss: 47.1719
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 48.8173
                       Mean reward: 73.93
               Mean episode length: 149.42
    Episode_Reward/reaching_object: 0.6537
     Episode_Reward/lifting_object: 13.3156
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 23.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 2.08s
                      Time elapsed: 00:17:17
                               ETA: 00:54:37

################################################################################
                     [1m Learning iteration 481/2000 [0m                      

                       Computation: 47376 steps/s (collection: 1.979s, learning 0.096s)
             Mean action noise std: 2.26
          Mean value_function loss: 55.2794
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 48.8179
                       Mean reward: 69.36
               Mean episode length: 138.92
    Episode_Reward/reaching_object: 0.6364
     Episode_Reward/lifting_object: 13.6002
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 22.5417
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 2.07s
                      Time elapsed: 00:17:19
                               ETA: 00:54:35

################################################################################
                     [1m Learning iteration 482/2000 [0m                      

                       Computation: 47553 steps/s (collection: 1.974s, learning 0.093s)
             Mean action noise std: 2.26
          Mean value_function loss: 48.0618
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 48.8189
                       Mean reward: 71.09
               Mean episode length: 139.81
    Episode_Reward/reaching_object: 0.6390
     Episode_Reward/lifting_object: 13.3776
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 22.8750
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 2.07s
                      Time elapsed: 00:17:21
                               ETA: 00:54:32

################################################################################
                     [1m Learning iteration 483/2000 [0m                      

                       Computation: 47753 steps/s (collection: 1.965s, learning 0.094s)
             Mean action noise std: 2.26
          Mean value_function loss: 58.7326
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 48.8216
                       Mean reward: 68.99
               Mean episode length: 133.62
    Episode_Reward/reaching_object: 0.6599
     Episode_Reward/lifting_object: 13.5783
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 23.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 2.06s
                      Time elapsed: 00:17:23
                               ETA: 00:54:30

################################################################################
                     [1m Learning iteration 484/2000 [0m                      

                       Computation: 47096 steps/s (collection: 1.981s, learning 0.107s)
             Mean action noise std: 2.26
          Mean value_function loss: 57.2403
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 48.8224
                       Mean reward: 64.75
               Mean episode length: 144.08
    Episode_Reward/reaching_object: 0.6778
     Episode_Reward/lifting_object: 13.7705
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 24.0833
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 2.09s
                      Time elapsed: 00:17:25
                               ETA: 00:54:27

################################################################################
                     [1m Learning iteration 485/2000 [0m                      

                       Computation: 42560 steps/s (collection: 2.113s, learning 0.197s)
             Mean action noise std: 2.26
          Mean value_function loss: 49.1376
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 48.8215
                       Mean reward: 76.85
               Mean episode length: 160.05
    Episode_Reward/reaching_object: 0.6785
     Episode_Reward/lifting_object: 14.3861
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.8333
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 2.31s
                      Time elapsed: 00:17:27
                               ETA: 00:54:26

################################################################################
                     [1m Learning iteration 486/2000 [0m                      

                       Computation: 46636 steps/s (collection: 2.012s, learning 0.096s)
             Mean action noise std: 2.26
          Mean value_function loss: 64.1999
               Mean surrogate loss: 0.0095
                 Mean entropy loss: 48.8212
                       Mean reward: 71.55
               Mean episode length: 135.47
    Episode_Reward/reaching_object: 0.6838
     Episode_Reward/lifting_object: 14.1298
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 21.2917
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 2.11s
                      Time elapsed: 00:17:29
                               ETA: 00:54:23

################################################################################
                     [1m Learning iteration 487/2000 [0m                      

                       Computation: 46153 steps/s (collection: 2.017s, learning 0.113s)
             Mean action noise std: 2.26
          Mean value_function loss: 63.4507
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 48.8217
                       Mean reward: 79.96
               Mean episode length: 155.65
    Episode_Reward/reaching_object: 0.6940
     Episode_Reward/lifting_object: 14.3255
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.6250
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 2.13s
                      Time elapsed: 00:17:31
                               ETA: 00:54:21

################################################################################
                     [1m Learning iteration 488/2000 [0m                      

                       Computation: 47809 steps/s (collection: 1.968s, learning 0.088s)
             Mean action noise std: 2.26
          Mean value_function loss: 71.7999
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 48.8231
                       Mean reward: 78.56
               Mean episode length: 157.91
    Episode_Reward/reaching_object: 0.6614
     Episode_Reward/lifting_object: 13.7632
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 24.5417
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 2.06s
                      Time elapsed: 00:17:34
                               ETA: 00:54:19

################################################################################
                     [1m Learning iteration 489/2000 [0m                      

                       Computation: 47450 steps/s (collection: 1.980s, learning 0.092s)
             Mean action noise std: 2.26
          Mean value_function loss: 63.8282
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 48.8251
                       Mean reward: 71.02
               Mean episode length: 141.43
    Episode_Reward/reaching_object: 0.6615
     Episode_Reward/lifting_object: 14.0106
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 2.07s
                      Time elapsed: 00:17:36
                               ETA: 00:54:16

################################################################################
                     [1m Learning iteration 490/2000 [0m                      

                       Computation: 43793 steps/s (collection: 2.157s, learning 0.088s)
             Mean action noise std: 2.26
          Mean value_function loss: 55.0800
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 48.8260
                       Mean reward: 72.64
               Mean episode length: 144.32
    Episode_Reward/reaching_object: 0.6706
     Episode_Reward/lifting_object: 14.1190
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 22.1250
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 2.24s
                      Time elapsed: 00:17:38
                               ETA: 00:54:14

################################################################################
                     [1m Learning iteration 491/2000 [0m                      

                       Computation: 47134 steps/s (collection: 1.976s, learning 0.110s)
             Mean action noise std: 2.26
          Mean value_function loss: 64.2815
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 48.8260
                       Mean reward: 78.64
               Mean episode length: 139.57
    Episode_Reward/reaching_object: 0.6632
     Episode_Reward/lifting_object: 14.1406
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.3750
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 2.09s
                      Time elapsed: 00:17:40
                               ETA: 00:54:12

################################################################################
                     [1m Learning iteration 492/2000 [0m                      

                       Computation: 42757 steps/s (collection: 2.139s, learning 0.160s)
             Mean action noise std: 2.26
          Mean value_function loss: 55.7674
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.8268
                       Mean reward: 79.51
               Mean episode length: 151.08
    Episode_Reward/reaching_object: 0.6861
     Episode_Reward/lifting_object: 14.7067
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 22.4583
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 2.30s
                      Time elapsed: 00:17:42
                               ETA: 00:54:10

################################################################################
                     [1m Learning iteration 493/2000 [0m                      

                       Computation: 43110 steps/s (collection: 2.133s, learning 0.147s)
             Mean action noise std: 2.26
          Mean value_function loss: 57.4710
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 48.8287
                       Mean reward: 80.47
               Mean episode length: 148.00
    Episode_Reward/reaching_object: 0.6992
     Episode_Reward/lifting_object: 15.8767
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 22.7917
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 2.28s
                      Time elapsed: 00:17:45
                               ETA: 00:54:08

################################################################################
                     [1m Learning iteration 494/2000 [0m                      

                       Computation: 46399 steps/s (collection: 2.007s, learning 0.112s)
             Mean action noise std: 2.26
          Mean value_function loss: 61.6984
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 48.8291
                       Mean reward: 77.35
               Mean episode length: 143.75
    Episode_Reward/reaching_object: 0.7202
     Episode_Reward/lifting_object: 15.6573
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 21.3333
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 2.12s
                      Time elapsed: 00:17:47
                               ETA: 00:54:06

################################################################################
                     [1m Learning iteration 495/2000 [0m                      

                       Computation: 47500 steps/s (collection: 1.967s, learning 0.103s)
             Mean action noise std: 2.26
          Mean value_function loss: 62.4323
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 48.8295
                       Mean reward: 83.42
               Mean episode length: 157.90
    Episode_Reward/reaching_object: 0.6978
     Episode_Reward/lifting_object: 15.5468
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 21.4583
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 2.07s
                      Time elapsed: 00:17:49
                               ETA: 00:54:04

################################################################################
                     [1m Learning iteration 496/2000 [0m                      

                       Computation: 46344 steps/s (collection: 2.024s, learning 0.098s)
             Mean action noise std: 2.26
          Mean value_function loss: 60.4138
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 48.8300
                       Mean reward: 79.10
               Mean episode length: 149.54
    Episode_Reward/reaching_object: 0.6727
     Episode_Reward/lifting_object: 15.3762
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.0833
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 2.12s
                      Time elapsed: 00:17:51
                               ETA: 00:54:02

################################################################################
                     [1m Learning iteration 497/2000 [0m                      

                       Computation: 46648 steps/s (collection: 1.989s, learning 0.119s)
             Mean action noise std: 2.26
          Mean value_function loss: 64.1636
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 48.8304
                       Mean reward: 79.54
               Mean episode length: 147.42
    Episode_Reward/reaching_object: 0.6979
     Episode_Reward/lifting_object: 16.0375
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 19.9167
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 2.11s
                      Time elapsed: 00:17:53
                               ETA: 00:53:59

################################################################################
                     [1m Learning iteration 498/2000 [0m                      

                       Computation: 46480 steps/s (collection: 2.002s, learning 0.113s)
             Mean action noise std: 2.26
          Mean value_function loss: 59.6071
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 48.8314
                       Mean reward: 83.66
               Mean episode length: 163.49
    Episode_Reward/reaching_object: 0.7100
     Episode_Reward/lifting_object: 15.6716
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 5.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 21.1250
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 2.11s
                      Time elapsed: 00:17:55
                               ETA: 00:53:57

################################################################################
                     [1m Learning iteration 499/2000 [0m                      

                       Computation: 47644 steps/s (collection: 1.973s, learning 0.091s)
             Mean action noise std: 2.26
          Mean value_function loss: 57.2794
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 48.8321
                       Mean reward: 82.69
               Mean episode length: 138.47
    Episode_Reward/reaching_object: 0.6895
     Episode_Reward/lifting_object: 15.5219
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 23.3333
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 2.06s
                      Time elapsed: 00:17:57
                               ETA: 00:53:55

################################################################################
                     [1m Learning iteration 500/2000 [0m                      

                       Computation: 46410 steps/s (collection: 1.998s, learning 0.120s)
             Mean action noise std: 2.26
          Mean value_function loss: 69.9763
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 48.8334
                       Mean reward: 76.80
               Mean episode length: 147.01
    Episode_Reward/reaching_object: 0.7046
     Episode_Reward/lifting_object: 15.9758
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 22.3333
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 2.12s
                      Time elapsed: 00:17:59
                               ETA: 00:53:52

################################################################################
                     [1m Learning iteration 501/2000 [0m                      

                       Computation: 44696 steps/s (collection: 2.072s, learning 0.127s)
             Mean action noise std: 2.26
          Mean value_function loss: 58.2219
               Mean surrogate loss: 0.0112
                 Mean entropy loss: 48.8352
                       Mean reward: 81.79
               Mean episode length: 142.34
    Episode_Reward/reaching_object: 0.6982
     Episode_Reward/lifting_object: 16.0125
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 22.2917
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 2.20s
                      Time elapsed: 00:18:01
                               ETA: 00:53:50

################################################################################
                     [1m Learning iteration 502/2000 [0m                      

                       Computation: 47138 steps/s (collection: 1.984s, learning 0.102s)
             Mean action noise std: 2.26
          Mean value_function loss: 63.2852
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 48.8355
                       Mean reward: 92.40
               Mean episode length: 163.60
    Episode_Reward/reaching_object: 0.7093
     Episode_Reward/lifting_object: 16.6881
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 19.3750
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 2.09s
                      Time elapsed: 00:18:04
                               ETA: 00:53:48

################################################################################
                     [1m Learning iteration 503/2000 [0m                      

                       Computation: 46664 steps/s (collection: 1.975s, learning 0.132s)
             Mean action noise std: 2.26
          Mean value_function loss: 58.1693
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 48.8359
                       Mean reward: 80.81
               Mean episode length: 142.52
    Episode_Reward/reaching_object: 0.7027
     Episode_Reward/lifting_object: 16.4372
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 21.5833
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 2.11s
                      Time elapsed: 00:18:06
                               ETA: 00:53:46

################################################################################
                     [1m Learning iteration 504/2000 [0m                      

                       Computation: 46729 steps/s (collection: 1.999s, learning 0.105s)
             Mean action noise std: 2.26
          Mean value_function loss: 65.8755
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 48.8367
                       Mean reward: 94.65
               Mean episode length: 150.24
    Episode_Reward/reaching_object: 0.6909
     Episode_Reward/lifting_object: 16.5850
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 20.5417
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 2.10s
                      Time elapsed: 00:18:08
                               ETA: 00:53:43

################################################################################
                     [1m Learning iteration 505/2000 [0m                      

                       Computation: 47518 steps/s (collection: 1.951s, learning 0.118s)
             Mean action noise std: 2.26
          Mean value_function loss: 64.1170
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 48.8357
                       Mean reward: 89.08
               Mean episode length: 150.21
    Episode_Reward/reaching_object: 0.6973
     Episode_Reward/lifting_object: 16.7546
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 23.4167
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 2.07s
                      Time elapsed: 00:18:10
                               ETA: 00:53:41

################################################################################
                     [1m Learning iteration 506/2000 [0m                      

                       Computation: 46224 steps/s (collection: 1.997s, learning 0.130s)
             Mean action noise std: 2.26
          Mean value_function loss: 73.2020
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 48.8339
                       Mean reward: 77.15
               Mean episode length: 143.96
    Episode_Reward/reaching_object: 0.6741
     Episode_Reward/lifting_object: 16.0548
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 24.3333
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 2.13s
                      Time elapsed: 00:18:12
                               ETA: 00:53:39

################################################################################
                     [1m Learning iteration 507/2000 [0m                      

                       Computation: 45710 steps/s (collection: 2.021s, learning 0.129s)
             Mean action noise std: 2.27
          Mean value_function loss: 73.4698
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 48.8344
                       Mean reward: 91.49
               Mean episode length: 144.09
    Episode_Reward/reaching_object: 0.6672
     Episode_Reward/lifting_object: 16.6643
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 23.2917
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 2.15s
                      Time elapsed: 00:18:14
                               ETA: 00:53:36

################################################################################
                     [1m Learning iteration 508/2000 [0m                      

                       Computation: 46039 steps/s (collection: 2.043s, learning 0.093s)
             Mean action noise std: 2.27
          Mean value_function loss: 73.9398
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 48.8357
                       Mean reward: 96.18
               Mean episode length: 149.15
    Episode_Reward/reaching_object: 0.6697
     Episode_Reward/lifting_object: 16.6186
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 26.5417
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 2.14s
                      Time elapsed: 00:18:16
                               ETA: 00:53:34

################################################################################
                     [1m Learning iteration 509/2000 [0m                      

                       Computation: 46576 steps/s (collection: 1.988s, learning 0.122s)
             Mean action noise std: 2.27
          Mean value_function loss: 77.6685
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 48.8388
                       Mean reward: 70.71
               Mean episode length: 125.76
    Episode_Reward/reaching_object: 0.6562
     Episode_Reward/lifting_object: 15.9715
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 25.7500
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 2.11s
                      Time elapsed: 00:18:18
                               ETA: 00:53:32

################################################################################
                     [1m Learning iteration 510/2000 [0m                      

                       Computation: 44656 steps/s (collection: 2.057s, learning 0.145s)
             Mean action noise std: 2.27
          Mean value_function loss: 77.7138
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 48.8428
                       Mean reward: 87.59
               Mean episode length: 135.94
    Episode_Reward/reaching_object: 0.6605
     Episode_Reward/lifting_object: 17.0895
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.1667
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 2.20s
                      Time elapsed: 00:18:21
                               ETA: 00:53:30

################################################################################
                     [1m Learning iteration 511/2000 [0m                      

                       Computation: 44559 steps/s (collection: 2.051s, learning 0.156s)
             Mean action noise std: 2.27
          Mean value_function loss: 76.8437
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 48.8440
                       Mean reward: 82.33
               Mean episode length: 140.54
    Episode_Reward/reaching_object: 0.6394
     Episode_Reward/lifting_object: 16.3980
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 3.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 25.1667
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 2.21s
                      Time elapsed: 00:18:23
                               ETA: 00:53:28

################################################################################
                     [1m Learning iteration 512/2000 [0m                      

                       Computation: 45493 steps/s (collection: 2.049s, learning 0.112s)
             Mean action noise std: 2.27
          Mean value_function loss: 62.9785
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 48.8459
                       Mean reward: 81.33
               Mean episode length: 130.64
    Episode_Reward/reaching_object: 0.5975
     Episode_Reward/lifting_object: 15.3587
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 3.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 25.7500
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 2.16s
                      Time elapsed: 00:18:25
                               ETA: 00:53:26

################################################################################
                     [1m Learning iteration 513/2000 [0m                      

                       Computation: 45152 steps/s (collection: 2.033s, learning 0.145s)
             Mean action noise std: 2.27
          Mean value_function loss: 65.1899
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 48.8475
                       Mean reward: 88.82
               Mean episode length: 144.53
    Episode_Reward/reaching_object: 0.6074
     Episode_Reward/lifting_object: 16.1058
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 26.8750
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 2.18s
                      Time elapsed: 00:18:27
                               ETA: 00:53:24

################################################################################
                     [1m Learning iteration 514/2000 [0m                      

                       Computation: 46575 steps/s (collection: 2.021s, learning 0.090s)
             Mean action noise std: 2.27
          Mean value_function loss: 63.9543
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 48.8494
                       Mean reward: 79.95
               Mean episode length: 132.19
    Episode_Reward/reaching_object: 0.6077
     Episode_Reward/lifting_object: 15.9067
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 2.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 26.9583
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 2.11s
                      Time elapsed: 00:18:29
                               ETA: 00:53:21

################################################################################
                     [1m Learning iteration 515/2000 [0m                      

                       Computation: 47110 steps/s (collection: 1.994s, learning 0.093s)
             Mean action noise std: 2.27
          Mean value_function loss: 65.1108
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.8494
                       Mean reward: 87.72
               Mean episode length: 145.76
    Episode_Reward/reaching_object: 0.6068
     Episode_Reward/lifting_object: 16.1806
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 26.9583
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 2.09s
                      Time elapsed: 00:18:31
                               ETA: 00:53:19

################################################################################
                     [1m Learning iteration 516/2000 [0m                      

                       Computation: 46439 steps/s (collection: 2.013s, learning 0.104s)
             Mean action noise std: 2.27
          Mean value_function loss: 75.4322
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 48.8477
                       Mean reward: 89.32
               Mean episode length: 143.39
    Episode_Reward/reaching_object: 0.6097
     Episode_Reward/lifting_object: 16.2039
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 29.5833
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 2.12s
                      Time elapsed: 00:18:33
                               ETA: 00:53:17

################################################################################
                     [1m Learning iteration 517/2000 [0m                      

                       Computation: 42694 steps/s (collection: 2.127s, learning 0.175s)
             Mean action noise std: 2.27
          Mean value_function loss: 78.2376
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 48.8471
                       Mean reward: 85.32
               Mean episode length: 137.69
    Episode_Reward/reaching_object: 0.6006
     Episode_Reward/lifting_object: 16.1516
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.8750
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 2.30s
                      Time elapsed: 00:18:36
                               ETA: 00:53:15

################################################################################
                     [1m Learning iteration 518/2000 [0m                      

                       Computation: 45867 steps/s (collection: 2.023s, learning 0.121s)
             Mean action noise std: 2.27
          Mean value_function loss: 84.4791
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 48.8476
                       Mean reward: 81.69
               Mean episode length: 134.13
    Episode_Reward/reaching_object: 0.5877
     Episode_Reward/lifting_object: 16.0515
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.3750
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 2.14s
                      Time elapsed: 00:18:38
                               ETA: 00:53:13

################################################################################
                     [1m Learning iteration 519/2000 [0m                      

                       Computation: 43133 steps/s (collection: 2.130s, learning 0.149s)
             Mean action noise std: 2.27
          Mean value_function loss: 72.7847
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 48.8477
                       Mean reward: 78.07
               Mean episode length: 127.01
    Episode_Reward/reaching_object: 0.6041
     Episode_Reward/lifting_object: 15.9199
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 26.3333
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 2.28s
                      Time elapsed: 00:18:40
                               ETA: 00:53:11

################################################################################
                     [1m Learning iteration 520/2000 [0m                      

                       Computation: 45754 steps/s (collection: 2.029s, learning 0.120s)
             Mean action noise std: 2.27
          Mean value_function loss: 67.4305
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 48.8471
                       Mean reward: 93.58
               Mean episode length: 141.72
    Episode_Reward/reaching_object: 0.6086
     Episode_Reward/lifting_object: 16.5630
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 29.2500
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 2.15s
                      Time elapsed: 00:18:42
                               ETA: 00:53:09

################################################################################
                     [1m Learning iteration 521/2000 [0m                      

                       Computation: 46827 steps/s (collection: 2.002s, learning 0.098s)
             Mean action noise std: 2.27
          Mean value_function loss: 79.9841
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 48.8467
                       Mean reward: 85.66
               Mean episode length: 132.30
    Episode_Reward/reaching_object: 0.5966
     Episode_Reward/lifting_object: 16.6388
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.8750
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 2.10s
                      Time elapsed: 00:18:44
                               ETA: 00:53:07

################################################################################
                     [1m Learning iteration 522/2000 [0m                      

                       Computation: 45800 steps/s (collection: 2.050s, learning 0.097s)
             Mean action noise std: 2.27
          Mean value_function loss: 71.1609
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 48.8467
                       Mean reward: 89.30
               Mean episode length: 135.01
    Episode_Reward/reaching_object: 0.5945
     Episode_Reward/lifting_object: 16.6663
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 31.5417
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 2.15s
                      Time elapsed: 00:18:47
                               ETA: 00:53:04

################################################################################
                     [1m Learning iteration 523/2000 [0m                      

                       Computation: 46563 steps/s (collection: 2.022s, learning 0.089s)
             Mean action noise std: 2.27
          Mean value_function loss: 71.9362
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 48.8459
                       Mean reward: 82.93
               Mean episode length: 124.79
    Episode_Reward/reaching_object: 0.5748
     Episode_Reward/lifting_object: 16.0439
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 30.2083
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 2.11s
                      Time elapsed: 00:18:49
                               ETA: 00:53:02

################################################################################
                     [1m Learning iteration 524/2000 [0m                      

                       Computation: 44632 steps/s (collection: 2.081s, learning 0.121s)
             Mean action noise std: 2.27
          Mean value_function loss: 84.0742
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 48.8446
                       Mean reward: 90.10
               Mean episode length: 132.23
    Episode_Reward/reaching_object: 0.5903
     Episode_Reward/lifting_object: 16.7917
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 29.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 2.20s
                      Time elapsed: 00:18:51
                               ETA: 00:53:00

################################################################################
                     [1m Learning iteration 525/2000 [0m                      

                       Computation: 43516 steps/s (collection: 2.058s, learning 0.201s)
             Mean action noise std: 2.27
          Mean value_function loss: 90.1510
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 48.8453
                       Mean reward: 87.31
               Mean episode length: 129.09
    Episode_Reward/reaching_object: 0.5724
     Episode_Reward/lifting_object: 16.0318
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 27.5000
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 2.26s
                      Time elapsed: 00:18:53
                               ETA: 00:52:58

################################################################################
                     [1m Learning iteration 526/2000 [0m                      

                       Computation: 45885 steps/s (collection: 2.025s, learning 0.118s)
             Mean action noise std: 2.27
          Mean value_function loss: 77.5327
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 48.8456
                       Mean reward: 91.93
               Mean episode length: 131.99
    Episode_Reward/reaching_object: 0.5727
     Episode_Reward/lifting_object: 16.6189
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 31.1250
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 2.14s
                      Time elapsed: 00:18:55
                               ETA: 00:52:56

################################################################################
                     [1m Learning iteration 527/2000 [0m                      

                       Computation: 44446 steps/s (collection: 2.094s, learning 0.118s)
             Mean action noise std: 2.27
          Mean value_function loss: 83.1160
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 48.8457
                       Mean reward: 82.99
               Mean episode length: 121.11
    Episode_Reward/reaching_object: 0.5836
     Episode_Reward/lifting_object: 16.9347
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 29.7500
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 2.21s
                      Time elapsed: 00:18:57
                               ETA: 00:52:54

################################################################################
                     [1m Learning iteration 528/2000 [0m                      

                       Computation: 45677 steps/s (collection: 2.029s, learning 0.124s)
             Mean action noise std: 2.27
          Mean value_function loss: 78.2148
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 48.8458
                       Mean reward: 86.75
               Mean episode length: 122.98
    Episode_Reward/reaching_object: 0.5862
     Episode_Reward/lifting_object: 17.4732
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 34.4583
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 2.15s
                      Time elapsed: 00:19:00
                               ETA: 00:52:52

################################################################################
                     [1m Learning iteration 529/2000 [0m                      

                       Computation: 43543 steps/s (collection: 2.114s, learning 0.143s)
             Mean action noise std: 2.27
          Mean value_function loss: 87.0672
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 48.8460
                       Mean reward: 97.09
               Mean episode length: 129.64
    Episode_Reward/reaching_object: 0.5876
     Episode_Reward/lifting_object: 17.3346
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 30.0417
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 2.26s
                      Time elapsed: 00:19:02
                               ETA: 00:52:50

################################################################################
                     [1m Learning iteration 530/2000 [0m                      

                       Computation: 44947 steps/s (collection: 2.079s, learning 0.109s)
             Mean action noise std: 2.27
          Mean value_function loss: 82.9037
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 48.8454
                       Mean reward: 104.01
               Mean episode length: 133.70
    Episode_Reward/reaching_object: 0.6086
     Episode_Reward/lifting_object: 17.9295
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 29.3333
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 2.19s
                      Time elapsed: 00:19:04
                               ETA: 00:52:48

################################################################################
                     [1m Learning iteration 531/2000 [0m                      

                       Computation: 43007 steps/s (collection: 2.131s, learning 0.155s)
             Mean action noise std: 2.27
          Mean value_function loss: 102.4685
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 48.8459
                       Mean reward: 91.52
               Mean episode length: 124.18
    Episode_Reward/reaching_object: 0.5970
     Episode_Reward/lifting_object: 17.8999
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.2500
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 2.29s
                      Time elapsed: 00:19:06
                               ETA: 00:52:46

################################################################################
                     [1m Learning iteration 532/2000 [0m                      

                       Computation: 44647 steps/s (collection: 2.107s, learning 0.095s)
             Mean action noise std: 2.27
          Mean value_function loss: 85.1935
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 48.8473
                       Mean reward: 99.56
               Mean episode length: 132.48
    Episode_Reward/reaching_object: 0.6157
     Episode_Reward/lifting_object: 18.1977
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 31.5417
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 2.20s
                      Time elapsed: 00:19:09
                               ETA: 00:52:44

################################################################################
                     [1m Learning iteration 533/2000 [0m                      

                       Computation: 46374 steps/s (collection: 2.031s, learning 0.089s)
             Mean action noise std: 2.27
          Mean value_function loss: 94.4971
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 48.8478
                       Mean reward: 83.87
               Mean episode length: 121.35
    Episode_Reward/reaching_object: 0.5949
     Episode_Reward/lifting_object: 17.3125
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0360
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.3750
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 2.12s
                      Time elapsed: 00:19:11
                               ETA: 00:52:42

################################################################################
                     [1m Learning iteration 534/2000 [0m                      

                       Computation: 44419 steps/s (collection: 2.106s, learning 0.107s)
             Mean action noise std: 2.27
          Mean value_function loss: 85.5137
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 48.8482
                       Mean reward: 92.02
               Mean episode length: 118.60
    Episode_Reward/reaching_object: 0.5943
     Episode_Reward/lifting_object: 18.2494
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.5417
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 2.21s
                      Time elapsed: 00:19:13
                               ETA: 00:52:40

################################################################################
                     [1m Learning iteration 535/2000 [0m                      

                       Computation: 43990 steps/s (collection: 2.069s, learning 0.166s)
             Mean action noise std: 2.27
          Mean value_function loss: 83.8077
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 48.8486
                       Mean reward: 84.44
               Mean episode length: 113.37
    Episode_Reward/reaching_object: 0.5904
     Episode_Reward/lifting_object: 17.7538
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 32.3750
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 2.23s
                      Time elapsed: 00:19:15
                               ETA: 00:52:38

################################################################################
                     [1m Learning iteration 536/2000 [0m                      

                       Computation: 42867 steps/s (collection: 2.194s, learning 0.100s)
             Mean action noise std: 2.27
          Mean value_function loss: 88.5289
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 48.8490
                       Mean reward: 104.22
               Mean episode length: 138.51
    Episode_Reward/reaching_object: 0.6078
     Episode_Reward/lifting_object: 18.7312
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 33.8750
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 2.29s
                      Time elapsed: 00:19:17
                               ETA: 00:52:36

################################################################################
                     [1m Learning iteration 537/2000 [0m                      

                       Computation: 45865 steps/s (collection: 2.045s, learning 0.098s)
             Mean action noise std: 2.27
          Mean value_function loss: 106.7977
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 48.8492
                       Mean reward: 85.95
               Mean episode length: 121.53
    Episode_Reward/reaching_object: 0.5829
     Episode_Reward/lifting_object: 17.8607
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 32.5417
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 2.14s
                      Time elapsed: 00:19:20
                               ETA: 00:52:34

################################################################################
                     [1m Learning iteration 538/2000 [0m                      

                       Computation: 44253 steps/s (collection: 2.094s, learning 0.127s)
             Mean action noise std: 2.27
          Mean value_function loss: 86.0658
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 48.8505
                       Mean reward: 98.29
               Mean episode length: 117.93
    Episode_Reward/reaching_object: 0.6050
     Episode_Reward/lifting_object: 18.4708
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 32.5000
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 2.22s
                      Time elapsed: 00:19:22
                               ETA: 00:52:32

################################################################################
                     [1m Learning iteration 539/2000 [0m                      

                       Computation: 44157 steps/s (collection: 2.101s, learning 0.126s)
             Mean action noise std: 2.27
          Mean value_function loss: 89.0708
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 48.8504
                       Mean reward: 93.20
               Mean episode length: 123.07
    Episode_Reward/reaching_object: 0.5969
     Episode_Reward/lifting_object: 19.1317
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 32.8333
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 2.23s
                      Time elapsed: 00:19:24
                               ETA: 00:52:30

################################################################################
                     [1m Learning iteration 540/2000 [0m                      

                       Computation: 44393 steps/s (collection: 2.030s, learning 0.185s)
             Mean action noise std: 2.27
          Mean value_function loss: 89.9727
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 48.8509
                       Mean reward: 88.62
               Mean episode length: 110.25
    Episode_Reward/reaching_object: 0.5591
     Episode_Reward/lifting_object: 18.1268
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0330
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 31.6250
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 2.21s
                      Time elapsed: 00:19:26
                               ETA: 00:52:28

################################################################################
                     [1m Learning iteration 541/2000 [0m                      

                       Computation: 44874 steps/s (collection: 2.097s, learning 0.094s)
             Mean action noise std: 2.27
          Mean value_function loss: 89.0398
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 48.8513
                       Mean reward: 98.03
               Mean episode length: 130.93
    Episode_Reward/reaching_object: 0.5825
     Episode_Reward/lifting_object: 18.6577
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 35.0833
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 2.19s
                      Time elapsed: 00:19:28
                               ETA: 00:52:26

################################################################################
                     [1m Learning iteration 542/2000 [0m                      

                       Computation: 45110 steps/s (collection: 2.068s, learning 0.112s)
             Mean action noise std: 2.27
          Mean value_function loss: 93.3986
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 48.8515
                       Mean reward: 82.85
               Mean episode length: 114.49
    Episode_Reward/reaching_object: 0.5593
     Episode_Reward/lifting_object: 17.4285
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 31.7917
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 2.18s
                      Time elapsed: 00:19:31
                               ETA: 00:52:24

################################################################################
                     [1m Learning iteration 543/2000 [0m                      

                       Computation: 43607 steps/s (collection: 2.138s, learning 0.116s)
             Mean action noise std: 2.27
          Mean value_function loss: 92.6201
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 48.8507
                       Mean reward: 95.91
               Mean episode length: 120.45
    Episode_Reward/reaching_object: 0.5535
     Episode_Reward/lifting_object: 18.1567
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 34.2917
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 2.25s
                      Time elapsed: 00:19:33
                               ETA: 00:52:22

################################################################################
                     [1m Learning iteration 544/2000 [0m                      

                       Computation: 45577 steps/s (collection: 2.062s, learning 0.095s)
             Mean action noise std: 2.27
          Mean value_function loss: 100.3173
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 48.8489
                       Mean reward: 90.19
               Mean episode length: 116.00
    Episode_Reward/reaching_object: 0.5563
     Episode_Reward/lifting_object: 18.6100
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 34.9583
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 2.16s
                      Time elapsed: 00:19:35
                               ETA: 00:52:20

################################################################################
                     [1m Learning iteration 545/2000 [0m                      

                       Computation: 42012 steps/s (collection: 2.200s, learning 0.140s)
             Mean action noise std: 2.27
          Mean value_function loss: 107.0835
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 48.8500
                       Mean reward: 93.89
               Mean episode length: 114.48
    Episode_Reward/reaching_object: 0.5504
     Episode_Reward/lifting_object: 18.5822
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 36.3333
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 2.34s
                      Time elapsed: 00:19:37
                               ETA: 00:52:18

################################################################################
                     [1m Learning iteration 546/2000 [0m                      

                       Computation: 44446 steps/s (collection: 2.109s, learning 0.103s)
             Mean action noise std: 2.27
          Mean value_function loss: 97.6696
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.8501
                       Mean reward: 94.89
               Mean episode length: 118.16
    Episode_Reward/reaching_object: 0.5327
     Episode_Reward/lifting_object: 17.6314
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 33.6667
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 2.21s
                      Time elapsed: 00:19:40
                               ETA: 00:52:16

################################################################################
                     [1m Learning iteration 547/2000 [0m                      

                       Computation: 44663 steps/s (collection: 2.083s, learning 0.118s)
             Mean action noise std: 2.27
          Mean value_function loss: 106.3788
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 48.8511
                       Mean reward: 91.74
               Mean episode length: 116.24
    Episode_Reward/reaching_object: 0.5532
     Episode_Reward/lifting_object: 18.8122
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 33.5000
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 2.20s
                      Time elapsed: 00:19:42
                               ETA: 00:52:14

################################################################################
                     [1m Learning iteration 548/2000 [0m                      

                       Computation: 44910 steps/s (collection: 2.081s, learning 0.108s)
             Mean action noise std: 2.27
          Mean value_function loss: 101.2567
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 48.8521
                       Mean reward: 99.92
               Mean episode length: 120.21
    Episode_Reward/reaching_object: 0.5315
     Episode_Reward/lifting_object: 18.0359
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 34.1667
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 2.19s
                      Time elapsed: 00:19:44
                               ETA: 00:52:12

################################################################################
                     [1m Learning iteration 549/2000 [0m                      

                       Computation: 46108 steps/s (collection: 2.023s, learning 0.109s)
             Mean action noise std: 2.27
          Mean value_function loss: 93.7946
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 48.8524
                       Mean reward: 101.43
               Mean episode length: 121.80
    Episode_Reward/reaching_object: 0.5449
     Episode_Reward/lifting_object: 18.4477
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 34.7500
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 2.13s
                      Time elapsed: 00:19:46
                               ETA: 00:52:10

################################################################################
                     [1m Learning iteration 550/2000 [0m                      

                       Computation: 46027 steps/s (collection: 2.041s, learning 0.095s)
             Mean action noise std: 2.27
          Mean value_function loss: 109.5908
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 48.8519
                       Mean reward: 88.43
               Mean episode length: 107.36
    Episode_Reward/reaching_object: 0.5117
     Episode_Reward/lifting_object: 17.6425
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 35.8750
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 2.14s
                      Time elapsed: 00:19:48
                               ETA: 00:52:08

################################################################################
                     [1m Learning iteration 551/2000 [0m                      

                       Computation: 44171 steps/s (collection: 2.079s, learning 0.146s)
             Mean action noise std: 2.27
          Mean value_function loss: 93.3012
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 48.8519
                       Mean reward: 101.39
               Mean episode length: 118.89
    Episode_Reward/reaching_object: 0.5411
     Episode_Reward/lifting_object: 18.2962
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 35.6667
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 2.23s
                      Time elapsed: 00:19:50
                               ETA: 00:52:06

################################################################################
                     [1m Learning iteration 552/2000 [0m                      

                       Computation: 45761 steps/s (collection: 2.053s, learning 0.096s)
             Mean action noise std: 2.27
          Mean value_function loss: 91.6613
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 48.8514
                       Mean reward: 88.96
               Mean episode length: 111.92
    Episode_Reward/reaching_object: 0.5402
     Episode_Reward/lifting_object: 18.4626
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0330
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 34.2500
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 2.15s
                      Time elapsed: 00:19:53
                               ETA: 00:52:03

################################################################################
                     [1m Learning iteration 553/2000 [0m                      

                       Computation: 46483 steps/s (collection: 2.022s, learning 0.093s)
             Mean action noise std: 2.27
          Mean value_function loss: 97.7814
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 48.8513
                       Mean reward: 90.75
               Mean episode length: 109.77
    Episode_Reward/reaching_object: 0.5309
     Episode_Reward/lifting_object: 18.1965
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 33.2083
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 2.11s
                      Time elapsed: 00:19:55
                               ETA: 00:52:01

################################################################################
                     [1m Learning iteration 554/2000 [0m                      

                       Computation: 46488 steps/s (collection: 2.017s, learning 0.098s)
             Mean action noise std: 2.27
          Mean value_function loss: 110.9613
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 48.8519
                       Mean reward: 101.87
               Mean episode length: 119.00
    Episode_Reward/reaching_object: 0.5502
     Episode_Reward/lifting_object: 19.1285
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 35.5417
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 2.11s
                      Time elapsed: 00:19:57
                               ETA: 00:51:59

################################################################################
                     [1m Learning iteration 555/2000 [0m                      

                       Computation: 46540 steps/s (collection: 2.018s, learning 0.094s)
             Mean action noise std: 2.27
          Mean value_function loss: 106.0388
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 48.8533
                       Mean reward: 98.59
               Mean episode length: 118.12
    Episode_Reward/reaching_object: 0.5248
     Episode_Reward/lifting_object: 18.3496
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 35.6667
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 2.11s
                      Time elapsed: 00:19:59
                               ETA: 00:51:57

################################################################################
                     [1m Learning iteration 556/2000 [0m                      

                       Computation: 45793 steps/s (collection: 2.057s, learning 0.090s)
             Mean action noise std: 2.27
          Mean value_function loss: 103.1928
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 48.8542
                       Mean reward: 92.37
               Mean episode length: 114.24
    Episode_Reward/reaching_object: 0.5525
     Episode_Reward/lifting_object: 19.3723
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 35.2917
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 2.15s
                      Time elapsed: 00:20:01
                               ETA: 00:51:54

################################################################################
                     [1m Learning iteration 557/2000 [0m                      

                       Computation: 46535 steps/s (collection: 2.010s, learning 0.103s)
             Mean action noise std: 2.27
          Mean value_function loss: 106.0880
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 48.8550
                       Mean reward: 107.80
               Mean episode length: 123.54
    Episode_Reward/reaching_object: 0.5306
     Episode_Reward/lifting_object: 19.1013
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 34.7083
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 2.11s
                      Time elapsed: 00:20:03
                               ETA: 00:51:52

################################################################################
                     [1m Learning iteration 558/2000 [0m                      

                       Computation: 44001 steps/s (collection: 1.999s, learning 0.235s)
             Mean action noise std: 2.27
          Mean value_function loss: 109.2646
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 48.8572
                       Mean reward: 101.63
               Mean episode length: 116.83
    Episode_Reward/reaching_object: 0.5463
     Episode_Reward/lifting_object: 19.7326
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 35.2917
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 2.23s
                      Time elapsed: 00:20:05
                               ETA: 00:51:50

################################################################################
                     [1m Learning iteration 559/2000 [0m                      

                       Computation: 42423 steps/s (collection: 2.202s, learning 0.116s)
             Mean action noise std: 2.27
          Mean value_function loss: 112.9429
               Mean surrogate loss: 0.0102
                 Mean entropy loss: 48.8611
                       Mean reward: 98.44
               Mean episode length: 113.51
    Episode_Reward/reaching_object: 0.5374
     Episode_Reward/lifting_object: 19.5015
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 33.9167
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 2.32s
                      Time elapsed: 00:20:08
                               ETA: 00:51:48

################################################################################
                     [1m Learning iteration 560/2000 [0m                      

                       Computation: 45261 steps/s (collection: 2.043s, learning 0.129s)
             Mean action noise std: 2.27
          Mean value_function loss: 103.9584
               Mean surrogate loss: 0.0111
                 Mean entropy loss: 48.8620
                       Mean reward: 94.33
               Mean episode length: 111.81
    Episode_Reward/reaching_object: 0.5487
     Episode_Reward/lifting_object: 20.0691
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 34.1667
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 2.17s
                      Time elapsed: 00:20:10
                               ETA: 00:51:46

################################################################################
                     [1m Learning iteration 561/2000 [0m                      

                       Computation: 45420 steps/s (collection: 2.048s, learning 0.117s)
             Mean action noise std: 2.27
          Mean value_function loss: 101.7923
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 48.8621
                       Mean reward: 96.60
               Mean episode length: 116.32
    Episode_Reward/reaching_object: 0.5345
     Episode_Reward/lifting_object: 19.5265
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 34.4167
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 2.16s
                      Time elapsed: 00:20:12
                               ETA: 00:51:44

################################################################################
                     [1m Learning iteration 562/2000 [0m                      

                       Computation: 43543 steps/s (collection: 2.069s, learning 0.188s)
             Mean action noise std: 2.27
          Mean value_function loss: 106.2867
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.8627
                       Mean reward: 101.38
               Mean episode length: 113.56
    Episode_Reward/reaching_object: 0.5412
     Episode_Reward/lifting_object: 19.9050
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 34.9583
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 2.26s
                      Time elapsed: 00:20:14
                               ETA: 00:51:42

################################################################################
                     [1m Learning iteration 563/2000 [0m                      

                       Computation: 40988 steps/s (collection: 2.249s, learning 0.149s)
             Mean action noise std: 2.27
          Mean value_function loss: 120.1446
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 48.8642
                       Mean reward: 102.66
               Mean episode length: 111.93
    Episode_Reward/reaching_object: 0.5473
     Episode_Reward/lifting_object: 20.8695
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 33.5833
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 2.40s
                      Time elapsed: 00:20:17
                               ETA: 00:51:41

################################################################################
                     [1m Learning iteration 564/2000 [0m                      

                       Computation: 46027 steps/s (collection: 2.026s, learning 0.110s)
             Mean action noise std: 2.27
          Mean value_function loss: 108.9002
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 48.8644
                       Mean reward: 111.71
               Mean episode length: 124.20
    Episode_Reward/reaching_object: 0.5659
     Episode_Reward/lifting_object: 20.6967
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 34.7500
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 2.14s
                      Time elapsed: 00:20:19
                               ETA: 00:51:39

################################################################################
                     [1m Learning iteration 565/2000 [0m                      

                       Computation: 44337 steps/s (collection: 2.115s, learning 0.103s)
             Mean action noise std: 2.27
          Mean value_function loss: 107.9683
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 48.8646
                       Mean reward: 100.85
               Mean episode length: 108.50
    Episode_Reward/reaching_object: 0.5414
     Episode_Reward/lifting_object: 20.3976
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 33.7917
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 2.22s
                      Time elapsed: 00:20:21
                               ETA: 00:51:37

################################################################################
                     [1m Learning iteration 566/2000 [0m                      

                       Computation: 46024 steps/s (collection: 2.009s, learning 0.127s)
             Mean action noise std: 2.27
          Mean value_function loss: 101.5124
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 48.8643
                       Mean reward: 104.03
               Mean episode length: 108.76
    Episode_Reward/reaching_object: 0.5501
     Episode_Reward/lifting_object: 21.2733
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 33.2500
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 2.14s
                      Time elapsed: 00:20:23
                               ETA: 00:51:34

################################################################################
                     [1m Learning iteration 567/2000 [0m                      

                       Computation: 45596 steps/s (collection: 2.039s, learning 0.117s)
             Mean action noise std: 2.27
          Mean value_function loss: 102.4644
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 48.8642
                       Mean reward: 107.34
               Mean episode length: 117.49
    Episode_Reward/reaching_object: 0.5557
     Episode_Reward/lifting_object: 20.7686
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 32.9167
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 2.16s
                      Time elapsed: 00:20:25
                               ETA: 00:51:32

################################################################################
                     [1m Learning iteration 568/2000 [0m                      

                       Computation: 44659 steps/s (collection: 2.102s, learning 0.099s)
             Mean action noise std: 2.27
          Mean value_function loss: 103.9276
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 48.8631
                       Mean reward: 116.13
               Mean episode length: 124.56
    Episode_Reward/reaching_object: 0.5572
     Episode_Reward/lifting_object: 21.0018
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 32.5000
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 2.20s
                      Time elapsed: 00:20:28
                               ETA: 00:51:30

################################################################################
                     [1m Learning iteration 569/2000 [0m                      

                       Computation: 44934 steps/s (collection: 2.090s, learning 0.098s)
             Mean action noise std: 2.27
          Mean value_function loss: 115.8457
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 48.8647
                       Mean reward: 96.16
               Mean episode length: 114.43
    Episode_Reward/reaching_object: 0.5747
     Episode_Reward/lifting_object: 20.7965
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 35.5417
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 2.19s
                      Time elapsed: 00:20:30
                               ETA: 00:51:28

################################################################################
                     [1m Learning iteration 570/2000 [0m                      

                       Computation: 44656 steps/s (collection: 2.084s, learning 0.118s)
             Mean action noise std: 2.27
          Mean value_function loss: 119.8829
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 48.8647
                       Mean reward: 110.32
               Mean episode length: 120.60
    Episode_Reward/reaching_object: 0.5559
     Episode_Reward/lifting_object: 20.0173
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 35.1667
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 2.20s
                      Time elapsed: 00:20:32
                               ETA: 00:51:26

################################################################################
                     [1m Learning iteration 571/2000 [0m                      

                       Computation: 45928 steps/s (collection: 2.037s, learning 0.103s)
             Mean action noise std: 2.28
          Mean value_function loss: 119.4642
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.8649
                       Mean reward: 113.43
               Mean episode length: 121.60
    Episode_Reward/reaching_object: 0.5738
     Episode_Reward/lifting_object: 21.2063
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 33.6667
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 2.14s
                      Time elapsed: 00:20:34
                               ETA: 00:51:24

################################################################################
                     [1m Learning iteration 572/2000 [0m                      

                       Computation: 45844 steps/s (collection: 2.043s, learning 0.102s)
             Mean action noise std: 2.28
          Mean value_function loss: 118.4262
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 48.8644
                       Mean reward: 119.81
               Mean episode length: 126.32
    Episode_Reward/reaching_object: 0.5540
     Episode_Reward/lifting_object: 20.8859
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 31.6667
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 2.14s
                      Time elapsed: 00:20:36
                               ETA: 00:51:22

################################################################################
                     [1m Learning iteration 573/2000 [0m                      

                       Computation: 45623 steps/s (collection: 2.064s, learning 0.091s)
             Mean action noise std: 2.28
          Mean value_function loss: 111.1048
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 48.8655
                       Mean reward: 110.95
               Mean episode length: 115.69
    Episode_Reward/reaching_object: 0.5529
     Episode_Reward/lifting_object: 21.1813
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 34.2083
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 2.15s
                      Time elapsed: 00:20:38
                               ETA: 00:51:19

################################################################################
                     [1m Learning iteration 574/2000 [0m                      

                       Computation: 44711 steps/s (collection: 2.074s, learning 0.125s)
             Mean action noise std: 2.28
          Mean value_function loss: 111.3498
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 48.8659
                       Mean reward: 112.61
               Mean episode length: 111.82
    Episode_Reward/reaching_object: 0.5671
     Episode_Reward/lifting_object: 21.7026
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 36.8333
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 2.20s
                      Time elapsed: 00:20:41
                               ETA: 00:51:17

################################################################################
                     [1m Learning iteration 575/2000 [0m                      

                       Computation: 44467 steps/s (collection: 2.117s, learning 0.094s)
             Mean action noise std: 2.28
          Mean value_function loss: 116.5056
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 48.8661
                       Mean reward: 119.75
               Mean episode length: 127.04
    Episode_Reward/reaching_object: 0.5672
     Episode_Reward/lifting_object: 21.7910
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 36.6667
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 2.21s
                      Time elapsed: 00:20:43
                               ETA: 00:51:15

################################################################################
                     [1m Learning iteration 576/2000 [0m                      

                       Computation: 45435 steps/s (collection: 2.068s, learning 0.096s)
             Mean action noise std: 2.28
          Mean value_function loss: 106.1246
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 48.8662
                       Mean reward: 105.11
               Mean episode length: 109.89
    Episode_Reward/reaching_object: 0.5630
     Episode_Reward/lifting_object: 21.2620
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0334
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 34.1667
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 2.16s
                      Time elapsed: 00:20:45
                               ETA: 00:51:13

################################################################################
                     [1m Learning iteration 577/2000 [0m                      

                       Computation: 45528 steps/s (collection: 2.063s, learning 0.096s)
             Mean action noise std: 2.28
          Mean value_function loss: 110.0580
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 48.8662
                       Mean reward: 109.01
               Mean episode length: 107.60
    Episode_Reward/reaching_object: 0.5361
     Episode_Reward/lifting_object: 21.6902
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 36.5833
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 2.16s
                      Time elapsed: 00:20:47
                               ETA: 00:51:11

################################################################################
                     [1m Learning iteration 578/2000 [0m                      

                       Computation: 45325 steps/s (collection: 2.031s, learning 0.138s)
             Mean action noise std: 2.28
          Mean value_function loss: 113.7460
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 48.8660
                       Mean reward: 105.56
               Mean episode length: 109.08
    Episode_Reward/reaching_object: 0.5437
     Episode_Reward/lifting_object: 21.8167
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 36.5833
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 2.17s
                      Time elapsed: 00:20:49
                               ETA: 00:51:09

################################################################################
                     [1m Learning iteration 579/2000 [0m                      

                       Computation: 46060 steps/s (collection: 2.004s, learning 0.130s)
             Mean action noise std: 2.28
          Mean value_function loss: 114.8419
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 48.8647
                       Mean reward: 115.64
               Mean episode length: 113.29
    Episode_Reward/reaching_object: 0.5371
     Episode_Reward/lifting_object: 22.0222
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 36.6250
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 2.13s
                      Time elapsed: 00:20:51
                               ETA: 00:51:07

################################################################################
                     [1m Learning iteration 580/2000 [0m                      

                       Computation: 46655 steps/s (collection: 1.986s, learning 0.121s)
             Mean action noise std: 2.28
          Mean value_function loss: 121.7627
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 48.8632
                       Mean reward: 100.74
               Mean episode length: 106.26
    Episode_Reward/reaching_object: 0.5277
     Episode_Reward/lifting_object: 21.2392
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 36.7500
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 2.11s
                      Time elapsed: 00:20:54
                               ETA: 00:51:04

################################################################################
                     [1m Learning iteration 581/2000 [0m                      

                       Computation: 46855 steps/s (collection: 1.990s, learning 0.108s)
             Mean action noise std: 2.28
          Mean value_function loss: 119.9253
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 48.8615
                       Mean reward: 107.76
               Mean episode length: 107.73
    Episode_Reward/reaching_object: 0.5220
     Episode_Reward/lifting_object: 21.3272
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 38.2500
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 2.10s
                      Time elapsed: 00:20:56
                               ETA: 00:51:02

################################################################################
                     [1m Learning iteration 582/2000 [0m                      

                       Computation: 46741 steps/s (collection: 1.995s, learning 0.108s)
             Mean action noise std: 2.28
          Mean value_function loss: 132.2718
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 48.8602
                       Mean reward: 101.69
               Mean episode length: 103.46
    Episode_Reward/reaching_object: 0.5181
     Episode_Reward/lifting_object: 21.4396
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 37.3333
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 2.10s
                      Time elapsed: 00:20:58
                               ETA: 00:51:00

################################################################################
                     [1m Learning iteration 583/2000 [0m                      

                       Computation: 47154 steps/s (collection: 1.992s, learning 0.093s)
             Mean action noise std: 2.28
          Mean value_function loss: 137.0515
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 48.8616
                       Mean reward: 107.36
               Mean episode length: 106.19
    Episode_Reward/reaching_object: 0.5198
     Episode_Reward/lifting_object: 21.1526
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 38.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 2.08s
                      Time elapsed: 00:21:00
                               ETA: 00:50:57

################################################################################
                     [1m Learning iteration 584/2000 [0m                      

                       Computation: 45056 steps/s (collection: 2.064s, learning 0.118s)
             Mean action noise std: 2.28
          Mean value_function loss: 131.1039
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 48.8601
                       Mean reward: 113.79
               Mean episode length: 111.49
    Episode_Reward/reaching_object: 0.5152
     Episode_Reward/lifting_object: 21.3299
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 37.5833
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 2.18s
                      Time elapsed: 00:21:02
                               ETA: 00:50:55

################################################################################
                     [1m Learning iteration 585/2000 [0m                      

                       Computation: 46749 steps/s (collection: 1.982s, learning 0.121s)
             Mean action noise std: 2.28
          Mean value_function loss: 133.6013
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 48.8615
                       Mean reward: 102.26
               Mean episode length: 101.29
    Episode_Reward/reaching_object: 0.5042
     Episode_Reward/lifting_object: 21.0530
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 37.3750
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 2.10s
                      Time elapsed: 00:21:04
                               ETA: 00:50:53

################################################################################
                     [1m Learning iteration 586/2000 [0m                      

                       Computation: 46789 steps/s (collection: 1.992s, learning 0.109s)
             Mean action noise std: 2.28
          Mean value_function loss: 129.8543
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 48.8615
                       Mean reward: 105.16
               Mean episode length: 103.12
    Episode_Reward/reaching_object: 0.5197
     Episode_Reward/lifting_object: 21.6944
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 37.6250
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 2.10s
                      Time elapsed: 00:21:06
                               ETA: 00:50:51

################################################################################
                     [1m Learning iteration 587/2000 [0m                      

                       Computation: 45089 steps/s (collection: 2.060s, learning 0.121s)
             Mean action noise std: 2.28
          Mean value_function loss: 124.0532
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 48.8596
                       Mean reward: 111.57
               Mean episode length: 108.40
    Episode_Reward/reaching_object: 0.5023
     Episode_Reward/lifting_object: 21.4780
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 38.9167
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 2.18s
                      Time elapsed: 00:21:08
                               ETA: 00:50:49

################################################################################
                     [1m Learning iteration 588/2000 [0m                      

                       Computation: 46028 steps/s (collection: 2.012s, learning 0.124s)
             Mean action noise std: 2.28
          Mean value_function loss: 127.9787
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 48.8579
                       Mean reward: 111.66
               Mean episode length: 112.89
    Episode_Reward/reaching_object: 0.5214
     Episode_Reward/lifting_object: 21.8418
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 37.2917
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 2.14s
                      Time elapsed: 00:21:11
                               ETA: 00:50:46

################################################################################
                     [1m Learning iteration 589/2000 [0m                      

                       Computation: 45571 steps/s (collection: 2.045s, learning 0.113s)
             Mean action noise std: 2.28
          Mean value_function loss: 128.2924
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 48.8579
                       Mean reward: 115.72
               Mean episode length: 109.04
    Episode_Reward/reaching_object: 0.4880
     Episode_Reward/lifting_object: 21.2043
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 38.5000
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 2.16s
                      Time elapsed: 00:21:13
                               ETA: 00:50:44

################################################################################
                     [1m Learning iteration 590/2000 [0m                      

                       Computation: 46890 steps/s (collection: 1.987s, learning 0.109s)
             Mean action noise std: 2.28
          Mean value_function loss: 135.3195
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 48.8579
                       Mean reward: 104.47
               Mean episode length: 102.88
    Episode_Reward/reaching_object: 0.4952
     Episode_Reward/lifting_object: 21.5518
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 39.5833
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 2.10s
                      Time elapsed: 00:21:15
                               ETA: 00:50:42

################################################################################
                     [1m Learning iteration 591/2000 [0m                      

                       Computation: 45826 steps/s (collection: 2.020s, learning 0.126s)
             Mean action noise std: 2.28
          Mean value_function loss: 123.6250
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 48.8581
                       Mean reward: 111.95
               Mean episode length: 106.35
    Episode_Reward/reaching_object: 0.5028
     Episode_Reward/lifting_object: 22.6364
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 39.1250
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 2.15s
                      Time elapsed: 00:21:17
                               ETA: 00:50:40

################################################################################
                     [1m Learning iteration 592/2000 [0m                      

                       Computation: 46921 steps/s (collection: 1.982s, learning 0.113s)
             Mean action noise std: 2.28
          Mean value_function loss: 125.8372
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 48.8590
                       Mean reward: 104.67
               Mean episode length: 100.13
    Episode_Reward/reaching_object: 0.4874
     Episode_Reward/lifting_object: 22.2447
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 39.0417
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 2.10s
                      Time elapsed: 00:21:19
                               ETA: 00:50:37

################################################################################
                     [1m Learning iteration 593/2000 [0m                      

                       Computation: 47308 steps/s (collection: 1.977s, learning 0.101s)
             Mean action noise std: 2.28
          Mean value_function loss: 129.0923
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 48.8606
                       Mean reward: 118.96
               Mean episode length: 110.38
    Episode_Reward/reaching_object: 0.4819
     Episode_Reward/lifting_object: 21.6494
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 37.7083
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 2.08s
                      Time elapsed: 00:21:21
                               ETA: 00:50:35

################################################################################
                     [1m Learning iteration 594/2000 [0m                      

                       Computation: 46379 steps/s (collection: 2.001s, learning 0.119s)
             Mean action noise std: 2.28
          Mean value_function loss: 122.7717
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 48.8604
                       Mean reward: 113.77
               Mean episode length: 108.68
    Episode_Reward/reaching_object: 0.4883
     Episode_Reward/lifting_object: 22.1116
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 37.1250
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 2.12s
                      Time elapsed: 00:21:23
                               ETA: 00:50:33

################################################################################
                     [1m Learning iteration 595/2000 [0m                      

                       Computation: 46328 steps/s (collection: 2.011s, learning 0.111s)
             Mean action noise std: 2.28
          Mean value_function loss: 121.6654
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 48.8602
                       Mean reward: 122.02
               Mean episode length: 111.34
    Episode_Reward/reaching_object: 0.4827
     Episode_Reward/lifting_object: 22.0948
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 39.7917
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 2.12s
                      Time elapsed: 00:21:25
                               ETA: 00:50:31

################################################################################
                     [1m Learning iteration 596/2000 [0m                      

                       Computation: 46449 steps/s (collection: 2.020s, learning 0.096s)
             Mean action noise std: 2.28
          Mean value_function loss: 27060626.3500
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 48.8603
                       Mean reward: 106.18
               Mean episode length: 104.32
    Episode_Reward/reaching_object: 0.4850
     Episode_Reward/lifting_object: 21.7262
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.1863
          Episode_Reward/joint_vel: -137.4872
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 40.0417
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 2.12s
                      Time elapsed: 00:21:27
                               ETA: 00:50:28

################################################################################
                     [1m Learning iteration 597/2000 [0m                      

                       Computation: 46094 steps/s (collection: 2.018s, learning 0.115s)
             Mean action noise std: 2.28
          Mean value_function loss: 116.8058
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 48.8605
                       Mean reward: 116.23
               Mean episode length: 107.88
    Episode_Reward/reaching_object: 0.4780
     Episode_Reward/lifting_object: 21.1983
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 38.0833
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 2.13s
                      Time elapsed: 00:21:30
                               ETA: 00:50:26

################################################################################
                     [1m Learning iteration 598/2000 [0m                      

                       Computation: 44304 steps/s (collection: 2.100s, learning 0.119s)
             Mean action noise std: 2.28
          Mean value_function loss: 127.7605
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 48.8607
                       Mean reward: 100.50
               Mean episode length: 102.67
    Episode_Reward/reaching_object: 0.4737
     Episode_Reward/lifting_object: 20.8308
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 39.4583
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 2.22s
                      Time elapsed: 00:21:32
                               ETA: 00:50:24

################################################################################
                     [1m Learning iteration 599/2000 [0m                      

                       Computation: 45928 steps/s (collection: 2.021s, learning 0.119s)
             Mean action noise std: 2.28
          Mean value_function loss: 128.1157
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 48.8597
                       Mean reward: 93.13
               Mean episode length: 97.91
    Episode_Reward/reaching_object: 0.4818
     Episode_Reward/lifting_object: 21.3076
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 38.6250
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 2.14s
                      Time elapsed: 00:21:34
                               ETA: 00:50:22

################################################################################
                     [1m Learning iteration 600/2000 [0m                      

                       Computation: 46441 steps/s (collection: 2.011s, learning 0.106s)
             Mean action noise std: 2.28
          Mean value_function loss: 124.5219
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 48.8582
                       Mean reward: 110.02
               Mean episode length: 100.95
    Episode_Reward/reaching_object: 0.4802
     Episode_Reward/lifting_object: 21.2728
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 38.6667
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 2.12s
                      Time elapsed: 00:21:36
                               ETA: 00:50:20

################################################################################
                     [1m Learning iteration 601/2000 [0m                      

                       Computation: 44086 steps/s (collection: 2.109s, learning 0.121s)
             Mean action noise std: 2.28
          Mean value_function loss: 133.0965
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 48.8584
                       Mean reward: 116.40
               Mean episode length: 104.72
    Episode_Reward/reaching_object: 0.4918
     Episode_Reward/lifting_object: 22.0452
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 38.4167
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 2.23s
                      Time elapsed: 00:21:38
                               ETA: 00:50:18

################################################################################
                     [1m Learning iteration 602/2000 [0m                      

                       Computation: 45404 steps/s (collection: 2.045s, learning 0.120s)
             Mean action noise std: 2.28
          Mean value_function loss: 125.2370
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 48.8585
                       Mean reward: 102.54
               Mean episode length: 108.17
    Episode_Reward/reaching_object: 0.4893
     Episode_Reward/lifting_object: 21.6629
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 37.7083
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 2.17s
                      Time elapsed: 00:21:40
                               ETA: 00:50:16

################################################################################
                     [1m Learning iteration 603/2000 [0m                      

                       Computation: 45232 steps/s (collection: 2.053s, learning 0.120s)
             Mean action noise std: 2.28
          Mean value_function loss: 125.8998
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 48.8585
                       Mean reward: 121.22
               Mean episode length: 111.19
    Episode_Reward/reaching_object: 0.4842
     Episode_Reward/lifting_object: 21.7272
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 39.3750
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 2.17s
                      Time elapsed: 00:21:43
                               ETA: 00:50:13

################################################################################
                     [1m Learning iteration 604/2000 [0m                      

                       Computation: 45502 steps/s (collection: 2.043s, learning 0.118s)
             Mean action noise std: 2.28
          Mean value_function loss: 128.6216
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 48.8586
                       Mean reward: 110.08
               Mean episode length: 103.47
    Episode_Reward/reaching_object: 0.4849
     Episode_Reward/lifting_object: 21.8352
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 39.5000
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 2.16s
                      Time elapsed: 00:21:45
                               ETA: 00:50:11

################################################################################
                     [1m Learning iteration 605/2000 [0m                      

                       Computation: 45446 steps/s (collection: 2.057s, learning 0.106s)
             Mean action noise std: 2.28
          Mean value_function loss: 124.8175
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 48.8586
                       Mean reward: 118.54
               Mean episode length: 105.73
    Episode_Reward/reaching_object: 0.4903
     Episode_Reward/lifting_object: 22.2374
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 40.4167
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 2.16s
                      Time elapsed: 00:21:47
                               ETA: 00:50:09

################################################################################
                     [1m Learning iteration 606/2000 [0m                      

                       Computation: 46562 steps/s (collection: 2.001s, learning 0.110s)
             Mean action noise std: 2.28
          Mean value_function loss: 138.7956
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 48.8588
                       Mean reward: 110.82
               Mean episode length: 101.83
    Episode_Reward/reaching_object: 0.4698
     Episode_Reward/lifting_object: 21.9235
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 40.8333
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 2.11s
                      Time elapsed: 00:21:49
                               ETA: 00:50:07

################################################################################
                     [1m Learning iteration 607/2000 [0m                      

                       Computation: 46239 steps/s (collection: 2.005s, learning 0.121s)
             Mean action noise std: 2.28
          Mean value_function loss: 133.6507
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 48.8590
                       Mean reward: 109.97
               Mean episode length: 101.36
    Episode_Reward/reaching_object: 0.4824
     Episode_Reward/lifting_object: 22.4116
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 39.6250
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 2.13s
                      Time elapsed: 00:21:51
                               ETA: 00:50:05

################################################################################
                     [1m Learning iteration 608/2000 [0m                      

                       Computation: 46209 steps/s (collection: 2.010s, learning 0.117s)
             Mean action noise std: 2.28
          Mean value_function loss: 145.7974
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 48.8590
                       Mean reward: 103.76
               Mean episode length: 97.80
    Episode_Reward/reaching_object: 0.4770
     Episode_Reward/lifting_object: 21.9341
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 38.8750
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 2.13s
                      Time elapsed: 00:21:53
                               ETA: 00:50:02

################################################################################
                     [1m Learning iteration 609/2000 [0m                      

                       Computation: 46123 steps/s (collection: 2.013s, learning 0.118s)
             Mean action noise std: 2.28
          Mean value_function loss: 132.2573
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 48.8590
                       Mean reward: 117.19
               Mean episode length: 101.83
    Episode_Reward/reaching_object: 0.4838
     Episode_Reward/lifting_object: 22.9170
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 38.6667
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 2.13s
                      Time elapsed: 00:21:55
                               ETA: 00:50:00

################################################################################
                     [1m Learning iteration 610/2000 [0m                      

                       Computation: 46025 steps/s (collection: 2.012s, learning 0.124s)
             Mean action noise std: 2.28
          Mean value_function loss: 133.6225
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 48.8591
                       Mean reward: 107.59
               Mean episode length: 97.60
    Episode_Reward/reaching_object: 0.4736
     Episode_Reward/lifting_object: 22.1342
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 40.5417
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 2.14s
                      Time elapsed: 00:21:58
                               ETA: 00:49:58

################################################################################
                     [1m Learning iteration 611/2000 [0m                      

                       Computation: 46193 steps/s (collection: 2.006s, learning 0.122s)
             Mean action noise std: 2.28
          Mean value_function loss: 126.8201
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 48.8590
                       Mean reward: 124.76
               Mean episode length: 115.96
    Episode_Reward/reaching_object: 0.4816
     Episode_Reward/lifting_object: 22.5962
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 40.5000
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 2.13s
                      Time elapsed: 00:22:00
                               ETA: 00:49:56

################################################################################
                     [1m Learning iteration 612/2000 [0m                      

                       Computation: 44408 steps/s (collection: 2.087s, learning 0.127s)
             Mean action noise std: 2.28
          Mean value_function loss: 131.3486
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 48.8590
                       Mean reward: 108.04
               Mean episode length: 101.88
    Episode_Reward/reaching_object: 0.4856
     Episode_Reward/lifting_object: 22.2482
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 40.3333
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 2.21s
                      Time elapsed: 00:22:02
                               ETA: 00:49:54

################################################################################
                     [1m Learning iteration 613/2000 [0m                      

                       Computation: 46324 steps/s (collection: 1.998s, learning 0.124s)
             Mean action noise std: 2.28
          Mean value_function loss: 133.1194
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 48.8590
                       Mean reward: 104.95
               Mean episode length: 98.35
    Episode_Reward/reaching_object: 0.4756
     Episode_Reward/lifting_object: 21.7966
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 38.5417
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 2.12s
                      Time elapsed: 00:22:04
                               ETA: 00:49:52

################################################################################
                     [1m Learning iteration 614/2000 [0m                      

                       Computation: 46485 steps/s (collection: 1.992s, learning 0.123s)
             Mean action noise std: 2.28
          Mean value_function loss: 135.6872
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 48.8590
                       Mean reward: 105.06
               Mean episode length: 99.80
    Episode_Reward/reaching_object: 0.4679
     Episode_Reward/lifting_object: 22.0564
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 40.0833
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 2.11s
                      Time elapsed: 00:22:06
                               ETA: 00:49:49

################################################################################
                     [1m Learning iteration 615/2000 [0m                      

                       Computation: 45932 steps/s (collection: 2.018s, learning 0.122s)
             Mean action noise std: 2.28
          Mean value_function loss: 147.5499
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 48.8590
                       Mean reward: 108.11
               Mean episode length: 97.80
    Episode_Reward/reaching_object: 0.4744
     Episode_Reward/lifting_object: 22.6460
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 43.6250
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 2.14s
                      Time elapsed: 00:22:08
                               ETA: 00:49:47

################################################################################
                     [1m Learning iteration 616/2000 [0m                      

                       Computation: 46508 steps/s (collection: 1.997s, learning 0.117s)
             Mean action noise std: 2.28
          Mean value_function loss: 143.2482
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 48.8589
                       Mean reward: 118.95
               Mean episode length: 105.79
    Episode_Reward/reaching_object: 0.4835
     Episode_Reward/lifting_object: 22.6356
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 39.6667
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 2.11s
                      Time elapsed: 00:22:10
                               ETA: 00:49:45

################################################################################
                     [1m Learning iteration 617/2000 [0m                      

                       Computation: 47279 steps/s (collection: 1.986s, learning 0.094s)
             Mean action noise std: 2.28
          Mean value_function loss: 137.2376
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 48.8590
                       Mean reward: 113.79
               Mean episode length: 100.40
    Episode_Reward/reaching_object: 0.4629
     Episode_Reward/lifting_object: 21.8090
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 41.7917
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 2.08s
                      Time elapsed: 00:22:12
                               ETA: 00:49:43

################################################################################
                     [1m Learning iteration 618/2000 [0m                      

                       Computation: 47479 steps/s (collection: 1.975s, learning 0.096s)
             Mean action noise std: 2.28
          Mean value_function loss: 141.0064
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 48.8590
                       Mean reward: 117.49
               Mean episode length: 100.71
    Episode_Reward/reaching_object: 0.4753
     Episode_Reward/lifting_object: 22.5274
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 39.8750
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 2.07s
                      Time elapsed: 00:22:15
                               ETA: 00:49:40

################################################################################
                     [1m Learning iteration 619/2000 [0m                      

                       Computation: 47723 steps/s (collection: 1.965s, learning 0.095s)
             Mean action noise std: 2.28
          Mean value_function loss: 145.0185
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 48.8587
                       Mean reward: 111.62
               Mean episode length: 98.06
    Episode_Reward/reaching_object: 0.4694
     Episode_Reward/lifting_object: 22.1545
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 41.1250
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 2.06s
                      Time elapsed: 00:22:17
                               ETA: 00:49:38

################################################################################
                     [1m Learning iteration 620/2000 [0m                      

                       Computation: 45254 steps/s (collection: 2.060s, learning 0.113s)
             Mean action noise std: 2.28
          Mean value_function loss: 138.0616
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 48.8578
                       Mean reward: 104.68
               Mean episode length: 92.37
    Episode_Reward/reaching_object: 0.4657
     Episode_Reward/lifting_object: 22.3107
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 40.0833
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 2.17s
                      Time elapsed: 00:22:19
                               ETA: 00:49:36

################################################################################
                     [1m Learning iteration 621/2000 [0m                      

                       Computation: 45686 steps/s (collection: 2.027s, learning 0.125s)
             Mean action noise std: 2.28
          Mean value_function loss: 140.7227
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 48.8552
                       Mean reward: 109.25
               Mean episode length: 95.93
    Episode_Reward/reaching_object: 0.4730
     Episode_Reward/lifting_object: 23.0270
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 41.4167
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 2.15s
                      Time elapsed: 00:22:21
                               ETA: 00:49:34

################################################################################
                     [1m Learning iteration 622/2000 [0m                      

                       Computation: 45352 steps/s (collection: 2.049s, learning 0.118s)
             Mean action noise std: 2.28
          Mean value_function loss: 151.5572
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 48.8518
                       Mean reward: 110.68
               Mean episode length: 97.31
    Episode_Reward/reaching_object: 0.4741
     Episode_Reward/lifting_object: 23.3463
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 40.3333
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 2.17s
                      Time elapsed: 00:22:23
                               ETA: 00:49:31

################################################################################
                     [1m Learning iteration 623/2000 [0m                      

                       Computation: 45834 steps/s (collection: 2.017s, learning 0.128s)
             Mean action noise std: 2.28
          Mean value_function loss: 148.5372
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 48.8531
                       Mean reward: 121.44
               Mean episode length: 100.47
    Episode_Reward/reaching_object: 0.4754
     Episode_Reward/lifting_object: 22.8575
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 40.0417
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 2.14s
                      Time elapsed: 00:22:25
                               ETA: 00:49:29

################################################################################
                     [1m Learning iteration 624/2000 [0m                      

                       Computation: 46017 steps/s (collection: 2.019s, learning 0.118s)
             Mean action noise std: 2.28
          Mean value_function loss: 152.7934
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 48.8540
                       Mean reward: 121.07
               Mean episode length: 99.51
    Episode_Reward/reaching_object: 0.4681
     Episode_Reward/lifting_object: 22.7716
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 41.5000
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 2.14s
                      Time elapsed: 00:22:27
                               ETA: 00:49:27

################################################################################
                     [1m Learning iteration 625/2000 [0m                      

                       Computation: 46499 steps/s (collection: 1.997s, learning 0.118s)
             Mean action noise std: 2.28
          Mean value_function loss: 172.8670
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 48.8543
                       Mean reward: 115.58
               Mean episode length: 100.78
    Episode_Reward/reaching_object: 0.4925
     Episode_Reward/lifting_object: 23.4357
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 41.5833
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 2.11s
                      Time elapsed: 00:22:29
                               ETA: 00:49:25

################################################################################
                     [1m Learning iteration 626/2000 [0m                      

                       Computation: 44946 steps/s (collection: 2.064s, learning 0.124s)
             Mean action noise std: 2.28
          Mean value_function loss: 151.1369
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 48.8531
                       Mean reward: 118.72
               Mean episode length: 103.53
    Episode_Reward/reaching_object: 0.4684
     Episode_Reward/lifting_object: 23.0796
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 40.0417
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 2.19s
                      Time elapsed: 00:22:32
                               ETA: 00:49:23

################################################################################
                     [1m Learning iteration 627/2000 [0m                      

                       Computation: 46071 steps/s (collection: 2.007s, learning 0.127s)
             Mean action noise std: 2.28
          Mean value_function loss: 153.4322
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 48.8514
                       Mean reward: 114.18
               Mean episode length: 98.89
    Episode_Reward/reaching_object: 0.4695
     Episode_Reward/lifting_object: 23.1061
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 41.7917
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 2.13s
                      Time elapsed: 00:22:34
                               ETA: 00:49:20

################################################################################
                     [1m Learning iteration 628/2000 [0m                      

                       Computation: 45793 steps/s (collection: 2.035s, learning 0.112s)
             Mean action noise std: 2.28
          Mean value_function loss: 151.9477
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 48.8506
                       Mean reward: 121.08
               Mean episode length: 101.65
    Episode_Reward/reaching_object: 0.4702
     Episode_Reward/lifting_object: 23.7636
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0271
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 41.2083
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 2.15s
                      Time elapsed: 00:22:36
                               ETA: 00:49:18

################################################################################
                     [1m Learning iteration 629/2000 [0m                      

                       Computation: 46865 steps/s (collection: 1.982s, learning 0.116s)
             Mean action noise std: 2.28
          Mean value_function loss: 154.3986
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 48.8502
                       Mean reward: 117.39
               Mean episode length: 101.37
    Episode_Reward/reaching_object: 0.4652
     Episode_Reward/lifting_object: 22.9946
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0271
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 41.1250
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 2.10s
                      Time elapsed: 00:22:38
                               ETA: 00:49:16

################################################################################
                     [1m Learning iteration 630/2000 [0m                      

                       Computation: 46741 steps/s (collection: 2.000s, learning 0.104s)
             Mean action noise std: 2.28
          Mean value_function loss: 150.8538
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 48.8498
                       Mean reward: 117.65
               Mean episode length: 95.96
    Episode_Reward/reaching_object: 0.4667
     Episode_Reward/lifting_object: 23.1757
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 42.6250
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 2.10s
                      Time elapsed: 00:22:40
                               ETA: 00:49:14

################################################################################
                     [1m Learning iteration 631/2000 [0m                      

                       Computation: 45626 steps/s (collection: 2.035s, learning 0.120s)
             Mean action noise std: 2.28
          Mean value_function loss: 143.0026
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 48.8501
                       Mean reward: 116.00
               Mean episode length: 98.03
    Episode_Reward/reaching_object: 0.4601
     Episode_Reward/lifting_object: 23.0490
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 41.4583
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 2.15s
                      Time elapsed: 00:22:42
                               ETA: 00:49:12

################################################################################
                     [1m Learning iteration 632/2000 [0m                      

                       Computation: 46322 steps/s (collection: 2.004s, learning 0.118s)
             Mean action noise std: 2.28
          Mean value_function loss: 157.6328
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 48.8487
                       Mean reward: 111.87
               Mean episode length: 92.97
    Episode_Reward/reaching_object: 0.4461
     Episode_Reward/lifting_object: 22.9316
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 43.8333
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 2.12s
                      Time elapsed: 00:22:44
                               ETA: 00:49:09

################################################################################
                     [1m Learning iteration 633/2000 [0m                      

                       Computation: 46713 steps/s (collection: 1.985s, learning 0.120s)
             Mean action noise std: 2.28
          Mean value_function loss: 153.6193
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 48.8483
                       Mean reward: 119.89
               Mean episode length: 95.76
    Episode_Reward/reaching_object: 0.4490
     Episode_Reward/lifting_object: 23.3412
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 43.6667
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 2.10s
                      Time elapsed: 00:22:47
                               ETA: 00:49:07

################################################################################
                     [1m Learning iteration 634/2000 [0m                      

                       Computation: 46723 steps/s (collection: 1.986s, learning 0.118s)
             Mean action noise std: 2.28
          Mean value_function loss: 158.1987
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 48.8489
                       Mean reward: 121.73
               Mean episode length: 99.93
    Episode_Reward/reaching_object: 0.4504
     Episode_Reward/lifting_object: 23.5021
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 42.9583
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 2.10s
                      Time elapsed: 00:22:49
                               ETA: 00:49:05

################################################################################
                     [1m Learning iteration 635/2000 [0m                      

                       Computation: 46673 steps/s (collection: 1.976s, learning 0.130s)
             Mean action noise std: 2.28
          Mean value_function loss: 151.5987
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 48.8486
                       Mean reward: 119.35
               Mean episode length: 97.71
    Episode_Reward/reaching_object: 0.4406
     Episode_Reward/lifting_object: 23.1933
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 42.7500
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 2.11s
                      Time elapsed: 00:22:51
                               ETA: 00:49:03

################################################################################
                     [1m Learning iteration 636/2000 [0m                      

                       Computation: 46173 steps/s (collection: 2.011s, learning 0.118s)
             Mean action noise std: 2.28
          Mean value_function loss: 142.6469
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 48.8478
                       Mean reward: 123.85
               Mean episode length: 99.88
    Episode_Reward/reaching_object: 0.4410
     Episode_Reward/lifting_object: 23.0510
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0271
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 41.1250
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 2.13s
                      Time elapsed: 00:22:53
                               ETA: 00:49:00

################################################################################
                     [1m Learning iteration 637/2000 [0m                      

                       Computation: 46399 steps/s (collection: 1.994s, learning 0.125s)
             Mean action noise std: 2.28
          Mean value_function loss: 153.1971
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 48.8473
                       Mean reward: 121.98
               Mean episode length: 95.11
    Episode_Reward/reaching_object: 0.4326
     Episode_Reward/lifting_object: 22.7575
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 43.1667
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 2.12s
                      Time elapsed: 00:22:55
                               ETA: 00:48:58

################################################################################
                     [1m Learning iteration 638/2000 [0m                      

                       Computation: 46086 steps/s (collection: 2.017s, learning 0.116s)
             Mean action noise std: 2.28
          Mean value_function loss: 187.4000
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 48.8484
                       Mean reward: 118.56
               Mean episode length: 96.57
    Episode_Reward/reaching_object: 0.4397
     Episode_Reward/lifting_object: 23.1396
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 44.3333
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 2.13s
                      Time elapsed: 00:22:57
                               ETA: 00:48:56

################################################################################
                     [1m Learning iteration 639/2000 [0m                      

                       Computation: 46800 steps/s (collection: 1.988s, learning 0.112s)
             Mean action noise std: 2.28
          Mean value_function loss: 158.4072
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 48.8505
                       Mean reward: 121.30
               Mean episode length: 97.76
    Episode_Reward/reaching_object: 0.4456
     Episode_Reward/lifting_object: 23.2980
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 45.1667
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 2.10s
                      Time elapsed: 00:22:59
                               ETA: 00:48:54

################################################################################
                     [1m Learning iteration 640/2000 [0m                      

                       Computation: 46273 steps/s (collection: 1.999s, learning 0.126s)
             Mean action noise std: 2.28
          Mean value_function loss: 142.4405
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 48.8514
                       Mean reward: 112.60
               Mean episode length: 90.23
    Episode_Reward/reaching_object: 0.4319
     Episode_Reward/lifting_object: 23.2743
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 41.0833
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 2.12s
                      Time elapsed: 00:23:01
                               ETA: 00:48:51

################################################################################
                     [1m Learning iteration 641/2000 [0m                      

                       Computation: 47676 steps/s (collection: 1.966s, learning 0.096s)
             Mean action noise std: 2.28
          Mean value_function loss: 156.4993
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 48.8523
                       Mean reward: 117.12
               Mean episode length: 93.42
    Episode_Reward/reaching_object: 0.4359
     Episode_Reward/lifting_object: 23.2658
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 40.7083
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 2.06s
                      Time elapsed: 00:23:03
                               ETA: 00:48:49

################################################################################
                     [1m Learning iteration 642/2000 [0m                      

                       Computation: 46944 steps/s (collection: 1.983s, learning 0.111s)
             Mean action noise std: 2.28
          Mean value_function loss: 2146.5214
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 48.8534
                       Mean reward: 127.64
               Mean episode length: 100.85
    Episode_Reward/reaching_object: 0.4483
     Episode_Reward/lifting_object: 23.6425
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -1.1921
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 42.8333
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 2.09s
                      Time elapsed: 00:23:06
                               ETA: 00:48:47

################################################################################
                     [1m Learning iteration 643/2000 [0m                      

                       Computation: 47106 steps/s (collection: 1.967s, learning 0.119s)
             Mean action noise std: 2.28
          Mean value_function loss: 164.6135
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 48.8538
                       Mean reward: 128.37
               Mean episode length: 101.76
    Episode_Reward/reaching_object: 0.4526
     Episode_Reward/lifting_object: 23.4973
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 43.5000
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 2.09s
                      Time elapsed: 00:23:08
                               ETA: 00:48:44

################################################################################
                     [1m Learning iteration 644/2000 [0m                      

                       Computation: 46364 steps/s (collection: 2.001s, learning 0.120s)
             Mean action noise std: 2.28
          Mean value_function loss: 158.6203
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 48.8548
                       Mean reward: 116.72
               Mean episode length: 93.01
    Episode_Reward/reaching_object: 0.4477
     Episode_Reward/lifting_object: 23.6554
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 46.4583
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 2.12s
                      Time elapsed: 00:23:10
                               ETA: 00:48:42

################################################################################
                     [1m Learning iteration 645/2000 [0m                      

                       Computation: 47237 steps/s (collection: 1.966s, learning 0.115s)
             Mean action noise std: 2.28
          Mean value_function loss: 157.8702
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 48.8539
                       Mean reward: 123.41
               Mean episode length: 96.09
    Episode_Reward/reaching_object: 0.4404
     Episode_Reward/lifting_object: 23.7928
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0271
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 41.1250
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 2.08s
                      Time elapsed: 00:23:12
                               ETA: 00:48:40

################################################################################
                     [1m Learning iteration 646/2000 [0m                      

                       Computation: 47430 steps/s (collection: 1.976s, learning 0.097s)
             Mean action noise std: 2.28
          Mean value_function loss: 166.1881
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 48.8551
                       Mean reward: 118.19
               Mean episode length: 92.57
    Episode_Reward/reaching_object: 0.4304
     Episode_Reward/lifting_object: 23.2498
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 45.3333
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 2.07s
                      Time elapsed: 00:23:14
                               ETA: 00:48:38

################################################################################
                     [1m Learning iteration 647/2000 [0m                      

                       Computation: 47964 steps/s (collection: 1.938s, learning 0.112s)
             Mean action noise std: 2.28
          Mean value_function loss: 166.5288
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 48.8558
                       Mean reward: 118.60
               Mean episode length: 94.72
    Episode_Reward/reaching_object: 0.4347
     Episode_Reward/lifting_object: 23.8977
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 43.6667
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 2.05s
                      Time elapsed: 00:23:16
                               ETA: 00:48:35

################################################################################
                     [1m Learning iteration 648/2000 [0m                      

                       Computation: 47191 steps/s (collection: 1.977s, learning 0.107s)
             Mean action noise std: 2.28
          Mean value_function loss: 159.7833
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 48.8561
                       Mean reward: 114.48
               Mean episode length: 92.21
    Episode_Reward/reaching_object: 0.4346
     Episode_Reward/lifting_object: 23.8557
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 42.5000
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 2.08s
                      Time elapsed: 00:23:18
                               ETA: 00:48:33

################################################################################
                     [1m Learning iteration 649/2000 [0m                      

                       Computation: 48117 steps/s (collection: 1.944s, learning 0.099s)
             Mean action noise std: 2.28
          Mean value_function loss: 168.2440
               Mean surrogate loss: 0.0105
                 Mean entropy loss: 48.8562
                       Mean reward: 122.46
               Mean episode length: 93.25
    Episode_Reward/reaching_object: 0.4411
     Episode_Reward/lifting_object: 23.9908
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 44.0417
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 2.04s
                      Time elapsed: 00:23:20
                               ETA: 00:48:30

################################################################################
                     [1m Learning iteration 650/2000 [0m                      

                       Computation: 48176 steps/s (collection: 1.951s, learning 0.090s)
             Mean action noise std: 2.28
          Mean value_function loss: 155.1923
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 48.8563
                       Mean reward: 120.09
               Mean episode length: 94.52
    Episode_Reward/reaching_object: 0.4348
     Episode_Reward/lifting_object: 23.3373
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 43.0417
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 2.04s
                      Time elapsed: 00:23:22
                               ETA: 00:48:28

################################################################################
                     [1m Learning iteration 651/2000 [0m                      

                       Computation: 46969 steps/s (collection: 1.978s, learning 0.115s)
             Mean action noise std: 2.28
          Mean value_function loss: 163.6313
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 48.8571
                       Mean reward: 122.43
               Mean episode length: 95.93
    Episode_Reward/reaching_object: 0.4334
     Episode_Reward/lifting_object: 23.8697
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 42.5417
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 2.09s
                      Time elapsed: 00:23:24
                               ETA: 00:48:26

################################################################################
                     [1m Learning iteration 652/2000 [0m                      

                       Computation: 48116 steps/s (collection: 1.935s, learning 0.108s)
             Mean action noise std: 2.28
          Mean value_function loss: 165.5965
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 48.8595
                       Mean reward: 128.47
               Mean episode length: 100.27
    Episode_Reward/reaching_object: 0.4405
     Episode_Reward/lifting_object: 24.3415
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 40.9167
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 2.04s
                      Time elapsed: 00:23:26
                               ETA: 00:48:23

################################################################################
                     [1m Learning iteration 653/2000 [0m                      

                       Computation: 48449 steps/s (collection: 1.930s, learning 0.099s)
             Mean action noise std: 2.28
          Mean value_function loss: 194.1279
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 48.8631
                       Mean reward: 116.82
               Mean episode length: 91.51
    Episode_Reward/reaching_object: 0.4382
     Episode_Reward/lifting_object: 24.0186
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 43.7083
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 2.03s
                      Time elapsed: 00:23:28
                               ETA: 00:48:21

################################################################################
                     [1m Learning iteration 654/2000 [0m                      

                       Computation: 48859 steps/s (collection: 1.918s, learning 0.094s)
             Mean action noise std: 2.28
          Mean value_function loss: 170.5660
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 48.8664
                       Mean reward: 124.38
               Mean episode length: 96.25
    Episode_Reward/reaching_object: 0.4499
     Episode_Reward/lifting_object: 24.4370
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 43.7083
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 2.01s
                      Time elapsed: 00:23:30
                               ETA: 00:48:19

################################################################################
                     [1m Learning iteration 655/2000 [0m                      

                       Computation: 48105 steps/s (collection: 1.950s, learning 0.094s)
             Mean action noise std: 2.28
          Mean value_function loss: 170.0537
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 48.8681
                       Mean reward: 128.61
               Mean episode length: 100.45
    Episode_Reward/reaching_object: 0.4416
     Episode_Reward/lifting_object: 24.0932
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 42.9583
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 2.04s
                      Time elapsed: 00:23:32
                               ETA: 00:48:16

################################################################################
                     [1m Learning iteration 656/2000 [0m                      

                       Computation: 48173 steps/s (collection: 1.938s, learning 0.103s)
             Mean action noise std: 2.28
          Mean value_function loss: 175.8882
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 48.8697
                       Mean reward: 129.05
               Mean episode length: 100.32
    Episode_Reward/reaching_object: 0.4415
     Episode_Reward/lifting_object: 24.2055
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 44.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 2.04s
                      Time elapsed: 00:23:34
                               ETA: 00:48:14

################################################################################
                     [1m Learning iteration 657/2000 [0m                      

                       Computation: 49099 steps/s (collection: 1.909s, learning 0.093s)
             Mean action noise std: 2.28
          Mean value_function loss: 176.9564
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 48.8711
                       Mean reward: 119.31
               Mean episode length: 93.24
    Episode_Reward/reaching_object: 0.4480
     Episode_Reward/lifting_object: 24.7370
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0268
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 43.4583
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 2.00s
                      Time elapsed: 00:23:36
                               ETA: 00:48:11

################################################################################
                     [1m Learning iteration 658/2000 [0m                      

                       Computation: 48568 steps/s (collection: 1.909s, learning 0.115s)
             Mean action noise std: 2.29
          Mean value_function loss: 173.2190
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 48.8717
                       Mean reward: 117.35
               Mean episode length: 91.08
    Episode_Reward/reaching_object: 0.4313
     Episode_Reward/lifting_object: 24.1119
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 43.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 2.02s
                      Time elapsed: 00:23:38
                               ETA: 00:48:09

################################################################################
                     [1m Learning iteration 659/2000 [0m                      

                       Computation: 48558 steps/s (collection: 1.909s, learning 0.115s)
             Mean action noise std: 2.29
          Mean value_function loss: 176.6117
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 48.8710
                       Mean reward: 137.85
               Mean episode length: 97.62
    Episode_Reward/reaching_object: 0.4323
     Episode_Reward/lifting_object: 24.8063
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 44.6667
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 2.02s
                      Time elapsed: 00:23:40
                               ETA: 00:48:07

################################################################################
                     [1m Learning iteration 660/2000 [0m                      

                       Computation: 49017 steps/s (collection: 1.897s, learning 0.108s)
             Mean action noise std: 2.29
          Mean value_function loss: 177.4287
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 48.8711
                       Mean reward: 118.11
               Mean episode length: 92.34
    Episode_Reward/reaching_object: 0.4322
     Episode_Reward/lifting_object: 24.2154
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 42.5833
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 2.01s
                      Time elapsed: 00:23:42
                               ETA: 00:48:04

################################################################################
                     [1m Learning iteration 661/2000 [0m                      

                       Computation: 48764 steps/s (collection: 1.902s, learning 0.114s)
             Mean action noise std: 2.29
          Mean value_function loss: 183.5703
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 48.8717
                       Mean reward: 121.50
               Mean episode length: 92.23
    Episode_Reward/reaching_object: 0.4279
     Episode_Reward/lifting_object: 24.6890
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 45.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 2.02s
                      Time elapsed: 00:23:44
                               ETA: 00:48:02

################################################################################
                     [1m Learning iteration 662/2000 [0m                      

                       Computation: 48980 steps/s (collection: 1.895s, learning 0.112s)
             Mean action noise std: 2.29
          Mean value_function loss: 178.5708
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 48.8726
                       Mean reward: 126.83
               Mean episode length: 95.45
    Episode_Reward/reaching_object: 0.4376
     Episode_Reward/lifting_object: 24.7844
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 45.0833
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 2.01s
                      Time elapsed: 00:23:46
                               ETA: 00:47:59

################################################################################
                     [1m Learning iteration 663/2000 [0m                      

                       Computation: 48077 steps/s (collection: 1.937s, learning 0.108s)
             Mean action noise std: 2.29
          Mean value_function loss: 177.0640
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 48.8740
                       Mean reward: 119.50
               Mean episode length: 88.10
    Episode_Reward/reaching_object: 0.4316
     Episode_Reward/lifting_object: 24.7369
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 42.6667
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 2.04s
                      Time elapsed: 00:23:48
                               ETA: 00:47:57

################################################################################
                     [1m Learning iteration 664/2000 [0m                      

                       Computation: 48406 steps/s (collection: 1.915s, learning 0.116s)
             Mean action noise std: 2.29
          Mean value_function loss: 184.3806
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 48.8757
                       Mean reward: 128.24
               Mean episode length: 97.10
    Episode_Reward/reaching_object: 0.4353
     Episode_Reward/lifting_object: 24.9251
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 45.7083
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 2.03s
                      Time elapsed: 00:23:51
                               ETA: 00:47:54

################################################################################
                     [1m Learning iteration 665/2000 [0m                      

                       Computation: 49066 steps/s (collection: 1.884s, learning 0.120s)
             Mean action noise std: 2.29
          Mean value_function loss: 174.8306
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 48.8774
                       Mean reward: 130.73
               Mean episode length: 96.94
    Episode_Reward/reaching_object: 0.4311
     Episode_Reward/lifting_object: 24.7549
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 45.4583
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 2.00s
                      Time elapsed: 00:23:53
                               ETA: 00:47:52

################################################################################
                     [1m Learning iteration 666/2000 [0m                      

                       Computation: 28319 steps/s (collection: 3.360s, learning 0.112s)
             Mean action noise std: 2.29
          Mean value_function loss: 184.7271
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 48.8790
                       Mean reward: 130.46
               Mean episode length: 93.41
    Episode_Reward/reaching_object: 0.4260
     Episode_Reward/lifting_object: 24.7960
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 44.2917
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 3.47s
                      Time elapsed: 00:23:56
                               ETA: 00:47:52

################################################################################
                     [1m Learning iteration 667/2000 [0m                      

                       Computation: 14599 steps/s (collection: 6.555s, learning 0.178s)
             Mean action noise std: 2.29
          Mean value_function loss: 195.1425
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 48.8799
                       Mean reward: 126.91
               Mean episode length: 92.18
    Episode_Reward/reaching_object: 0.4148
     Episode_Reward/lifting_object: 24.3867
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 44.2500
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 6.73s
                      Time elapsed: 00:24:03
                               ETA: 00:47:59

################################################################################
                     [1m Learning iteration 668/2000 [0m                      

                       Computation: 14799 steps/s (collection: 6.527s, learning 0.116s)
             Mean action noise std: 2.29
          Mean value_function loss: 191.2742
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.8801
                       Mean reward: 125.90
               Mean episode length: 90.16
    Episode_Reward/reaching_object: 0.4212
     Episode_Reward/lifting_object: 24.5162
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 44.6250
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 6.64s
                      Time elapsed: 00:24:09
                               ETA: 00:48:06

################################################################################
                     [1m Learning iteration 669/2000 [0m                      

                       Computation: 15023 steps/s (collection: 6.423s, learning 0.121s)
             Mean action noise std: 2.29
          Mean value_function loss: 184.1426
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 48.8801
                       Mean reward: 126.46
               Mean episode length: 93.01
    Episode_Reward/reaching_object: 0.4239
     Episode_Reward/lifting_object: 24.9309
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 44.7083
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 6.54s
                      Time elapsed: 00:24:16
                               ETA: 00:48:13

################################################################################
                     [1m Learning iteration 670/2000 [0m                      

                       Computation: 14614 steps/s (collection: 6.599s, learning 0.128s)
             Mean action noise std: 2.29
          Mean value_function loss: 184.6901
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 48.8810
                       Mean reward: 138.58
               Mean episode length: 98.90
    Episode_Reward/reaching_object: 0.4257
     Episode_Reward/lifting_object: 24.9875
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 45.9167
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 6.73s
                      Time elapsed: 00:24:23
                               ETA: 00:48:20

################################################################################
                     [1m Learning iteration 671/2000 [0m                      

                       Computation: 14646 steps/s (collection: 6.586s, learning 0.126s)
             Mean action noise std: 2.29
          Mean value_function loss: 191.0803
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 48.8820
                       Mean reward: 125.19
               Mean episode length: 90.56
    Episode_Reward/reaching_object: 0.4122
     Episode_Reward/lifting_object: 23.9980
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 45.4583
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 6.71s
                      Time elapsed: 00:24:29
                               ETA: 00:48:26

################################################################################
                     [1m Learning iteration 672/2000 [0m                      

                       Computation: 14054 steps/s (collection: 6.836s, learning 0.159s)
             Mean action noise std: 2.29
          Mean value_function loss: 185.7528
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 48.8852
                       Mean reward: 120.04
               Mean episode length: 86.60
    Episode_Reward/reaching_object: 0.4212
     Episode_Reward/lifting_object: 25.0880
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 43.7500
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 6.99s
                      Time elapsed: 00:24:36
                               ETA: 00:48:34

################################################################################
                     [1m Learning iteration 673/2000 [0m                      

                       Computation: 13880 steps/s (collection: 6.932s, learning 0.151s)
             Mean action noise std: 2.29
          Mean value_function loss: 181.9894
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 48.8874
                       Mean reward: 118.17
               Mean episode length: 85.62
    Episode_Reward/reaching_object: 0.4154
     Episode_Reward/lifting_object: 24.8409
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 45.9167
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 7.08s
                      Time elapsed: 00:24:43
                               ETA: 00:48:41

################################################################################
                     [1m Learning iteration 674/2000 [0m                      

                       Computation: 13875 steps/s (collection: 6.917s, learning 0.168s)
             Mean action noise std: 2.29
          Mean value_function loss: 176.0235
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 48.8887
                       Mean reward: 130.18
               Mean episode length: 92.40
    Episode_Reward/reaching_object: 0.4104
     Episode_Reward/lifting_object: 24.7386
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 43.7083
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 7.08s
                      Time elapsed: 00:24:50
                               ETA: 00:48:48

################################################################################
                     [1m Learning iteration 675/2000 [0m                      

                       Computation: 21265 steps/s (collection: 4.533s, learning 0.090s)
             Mean action noise std: 2.29
          Mean value_function loss: 184.2881
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 48.8896
                       Mean reward: 126.68
               Mean episode length: 89.99
    Episode_Reward/reaching_object: 0.4144
     Episode_Reward/lifting_object: 24.8253
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 44.5833
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 4.62s
                      Time elapsed: 00:24:55
                               ETA: 00:48:51

################################################################################
                     [1m Learning iteration 676/2000 [0m                      

                       Computation: 47921 steps/s (collection: 1.936s, learning 0.116s)
             Mean action noise std: 2.29
          Mean value_function loss: 200.6692
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 48.8923
                       Mean reward: 128.41
               Mean episode length: 92.22
    Episode_Reward/reaching_object: 0.4258
     Episode_Reward/lifting_object: 25.6006
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 44.9583
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 2.05s
                      Time elapsed: 00:24:57
                               ETA: 00:48:48

################################################################################
                     [1m Learning iteration 677/2000 [0m                      

                       Computation: 45441 steps/s (collection: 2.027s, learning 0.136s)
             Mean action noise std: 2.29
          Mean value_function loss: 191.4037
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 48.8924
                       Mean reward: 126.62
               Mean episode length: 89.03
    Episode_Reward/reaching_object: 0.4152
     Episode_Reward/lifting_object: 24.7340
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 45.1250
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 2.16s
                      Time elapsed: 00:24:59
                               ETA: 00:48:46

################################################################################
                     [1m Learning iteration 678/2000 [0m                      

                       Computation: 43816 steps/s (collection: 2.085s, learning 0.158s)
             Mean action noise std: 2.29
          Mean value_function loss: 198.9482
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 48.8917
                       Mean reward: 130.51
               Mean episode length: 95.34
    Episode_Reward/reaching_object: 0.4203
     Episode_Reward/lifting_object: 25.0333
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 44.0833
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 2.24s
                      Time elapsed: 00:25:02
                               ETA: 00:48:44

################################################################################
                     [1m Learning iteration 679/2000 [0m                      

                       Computation: 46284 steps/s (collection: 2.000s, learning 0.124s)
             Mean action noise std: 2.29
          Mean value_function loss: 201.3087
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 48.8927
                       Mean reward: 131.92
               Mean episode length: 97.70
    Episode_Reward/reaching_object: 0.4191
     Episode_Reward/lifting_object: 24.7561
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 47.7083
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 2.12s
                      Time elapsed: 00:25:04
                               ETA: 00:48:42

################################################################################
                     [1m Learning iteration 680/2000 [0m                      

                       Computation: 46588 steps/s (collection: 2.000s, learning 0.110s)
             Mean action noise std: 2.29
          Mean value_function loss: 199.2197
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 48.8938
                       Mean reward: 128.66
               Mean episode length: 90.57
    Episode_Reward/reaching_object: 0.4057
     Episode_Reward/lifting_object: 24.2321
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 46.2500
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 2.11s
                      Time elapsed: 00:25:06
                               ETA: 00:48:39

################################################################################
                     [1m Learning iteration 681/2000 [0m                      

                       Computation: 46250 steps/s (collection: 2.023s, learning 0.103s)
             Mean action noise std: 2.29
          Mean value_function loss: 226.0723
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 48.8939
                       Mean reward: 125.37
               Mean episode length: 91.30
    Episode_Reward/reaching_object: 0.3984
     Episode_Reward/lifting_object: 24.2708
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 45.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 2.13s
                      Time elapsed: 00:25:08
                               ETA: 00:48:37

################################################################################
                     [1m Learning iteration 682/2000 [0m                      

                       Computation: 45665 steps/s (collection: 2.052s, learning 0.101s)
             Mean action noise std: 2.29
          Mean value_function loss: 222.8161
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 48.8942
                       Mean reward: 111.05
               Mean episode length: 77.25
    Episode_Reward/reaching_object: 0.3950
     Episode_Reward/lifting_object: 24.0990
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 43.7500
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 2.15s
                      Time elapsed: 00:25:10
                               ETA: 00:48:35

################################################################################
                     [1m Learning iteration 683/2000 [0m                      

                       Computation: 48196 steps/s (collection: 1.936s, learning 0.104s)
             Mean action noise std: 2.29
          Mean value_function loss: 215.9617
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 48.8942
                       Mean reward: 122.14
               Mean episode length: 87.94
    Episode_Reward/reaching_object: 0.4079
     Episode_Reward/lifting_object: 24.8410
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 44.3750
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 2.04s
                      Time elapsed: 00:25:12
                               ETA: 00:48:32

################################################################################
                     [1m Learning iteration 684/2000 [0m                      

                       Computation: 48238 steps/s (collection: 1.925s, learning 0.113s)
             Mean action noise std: 2.29
          Mean value_function loss: 202.1698
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 48.8946
                       Mean reward: 127.45
               Mean episode length: 94.98
    Episode_Reward/reaching_object: 0.4224
     Episode_Reward/lifting_object: 25.7132
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 44.3750
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 2.04s
                      Time elapsed: 00:25:14
                               ETA: 00:48:29

################################################################################
                     [1m Learning iteration 685/2000 [0m                      

                       Computation: 47536 steps/s (collection: 1.953s, learning 0.115s)
             Mean action noise std: 2.29
          Mean value_function loss: 197.6505
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 48.8972
                       Mean reward: 119.72
               Mean episode length: 85.56
    Episode_Reward/reaching_object: 0.4125
     Episode_Reward/lifting_object: 25.5237
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 47.7083
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 2.07s
                      Time elapsed: 00:25:16
                               ETA: 00:48:27

################################################################################
                     [1m Learning iteration 686/2000 [0m                      

                       Computation: 48340 steps/s (collection: 1.925s, learning 0.109s)
             Mean action noise std: 2.29
          Mean value_function loss: 201.8810
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 48.8998
                       Mean reward: 116.04
               Mean episode length: 85.05
    Episode_Reward/reaching_object: 0.4240
     Episode_Reward/lifting_object: 26.5712
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 43.8333
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 2.03s
                      Time elapsed: 00:25:18
                               ETA: 00:48:24

################################################################################
                     [1m Learning iteration 687/2000 [0m                      

                       Computation: 48120 steps/s (collection: 1.934s, learning 0.109s)
             Mean action noise std: 2.29
          Mean value_function loss: 206.7171
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 48.8987
                       Mean reward: 124.94
               Mean episode length: 86.97
    Episode_Reward/reaching_object: 0.3997
     Episode_Reward/lifting_object: 25.4213
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 47.6667
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 2.04s
                      Time elapsed: 00:25:20
                               ETA: 00:48:22

################################################################################
                     [1m Learning iteration 688/2000 [0m                      

                       Computation: 48307 steps/s (collection: 1.925s, learning 0.110s)
             Mean action noise std: 2.29
          Mean value_function loss: 197.9866
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 48.8981
                       Mean reward: 123.04
               Mean episode length: 86.76
    Episode_Reward/reaching_object: 0.4101
     Episode_Reward/lifting_object: 25.6399
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 47.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 2.03s
                      Time elapsed: 00:25:22
                               ETA: 00:48:19

################################################################################
                     [1m Learning iteration 689/2000 [0m                      

                       Computation: 48677 steps/s (collection: 1.909s, learning 0.110s)
             Mean action noise std: 2.29
          Mean value_function loss: 199.8149
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 48.8981
                       Mean reward: 129.87
               Mean episode length: 86.41
    Episode_Reward/reaching_object: 0.3994
     Episode_Reward/lifting_object: 25.5134
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 46.5417
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 2.02s
                      Time elapsed: 00:25:24
                               ETA: 00:48:17

################################################################################
                     [1m Learning iteration 690/2000 [0m                      

                       Computation: 46803 steps/s (collection: 1.974s, learning 0.127s)
             Mean action noise std: 2.29
          Mean value_function loss: 205.7386
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 48.9000
                       Mean reward: 131.82
               Mean episode length: 90.90
    Episode_Reward/reaching_object: 0.3952
     Episode_Reward/lifting_object: 25.1481
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 47.3750
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 2.10s
                      Time elapsed: 00:25:26
                               ETA: 00:48:14

################################################################################
                     [1m Learning iteration 691/2000 [0m                      

                       Computation: 45867 steps/s (collection: 2.024s, learning 0.119s)
             Mean action noise std: 2.29
          Mean value_function loss: 207.0802
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 48.9010
                       Mean reward: 139.15
               Mean episode length: 95.72
    Episode_Reward/reaching_object: 0.4028
     Episode_Reward/lifting_object: 26.0332
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 46.0833
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 2.14s
                      Time elapsed: 00:25:29
                               ETA: 00:48:12

################################################################################
                     [1m Learning iteration 692/2000 [0m                      

                       Computation: 47840 steps/s (collection: 1.941s, learning 0.114s)
             Mean action noise std: 2.29
          Mean value_function loss: 204.2625
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 48.9010
                       Mean reward: 131.08
               Mean episode length: 87.50
    Episode_Reward/reaching_object: 0.3955
     Episode_Reward/lifting_object: 25.3049
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 45.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 2.05s
                      Time elapsed: 00:25:31
                               ETA: 00:48:09

################################################################################
                     [1m Learning iteration 693/2000 [0m                      

                       Computation: 47420 steps/s (collection: 1.956s, learning 0.117s)
             Mean action noise std: 2.29
          Mean value_function loss: 225.4398
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 48.9007
                       Mean reward: 130.52
               Mean episode length: 92.97
    Episode_Reward/reaching_object: 0.4018
     Episode_Reward/lifting_object: 25.7976
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 46.3333
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 2.07s
                      Time elapsed: 00:25:33
                               ETA: 00:48:07

################################################################################
                     [1m Learning iteration 694/2000 [0m                      

                       Computation: 47433 steps/s (collection: 1.961s, learning 0.112s)
             Mean action noise std: 2.29
          Mean value_function loss: 211.3101
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 48.9013
                       Mean reward: 126.14
               Mean episode length: 85.79
    Episode_Reward/reaching_object: 0.3961
     Episode_Reward/lifting_object: 25.4832
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 47.2083
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 2.07s
                      Time elapsed: 00:25:35
                               ETA: 00:48:05

################################################################################
                     [1m Learning iteration 695/2000 [0m                      

                       Computation: 47661 steps/s (collection: 1.969s, learning 0.094s)
             Mean action noise std: 2.29
          Mean value_function loss: 213.2670
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 48.9018
                       Mean reward: 124.69
               Mean episode length: 82.72
    Episode_Reward/reaching_object: 0.4036
     Episode_Reward/lifting_object: 26.3436
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 46.5417
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 2.06s
                      Time elapsed: 00:25:37
                               ETA: 00:48:02

################################################################################
                     [1m Learning iteration 696/2000 [0m                      

                       Computation: 48071 steps/s (collection: 1.937s, learning 0.108s)
             Mean action noise std: 2.29
          Mean value_function loss: 223.2082
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 48.9030
                       Mean reward: 128.82
               Mean episode length: 86.33
    Episode_Reward/reaching_object: 0.3866
     Episode_Reward/lifting_object: 24.9908
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 45.2500
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 2.04s
                      Time elapsed: 00:25:39
                               ETA: 00:48:00

################################################################################
                     [1m Learning iteration 697/2000 [0m                      

                       Computation: 48249 steps/s (collection: 1.926s, learning 0.112s)
             Mean action noise std: 2.29
          Mean value_function loss: 214.9268
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 48.9047
                       Mean reward: 129.67
               Mean episode length: 90.57
    Episode_Reward/reaching_object: 0.3963
     Episode_Reward/lifting_object: 25.2527
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 46.5417
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 2.04s
                      Time elapsed: 00:25:41
                               ETA: 00:47:57

################################################################################
                     [1m Learning iteration 698/2000 [0m                      

                       Computation: 46903 steps/s (collection: 1.968s, learning 0.128s)
             Mean action noise std: 2.29
          Mean value_function loss: 219.0913
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 48.9070
                       Mean reward: 131.79
               Mean episode length: 87.19
    Episode_Reward/reaching_object: 0.3954
     Episode_Reward/lifting_object: 25.5784
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 47.4583
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 2.10s
                      Time elapsed: 00:25:43
                               ETA: 00:47:55

################################################################################
                     [1m Learning iteration 699/2000 [0m                      

                       Computation: 45532 steps/s (collection: 2.056s, learning 0.103s)
             Mean action noise std: 2.29
          Mean value_function loss: 224.1358
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 48.9088
                       Mean reward: 134.61
               Mean episode length: 87.87
    Episode_Reward/reaching_object: 0.3960
     Episode_Reward/lifting_object: 25.4206
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 45.5000
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 2.16s
                      Time elapsed: 00:25:45
                               ETA: 00:47:52

################################################################################
                     [1m Learning iteration 700/2000 [0m                      

                       Computation: 47544 steps/s (collection: 1.956s, learning 0.112s)
             Mean action noise std: 2.29
          Mean value_function loss: 215.2100
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 48.9097
                       Mean reward: 126.24
               Mean episode length: 86.16
    Episode_Reward/reaching_object: 0.3974
     Episode_Reward/lifting_object: 25.4387
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 46.8333
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 2.07s
                      Time elapsed: 00:25:47
                               ETA: 00:47:50

################################################################################
                     [1m Learning iteration 701/2000 [0m                      

                       Computation: 48214 steps/s (collection: 1.934s, learning 0.105s)
             Mean action noise std: 2.29
          Mean value_function loss: 205.4698
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 48.9107
                       Mean reward: 132.15
               Mean episode length: 93.18
    Episode_Reward/reaching_object: 0.4022
     Episode_Reward/lifting_object: 26.0721
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 45.3333
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 2.04s
                      Time elapsed: 00:25:49
                               ETA: 00:47:47

################################################################################
                     [1m Learning iteration 702/2000 [0m                      

                       Computation: 48179 steps/s (collection: 1.939s, learning 0.101s)
             Mean action noise std: 2.29
          Mean value_function loss: 216.5495
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 48.9104
                       Mean reward: 131.24
               Mean episode length: 89.35
    Episode_Reward/reaching_object: 0.4072
     Episode_Reward/lifting_object: 25.9271
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 44.5000
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 2.04s
                      Time elapsed: 00:25:51
                               ETA: 00:47:45

################################################################################
                     [1m Learning iteration 703/2000 [0m                      

                       Computation: 47584 steps/s (collection: 1.965s, learning 0.101s)
             Mean action noise std: 2.29
          Mean value_function loss: 240.8141
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 48.9094
                       Mean reward: 139.94
               Mean episode length: 90.67
    Episode_Reward/reaching_object: 0.4131
     Episode_Reward/lifting_object: 26.4385
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 47.6667
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 2.07s
                      Time elapsed: 00:25:53
                               ETA: 00:47:42

################################################################################
                     [1m Learning iteration 704/2000 [0m                      

                       Computation: 49201 steps/s (collection: 1.911s, learning 0.087s)
             Mean action noise std: 2.29
          Mean value_function loss: 220.8223
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 48.9119
                       Mean reward: 137.71
               Mean episode length: 93.67
    Episode_Reward/reaching_object: 0.4124
     Episode_Reward/lifting_object: 27.1047
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 46.4167
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 2.00s
                      Time elapsed: 00:25:55
                               ETA: 00:47:40

################################################################################
                     [1m Learning iteration 705/2000 [0m                      

                       Computation: 49211 steps/s (collection: 1.902s, learning 0.096s)
             Mean action noise std: 2.29
          Mean value_function loss: 213.8818
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 48.9138
                       Mean reward: 136.49
               Mean episode length: 88.63
    Episode_Reward/reaching_object: 0.4028
     Episode_Reward/lifting_object: 26.1732
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 44.1250
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 2.00s
                      Time elapsed: 00:25:57
                               ETA: 00:47:37

################################################################################
                     [1m Learning iteration 706/2000 [0m                      

                       Computation: 48760 steps/s (collection: 1.909s, learning 0.107s)
             Mean action noise std: 2.29
          Mean value_function loss: 211.8563
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 48.9140
                       Mean reward: 123.97
               Mean episode length: 86.38
    Episode_Reward/reaching_object: 0.4034
     Episode_Reward/lifting_object: 26.5021
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 47.1667
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 2.02s
                      Time elapsed: 00:25:59
                               ETA: 00:47:35

################################################################################
                     [1m Learning iteration 707/2000 [0m                      

                       Computation: 49580 steps/s (collection: 1.870s, learning 0.113s)
             Mean action noise std: 2.30
          Mean value_function loss: 215.2142
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 48.9154
                       Mean reward: 130.99
               Mean episode length: 86.58
    Episode_Reward/reaching_object: 0.4014
     Episode_Reward/lifting_object: 26.7798
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 46.5417
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 1.98s
                      Time elapsed: 00:26:01
                               ETA: 00:47:32

################################################################################
                     [1m Learning iteration 708/2000 [0m                      

                       Computation: 49005 steps/s (collection: 1.896s, learning 0.110s)
             Mean action noise std: 2.30
          Mean value_function loss: 236.6520
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 48.9171
                       Mean reward: 137.51
               Mean episode length: 82.66
    Episode_Reward/reaching_object: 0.3970
     Episode_Reward/lifting_object: 26.4339
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 47.1250
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 2.01s
                      Time elapsed: 00:26:03
                               ETA: 00:47:29

################################################################################
                     [1m Learning iteration 709/2000 [0m                      

                       Computation: 49264 steps/s (collection: 1.905s, learning 0.090s)
             Mean action noise std: 2.30
          Mean value_function loss: 216.0487
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 48.9173
                       Mean reward: 137.27
               Mean episode length: 90.86
    Episode_Reward/reaching_object: 0.4106
     Episode_Reward/lifting_object: 27.2994
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 45.2083
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 2.00s
                      Time elapsed: 00:26:05
                               ETA: 00:47:27

################################################################################
                     [1m Learning iteration 710/2000 [0m                      

                       Computation: 48038 steps/s (collection: 1.936s, learning 0.111s)
             Mean action noise std: 2.30
          Mean value_function loss: 218.4323
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 48.9180
                       Mean reward: 135.33
               Mean episode length: 88.57
    Episode_Reward/reaching_object: 0.4017
     Episode_Reward/lifting_object: 27.1052
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 44.1667
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 2.05s
                      Time elapsed: 00:26:07
                               ETA: 00:47:24

################################################################################
                     [1m Learning iteration 711/2000 [0m                      

                       Computation: 49081 steps/s (collection: 1.891s, learning 0.112s)
             Mean action noise std: 2.30
          Mean value_function loss: 228.1426
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 48.9205
                       Mean reward: 151.32
               Mean episode length: 92.06
    Episode_Reward/reaching_object: 0.3974
     Episode_Reward/lifting_object: 26.9197
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 46.0833
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 2.00s
                      Time elapsed: 00:26:09
                               ETA: 00:47:22

################################################################################
                     [1m Learning iteration 712/2000 [0m                      

                       Computation: 47265 steps/s (collection: 1.969s, learning 0.111s)
             Mean action noise std: 2.30
          Mean value_function loss: 231.4424
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 48.9220
                       Mean reward: 136.61
               Mean episode length: 90.09
    Episode_Reward/reaching_object: 0.4126
     Episode_Reward/lifting_object: 27.5717
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 46.8333
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 2.08s
                      Time elapsed: 00:26:12
                               ETA: 00:47:19

################################################################################
                     [1m Learning iteration 713/2000 [0m                      

                       Computation: 48750 steps/s (collection: 1.907s, learning 0.109s)
             Mean action noise std: 2.30
          Mean value_function loss: 223.1813
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 48.9228
                       Mean reward: 143.61
               Mean episode length: 94.25
    Episode_Reward/reaching_object: 0.4114
     Episode_Reward/lifting_object: 27.7003
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 45.2500
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 2.02s
                      Time elapsed: 00:26:14
                               ETA: 00:47:17

################################################################################
                     [1m Learning iteration 714/2000 [0m                      

                       Computation: 47941 steps/s (collection: 1.938s, learning 0.112s)
             Mean action noise std: 2.30
          Mean value_function loss: 223.4914
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 48.9246
                       Mean reward: 147.08
               Mean episode length: 92.65
    Episode_Reward/reaching_object: 0.4042
     Episode_Reward/lifting_object: 27.3665
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 45.3750
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 2.05s
                      Time elapsed: 00:26:16
                               ETA: 00:47:14

################################################################################
                     [1m Learning iteration 715/2000 [0m                      

                       Computation: 48782 steps/s (collection: 1.921s, learning 0.094s)
             Mean action noise std: 2.30
          Mean value_function loss: 229.9590
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 48.9256
                       Mean reward: 144.93
               Mean episode length: 92.90
    Episode_Reward/reaching_object: 0.4004
     Episode_Reward/lifting_object: 26.9211
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 44.2083
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 2.02s
                      Time elapsed: 00:26:18
                               ETA: 00:47:12

################################################################################
                     [1m Learning iteration 716/2000 [0m                      

                       Computation: 49462 steps/s (collection: 1.898s, learning 0.090s)
             Mean action noise std: 2.30
          Mean value_function loss: 266.4168
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 48.9256
                       Mean reward: 133.04
               Mean episode length: 88.92
    Episode_Reward/reaching_object: 0.4101
     Episode_Reward/lifting_object: 27.6908
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 44.5000
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 1.99s
                      Time elapsed: 00:26:20
                               ETA: 00:47:09

################################################################################
                     [1m Learning iteration 717/2000 [0m                      

                       Computation: 49448 steps/s (collection: 1.902s, learning 0.086s)
             Mean action noise std: 2.30
          Mean value_function loss: 237.6793
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.9266
                       Mean reward: 139.11
               Mean episode length: 90.14
    Episode_Reward/reaching_object: 0.4132
     Episode_Reward/lifting_object: 28.0054
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 45.7917
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 1.99s
                      Time elapsed: 00:26:22
                               ETA: 00:47:07

################################################################################
                     [1m Learning iteration 718/2000 [0m                      

                       Computation: 49408 steps/s (collection: 1.904s, learning 0.086s)
             Mean action noise std: 2.30
          Mean value_function loss: 254.3114
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 48.9280
                       Mean reward: 145.25
               Mean episode length: 90.06
    Episode_Reward/reaching_object: 0.4174
     Episode_Reward/lifting_object: 28.4728
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 47.0833
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 1.99s
                      Time elapsed: 00:26:24
                               ETA: 00:47:04

################################################################################
                     [1m Learning iteration 719/2000 [0m                      

                       Computation: 47825 steps/s (collection: 1.967s, learning 0.089s)
             Mean action noise std: 2.30
          Mean value_function loss: 230.5182
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 48.9289
                       Mean reward: 148.78
               Mean episode length: 92.52
    Episode_Reward/reaching_object: 0.3982
     Episode_Reward/lifting_object: 26.8584
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 46.3333
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 2.06s
                      Time elapsed: 00:26:26
                               ETA: 00:47:02

################################################################################
                     [1m Learning iteration 720/2000 [0m                      

                       Computation: 48738 steps/s (collection: 1.927s, learning 0.090s)
             Mean action noise std: 2.30
          Mean value_function loss: 251.1768
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 48.9289
                       Mean reward: 141.23
               Mean episode length: 94.21
    Episode_Reward/reaching_object: 0.4030
     Episode_Reward/lifting_object: 27.5577
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 43.6250
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 2.02s
                      Time elapsed: 00:26:28
                               ETA: 00:46:59

################################################################################
                     [1m Learning iteration 721/2000 [0m                      

                       Computation: 48486 steps/s (collection: 1.940s, learning 0.087s)
             Mean action noise std: 2.30
          Mean value_function loss: 243.8071
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 48.9287
                       Mean reward: 133.52
               Mean episode length: 87.86
    Episode_Reward/reaching_object: 0.4142
     Episode_Reward/lifting_object: 28.0130
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 45.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 2.03s
                      Time elapsed: 00:26:30
                               ETA: 00:46:56

################################################################################
                     [1m Learning iteration 722/2000 [0m                      

                       Computation: 49659 steps/s (collection: 1.893s, learning 0.087s)
             Mean action noise std: 2.30
          Mean value_function loss: 247.1143
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 48.9290
                       Mean reward: 142.62
               Mean episode length: 89.01
    Episode_Reward/reaching_object: 0.4104
     Episode_Reward/lifting_object: 28.2970
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 46.1250
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 1.98s
                      Time elapsed: 00:26:32
                               ETA: 00:46:54

################################################################################
                     [1m Learning iteration 723/2000 [0m                      

                       Computation: 47824 steps/s (collection: 1.967s, learning 0.089s)
             Mean action noise std: 2.30
          Mean value_function loss: 235.7998
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 48.9304
                       Mean reward: 152.67
               Mean episode length: 95.00
    Episode_Reward/reaching_object: 0.4015
     Episode_Reward/lifting_object: 27.8670
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 47.0833
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 2.06s
                      Time elapsed: 00:26:34
                               ETA: 00:46:51

################################################################################
                     [1m Learning iteration 724/2000 [0m                      

                       Computation: 48358 steps/s (collection: 1.941s, learning 0.092s)
             Mean action noise std: 2.30
          Mean value_function loss: 248.0120
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 48.9310
                       Mean reward: 143.20
               Mean episode length: 89.59
    Episode_Reward/reaching_object: 0.4102
     Episode_Reward/lifting_object: 28.4510
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 45.8750
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 2.03s
                      Time elapsed: 00:26:36
                               ETA: 00:46:49

################################################################################
                     [1m Learning iteration 725/2000 [0m                      

                       Computation: 48629 steps/s (collection: 1.930s, learning 0.092s)
             Mean action noise std: 2.30
          Mean value_function loss: 242.5463
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 48.9298
                       Mean reward: 144.11
               Mean episode length: 87.01
    Episode_Reward/reaching_object: 0.4062
     Episode_Reward/lifting_object: 28.2448
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 45.2500
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 2.02s
                      Time elapsed: 00:26:38
                               ETA: 00:46:46

################################################################################
                     [1m Learning iteration 726/2000 [0m                      

                       Computation: 48580 steps/s (collection: 1.934s, learning 0.090s)
             Mean action noise std: 2.30
          Mean value_function loss: 259.7571
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 48.9294
                       Mean reward: 142.24
               Mean episode length: 87.07
    Episode_Reward/reaching_object: 0.4110
     Episode_Reward/lifting_object: 28.5968
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 45.3333
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 2.02s
                      Time elapsed: 00:26:40
                               ETA: 00:46:44

################################################################################
                     [1m Learning iteration 727/2000 [0m                      

                       Computation: 48809 steps/s (collection: 1.904s, learning 0.111s)
             Mean action noise std: 2.30
          Mean value_function loss: 238.8427
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 48.9297
                       Mean reward: 163.42
               Mean episode length: 102.01
    Episode_Reward/reaching_object: 0.4148
     Episode_Reward/lifting_object: 28.8474
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 46.0417
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 2.01s
                      Time elapsed: 00:26:42
                               ETA: 00:46:41

################################################################################
                     [1m Learning iteration 728/2000 [0m                      

                       Computation: 47985 steps/s (collection: 1.950s, learning 0.099s)
             Mean action noise std: 2.30
          Mean value_function loss: 241.1675
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 48.9304
                       Mean reward: 144.62
               Mean episode length: 91.62
    Episode_Reward/reaching_object: 0.4047
     Episode_Reward/lifting_object: 28.3110
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 45.0417
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 2.05s
                      Time elapsed: 00:26:44
                               ETA: 00:46:39

################################################################################
                     [1m Learning iteration 729/2000 [0m                      

                       Computation: 48836 steps/s (collection: 1.909s, learning 0.104s)
             Mean action noise std: 2.30
          Mean value_function loss: 256.2645
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 48.9303
                       Mean reward: 144.97
               Mean episode length: 89.95
    Episode_Reward/reaching_object: 0.4100
     Episode_Reward/lifting_object: 28.4931
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 45.8750
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 2.01s
                      Time elapsed: 00:26:46
                               ETA: 00:46:36

################################################################################
                     [1m Learning iteration 730/2000 [0m                      

                       Computation: 47629 steps/s (collection: 1.950s, learning 0.114s)
             Mean action noise std: 2.30
          Mean value_function loss: 312.5185
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 48.9298
                       Mean reward: 131.26
               Mean episode length: 83.57
    Episode_Reward/reaching_object: 0.3906
     Episode_Reward/lifting_object: 27.4377
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 45.1667
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 2.06s
                      Time elapsed: 00:26:48
                               ETA: 00:46:34

################################################################################
                     [1m Learning iteration 731/2000 [0m                      

                       Computation: 47724 steps/s (collection: 1.946s, learning 0.114s)
             Mean action noise std: 2.30
          Mean value_function loss: 295.7652
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 48.9293
                       Mean reward: 146.70
               Mean episode length: 90.47
    Episode_Reward/reaching_object: 0.4073
     Episode_Reward/lifting_object: 28.4761
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 45.0417
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 2.06s
                      Time elapsed: 00:26:50
                               ETA: 00:46:31

################################################################################
                     [1m Learning iteration 732/2000 [0m                      

                       Computation: 48062 steps/s (collection: 1.931s, learning 0.115s)
             Mean action noise std: 2.30
          Mean value_function loss: 241.3614
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 48.9294
                       Mean reward: 153.66
               Mean episode length: 93.29
    Episode_Reward/reaching_object: 0.4078
     Episode_Reward/lifting_object: 29.5784
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 43.7083
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 2.05s
                      Time elapsed: 00:26:52
                               ETA: 00:46:29

################################################################################
                     [1m Learning iteration 733/2000 [0m                      

                       Computation: 49051 steps/s (collection: 1.891s, learning 0.113s)
             Mean action noise std: 2.30
          Mean value_function loss: 249.6315
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 48.9307
                       Mean reward: 150.86
               Mean episode length: 93.28
    Episode_Reward/reaching_object: 0.4083
     Episode_Reward/lifting_object: 29.2815
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 46.0417
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 2.00s
                      Time elapsed: 00:26:54
                               ETA: 00:46:26

################################################################################
                     [1m Learning iteration 734/2000 [0m                      

                       Computation: 48770 steps/s (collection: 1.901s, learning 0.115s)
             Mean action noise std: 2.30
          Mean value_function loss: 259.4352
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 48.9315
                       Mean reward: 150.30
               Mean episode length: 92.02
    Episode_Reward/reaching_object: 0.4125
     Episode_Reward/lifting_object: 29.6062
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 45.9167
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 2.02s
                      Time elapsed: 00:26:56
                               ETA: 00:46:24

################################################################################
                     [1m Learning iteration 735/2000 [0m                      

                       Computation: 49063 steps/s (collection: 1.895s, learning 0.109s)
             Mean action noise std: 2.30
          Mean value_function loss: 293.5544
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 48.9315
                       Mean reward: 144.78
               Mean episode length: 89.37
    Episode_Reward/reaching_object: 0.3984
     Episode_Reward/lifting_object: 28.1008
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 47.2917
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 2.00s
                      Time elapsed: 00:26:58
                               ETA: 00:46:21

################################################################################
                     [1m Learning iteration 736/2000 [0m                      

                       Computation: 48842 steps/s (collection: 1.908s, learning 0.105s)
             Mean action noise std: 2.30
          Mean value_function loss: 253.3406
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 48.9297
                       Mean reward: 148.46
               Mean episode length: 88.68
    Episode_Reward/reaching_object: 0.4111
     Episode_Reward/lifting_object: 29.3272
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 43.4167
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 2.01s
                      Time elapsed: 00:27:00
                               ETA: 00:46:19

################################################################################
                     [1m Learning iteration 737/2000 [0m                      

                       Computation: 49078 steps/s (collection: 1.892s, learning 0.111s)
             Mean action noise std: 2.30
          Mean value_function loss: 270.8841
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 48.9297
                       Mean reward: 146.71
               Mean episode length: 91.47
    Episode_Reward/reaching_object: 0.4026
     Episode_Reward/lifting_object: 28.7091
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 45.1250
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 2.00s
                      Time elapsed: 00:27:02
                               ETA: 00:46:16

################################################################################
                     [1m Learning iteration 738/2000 [0m                      

                       Computation: 48402 steps/s (collection: 1.916s, learning 0.115s)
             Mean action noise std: 2.30
          Mean value_function loss: 254.3755
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 48.9302
                       Mean reward: 155.59
               Mean episode length: 92.35
    Episode_Reward/reaching_object: 0.3982
     Episode_Reward/lifting_object: 28.9753
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 43.7917
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 2.03s
                      Time elapsed: 00:27:04
                               ETA: 00:46:14

################################################################################
                     [1m Learning iteration 739/2000 [0m                      

                       Computation: 47487 steps/s (collection: 1.955s, learning 0.116s)
             Mean action noise std: 2.30
          Mean value_function loss: 258.8194
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 48.9304
                       Mean reward: 134.84
               Mean episode length: 87.91
    Episode_Reward/reaching_object: 0.4075
     Episode_Reward/lifting_object: 28.9551
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 45.8333
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 2.07s
                      Time elapsed: 00:27:06
                               ETA: 00:46:11

################################################################################
                     [1m Learning iteration 740/2000 [0m                      

                       Computation: 48408 steps/s (collection: 1.916s, learning 0.115s)
             Mean action noise std: 2.30
          Mean value_function loss: 254.0977
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 48.9313
                       Mean reward: 145.65
               Mean episode length: 90.55
    Episode_Reward/reaching_object: 0.4046
     Episode_Reward/lifting_object: 29.4210
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 45.9167
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 2.03s
                      Time elapsed: 00:27:08
                               ETA: 00:46:09

################################################################################
                     [1m Learning iteration 741/2000 [0m                      

                       Computation: 48940 steps/s (collection: 1.899s, learning 0.110s)
             Mean action noise std: 2.30
          Mean value_function loss: 254.0276
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 48.9317
                       Mean reward: 146.18
               Mean episode length: 89.96
    Episode_Reward/reaching_object: 0.4137
     Episode_Reward/lifting_object: 30.0623
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 45.5833
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 2.01s
                      Time elapsed: 00:27:10
                               ETA: 00:46:06

################################################################################
                     [1m Learning iteration 742/2000 [0m                      

                       Computation: 48673 steps/s (collection: 1.913s, learning 0.107s)
             Mean action noise std: 2.30
          Mean value_function loss: 263.8428
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 48.9322
                       Mean reward: 146.05
               Mean episode length: 89.27
    Episode_Reward/reaching_object: 0.3946
     Episode_Reward/lifting_object: 28.9920
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 44.6250
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 2.02s
                      Time elapsed: 00:27:12
                               ETA: 00:46:04

################################################################################
                     [1m Learning iteration 743/2000 [0m                      

                       Computation: 48174 steps/s (collection: 1.927s, learning 0.113s)
             Mean action noise std: 2.30
          Mean value_function loss: 288.0793
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 48.9325
                       Mean reward: 139.49
               Mean episode length: 86.29
    Episode_Reward/reaching_object: 0.3916
     Episode_Reward/lifting_object: 28.7148
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 47.6667
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 2.04s
                      Time elapsed: 00:27:14
                               ETA: 00:46:02

################################################################################
                     [1m Learning iteration 744/2000 [0m                      

                       Computation: 48714 steps/s (collection: 1.910s, learning 0.108s)
             Mean action noise std: 2.30
          Mean value_function loss: 259.1967
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 48.9335
                       Mean reward: 146.41
               Mean episode length: 88.86
    Episode_Reward/reaching_object: 0.4009
     Episode_Reward/lifting_object: 29.2992
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 43.2500
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 2.02s
                      Time elapsed: 00:27:16
                               ETA: 00:45:59

################################################################################
                     [1m Learning iteration 745/2000 [0m                      

                       Computation: 48991 steps/s (collection: 1.915s, learning 0.092s)
             Mean action noise std: 2.30
          Mean value_function loss: 263.3635
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 48.9345
                       Mean reward: 160.18
               Mean episode length: 96.56
    Episode_Reward/reaching_object: 0.3975
     Episode_Reward/lifting_object: 29.4359
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 44.9583
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 2.01s
                      Time elapsed: 00:27:18
                               ETA: 00:45:56

################################################################################
                     [1m Learning iteration 746/2000 [0m                      

                       Computation: 49466 steps/s (collection: 1.883s, learning 0.104s)
             Mean action noise std: 2.30
          Mean value_function loss: 302.0449
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 48.9354
                       Mean reward: 155.83
               Mean episode length: 92.18
    Episode_Reward/reaching_object: 0.4015
     Episode_Reward/lifting_object: 29.6869
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 45.7917
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 1.99s
                      Time elapsed: 00:27:20
                               ETA: 00:45:54

################################################################################
                     [1m Learning iteration 747/2000 [0m                      

                       Computation: 49057 steps/s (collection: 1.906s, learning 0.098s)
             Mean action noise std: 2.30
          Mean value_function loss: 298.0693
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 48.9375
                       Mean reward: 146.83
               Mean episode length: 91.59
    Episode_Reward/reaching_object: 0.4049
     Episode_Reward/lifting_object: 29.7093
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 46.9583
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 2.00s
                      Time elapsed: 00:27:22
                               ETA: 00:45:51

################################################################################
                     [1m Learning iteration 748/2000 [0m                      

                       Computation: 47712 steps/s (collection: 1.962s, learning 0.098s)
             Mean action noise std: 2.30
          Mean value_function loss: 289.4643
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 48.9393
                       Mean reward: 158.68
               Mean episode length: 91.95
    Episode_Reward/reaching_object: 0.4043
     Episode_Reward/lifting_object: 29.5309
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 44.0833
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 2.06s
                      Time elapsed: 00:27:24
                               ETA: 00:45:49

################################################################################
                     [1m Learning iteration 749/2000 [0m                      

                       Computation: 47745 steps/s (collection: 1.945s, learning 0.114s)
             Mean action noise std: 2.30
          Mean value_function loss: 289.0778
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 48.9398
                       Mean reward: 139.24
               Mean episode length: 83.67
    Episode_Reward/reaching_object: 0.4041
     Episode_Reward/lifting_object: 29.9719
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 45.9583
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 2.06s
                      Time elapsed: 00:27:26
                               ETA: 00:45:47

################################################################################
                     [1m Learning iteration 750/2000 [0m                      

                       Computation: 48984 steps/s (collection: 1.900s, learning 0.107s)
             Mean action noise std: 2.30
          Mean value_function loss: 277.3956
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 48.9398
                       Mean reward: 151.48
               Mean episode length: 87.79
    Episode_Reward/reaching_object: 0.3941
     Episode_Reward/lifting_object: 29.1198
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 45.7083
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 2.01s
                      Time elapsed: 00:27:28
                               ETA: 00:45:44

################################################################################
                     [1m Learning iteration 751/2000 [0m                      

                       Computation: 47513 steps/s (collection: 1.973s, learning 0.096s)
             Mean action noise std: 2.30
          Mean value_function loss: 274.1201
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 48.9421
                       Mean reward: 158.45
               Mean episode length: 90.09
    Episode_Reward/reaching_object: 0.4042
     Episode_Reward/lifting_object: 30.4668
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 47.7083
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 2.07s
                      Time elapsed: 00:27:31
                               ETA: 00:45:42

################################################################################
                     [1m Learning iteration 752/2000 [0m                      

                       Computation: 48511 steps/s (collection: 1.913s, learning 0.114s)
             Mean action noise std: 2.30
          Mean value_function loss: 272.9209
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 48.9439
                       Mean reward: 157.08
               Mean episode length: 87.80
    Episode_Reward/reaching_object: 0.4012
     Episode_Reward/lifting_object: 30.1049
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 44.7083
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 2.03s
                      Time elapsed: 00:27:33
                               ETA: 00:45:39

################################################################################
                     [1m Learning iteration 753/2000 [0m                      

                       Computation: 48892 steps/s (collection: 1.902s, learning 0.109s)
             Mean action noise std: 2.30
          Mean value_function loss: 270.6615
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 48.9447
                       Mean reward: 146.84
               Mean episode length: 87.37
    Episode_Reward/reaching_object: 0.3915
     Episode_Reward/lifting_object: 29.3577
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 46.1250
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 2.01s
                      Time elapsed: 00:27:35
                               ETA: 00:45:37

################################################################################
                     [1m Learning iteration 754/2000 [0m                      

                       Computation: 48709 steps/s (collection: 1.899s, learning 0.119s)
             Mean action noise std: 2.30
          Mean value_function loss: 294.8463
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 48.9453
                       Mean reward: 161.78
               Mean episode length: 88.21
    Episode_Reward/reaching_object: 0.3919
     Episode_Reward/lifting_object: 30.2680
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 47.8750
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 2.02s
                      Time elapsed: 00:27:37
                               ETA: 00:45:34

################################################################################
                     [1m Learning iteration 755/2000 [0m                      

                       Computation: 48479 steps/s (collection: 1.916s, learning 0.112s)
             Mean action noise std: 2.30
          Mean value_function loss: 280.8127
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 48.9450
                       Mean reward: 149.32
               Mean episode length: 86.33
    Episode_Reward/reaching_object: 0.3877
     Episode_Reward/lifting_object: 30.1748
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 47.3333
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 2.03s
                      Time elapsed: 00:27:39
                               ETA: 00:45:32

################################################################################
                     [1m Learning iteration 756/2000 [0m                      

                       Computation: 49048 steps/s (collection: 1.904s, learning 0.101s)
             Mean action noise std: 2.30
          Mean value_function loss: 291.4738
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 48.9445
                       Mean reward: 165.93
               Mean episode length: 89.48
    Episode_Reward/reaching_object: 0.3955
     Episode_Reward/lifting_object: 30.0437
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 45.5000
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 2.00s
                      Time elapsed: 00:27:41
                               ETA: 00:45:29

################################################################################
                     [1m Learning iteration 757/2000 [0m                      

                       Computation: 49110 steps/s (collection: 1.889s, learning 0.113s)
             Mean action noise std: 2.30
          Mean value_function loss: 317.5523
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 48.9445
                       Mean reward: 142.68
               Mean episode length: 83.07
    Episode_Reward/reaching_object: 0.3910
     Episode_Reward/lifting_object: 29.9382
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 48.2500
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 2.00s
                      Time elapsed: 00:27:43
                               ETA: 00:45:27

################################################################################
                     [1m Learning iteration 758/2000 [0m                      

                       Computation: 49456 steps/s (collection: 1.874s, learning 0.114s)
             Mean action noise std: 2.30
          Mean value_function loss: 344.6547
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 48.9455
                       Mean reward: 145.08
               Mean episode length: 82.31
    Episode_Reward/reaching_object: 0.3836
     Episode_Reward/lifting_object: 29.8744
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 45.2083
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 1.99s
                      Time elapsed: 00:27:45
                               ETA: 00:45:24

################################################################################
                     [1m Learning iteration 759/2000 [0m                      

                       Computation: 49160 steps/s (collection: 1.883s, learning 0.117s)
             Mean action noise std: 2.30
          Mean value_function loss: 366.8324
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 48.9490
                       Mean reward: 154.18
               Mean episode length: 92.82
    Episode_Reward/reaching_object: 0.3825
     Episode_Reward/lifting_object: 29.3362
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 49.1667
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 2.00s
                      Time elapsed: 00:27:47
                               ETA: 00:45:22

################################################################################
                     [1m Learning iteration 760/2000 [0m                      

                       Computation: 47998 steps/s (collection: 1.927s, learning 0.121s)
             Mean action noise std: 2.30
          Mean value_function loss: 281.7243
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 48.9501
                       Mean reward: 161.01
               Mean episode length: 92.46
    Episode_Reward/reaching_object: 0.3905
     Episode_Reward/lifting_object: 30.2875
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 46.2500
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 2.05s
                      Time elapsed: 00:27:49
                               ETA: 00:45:19

################################################################################
                     [1m Learning iteration 761/2000 [0m                      

                       Computation: 48135 steps/s (collection: 1.921s, learning 0.122s)
             Mean action noise std: 2.30
          Mean value_function loss: 295.7622
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 48.9508
                       Mean reward: 136.09
               Mean episode length: 88.33
    Episode_Reward/reaching_object: 0.3878
     Episode_Reward/lifting_object: 29.8918
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 48.4167
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 2.04s
                      Time elapsed: 00:27:51
                               ETA: 00:45:17

################################################################################
                     [1m Learning iteration 762/2000 [0m                      

                       Computation: 47923 steps/s (collection: 1.941s, learning 0.111s)
             Mean action noise std: 2.30
          Mean value_function loss: 340.0509
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 48.9515
                       Mean reward: 144.65
               Mean episode length: 83.65
    Episode_Reward/reaching_object: 0.3763
     Episode_Reward/lifting_object: 29.2938
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 47.7083
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 2.05s
                      Time elapsed: 00:27:53
                               ETA: 00:45:14

################################################################################
                     [1m Learning iteration 763/2000 [0m                      

                       Computation: 49007 steps/s (collection: 1.897s, learning 0.109s)
             Mean action noise std: 2.30
          Mean value_function loss: 295.9637
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 48.9515
                       Mean reward: 143.82
               Mean episode length: 86.32
    Episode_Reward/reaching_object: 0.3771
     Episode_Reward/lifting_object: 29.2634
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 49.2917
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 2.01s
                      Time elapsed: 00:27:55
                               ETA: 00:45:12

################################################################################
                     [1m Learning iteration 764/2000 [0m                      

                       Computation: 48040 steps/s (collection: 1.942s, learning 0.105s)
             Mean action noise std: 2.30
          Mean value_function loss: 311.9187
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 48.9509
                       Mean reward: 152.62
               Mean episode length: 86.07
    Episode_Reward/reaching_object: 0.3755
     Episode_Reward/lifting_object: 29.7364
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 48.9167
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 2.05s
                      Time elapsed: 00:27:57
                               ETA: 00:45:09

################################################################################
                     [1m Learning iteration 765/2000 [0m                      

                       Computation: 48450 steps/s (collection: 1.915s, learning 0.114s)
             Mean action noise std: 2.30
          Mean value_function loss: 309.5325
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 48.9515
                       Mean reward: 145.12
               Mean episode length: 80.37
    Episode_Reward/reaching_object: 0.3695
     Episode_Reward/lifting_object: 29.1967
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 46.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 2.03s
                      Time elapsed: 00:27:59
                               ETA: 00:45:07

################################################################################
                     [1m Learning iteration 766/2000 [0m                      

                       Computation: 47702 steps/s (collection: 1.948s, learning 0.113s)
             Mean action noise std: 2.30
          Mean value_function loss: 299.3104
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 48.9532
                       Mean reward: 162.29
               Mean episode length: 88.95
    Episode_Reward/reaching_object: 0.3701
     Episode_Reward/lifting_object: 29.0380
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 49.1250
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 2.06s
                      Time elapsed: 00:28:01
                               ETA: 00:45:05

################################################################################
                     [1m Learning iteration 767/2000 [0m                      

                       Computation: 48872 steps/s (collection: 1.902s, learning 0.109s)
             Mean action noise std: 2.30
          Mean value_function loss: 333.2535
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 48.9531
                       Mean reward: 148.23
               Mean episode length: 85.48
    Episode_Reward/reaching_object: 0.3692
     Episode_Reward/lifting_object: 29.7104
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 48.2083
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 2.01s
                      Time elapsed: 00:28:03
                               ETA: 00:45:02

################################################################################
                     [1m Learning iteration 768/2000 [0m                      

                       Computation: 48107 steps/s (collection: 1.931s, learning 0.113s)
             Mean action noise std: 2.30
          Mean value_function loss: 339.7301
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 48.9531
                       Mean reward: 157.68
               Mean episode length: 85.05
    Episode_Reward/reaching_object: 0.3818
     Episode_Reward/lifting_object: 30.2033
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 49.3750
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 2.04s
                      Time elapsed: 00:28:05
                               ETA: 00:45:00

################################################################################
                     [1m Learning iteration 769/2000 [0m                      

                       Computation: 49506 steps/s (collection: 1.898s, learning 0.088s)
             Mean action noise std: 2.30
          Mean value_function loss: 323.2253
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 48.9524
                       Mean reward: 150.13
               Mean episode length: 83.83
    Episode_Reward/reaching_object: 0.3680
     Episode_Reward/lifting_object: 29.7320
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 47.7083
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 1.99s
                      Time elapsed: 00:28:07
                               ETA: 00:44:57

################################################################################
                     [1m Learning iteration 770/2000 [0m                      

                       Computation: 48928 steps/s (collection: 1.906s, learning 0.103s)
             Mean action noise std: 2.30
          Mean value_function loss: 344.8538
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 48.9523
                       Mean reward: 146.38
               Mean episode length: 81.85
    Episode_Reward/reaching_object: 0.3687
     Episode_Reward/lifting_object: 29.8434
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 47.8750
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 2.01s
                      Time elapsed: 00:28:09
                               ETA: 00:44:55

################################################################################
                     [1m Learning iteration 771/2000 [0m                      

                       Computation: 48811 steps/s (collection: 1.902s, learning 0.112s)
             Mean action noise std: 2.30
          Mean value_function loss: 335.9156
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 48.9532
                       Mean reward: 155.51
               Mean episode length: 82.89
    Episode_Reward/reaching_object: 0.3701
     Episode_Reward/lifting_object: 30.4272
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 49.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 2.01s
                      Time elapsed: 00:28:11
                               ETA: 00:44:52

################################################################################
                     [1m Learning iteration 772/2000 [0m                      

                       Computation: 48808 steps/s (collection: 1.897s, learning 0.117s)
             Mean action noise std: 2.30
          Mean value_function loss: 323.2218
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 48.9552
                       Mean reward: 140.40
               Mean episode length: 80.78
    Episode_Reward/reaching_object: 0.3618
     Episode_Reward/lifting_object: 29.6374
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 48.8333
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 2.01s
                      Time elapsed: 00:28:13
                               ETA: 00:44:50

################################################################################
                     [1m Learning iteration 773/2000 [0m                      

                       Computation: 47976 steps/s (collection: 1.939s, learning 0.110s)
             Mean action noise std: 2.30
          Mean value_function loss: 318.6101
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 48.9557
                       Mean reward: 139.77
               Mean episode length: 79.01
    Episode_Reward/reaching_object: 0.3567
     Episode_Reward/lifting_object: 29.9322
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 48.7083
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 2.05s
                      Time elapsed: 00:28:15
                               ETA: 00:44:47

################################################################################
                     [1m Learning iteration 774/2000 [0m                      

                       Computation: 48942 steps/s (collection: 1.895s, learning 0.114s)
             Mean action noise std: 2.30
          Mean value_function loss: 378.1497
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 48.9556
                       Mean reward: 144.36
               Mean episode length: 86.44
    Episode_Reward/reaching_object: 0.3642
     Episode_Reward/lifting_object: 30.2433
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 48.7083
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 2.01s
                      Time elapsed: 00:28:17
                               ETA: 00:44:45

################################################################################
                     [1m Learning iteration 775/2000 [0m                      

                       Computation: 49359 steps/s (collection: 1.882s, learning 0.110s)
             Mean action noise std: 2.30
          Mean value_function loss: 338.9660
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 48.9548
                       Mean reward: 163.56
               Mean episode length: 86.60
    Episode_Reward/reaching_object: 0.3650
     Episode_Reward/lifting_object: 30.3683
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 47.8333
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 1.99s
                      Time elapsed: 00:28:19
                               ETA: 00:44:42

################################################################################
                     [1m Learning iteration 776/2000 [0m                      

                       Computation: 49137 steps/s (collection: 1.890s, learning 0.111s)
             Mean action noise std: 2.30
          Mean value_function loss: 328.1830
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 48.9533
                       Mean reward: 157.75
               Mean episode length: 85.21
    Episode_Reward/reaching_object: 0.3620
     Episode_Reward/lifting_object: 30.4901
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 48.1667
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 2.00s
                      Time elapsed: 00:28:21
                               ETA: 00:44:40

################################################################################
                     [1m Learning iteration 777/2000 [0m                      

                       Computation: 49041 steps/s (collection: 1.894s, learning 0.110s)
             Mean action noise std: 2.30
          Mean value_function loss: 356.7786
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 48.9509
                       Mean reward: 157.70
               Mean episode length: 84.21
    Episode_Reward/reaching_object: 0.3646
     Episode_Reward/lifting_object: 30.6281
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 48.5000
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 2.00s
                      Time elapsed: 00:28:23
                               ETA: 00:44:37

################################################################################
                     [1m Learning iteration 778/2000 [0m                      

                       Computation: 47973 steps/s (collection: 1.940s, learning 0.110s)
             Mean action noise std: 2.30
          Mean value_function loss: 341.2149
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 48.9512
                       Mean reward: 160.77
               Mean episode length: 84.77
    Episode_Reward/reaching_object: 0.3734
     Episode_Reward/lifting_object: 32.5956
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 49.6667
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 2.05s
                      Time elapsed: 00:28:25
                               ETA: 00:44:35

################################################################################
                     [1m Learning iteration 779/2000 [0m                      

                       Computation: 49117 steps/s (collection: 1.881s, learning 0.121s)
             Mean action noise std: 2.30
          Mean value_function loss: 349.2879
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 48.9514
                       Mean reward: 160.57
               Mean episode length: 86.16
    Episode_Reward/reaching_object: 0.3574
     Episode_Reward/lifting_object: 31.0382
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 48.4167
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 2.00s
                      Time elapsed: 00:28:27
                               ETA: 00:44:32

################################################################################
                     [1m Learning iteration 780/2000 [0m                      

                       Computation: 47994 steps/s (collection: 1.944s, learning 0.104s)
             Mean action noise std: 2.30
          Mean value_function loss: 367.3823
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 48.9508
                       Mean reward: 152.64
               Mean episode length: 84.60
    Episode_Reward/reaching_object: 0.3628
     Episode_Reward/lifting_object: 31.7698
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 45.7500
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 2.05s
                      Time elapsed: 00:28:29
                               ETA: 00:44:30

################################################################################
                     [1m Learning iteration 781/2000 [0m                      

                       Computation: 49778 steps/s (collection: 1.879s, learning 0.096s)
             Mean action noise std: 2.30
          Mean value_function loss: 369.8799
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 48.9509
                       Mean reward: 157.02
               Mean episode length: 79.70
    Episode_Reward/reaching_object: 0.3588
     Episode_Reward/lifting_object: 31.8773
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 49.2917
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 1.97s
                      Time elapsed: 00:28:31
                               ETA: 00:44:28

################################################################################
                     [1m Learning iteration 782/2000 [0m                      

                       Computation: 48925 steps/s (collection: 1.896s, learning 0.113s)
             Mean action noise std: 2.30
          Mean value_function loss: 362.0530
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 48.9515
                       Mean reward: 152.84
               Mean episode length: 81.29
    Episode_Reward/reaching_object: 0.3603
     Episode_Reward/lifting_object: 31.6904
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 45.6667
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 2.01s
                      Time elapsed: 00:28:33
                               ETA: 00:44:25

################################################################################
                     [1m Learning iteration 783/2000 [0m                      

                       Computation: 48776 steps/s (collection: 1.923s, learning 0.093s)
             Mean action noise std: 2.30
          Mean value_function loss: 381.1284
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 48.9506
                       Mean reward: 176.16
               Mean episode length: 91.38
    Episode_Reward/reaching_object: 0.3669
     Episode_Reward/lifting_object: 33.1291
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 47.7917
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 2.02s
                      Time elapsed: 00:28:35
                               ETA: 00:44:23

################################################################################
                     [1m Learning iteration 784/2000 [0m                      

                       Computation: 49313 steps/s (collection: 1.871s, learning 0.122s)
             Mean action noise std: 2.30
          Mean value_function loss: 375.5655
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 48.9492
                       Mean reward: 176.63
               Mean episode length: 87.59
    Episode_Reward/reaching_object: 0.3716
     Episode_Reward/lifting_object: 33.6254
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 50.7917
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 1.99s
                      Time elapsed: 00:28:37
                               ETA: 00:44:20

################################################################################
                     [1m Learning iteration 785/2000 [0m                      

                       Computation: 46990 steps/s (collection: 1.932s, learning 0.160s)
             Mean action noise std: 2.30
          Mean value_function loss: 390.6677
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 48.9486
                       Mean reward: 162.10
               Mean episode length: 81.31
    Episode_Reward/reaching_object: 0.3624
     Episode_Reward/lifting_object: 33.5081
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 44.2917
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 2.09s
                      Time elapsed: 00:28:39
                               ETA: 00:44:18

################################################################################
                     [1m Learning iteration 786/2000 [0m                      

                       Computation: 47435 steps/s (collection: 1.952s, learning 0.121s)
             Mean action noise std: 2.30
          Mean value_function loss: 394.6988
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 48.9480
                       Mean reward: 154.13
               Mean episode length: 82.26
    Episode_Reward/reaching_object: 0.3582
     Episode_Reward/lifting_object: 33.6590
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 47.2083
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 2.07s
                      Time elapsed: 00:28:41
                               ETA: 00:44:15

################################################################################
                     [1m Learning iteration 787/2000 [0m                      

                       Computation: 48709 steps/s (collection: 1.886s, learning 0.133s)
             Mean action noise std: 2.30
          Mean value_function loss: 413.7716
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 48.9476
                       Mean reward: 173.80
               Mean episode length: 87.32
    Episode_Reward/reaching_object: 0.3646
     Episode_Reward/lifting_object: 34.1321
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 46.7917
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 2.02s
                      Time elapsed: 00:28:43
                               ETA: 00:44:13

################################################################################
                     [1m Learning iteration 788/2000 [0m                      

                       Computation: 48721 steps/s (collection: 1.907s, learning 0.111s)
             Mean action noise std: 2.31
          Mean value_function loss: 429.9088
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 48.9476
                       Mean reward: 188.37
               Mean episode length: 87.41
    Episode_Reward/reaching_object: 0.3782
     Episode_Reward/lifting_object: 36.4009
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 45.2500
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 2.02s
                      Time elapsed: 00:28:45
                               ETA: 00:44:11

################################################################################
                     [1m Learning iteration 789/2000 [0m                      

                       Computation: 50736 steps/s (collection: 1.832s, learning 0.106s)
             Mean action noise std: 2.31
          Mean value_function loss: 431.5169
               Mean surrogate loss: 0.0137
                 Mean entropy loss: 48.9503
                       Mean reward: 178.04
               Mean episode length: 84.05
    Episode_Reward/reaching_object: 0.3811
     Episode_Reward/lifting_object: 36.9414
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 45.6250
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 1.94s
                      Time elapsed: 00:28:47
                               ETA: 00:44:08

################################################################################
                     [1m Learning iteration 790/2000 [0m                      

                       Computation: 49014 steps/s (collection: 1.894s, learning 0.112s)
             Mean action noise std: 2.31
          Mean value_function loss: 420.9043
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 48.9507
                       Mean reward: 181.91
               Mean episode length: 83.48
    Episode_Reward/reaching_object: 0.3826
     Episode_Reward/lifting_object: 37.5353
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 45.0833
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 2.01s
                      Time elapsed: 00:28:49
                               ETA: 00:44:05

################################################################################
                     [1m Learning iteration 791/2000 [0m                      

                       Computation: 50058 steps/s (collection: 1.861s, learning 0.103s)
             Mean action noise std: 2.31
          Mean value_function loss: 437.6326
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 48.9506
                       Mean reward: 183.68
               Mean episode length: 86.87
    Episode_Reward/reaching_object: 0.3817
     Episode_Reward/lifting_object: 37.5479
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 46.0417
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 1.96s
                      Time elapsed: 00:28:51
                               ETA: 00:44:03

################################################################################
                     [1m Learning iteration 792/2000 [0m                      

                       Computation: 49502 steps/s (collection: 1.867s, learning 0.119s)
             Mean action noise std: 2.31
          Mean value_function loss: 438.2685
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 48.9512
                       Mean reward: 210.06
               Mean episode length: 91.28
    Episode_Reward/reaching_object: 0.3943
     Episode_Reward/lifting_object: 38.9969
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 44.8750
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 1.99s
                      Time elapsed: 00:28:53
                               ETA: 00:44:00

################################################################################
                     [1m Learning iteration 793/2000 [0m                      

                       Computation: 50208 steps/s (collection: 1.849s, learning 0.109s)
             Mean action noise std: 2.31
          Mean value_function loss: 443.0079
               Mean surrogate loss: 0.0124
                 Mean entropy loss: 48.9510
                       Mean reward: 225.01
               Mean episode length: 96.89
    Episode_Reward/reaching_object: 0.3910
     Episode_Reward/lifting_object: 39.4457
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 46.1667
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 1.96s
                      Time elapsed: 00:28:55
                               ETA: 00:43:58

################################################################################
                     [1m Learning iteration 794/2000 [0m                      

                       Computation: 49547 steps/s (collection: 1.881s, learning 0.103s)
             Mean action noise std: 2.31
          Mean value_function loss: 456.8622
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 48.9515
                       Mean reward: 190.95
               Mean episode length: 89.07
    Episode_Reward/reaching_object: 0.3893
     Episode_Reward/lifting_object: 39.3382
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 47.6250
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 1.98s
                      Time elapsed: 00:28:57
                               ETA: 00:43:55

################################################################################
                     [1m Learning iteration 795/2000 [0m                      

                       Computation: 49975 steps/s (collection: 1.861s, learning 0.106s)
             Mean action noise std: 2.31
          Mean value_function loss: 459.7842
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 48.9518
                       Mean reward: 206.61
               Mean episode length: 89.63
    Episode_Reward/reaching_object: 0.3920
     Episode_Reward/lifting_object: 40.2055
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 45.6250
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 1.97s
                      Time elapsed: 00:28:59
                               ETA: 00:43:53

################################################################################
                     [1m Learning iteration 796/2000 [0m                      

                       Computation: 49295 steps/s (collection: 1.878s, learning 0.116s)
             Mean action noise std: 2.31
          Mean value_function loss: 458.4163
               Mean surrogate loss: 0.0082
                 Mean entropy loss: 48.9525
                       Mean reward: 211.41
               Mean episode length: 90.53
    Episode_Reward/reaching_object: 0.3861
     Episode_Reward/lifting_object: 40.5388
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 45.0833
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 1.99s
                      Time elapsed: 00:29:01
                               ETA: 00:43:50

################################################################################
                     [1m Learning iteration 797/2000 [0m                      

                       Computation: 49299 steps/s (collection: 1.878s, learning 0.116s)
             Mean action noise std: 2.31
          Mean value_function loss: 448.3048
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 48.9531
                       Mean reward: 207.34
               Mean episode length: 89.05
    Episode_Reward/reaching_object: 0.3798
     Episode_Reward/lifting_object: 39.5000
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 45.4167
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 1.99s
                      Time elapsed: 00:29:03
                               ETA: 00:43:48

################################################################################
                     [1m Learning iteration 798/2000 [0m                      

                       Computation: 49174 steps/s (collection: 1.888s, learning 0.111s)
             Mean action noise std: 2.31
          Mean value_function loss: 453.5236
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 48.9535
                       Mean reward: 188.50
               Mean episode length: 86.78
    Episode_Reward/reaching_object: 0.3887
     Episode_Reward/lifting_object: 39.8706
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 46.8333
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 2.00s
                      Time elapsed: 00:29:05
                               ETA: 00:43:46

################################################################################
                     [1m Learning iteration 799/2000 [0m                      

                       Computation: 49649 steps/s (collection: 1.883s, learning 0.097s)
             Mean action noise std: 2.31
          Mean value_function loss: 442.9827
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 48.9536
                       Mean reward: 212.59
               Mean episode length: 92.77
    Episode_Reward/reaching_object: 0.3743
     Episode_Reward/lifting_object: 38.5817
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 45.6667
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 1.98s
                      Time elapsed: 00:29:07
                               ETA: 00:43:43

################################################################################
                     [1m Learning iteration 800/2000 [0m                      

                       Computation: 49264 steps/s (collection: 1.873s, learning 0.123s)
             Mean action noise std: 2.31
          Mean value_function loss: 465.6001
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 48.9538
                       Mean reward: 205.85
               Mean episode length: 89.54
    Episode_Reward/reaching_object: 0.3858
     Episode_Reward/lifting_object: 40.2591
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 46.6250
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 2.00s
                      Time elapsed: 00:29:09
                               ETA: 00:43:41

################################################################################
                     [1m Learning iteration 801/2000 [0m                      

                       Computation: 50196 steps/s (collection: 1.865s, learning 0.093s)
             Mean action noise std: 2.31
          Mean value_function loss: 463.5520
               Mean surrogate loss: 0.0100
                 Mean entropy loss: 48.9546
                       Mean reward: 161.60
               Mean episode length: 79.20
    Episode_Reward/reaching_object: 0.3627
     Episode_Reward/lifting_object: 36.4358
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 45.9167
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 1.96s
                      Time elapsed: 00:29:11
                               ETA: 00:43:38

################################################################################
                     [1m Learning iteration 802/2000 [0m                      

                       Computation: 49736 steps/s (collection: 1.866s, learning 0.111s)
             Mean action noise std: 2.31
          Mean value_function loss: 435.5688
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 48.9552
                       Mean reward: 192.59
               Mean episode length: 90.45
    Episode_Reward/reaching_object: 0.3814
     Episode_Reward/lifting_object: 38.4692
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 44.5000
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 1.98s
                      Time elapsed: 00:29:13
                               ETA: 00:43:36

################################################################################
                     [1m Learning iteration 803/2000 [0m                      

                       Computation: 49157 steps/s (collection: 1.882s, learning 0.118s)
             Mean action noise std: 2.31
          Mean value_function loss: 430.7096
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 48.9561
                       Mean reward: 202.59
               Mean episode length: 91.13
    Episode_Reward/reaching_object: 0.3873
     Episode_Reward/lifting_object: 38.9067
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 40.7917
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 2.00s
                      Time elapsed: 00:29:15
                               ETA: 00:43:33

################################################################################
                     [1m Learning iteration 804/2000 [0m                      

                       Computation: 49400 steps/s (collection: 1.863s, learning 0.127s)
             Mean action noise std: 2.31
          Mean value_function loss: 456.8368
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 48.9562
                       Mean reward: 207.15
               Mean episode length: 88.58
    Episode_Reward/reaching_object: 0.3895
     Episode_Reward/lifting_object: 40.0775
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 43.7917
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 1.99s
                      Time elapsed: 00:29:17
                               ETA: 00:43:31

################################################################################
                     [1m Learning iteration 805/2000 [0m                      

                       Computation: 49941 steps/s (collection: 1.850s, learning 0.118s)
             Mean action noise std: 2.31
          Mean value_function loss: 462.6610
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 48.9570
                       Mean reward: 245.03
               Mean episode length: 99.98
    Episode_Reward/reaching_object: 0.4138
     Episode_Reward/lifting_object: 43.2252
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 42.0417
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 1.97s
                      Time elapsed: 00:29:19
                               ETA: 00:43:28

################################################################################
                     [1m Learning iteration 806/2000 [0m                      

                       Computation: 49223 steps/s (collection: 1.883s, learning 0.115s)
             Mean action noise std: 2.31
          Mean value_function loss: 449.1574
               Mean surrogate loss: 0.0156
                 Mean entropy loss: 48.9577
                       Mean reward: 242.40
               Mean episode length: 98.13
    Episode_Reward/reaching_object: 0.4003
     Episode_Reward/lifting_object: 41.6059
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 42.5000
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 2.00s
                      Time elapsed: 00:29:21
                               ETA: 00:43:26

################################################################################
                     [1m Learning iteration 807/2000 [0m                      

                       Computation: 50385 steps/s (collection: 1.837s, learning 0.114s)
             Mean action noise std: 2.31
          Mean value_function loss: 461.0392
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 48.9579
                       Mean reward: 193.32
               Mean episode length: 88.31
    Episode_Reward/reaching_object: 0.4127
     Episode_Reward/lifting_object: 43.5072
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 43.0417
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 1.95s
                      Time elapsed: 00:29:23
                               ETA: 00:43:23

################################################################################
                     [1m Learning iteration 808/2000 [0m                      

                       Computation: 49103 steps/s (collection: 1.892s, learning 0.110s)
             Mean action noise std: 2.31
          Mean value_function loss: 450.8833
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 48.9580
                       Mean reward: 214.17
               Mean episode length: 91.11
    Episode_Reward/reaching_object: 0.4116
     Episode_Reward/lifting_object: 43.2653
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 41.5000
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 2.00s
                      Time elapsed: 00:29:25
                               ETA: 00:43:21

################################################################################
                     [1m Learning iteration 809/2000 [0m                      

                       Computation: 49286 steps/s (collection: 1.879s, learning 0.115s)
             Mean action noise std: 2.31
          Mean value_function loss: 477.1288
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 48.9580
                       Mean reward: 202.77
               Mean episode length: 94.57
    Episode_Reward/reaching_object: 0.4105
     Episode_Reward/lifting_object: 42.5698
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 41.5417
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 1.99s
                      Time elapsed: 00:29:27
                               ETA: 00:43:18

################################################################################
                     [1m Learning iteration 810/2000 [0m                      

                       Computation: 49594 steps/s (collection: 1.872s, learning 0.110s)
             Mean action noise std: 2.31
          Mean value_function loss: 457.7804
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 48.9583
                       Mean reward: 237.96
               Mean episode length: 100.12
    Episode_Reward/reaching_object: 0.4321
     Episode_Reward/lifting_object: 44.9400
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 41.1250
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 1.98s
                      Time elapsed: 00:29:29
                               ETA: 00:43:16

################################################################################
                     [1m Learning iteration 811/2000 [0m                      

                       Computation: 48857 steps/s (collection: 1.898s, learning 0.114s)
             Mean action noise std: 2.31
          Mean value_function loss: 481.2284
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 48.9591
                       Mean reward: 231.64
               Mean episode length: 97.30
    Episode_Reward/reaching_object: 0.4401
     Episode_Reward/lifting_object: 45.8695
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 42.7500
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 2.01s
                      Time elapsed: 00:29:31
                               ETA: 00:43:13

################################################################################
                     [1m Learning iteration 812/2000 [0m                      

                       Computation: 49462 steps/s (collection: 1.873s, learning 0.115s)
             Mean action noise std: 2.31
          Mean value_function loss: 473.9450
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 48.9597
                       Mean reward: 224.89
               Mean episode length: 94.11
    Episode_Reward/reaching_object: 0.4431
     Episode_Reward/lifting_object: 46.7457
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 40.7083
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 1.99s
                      Time elapsed: 00:29:33
                               ETA: 00:43:11

################################################################################
                     [1m Learning iteration 813/2000 [0m                      

                       Computation: 49364 steps/s (collection: 1.878s, learning 0.113s)
             Mean action noise std: 2.31
          Mean value_function loss: 487.4025
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 48.9609
                       Mean reward: 205.51
               Mean episode length: 88.24
    Episode_Reward/reaching_object: 0.4327
     Episode_Reward/lifting_object: 46.6847
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 39.3333
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 1.99s
                      Time elapsed: 00:29:35
                               ETA: 00:43:08

################################################################################
                     [1m Learning iteration 814/2000 [0m                      

                       Computation: 49772 steps/s (collection: 1.863s, learning 0.112s)
             Mean action noise std: 2.31
          Mean value_function loss: 518.7214
               Mean surrogate loss: 0.0095
                 Mean entropy loss: 48.9612
                       Mean reward: 250.44
               Mean episode length: 100.88
    Episode_Reward/reaching_object: 0.4378
     Episode_Reward/lifting_object: 47.6193
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 42.5417
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 1.98s
                      Time elapsed: 00:29:37
                               ETA: 00:43:06

################################################################################
                     [1m Learning iteration 815/2000 [0m                      

                       Computation: 48414 steps/s (collection: 1.912s, learning 0.118s)
             Mean action noise std: 2.31
          Mean value_function loss: 509.1217
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 48.9614
                       Mean reward: 265.64
               Mean episode length: 104.27
    Episode_Reward/reaching_object: 0.4407
     Episode_Reward/lifting_object: 48.6056
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 41.9583
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 2.03s
                      Time elapsed: 00:29:39
                               ETA: 00:43:04

################################################################################
                     [1m Learning iteration 816/2000 [0m                      

                       Computation: 49446 steps/s (collection: 1.874s, learning 0.115s)
             Mean action noise std: 2.31
          Mean value_function loss: 519.1664
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 48.9612
                       Mean reward: 243.74
               Mean episode length: 97.35
    Episode_Reward/reaching_object: 0.4412
     Episode_Reward/lifting_object: 49.0037
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 40.7083
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 1.99s
                      Time elapsed: 00:29:41
                               ETA: 00:43:01

################################################################################
                     [1m Learning iteration 817/2000 [0m                      

                       Computation: 49287 steps/s (collection: 1.879s, learning 0.115s)
             Mean action noise std: 2.31
          Mean value_function loss: 498.6980
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 48.9598
                       Mean reward: 241.54
               Mean episode length: 95.82
    Episode_Reward/reaching_object: 0.4318
     Episode_Reward/lifting_object: 46.8798
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 38.0417
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 1.99s
                      Time elapsed: 00:29:43
                               ETA: 00:42:59

################################################################################
                     [1m Learning iteration 818/2000 [0m                      

                       Computation: 48963 steps/s (collection: 1.899s, learning 0.109s)
             Mean action noise std: 2.31
          Mean value_function loss: 502.1428
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 48.9582
                       Mean reward: 239.67
               Mean episode length: 94.35
    Episode_Reward/reaching_object: 0.4313
     Episode_Reward/lifting_object: 47.1969
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 38.5833
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 2.01s
                      Time elapsed: 00:29:45
                               ETA: 00:42:56

################################################################################
                     [1m Learning iteration 819/2000 [0m                      

                       Computation: 49696 steps/s (collection: 1.865s, learning 0.113s)
             Mean action noise std: 2.31
          Mean value_function loss: 509.8938
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 48.9567
                       Mean reward: 266.80
               Mean episode length: 105.81
    Episode_Reward/reaching_object: 0.4494
     Episode_Reward/lifting_object: 49.2333
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 39.3750
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 1.98s
                      Time elapsed: 00:29:47
                               ETA: 00:42:54

################################################################################
                     [1m Learning iteration 820/2000 [0m                      

                       Computation: 50090 steps/s (collection: 1.849s, learning 0.114s)
             Mean action noise std: 2.31
          Mean value_function loss: 528.2882
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 48.9578
                       Mean reward: 275.47
               Mean episode length: 107.64
    Episode_Reward/reaching_object: 0.4599
     Episode_Reward/lifting_object: 50.1396
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 38.4167
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 1.96s
                      Time elapsed: 00:29:49
                               ETA: 00:42:51

################################################################################
                     [1m Learning iteration 821/2000 [0m                      

                       Computation: 50124 steps/s (collection: 1.851s, learning 0.111s)
             Mean action noise std: 2.31
          Mean value_function loss: 518.2126
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 48.9613
                       Mean reward: 250.57
               Mean episode length: 104.38
    Episode_Reward/reaching_object: 0.4600
     Episode_Reward/lifting_object: 50.3558
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 37.8333
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 1.96s
                      Time elapsed: 00:29:51
                               ETA: 00:42:49

################################################################################
                     [1m Learning iteration 822/2000 [0m                      

                       Computation: 49646 steps/s (collection: 1.858s, learning 0.123s)
             Mean action noise std: 2.31
          Mean value_function loss: 460.4307
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 48.9622
                       Mean reward: 273.13
               Mean episode length: 110.80
    Episode_Reward/reaching_object: 0.4774
     Episode_Reward/lifting_object: 52.6620
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 34.8750
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 1.98s
                      Time elapsed: 00:29:53
                               ETA: 00:42:46

################################################################################
                     [1m Learning iteration 823/2000 [0m                      

                       Computation: 49744 steps/s (collection: 1.866s, learning 0.111s)
             Mean action noise std: 2.31
          Mean value_function loss: 460.1216
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 48.9624
                       Mean reward: 293.81
               Mean episode length: 117.82
    Episode_Reward/reaching_object: 0.4798
     Episode_Reward/lifting_object: 51.6857
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 34.3750
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 1.98s
                      Time elapsed: 00:29:55
                               ETA: 00:42:44

################################################################################
                     [1m Learning iteration 824/2000 [0m                      

                       Computation: 49076 steps/s (collection: 1.891s, learning 0.113s)
             Mean action noise std: 2.31
          Mean value_function loss: 432.8317
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 48.9637
                       Mean reward: 268.17
               Mean episode length: 110.19
    Episode_Reward/reaching_object: 0.4813
     Episode_Reward/lifting_object: 52.1056
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 32.0833
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 2.00s
                      Time elapsed: 00:29:57
                               ETA: 00:42:41

################################################################################
                     [1m Learning iteration 825/2000 [0m                      

                       Computation: 49848 steps/s (collection: 1.854s, learning 0.118s)
             Mean action noise std: 2.31
          Mean value_function loss: 419.6685
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 48.9649
                       Mean reward: 289.20
               Mean episode length: 112.92
    Episode_Reward/reaching_object: 0.4924
     Episode_Reward/lifting_object: 53.1692
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.6250
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 1.97s
                      Time elapsed: 00:29:59
                               ETA: 00:42:39

################################################################################
                     [1m Learning iteration 826/2000 [0m                      

                       Computation: 49890 steps/s (collection: 1.857s, learning 0.113s)
             Mean action noise std: 2.31
          Mean value_function loss: 443.6634
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 48.9639
                       Mean reward: 273.11
               Mean episode length: 120.99
    Episode_Reward/reaching_object: 0.5117
     Episode_Reward/lifting_object: 55.1388
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 34.0417
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 1.97s
                      Time elapsed: 00:30:01
                               ETA: 00:42:36

################################################################################
                     [1m Learning iteration 827/2000 [0m                      

                       Computation: 49875 steps/s (collection: 1.861s, learning 0.110s)
             Mean action noise std: 2.31
          Mean value_function loss: 482.3167
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 48.9606
                       Mean reward: 307.04
               Mean episode length: 118.89
    Episode_Reward/reaching_object: 0.5252
     Episode_Reward/lifting_object: 57.2560
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 33.0417
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 1.97s
                      Time elapsed: 00:30:03
                               ETA: 00:42:34

################################################################################
                     [1m Learning iteration 828/2000 [0m                      

                       Computation: 48047 steps/s (collection: 1.920s, learning 0.126s)
             Mean action noise std: 2.31
          Mean value_function loss: 486.6538
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 48.9594
                       Mean reward: 315.66
               Mean episode length: 119.67
    Episode_Reward/reaching_object: 0.5341
     Episode_Reward/lifting_object: 58.0142
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 31.6250
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 2.05s
                      Time elapsed: 00:30:05
                               ETA: 00:42:32

################################################################################
                     [1m Learning iteration 829/2000 [0m                      

                       Computation: 49162 steps/s (collection: 1.889s, learning 0.111s)
             Mean action noise std: 2.31
          Mean value_function loss: 513.1769
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 48.9619
                       Mean reward: 284.58
               Mean episode length: 117.14
    Episode_Reward/reaching_object: 0.5306
     Episode_Reward/lifting_object: 57.6213
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 32.0833
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 2.00s
                      Time elapsed: 00:30:07
                               ETA: 00:42:29

################################################################################
                     [1m Learning iteration 830/2000 [0m                      

                       Computation: 48943 steps/s (collection: 1.884s, learning 0.125s)
             Mean action noise std: 2.31
          Mean value_function loss: 445.7684
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 48.9629
                       Mean reward: 275.66
               Mean episode length: 112.72
    Episode_Reward/reaching_object: 0.5338
     Episode_Reward/lifting_object: 58.4526
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 32.2500
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 2.01s
                      Time elapsed: 00:30:09
                               ETA: 00:42:27

################################################################################
                     [1m Learning iteration 831/2000 [0m                      

                       Computation: 49342 steps/s (collection: 1.868s, learning 0.124s)
             Mean action noise std: 2.31
          Mean value_function loss: 471.9473
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 48.9601
                       Mean reward: 313.20
               Mean episode length: 116.40
    Episode_Reward/reaching_object: 0.5540
     Episode_Reward/lifting_object: 62.9427
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 34.1667
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 1.99s
                      Time elapsed: 00:30:11
                               ETA: 00:42:24

################################################################################
                     [1m Learning iteration 832/2000 [0m                      

                       Computation: 50230 steps/s (collection: 1.837s, learning 0.120s)
             Mean action noise std: 2.31
          Mean value_function loss: 497.5732
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 48.9594
                       Mean reward: 346.00
               Mean episode length: 133.62
    Episode_Reward/reaching_object: 0.5511
     Episode_Reward/lifting_object: 62.6323
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.3333
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 1.96s
                      Time elapsed: 00:30:13
                               ETA: 00:42:22

################################################################################
                     [1m Learning iteration 833/2000 [0m                      

                       Computation: 49789 steps/s (collection: 1.856s, learning 0.119s)
             Mean action noise std: 2.31
          Mean value_function loss: 485.0541
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 48.9594
                       Mean reward: 294.28
               Mean episode length: 116.65
    Episode_Reward/reaching_object: 0.5392
     Episode_Reward/lifting_object: 61.6613
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 31.9583
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 1.97s
                      Time elapsed: 00:30:15
                               ETA: 00:42:19

################################################################################
                     [1m Learning iteration 834/2000 [0m                      

                       Computation: 49236 steps/s (collection: 1.877s, learning 0.120s)
             Mean action noise std: 2.31
          Mean value_function loss: 489.0379
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 48.9596
                       Mean reward: 297.44
               Mean episode length: 111.27
    Episode_Reward/reaching_object: 0.5405
     Episode_Reward/lifting_object: 61.3628
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 30.6667
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 2.00s
                      Time elapsed: 00:30:17
                               ETA: 00:42:17

################################################################################
                     [1m Learning iteration 835/2000 [0m                      

                       Computation: 49977 steps/s (collection: 1.864s, learning 0.103s)
             Mean action noise std: 2.31
          Mean value_function loss: 464.7920
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 48.9593
                       Mean reward: 349.55
               Mean episode length: 129.60
    Episode_Reward/reaching_object: 0.5610
     Episode_Reward/lifting_object: 65.1835
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 32.4167
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 1.97s
                      Time elapsed: 00:30:19
                               ETA: 00:42:14

################################################################################
                     [1m Learning iteration 836/2000 [0m                      

                       Computation: 48932 steps/s (collection: 1.902s, learning 0.107s)
             Mean action noise std: 2.31
          Mean value_function loss: 494.8802
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 48.9591
                       Mean reward: 344.10
               Mean episode length: 125.19
    Episode_Reward/reaching_object: 0.5495
     Episode_Reward/lifting_object: 63.4778
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 31.0833
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 2.01s
                      Time elapsed: 00:30:21
                               ETA: 00:42:12

################################################################################
                     [1m Learning iteration 837/2000 [0m                      

                       Computation: 50626 steps/s (collection: 1.838s, learning 0.104s)
             Mean action noise std: 2.31
          Mean value_function loss: 443.5479
               Mean surrogate loss: 0.0082
                 Mean entropy loss: 48.9593
                       Mean reward: 302.20
               Mean episode length: 116.51
    Episode_Reward/reaching_object: 0.5419
     Episode_Reward/lifting_object: 61.4937
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.1250
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 1.94s
                      Time elapsed: 00:30:23
                               ETA: 00:42:10

################################################################################
                     [1m Learning iteration 838/2000 [0m                      

                       Computation: 49925 steps/s (collection: 1.861s, learning 0.108s)
             Mean action noise std: 2.31
          Mean value_function loss: 459.6429
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 48.9593
                       Mean reward: 371.88
               Mean episode length: 134.47
    Episode_Reward/reaching_object: 0.5639
     Episode_Reward/lifting_object: 66.1346
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 27.7500
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 1.97s
                      Time elapsed: 00:30:24
                               ETA: 00:42:07

################################################################################
                     [1m Learning iteration 839/2000 [0m                      

                       Computation: 50166 steps/s (collection: 1.846s, learning 0.114s)
             Mean action noise std: 2.31
          Mean value_function loss: 441.4098
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 48.9590
                       Mean reward: 297.04
               Mean episode length: 115.23
    Episode_Reward/reaching_object: 0.5841
     Episode_Reward/lifting_object: 68.1386
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 26.9167
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 1.96s
                      Time elapsed: 00:30:26
                               ETA: 00:42:05

################################################################################
                     [1m Learning iteration 840/2000 [0m                      

                       Computation: 50011 steps/s (collection: 1.860s, learning 0.106s)
             Mean action noise std: 2.31
          Mean value_function loss: 443.0831
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 48.9576
                       Mean reward: 356.14
               Mean episode length: 130.71
    Episode_Reward/reaching_object: 0.6001
     Episode_Reward/lifting_object: 69.5931
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.7917
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 1.97s
                      Time elapsed: 00:30:28
                               ETA: 00:42:02

################################################################################
                     [1m Learning iteration 841/2000 [0m                      

                       Computation: 49546 steps/s (collection: 1.872s, learning 0.113s)
             Mean action noise std: 2.31
          Mean value_function loss: 450.6887
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 48.9572
                       Mean reward: 319.80
               Mean episode length: 120.59
    Episode_Reward/reaching_object: 0.5878
     Episode_Reward/lifting_object: 67.4841
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.4583
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 1.98s
                      Time elapsed: 00:30:30
                               ETA: 00:42:00

################################################################################
                     [1m Learning iteration 842/2000 [0m                      

                       Computation: 49663 steps/s (collection: 1.867s, learning 0.112s)
             Mean action noise std: 2.31
          Mean value_function loss: 420.8412
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.9567
                       Mean reward: 373.78
               Mean episode length: 137.41
    Episode_Reward/reaching_object: 0.5964
     Episode_Reward/lifting_object: 68.6180
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 27.6667
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 1.98s
                      Time elapsed: 00:30:32
                               ETA: 00:41:57

################################################################################
                     [1m Learning iteration 843/2000 [0m                      

                       Computation: 49976 steps/s (collection: 1.859s, learning 0.108s)
             Mean action noise std: 2.31
          Mean value_function loss: 438.7030
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 48.9551
                       Mean reward: 346.22
               Mean episode length: 134.31
    Episode_Reward/reaching_object: 0.5906
     Episode_Reward/lifting_object: 67.8985
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 3.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 26.6667
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 1.97s
                      Time elapsed: 00:30:34
                               ETA: 00:41:55

################################################################################
                     [1m Learning iteration 844/2000 [0m                      

                       Computation: 49263 steps/s (collection: 1.884s, learning 0.111s)
             Mean action noise std: 2.31
          Mean value_function loss: 426.2552
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 48.9549
                       Mean reward: 369.61
               Mean episode length: 140.09
    Episode_Reward/reaching_object: 0.6048
     Episode_Reward/lifting_object: 69.5966
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 4.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 27.2500
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 2.00s
                      Time elapsed: 00:30:36
                               ETA: 00:41:52

################################################################################
                     [1m Learning iteration 845/2000 [0m                      

                       Computation: 50017 steps/s (collection: 1.849s, learning 0.116s)
             Mean action noise std: 2.31
          Mean value_function loss: 420.5613
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 48.9550
                       Mean reward: 347.05
               Mean episode length: 128.53
    Episode_Reward/reaching_object: 0.6303
     Episode_Reward/lifting_object: 73.5763
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 24.9583
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 1.97s
                      Time elapsed: 00:30:38
                               ETA: 00:41:50

################################################################################
                     [1m Learning iteration 846/2000 [0m                      

                       Computation: 50284 steps/s (collection: 1.839s, learning 0.116s)
             Mean action noise std: 2.31
          Mean value_function loss: 438.0063
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 48.9557
                       Mean reward: 332.48
               Mean episode length: 125.29
    Episode_Reward/reaching_object: 0.5856
     Episode_Reward/lifting_object: 66.8777
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 27.5417
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 1.95s
                      Time elapsed: 00:30:40
                               ETA: 00:41:47

################################################################################
                     [1m Learning iteration 847/2000 [0m                      

                       Computation: 49867 steps/s (collection: 1.863s, learning 0.108s)
             Mean action noise std: 2.31
          Mean value_function loss: 429.2695
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 48.9574
                       Mean reward: 366.42
               Mean episode length: 133.80
    Episode_Reward/reaching_object: 0.6188
     Episode_Reward/lifting_object: 72.1060
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 25.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 1.97s
                      Time elapsed: 00:30:42
                               ETA: 00:41:45

################################################################################
                     [1m Learning iteration 848/2000 [0m                      

                       Computation: 49890 steps/s (collection: 1.875s, learning 0.095s)
             Mean action noise std: 2.31
          Mean value_function loss: 442.3180
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 48.9576
                       Mean reward: 333.21
               Mean episode length: 124.75
    Episode_Reward/reaching_object: 0.6133
     Episode_Reward/lifting_object: 70.9163
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 26.8333
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 1.97s
                      Time elapsed: 00:30:44
                               ETA: 00:41:43

################################################################################
                     [1m Learning iteration 849/2000 [0m                      

                       Computation: 50020 steps/s (collection: 1.857s, learning 0.108s)
             Mean action noise std: 2.31
          Mean value_function loss: 428.0096
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 48.9566
                       Mean reward: 379.28
               Mean episode length: 140.84
    Episode_Reward/reaching_object: 0.6423
     Episode_Reward/lifting_object: 75.0621
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 24.4583
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 1.97s
                      Time elapsed: 00:30:46
                               ETA: 00:41:40

################################################################################
                     [1m Learning iteration 850/2000 [0m                      

                       Computation: 49932 steps/s (collection: 1.841s, learning 0.128s)
             Mean action noise std: 2.31
          Mean value_function loss: 424.2707
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 48.9543
                       Mean reward: 387.83
               Mean episode length: 139.73
    Episode_Reward/reaching_object: 0.6270
     Episode_Reward/lifting_object: 73.6705
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 4.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 24.4167
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 1.97s
                      Time elapsed: 00:30:48
                               ETA: 00:41:38

################################################################################
                     [1m Learning iteration 851/2000 [0m                      

                       Computation: 50325 steps/s (collection: 1.844s, learning 0.109s)
             Mean action noise std: 2.31
          Mean value_function loss: 428.7892
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 48.9520
                       Mean reward: 350.69
               Mean episode length: 130.54
    Episode_Reward/reaching_object: 0.6661
     Episode_Reward/lifting_object: 78.9335
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 23.9167
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 1.95s
                      Time elapsed: 00:30:50
                               ETA: 00:41:35

################################################################################
                     [1m Learning iteration 852/2000 [0m                      

                       Computation: 50195 steps/s (collection: 1.839s, learning 0.120s)
             Mean action noise std: 2.31
          Mean value_function loss: 433.1817
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 48.9505
                       Mean reward: 421.19
               Mean episode length: 147.80
    Episode_Reward/reaching_object: 0.6727
     Episode_Reward/lifting_object: 79.3846
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 23.8750
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 1.96s
                      Time elapsed: 00:30:52
                               ETA: 00:41:33

################################################################################
                     [1m Learning iteration 853/2000 [0m                      

                       Computation: 50216 steps/s (collection: 1.845s, learning 0.113s)
             Mean action noise std: 2.31
          Mean value_function loss: 425.2918
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 48.9500
                       Mean reward: 391.11
               Mean episode length: 137.80
    Episode_Reward/reaching_object: 0.6682
     Episode_Reward/lifting_object: 80.6974
      Episode_Reward/object_height: 0.0274
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 23.8333
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 1.96s
                      Time elapsed: 00:30:54
                               ETA: 00:41:30

################################################################################
                     [1m Learning iteration 854/2000 [0m                      

                       Computation: 50694 steps/s (collection: 1.834s, learning 0.105s)
             Mean action noise std: 2.31
          Mean value_function loss: 430.1650
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 48.9505
                       Mean reward: 440.13
               Mean episode length: 151.65
    Episode_Reward/reaching_object: 0.6621
     Episode_Reward/lifting_object: 78.6399
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 23.5000
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 1.94s
                      Time elapsed: 00:30:56
                               ETA: 00:41:28

################################################################################
                     [1m Learning iteration 855/2000 [0m                      

                       Computation: 50397 steps/s (collection: 1.848s, learning 0.103s)
             Mean action noise std: 2.31
          Mean value_function loss: 437.3128
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 48.9520
                       Mean reward: 344.88
               Mean episode length: 129.61
    Episode_Reward/reaching_object: 0.6524
     Episode_Reward/lifting_object: 79.1165
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 23.9167
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 1.95s
                      Time elapsed: 00:30:58
                               ETA: 00:41:25

################################################################################
                     [1m Learning iteration 856/2000 [0m                      

                       Computation: 48357 steps/s (collection: 1.915s, learning 0.118s)
             Mean action noise std: 2.31
          Mean value_function loss: 428.1636
               Mean surrogate loss: 0.0162
                 Mean entropy loss: 48.9526
                       Mean reward: 411.32
               Mean episode length: 144.83
    Episode_Reward/reaching_object: 0.6984
     Episode_Reward/lifting_object: 85.1978
      Episode_Reward/object_height: 0.0307
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 23.7500
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 2.03s
                      Time elapsed: 00:31:00
                               ETA: 00:41:23

################################################################################
                     [1m Learning iteration 857/2000 [0m                      

                       Computation: 50000 steps/s (collection: 1.854s, learning 0.112s)
             Mean action noise std: 2.31
          Mean value_function loss: 433.5454
               Mean surrogate loss: 0.0169
                 Mean entropy loss: 48.9526
                       Mean reward: 414.07
               Mean episode length: 143.13
    Episode_Reward/reaching_object: 0.6876
     Episode_Reward/lifting_object: 84.0234
      Episode_Reward/object_height: 0.0296
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 23.1250
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 1.97s
                      Time elapsed: 00:31:02
                               ETA: 00:41:21

################################################################################
                     [1m Learning iteration 858/2000 [0m                      

                       Computation: 45052 steps/s (collection: 2.012s, learning 0.170s)
             Mean action noise std: 2.31
          Mean value_function loss: 471.8557
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 48.9527
                       Mean reward: 488.75
               Mean episode length: 155.95
    Episode_Reward/reaching_object: 0.6930
     Episode_Reward/lifting_object: 85.9081
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 23.7083
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 2.18s
                      Time elapsed: 00:31:04
                               ETA: 00:41:18

################################################################################
                     [1m Learning iteration 859/2000 [0m                      

                       Computation: 43524 steps/s (collection: 2.091s, learning 0.168s)
             Mean action noise std: 2.31
          Mean value_function loss: 483.6835
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 48.9531
                       Mean reward: 413.46
               Mean episode length: 139.17
    Episode_Reward/reaching_object: 0.6674
     Episode_Reward/lifting_object: 83.0297
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 26.9167
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 2.26s
                      Time elapsed: 00:31:06
                               ETA: 00:41:16

################################################################################
                     [1m Learning iteration 860/2000 [0m                      

                       Computation: 43750 steps/s (collection: 2.123s, learning 0.124s)
             Mean action noise std: 2.31
          Mean value_function loss: 475.1018
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 48.9557
                       Mean reward: 418.88
               Mean episode length: 143.44
    Episode_Reward/reaching_object: 0.6721
     Episode_Reward/lifting_object: 84.0100
      Episode_Reward/object_height: 0.0310
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 26.3750
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 2.25s
                      Time elapsed: 00:31:09
                               ETA: 00:41:14

################################################################################
                     [1m Learning iteration 861/2000 [0m                      

                       Computation: 48692 steps/s (collection: 1.902s, learning 0.117s)
             Mean action noise std: 2.31
          Mean value_function loss: 471.2418
               Mean surrogate loss: 0.0093
                 Mean entropy loss: 48.9570
                       Mean reward: 435.61
               Mean episode length: 144.84
    Episode_Reward/reaching_object: 0.6675
     Episode_Reward/lifting_object: 84.0429
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 24.8333
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 2.02s
                      Time elapsed: 00:31:11
                               ETA: 00:41:12

################################################################################
                     [1m Learning iteration 862/2000 [0m                      

                       Computation: 47747 steps/s (collection: 1.927s, learning 0.132s)
             Mean action noise std: 2.31
          Mean value_function loss: 467.0388
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 48.9572
                       Mean reward: 455.02
               Mean episode length: 147.45
    Episode_Reward/reaching_object: 0.6869
     Episode_Reward/lifting_object: 86.2228
      Episode_Reward/object_height: 0.0330
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 23.2500
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 2.06s
                      Time elapsed: 00:31:13
                               ETA: 00:41:10

################################################################################
                     [1m Learning iteration 863/2000 [0m                      

                       Computation: 50505 steps/s (collection: 1.830s, learning 0.116s)
             Mean action noise std: 2.31
          Mean value_function loss: 454.5735
               Mean surrogate loss: 0.0125
                 Mean entropy loss: 48.9572
                       Mean reward: 414.89
               Mean episode length: 140.17
    Episode_Reward/reaching_object: 0.6898
     Episode_Reward/lifting_object: 86.5244
      Episode_Reward/object_height: 0.0330
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 22.0000
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 1.95s
                      Time elapsed: 00:31:15
                               ETA: 00:41:07

################################################################################
                     [1m Learning iteration 864/2000 [0m                      

                       Computation: 49514 steps/s (collection: 1.876s, learning 0.110s)
             Mean action noise std: 2.31
          Mean value_function loss: 428.6077
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 48.9573
                       Mean reward: 425.70
               Mean episode length: 138.18
    Episode_Reward/reaching_object: 0.6944
     Episode_Reward/lifting_object: 88.0612
      Episode_Reward/object_height: 0.0338
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 19.4167
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 1.99s
                      Time elapsed: 00:31:17
                               ETA: 00:41:05

################################################################################
                     [1m Learning iteration 865/2000 [0m                      

                       Computation: 50584 steps/s (collection: 1.830s, learning 0.114s)
             Mean action noise std: 2.31
          Mean value_function loss: 457.2618
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 48.9574
                       Mean reward: 435.12
               Mean episode length: 140.41
    Episode_Reward/reaching_object: 0.6836
     Episode_Reward/lifting_object: 85.3871
      Episode_Reward/object_height: 0.0325
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 3.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 23.8750
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 1.94s
                      Time elapsed: 00:31:19
                               ETA: 00:41:02

################################################################################
                     [1m Learning iteration 866/2000 [0m                      

                       Computation: 50702 steps/s (collection: 1.838s, learning 0.101s)
             Mean action noise std: 2.31
          Mean value_function loss: 469.8602
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 48.9576
                       Mean reward: 448.55
               Mean episode length: 151.65
    Episode_Reward/reaching_object: 0.7121
     Episode_Reward/lifting_object: 91.2273
      Episode_Reward/object_height: 0.0349
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 3.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 25.1667
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 1.94s
                      Time elapsed: 00:31:20
                               ETA: 00:41:00

################################################################################
                     [1m Learning iteration 867/2000 [0m                      

                       Computation: 50797 steps/s (collection: 1.823s, learning 0.113s)
             Mean action noise std: 2.31
          Mean value_function loss: 474.5101
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 48.9590
                       Mean reward: 431.47
               Mean episode length: 141.87
    Episode_Reward/reaching_object: 0.6966
     Episode_Reward/lifting_object: 88.9027
      Episode_Reward/object_height: 0.0351
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 5.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 28.5833
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 1.94s
                      Time elapsed: 00:31:22
                               ETA: 00:40:57

################################################################################
                     [1m Learning iteration 868/2000 [0m                      

                       Computation: 50069 steps/s (collection: 1.842s, learning 0.122s)
             Mean action noise std: 2.31
          Mean value_function loss: 480.6670
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 48.9616
                       Mean reward: 447.32
               Mean episode length: 144.37
    Episode_Reward/reaching_object: 0.6752
     Episode_Reward/lifting_object: 86.3742
      Episode_Reward/object_height: 0.0338
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 27.9167
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 1.96s
                      Time elapsed: 00:31:24
                               ETA: 00:40:55

################################################################################
                     [1m Learning iteration 869/2000 [0m                      

                       Computation: 50390 steps/s (collection: 1.839s, learning 0.112s)
             Mean action noise std: 2.31
          Mean value_function loss: 451.5731
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 48.9653
                       Mean reward: 433.07
               Mean episode length: 145.35
    Episode_Reward/reaching_object: 0.6900
     Episode_Reward/lifting_object: 88.7357
      Episode_Reward/object_height: 0.0348
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 22.6667
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 1.95s
                      Time elapsed: 00:31:26
                               ETA: 00:40:52

################################################################################
                     [1m Learning iteration 870/2000 [0m                      

                       Computation: 51539 steps/s (collection: 1.814s, learning 0.093s)
             Mean action noise std: 2.31
          Mean value_function loss: 446.8360
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 48.9693
                       Mean reward: 432.10
               Mean episode length: 144.39
    Episode_Reward/reaching_object: 0.6902
     Episode_Reward/lifting_object: 87.6518
      Episode_Reward/object_height: 0.0346
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 20.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 1.91s
                      Time elapsed: 00:31:28
                               ETA: 00:40:50

################################################################################
                     [1m Learning iteration 871/2000 [0m                      

                       Computation: 50529 steps/s (collection: 1.850s, learning 0.096s)
             Mean action noise std: 2.31
          Mean value_function loss: 440.6858
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 48.9708
                       Mean reward: 431.99
               Mean episode length: 140.55
    Episode_Reward/reaching_object: 0.7021
     Episode_Reward/lifting_object: 89.3381
      Episode_Reward/object_height: 0.0364
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 21.6667
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 1.95s
                      Time elapsed: 00:31:30
                               ETA: 00:40:47

################################################################################
                     [1m Learning iteration 872/2000 [0m                      

                       Computation: 49789 steps/s (collection: 1.853s, learning 0.122s)
             Mean action noise std: 2.31
          Mean value_function loss: 422.2052
               Mean surrogate loss: 0.0133
                 Mean entropy loss: 48.9708
                       Mean reward: 410.76
               Mean episode length: 135.28
    Episode_Reward/reaching_object: 0.6965
     Episode_Reward/lifting_object: 88.9717
      Episode_Reward/object_height: 0.0355
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 4.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 18.1667
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 1.97s
                      Time elapsed: 00:31:32
                               ETA: 00:40:45

################################################################################
                     [1m Learning iteration 873/2000 [0m                      

                       Computation: 50848 steps/s (collection: 1.828s, learning 0.105s)
             Mean action noise std: 2.31
          Mean value_function loss: 441.3625
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 48.9707
                       Mean reward: 489.42
               Mean episode length: 155.15
    Episode_Reward/reaching_object: 0.7407
     Episode_Reward/lifting_object: 95.4473
      Episode_Reward/object_height: 0.0395
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 21.4583
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 1.93s
                      Time elapsed: 00:31:34
                               ETA: 00:40:43

################################################################################
                     [1m Learning iteration 874/2000 [0m                      

                       Computation: 49686 steps/s (collection: 1.854s, learning 0.125s)
             Mean action noise std: 2.31
          Mean value_function loss: 463.0680
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 48.9724
                       Mean reward: 479.75
               Mean episode length: 156.51
    Episode_Reward/reaching_object: 0.7213
     Episode_Reward/lifting_object: 93.8071
      Episode_Reward/object_height: 0.0379
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 4.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 24.0833
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 1.98s
                      Time elapsed: 00:31:36
                               ETA: 00:40:40

################################################################################
                     [1m Learning iteration 875/2000 [0m                      

                       Computation: 50144 steps/s (collection: 1.843s, learning 0.117s)
             Mean action noise std: 2.31
          Mean value_function loss: 425.2552
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 48.9751
                       Mean reward: 499.33
               Mean episode length: 154.55
    Episode_Reward/reaching_object: 0.7111
     Episode_Reward/lifting_object: 92.4887
      Episode_Reward/object_height: 0.0378
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 3.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 20.9167
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 1.96s
                      Time elapsed: 00:31:38
                               ETA: 00:40:38

################################################################################
                     [1m Learning iteration 876/2000 [0m                      

                       Computation: 50949 steps/s (collection: 1.815s, learning 0.115s)
             Mean action noise std: 2.31
          Mean value_function loss: 442.8790
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 48.9764
                       Mean reward: 444.99
               Mean episode length: 146.46
    Episode_Reward/reaching_object: 0.7267
     Episode_Reward/lifting_object: 94.3670
      Episode_Reward/object_height: 0.0397
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 19.7083
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 1.93s
                      Time elapsed: 00:31:40
                               ETA: 00:40:35

################################################################################
                     [1m Learning iteration 877/2000 [0m                      

                       Computation: 50095 steps/s (collection: 1.843s, learning 0.120s)
             Mean action noise std: 2.31
          Mean value_function loss: 433.2146
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 48.9791
                       Mean reward: 485.14
               Mean episode length: 157.20
    Episode_Reward/reaching_object: 0.7524
     Episode_Reward/lifting_object: 98.0522
      Episode_Reward/object_height: 0.0426
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 21.8750
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 1.96s
                      Time elapsed: 00:31:42
                               ETA: 00:40:33

################################################################################
                     [1m Learning iteration 878/2000 [0m                      

                       Computation: 50532 steps/s (collection: 1.830s, learning 0.115s)
             Mean action noise std: 2.31
          Mean value_function loss: 418.6431
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 48.9805
                       Mean reward: 480.12
               Mean episode length: 155.81
    Episode_Reward/reaching_object: 0.7471
     Episode_Reward/lifting_object: 97.1951
      Episode_Reward/object_height: 0.0421
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 19.5417
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 1.95s
                      Time elapsed: 00:31:44
                               ETA: 00:40:30

################################################################################
                     [1m Learning iteration 879/2000 [0m                      

                       Computation: 50674 steps/s (collection: 1.825s, learning 0.115s)
             Mean action noise std: 2.31
          Mean value_function loss: 413.2906
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 48.9809
                       Mean reward: 478.84
               Mean episode length: 152.29
    Episode_Reward/reaching_object: 0.7671
     Episode_Reward/lifting_object: 99.6613
      Episode_Reward/object_height: 0.0433
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 19.6667
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 1.94s
                      Time elapsed: 00:31:46
                               ETA: 00:40:28

################################################################################
                     [1m Learning iteration 880/2000 [0m                      

                       Computation: 49901 steps/s (collection: 1.860s, learning 0.110s)
             Mean action noise std: 2.31
          Mean value_function loss: 394.8228
               Mean surrogate loss: 0.0103
                 Mean entropy loss: 48.9802
                       Mean reward: 514.14
               Mean episode length: 161.98
    Episode_Reward/reaching_object: 0.7562
     Episode_Reward/lifting_object: 97.9956
      Episode_Reward/object_height: 0.0424
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.8750
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 1.97s
                      Time elapsed: 00:31:48
                               ETA: 00:40:25

################################################################################
                     [1m Learning iteration 881/2000 [0m                      

                       Computation: 50290 steps/s (collection: 1.847s, learning 0.108s)
             Mean action noise std: 2.31
          Mean value_function loss: 386.2684
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 48.9802
                       Mean reward: 492.80
               Mean episode length: 160.12
    Episode_Reward/reaching_object: 0.7701
     Episode_Reward/lifting_object: 99.8220
      Episode_Reward/object_height: 0.0437
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.4583
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 1.95s
                      Time elapsed: 00:31:50
                               ETA: 00:40:23

################################################################################
                     [1m Learning iteration 882/2000 [0m                      

                       Computation: 50120 steps/s (collection: 1.863s, learning 0.099s)
             Mean action noise std: 2.31
          Mean value_function loss: 385.0712
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 48.9802
                       Mean reward: 437.85
               Mean episode length: 142.53
    Episode_Reward/reaching_object: 0.7094
     Episode_Reward/lifting_object: 90.2006
      Episode_Reward/object_height: 0.0378
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 5.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.4583
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 1.96s
                      Time elapsed: 00:31:52
                               ETA: 00:40:21

################################################################################
                     [1m Learning iteration 883/2000 [0m                      

                       Computation: 49981 steps/s (collection: 1.848s, learning 0.119s)
             Mean action noise std: 2.31
          Mean value_function loss: 400.9853
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 48.9801
                       Mean reward: 486.78
               Mean episode length: 157.75
    Episode_Reward/reaching_object: 0.7799
     Episode_Reward/lifting_object: 99.8225
      Episode_Reward/object_height: 0.0441
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 18.6667
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 1.97s
                      Time elapsed: 00:31:54
                               ETA: 00:40:18

################################################################################
                     [1m Learning iteration 884/2000 [0m                      

                       Computation: 50187 steps/s (collection: 1.844s, learning 0.115s)
             Mean action noise std: 2.31
          Mean value_function loss: 394.7011
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 48.9805
                       Mean reward: 548.93
               Mean episode length: 174.25
    Episode_Reward/reaching_object: 0.8172
     Episode_Reward/lifting_object: 105.5806
      Episode_Reward/object_height: 0.0467
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 17.1250
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 1.96s
                      Time elapsed: 00:31:56
                               ETA: 00:40:16

################################################################################
                     [1m Learning iteration 885/2000 [0m                      

                       Computation: 50683 steps/s (collection: 1.829s, learning 0.111s)
             Mean action noise std: 2.31
          Mean value_function loss: 408.9337
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 48.9812
                       Mean reward: 555.24
               Mean episode length: 168.88
    Episode_Reward/reaching_object: 0.8214
     Episode_Reward/lifting_object: 106.5124
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 16.7500
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 1.94s
                      Time elapsed: 00:31:58
                               ETA: 00:40:13

################################################################################
                     [1m Learning iteration 886/2000 [0m                      

                       Computation: 49136 steps/s (collection: 1.882s, learning 0.119s)
             Mean action noise std: 2.31
          Mean value_function loss: 426.9712
               Mean surrogate loss: 0.0112
                 Mean entropy loss: 48.9817
                       Mean reward: 500.34
               Mean episode length: 161.10
    Episode_Reward/reaching_object: 0.8175
     Episode_Reward/lifting_object: 106.0862
      Episode_Reward/object_height: 0.0469
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 20.3333
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 2.00s
                      Time elapsed: 00:32:00
                               ETA: 00:40:11

################################################################################
                     [1m Learning iteration 887/2000 [0m                      

                       Computation: 51314 steps/s (collection: 1.804s, learning 0.112s)
             Mean action noise std: 2.31
          Mean value_function loss: 424.1165
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 48.9817
                       Mean reward: 562.40
               Mean episode length: 172.87
    Episode_Reward/reaching_object: 0.8270
     Episode_Reward/lifting_object: 108.3975
      Episode_Reward/object_height: 0.0476
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 18.8750
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 1.92s
                      Time elapsed: 00:32:01
                               ETA: 00:40:08

################################################################################
                     [1m Learning iteration 888/2000 [0m                      

                       Computation: 49867 steps/s (collection: 1.862s, learning 0.109s)
             Mean action noise std: 2.31
          Mean value_function loss: 410.5658
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 48.9817
                       Mean reward: 582.25
               Mean episode length: 176.83
    Episode_Reward/reaching_object: 0.8505
     Episode_Reward/lifting_object: 112.0663
      Episode_Reward/object_height: 0.0510
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.5000
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 1.97s
                      Time elapsed: 00:32:03
                               ETA: 00:40:06

################################################################################
                     [1m Learning iteration 889/2000 [0m                      

                       Computation: 48123 steps/s (collection: 1.923s, learning 0.120s)
             Mean action noise std: 2.31
          Mean value_function loss: 408.7434
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 48.9818
                       Mean reward: 534.66
               Mean episode length: 165.30
    Episode_Reward/reaching_object: 0.8157
     Episode_Reward/lifting_object: 106.5533
      Episode_Reward/object_height: 0.0475
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.3333
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 2.04s
                      Time elapsed: 00:32:05
                               ETA: 00:40:04

################################################################################
                     [1m Learning iteration 890/2000 [0m                      

                       Computation: 48825 steps/s (collection: 1.906s, learning 0.107s)
             Mean action noise std: 2.31
          Mean value_function loss: 417.2336
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 48.9811
                       Mean reward: 472.15
               Mean episode length: 148.62
    Episode_Reward/reaching_object: 0.7945
     Episode_Reward/lifting_object: 103.0811
      Episode_Reward/object_height: 0.0459
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 17.8750
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 2.01s
                      Time elapsed: 00:32:07
                               ETA: 00:40:01

################################################################################
                     [1m Learning iteration 891/2000 [0m                      

                       Computation: 49154 steps/s (collection: 1.880s, learning 0.120s)
             Mean action noise std: 2.31
          Mean value_function loss: 414.2392
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 48.9820
                       Mean reward: 468.20
               Mean episode length: 150.45
    Episode_Reward/reaching_object: 0.7902
     Episode_Reward/lifting_object: 101.5113
      Episode_Reward/object_height: 0.0440
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.6667
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 2.00s
                      Time elapsed: 00:32:09
                               ETA: 00:39:59

################################################################################
                     [1m Learning iteration 892/2000 [0m                      

                       Computation: 49611 steps/s (collection: 1.863s, learning 0.119s)
             Mean action noise std: 2.31
          Mean value_function loss: 426.2331
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 48.9838
                       Mean reward: 510.66
               Mean episode length: 164.38
    Episode_Reward/reaching_object: 0.7794
     Episode_Reward/lifting_object: 99.8177
      Episode_Reward/object_height: 0.0429
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 19.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 1.98s
                      Time elapsed: 00:32:11
                               ETA: 00:39:57

################################################################################
                     [1m Learning iteration 893/2000 [0m                      

                       Computation: 48920 steps/s (collection: 1.890s, learning 0.119s)
             Mean action noise std: 2.31
          Mean value_function loss: 401.8913
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 48.9853
                       Mean reward: 517.82
               Mean episode length: 164.45
    Episode_Reward/reaching_object: 0.8252
     Episode_Reward/lifting_object: 106.1824
      Episode_Reward/object_height: 0.0466
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 2.01s
                      Time elapsed: 00:32:13
                               ETA: 00:39:54

################################################################################
                     [1m Learning iteration 894/2000 [0m                      

                       Computation: 50078 steps/s (collection: 1.862s, learning 0.101s)
             Mean action noise std: 2.31
          Mean value_function loss: 401.4861
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 48.9868
                       Mean reward: 518.31
               Mean episode length: 161.70
    Episode_Reward/reaching_object: 0.8592
     Episode_Reward/lifting_object: 111.5333
      Episode_Reward/object_height: 0.0509
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 16.3750
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 1.96s
                      Time elapsed: 00:32:15
                               ETA: 00:39:52

################################################################################
                     [1m Learning iteration 895/2000 [0m                      

                       Computation: 48514 steps/s (collection: 1.925s, learning 0.101s)
             Mean action noise std: 2.31
          Mean value_function loss: 386.6131
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 48.9878
                       Mean reward: 543.23
               Mean episode length: 169.25
    Episode_Reward/reaching_object: 0.8316
     Episode_Reward/lifting_object: 106.8357
      Episode_Reward/object_height: 0.0467
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.2083
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 2.03s
                      Time elapsed: 00:32:17
                               ETA: 00:39:50

################################################################################
                     [1m Learning iteration 896/2000 [0m                      

                       Computation: 48814 steps/s (collection: 1.917s, learning 0.097s)
             Mean action noise std: 2.31
          Mean value_function loss: 407.9642
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 48.9903
                       Mean reward: 581.85
               Mean episode length: 179.68
    Episode_Reward/reaching_object: 0.8556
     Episode_Reward/lifting_object: 110.5541
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.8333
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 2.01s
                      Time elapsed: 00:32:19
                               ETA: 00:39:47

################################################################################
                     [1m Learning iteration 897/2000 [0m                      

                       Computation: 48644 steps/s (collection: 1.925s, learning 0.096s)
             Mean action noise std: 2.31
          Mean value_function loss: 427.5800
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 48.9936
                       Mean reward: 533.71
               Mean episode length: 164.69
    Episode_Reward/reaching_object: 0.8692
     Episode_Reward/lifting_object: 113.3769
      Episode_Reward/object_height: 0.0500
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.3333
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 2.02s
                      Time elapsed: 00:32:22
                               ETA: 00:39:45

################################################################################
                     [1m Learning iteration 898/2000 [0m                      

                       Computation: 49030 steps/s (collection: 1.915s, learning 0.090s)
             Mean action noise std: 2.31
          Mean value_function loss: 431.5579
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 48.9945
                       Mean reward: 472.05
               Mean episode length: 147.08
    Episode_Reward/reaching_object: 0.7867
     Episode_Reward/lifting_object: 101.1977
      Episode_Reward/object_height: 0.0431
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.0417
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 2.00s
                      Time elapsed: 00:32:24
                               ETA: 00:39:42

################################################################################
                     [1m Learning iteration 899/2000 [0m                      

                       Computation: 49151 steps/s (collection: 1.904s, learning 0.096s)
             Mean action noise std: 2.32
          Mean value_function loss: 414.3625
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 48.9948
                       Mean reward: 537.25
               Mean episode length: 168.21
    Episode_Reward/reaching_object: 0.7976
     Episode_Reward/lifting_object: 102.2370
      Episode_Reward/object_height: 0.0447
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.8750
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 2.00s
                      Time elapsed: 00:32:26
                               ETA: 00:39:40

################################################################################
                     [1m Learning iteration 900/2000 [0m                      

                       Computation: 48810 steps/s (collection: 1.909s, learning 0.105s)
             Mean action noise std: 2.32
          Mean value_function loss: 387.2665
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 48.9998
                       Mean reward: 578.50
               Mean episode length: 175.77
    Episode_Reward/reaching_object: 0.8287
     Episode_Reward/lifting_object: 107.1330
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.0417
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 2.01s
                      Time elapsed: 00:32:28
                               ETA: 00:39:38

################################################################################
                     [1m Learning iteration 901/2000 [0m                      

                       Computation: 47162 steps/s (collection: 1.965s, learning 0.119s)
             Mean action noise std: 2.32
          Mean value_function loss: 360.0570
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 49.0006
                       Mean reward: 638.06
               Mean episode length: 185.88
    Episode_Reward/reaching_object: 0.8824
     Episode_Reward/lifting_object: 116.4424
      Episode_Reward/object_height: 0.0530
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 2.08s
                      Time elapsed: 00:32:30
                               ETA: 00:39:36

################################################################################
                     [1m Learning iteration 902/2000 [0m                      

                       Computation: 49789 steps/s (collection: 1.876s, learning 0.098s)
             Mean action noise std: 2.32
          Mean value_function loss: 357.8202
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 49.0004
                       Mean reward: 575.32
               Mean episode length: 175.38
    Episode_Reward/reaching_object: 0.8926
     Episode_Reward/lifting_object: 117.6695
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 1.97s
                      Time elapsed: 00:32:32
                               ETA: 00:39:33

################################################################################
                     [1m Learning iteration 903/2000 [0m                      

                       Computation: 49924 steps/s (collection: 1.875s, learning 0.094s)
             Mean action noise std: 2.32
          Mean value_function loss: 383.8667
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 49.0022
                       Mean reward: 566.78
               Mean episode length: 172.62
    Episode_Reward/reaching_object: 0.8670
     Episode_Reward/lifting_object: 113.9908
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 1.97s
                      Time elapsed: 00:32:34
                               ETA: 00:39:31

################################################################################
                     [1m Learning iteration 904/2000 [0m                      

                       Computation: 49789 steps/s (collection: 1.880s, learning 0.094s)
             Mean action noise std: 2.32
          Mean value_function loss: 391.9847
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 49.0047
                       Mean reward: 583.85
               Mean episode length: 179.22
    Episode_Reward/reaching_object: 0.8702
     Episode_Reward/lifting_object: 115.1146
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.6667
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 1.97s
                      Time elapsed: 00:32:36
                               ETA: 00:39:28

################################################################################
                     [1m Learning iteration 905/2000 [0m                      

                       Computation: 48796 steps/s (collection: 1.918s, learning 0.097s)
             Mean action noise std: 2.32
          Mean value_function loss: 413.1988
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 49.0057
                       Mean reward: 644.23
               Mean episode length: 190.65
    Episode_Reward/reaching_object: 0.8917
     Episode_Reward/lifting_object: 118.0351
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 2.01s
                      Time elapsed: 00:32:38
                               ETA: 00:39:26

################################################################################
                     [1m Learning iteration 906/2000 [0m                      

                       Computation: 49494 steps/s (collection: 1.894s, learning 0.093s)
             Mean action noise std: 2.32
          Mean value_function loss: 394.0262
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 49.0092
                       Mean reward: 612.01
               Mean episode length: 184.00
    Episode_Reward/reaching_object: 0.8892
     Episode_Reward/lifting_object: 117.1223
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.8333
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 1.99s
                      Time elapsed: 00:32:40
                               ETA: 00:39:24

################################################################################
                     [1m Learning iteration 907/2000 [0m                      

                       Computation: 48233 steps/s (collection: 1.941s, learning 0.098s)
             Mean action noise std: 2.32
          Mean value_function loss: 373.3986
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 49.0106
                       Mean reward: 595.50
               Mean episode length: 179.84
    Episode_Reward/reaching_object: 0.8818
     Episode_Reward/lifting_object: 115.8023
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.3333
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 2.04s
                      Time elapsed: 00:32:42
                               ETA: 00:39:21

################################################################################
                     [1m Learning iteration 908/2000 [0m                      

                       Computation: 47308 steps/s (collection: 1.978s, learning 0.100s)
             Mean action noise std: 2.32
          Mean value_function loss: 371.8629
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 49.0110
                       Mean reward: 624.02
               Mean episode length: 183.29
    Episode_Reward/reaching_object: 0.8976
     Episode_Reward/lifting_object: 119.4053
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 2.08s
                      Time elapsed: 00:32:44
                               ETA: 00:39:19

################################################################################
                     [1m Learning iteration 909/2000 [0m                      

                       Computation: 48128 steps/s (collection: 1.916s, learning 0.126s)
             Mean action noise std: 2.32
          Mean value_function loss: 387.6719
               Mean surrogate loss: 0.0110
                 Mean entropy loss: 49.0112
                       Mean reward: 572.19
               Mean episode length: 177.22
    Episode_Reward/reaching_object: 0.8679
     Episode_Reward/lifting_object: 114.3298
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 2.04s
                      Time elapsed: 00:32:46
                               ETA: 00:39:17

################################################################################
                     [1m Learning iteration 910/2000 [0m                      

                       Computation: 49005 steps/s (collection: 1.878s, learning 0.128s)
             Mean action noise std: 2.32
          Mean value_function loss: 408.5421
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 49.0113
                       Mean reward: 525.18
               Mean episode length: 157.78
    Episode_Reward/reaching_object: 0.8581
     Episode_Reward/lifting_object: 113.6822
      Episode_Reward/object_height: 0.0522
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.9583
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 2.01s
                      Time elapsed: 00:32:48
                               ETA: 00:39:14

################################################################################
                     [1m Learning iteration 911/2000 [0m                      

                       Computation: 48907 steps/s (collection: 1.893s, learning 0.117s)
             Mean action noise std: 2.32
          Mean value_function loss: 396.3189
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 49.0110
                       Mean reward: 553.30
               Mean episode length: 166.45
    Episode_Reward/reaching_object: 0.8818
     Episode_Reward/lifting_object: 116.0685
      Episode_Reward/object_height: 0.0544
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 2.01s
                      Time elapsed: 00:32:50
                               ETA: 00:39:12

################################################################################
                     [1m Learning iteration 912/2000 [0m                      

                       Computation: 50160 steps/s (collection: 1.842s, learning 0.118s)
             Mean action noise std: 2.32
          Mean value_function loss: 401.4821
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 49.0117
                       Mean reward: 581.82
               Mean episode length: 174.58
    Episode_Reward/reaching_object: 0.8249
     Episode_Reward/lifting_object: 107.5392
      Episode_Reward/object_height: 0.0496
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.6250
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 1.96s
                      Time elapsed: 00:32:52
                               ETA: 00:39:10

################################################################################
                     [1m Learning iteration 913/2000 [0m                      

                       Computation: 49363 steps/s (collection: 1.875s, learning 0.116s)
             Mean action noise std: 2.32
          Mean value_function loss: 407.7842
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 49.0141
                       Mean reward: 509.23
               Mean episode length: 160.61
    Episode_Reward/reaching_object: 0.8369
     Episode_Reward/lifting_object: 109.0160
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.9167
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 1.99s
                      Time elapsed: 00:32:54
                               ETA: 00:39:07

################################################################################
                     [1m Learning iteration 914/2000 [0m                      

                       Computation: 49277 steps/s (collection: 1.879s, learning 0.116s)
             Mean action noise std: 2.32
          Mean value_function loss: 388.5905
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 49.0151
                       Mean reward: 601.32
               Mean episode length: 180.69
    Episode_Reward/reaching_object: 0.8734
     Episode_Reward/lifting_object: 115.2851
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 1.99s
                      Time elapsed: 00:32:56
                               ETA: 00:39:05

################################################################################
                     [1m Learning iteration 915/2000 [0m                      

                       Computation: 46673 steps/s (collection: 1.977s, learning 0.129s)
             Mean action noise std: 2.32
          Mean value_function loss: 414.1685
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 49.0171
                       Mean reward: 522.36
               Mean episode length: 164.45
    Episode_Reward/reaching_object: 0.8820
     Episode_Reward/lifting_object: 115.8712
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 15.0417
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 2.11s
                      Time elapsed: 00:32:58
                               ETA: 00:39:03

################################################################################
                     [1m Learning iteration 916/2000 [0m                      

                       Computation: 45779 steps/s (collection: 2.042s, learning 0.105s)
             Mean action noise std: 2.32
          Mean value_function loss: 404.7006
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 49.0208
                       Mean reward: 603.53
               Mean episode length: 180.69
    Episode_Reward/reaching_object: 0.8915
     Episode_Reward/lifting_object: 117.2150
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 2.15s
                      Time elapsed: 00:33:00
                               ETA: 00:39:01

################################################################################
                     [1m Learning iteration 917/2000 [0m                      

                       Computation: 48485 steps/s (collection: 1.907s, learning 0.121s)
             Mean action noise std: 2.32
          Mean value_function loss: 404.2974
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 49.0229
                       Mean reward: 583.92
               Mean episode length: 178.31
    Episode_Reward/reaching_object: 0.8826
     Episode_Reward/lifting_object: 115.9335
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 2.03s
                      Time elapsed: 00:33:02
                               ETA: 00:38:58

################################################################################
                     [1m Learning iteration 918/2000 [0m                      

                       Computation: 47757 steps/s (collection: 1.940s, learning 0.118s)
             Mean action noise std: 2.32
          Mean value_function loss: 394.4470
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 49.0232
                       Mean reward: 582.28
               Mean episode length: 176.68
    Episode_Reward/reaching_object: 0.8536
     Episode_Reward/lifting_object: 111.5780
      Episode_Reward/object_height: 0.0501
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 2.06s
                      Time elapsed: 00:33:04
                               ETA: 00:38:56

################################################################################
                     [1m Learning iteration 919/2000 [0m                      

                       Computation: 49088 steps/s (collection: 1.908s, learning 0.095s)
             Mean action noise std: 2.32
          Mean value_function loss: 398.1967
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 49.0212
                       Mean reward: 567.79
               Mean episode length: 168.00
    Episode_Reward/reaching_object: 0.8628
     Episode_Reward/lifting_object: 113.5070
      Episode_Reward/object_height: 0.0503
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.8333
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 2.00s
                      Time elapsed: 00:33:06
                               ETA: 00:38:54

################################################################################
                     [1m Learning iteration 920/2000 [0m                      

                       Computation: 49363 steps/s (collection: 1.894s, learning 0.098s)
             Mean action noise std: 2.32
          Mean value_function loss: 415.2302
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 49.0166
                       Mean reward: 578.43
               Mean episode length: 169.05
    Episode_Reward/reaching_object: 0.8949
     Episode_Reward/lifting_object: 118.3282
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 15.6667
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 1.99s
                      Time elapsed: 00:33:08
                               ETA: 00:38:51

################################################################################
                     [1m Learning iteration 921/2000 [0m                      

                       Computation: 48849 steps/s (collection: 1.891s, learning 0.122s)
             Mean action noise std: 2.32
          Mean value_function loss: 377.2267
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 49.0194
                       Mean reward: 561.70
               Mean episode length: 169.85
    Episode_Reward/reaching_object: 0.8956
     Episode_Reward/lifting_object: 118.5795
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 2.01s
                      Time elapsed: 00:33:10
                               ETA: 00:38:49

################################################################################
                     [1m Learning iteration 922/2000 [0m                      

                       Computation: 48601 steps/s (collection: 1.897s, learning 0.126s)
             Mean action noise std: 2.32
          Mean value_function loss: 381.4493
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 49.0202
                       Mean reward: 576.89
               Mean episode length: 173.57
    Episode_Reward/reaching_object: 0.8632
     Episode_Reward/lifting_object: 113.7806
      Episode_Reward/object_height: 0.0508
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 2.02s
                      Time elapsed: 00:33:12
                               ETA: 00:38:47

################################################################################
                     [1m Learning iteration 923/2000 [0m                      

                       Computation: 48868 steps/s (collection: 1.883s, learning 0.129s)
             Mean action noise std: 2.32
          Mean value_function loss: 374.5463
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 49.0194
                       Mean reward: 571.93
               Mean episode length: 172.47
    Episode_Reward/reaching_object: 0.9167
     Episode_Reward/lifting_object: 122.1197
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 2.01s
                      Time elapsed: 00:33:14
                               ETA: 00:38:44

################################################################################
                     [1m Learning iteration 924/2000 [0m                      

                       Computation: 47968 steps/s (collection: 1.919s, learning 0.131s)
             Mean action noise std: 2.32
          Mean value_function loss: 379.3166
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 49.0201
                       Mean reward: 616.29
               Mean episode length: 183.04
    Episode_Reward/reaching_object: 0.9293
     Episode_Reward/lifting_object: 124.0536
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.7500
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 2.05s
                      Time elapsed: 00:33:16
                               ETA: 00:38:42

################################################################################
                     [1m Learning iteration 925/2000 [0m                      

                       Computation: 49064 steps/s (collection: 1.884s, learning 0.119s)
             Mean action noise std: 2.32
          Mean value_function loss: 342.1699
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 49.0184
                       Mean reward: 577.73
               Mean episode length: 173.23
    Episode_Reward/reaching_object: 0.9458
     Episode_Reward/lifting_object: 125.4452
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 2.00s
                      Time elapsed: 00:33:18
                               ETA: 00:38:40

################################################################################
                     [1m Learning iteration 926/2000 [0m                      

                       Computation: 48751 steps/s (collection: 1.900s, learning 0.117s)
             Mean action noise std: 2.32
          Mean value_function loss: 349.5130
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 49.0177
                       Mean reward: 651.50
               Mean episode length: 195.57
    Episode_Reward/reaching_object: 0.9541
     Episode_Reward/lifting_object: 126.6215
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 2.02s
                      Time elapsed: 00:33:20
                               ETA: 00:38:37

################################################################################
                     [1m Learning iteration 927/2000 [0m                      

                       Computation: 49458 steps/s (collection: 1.873s, learning 0.115s)
             Mean action noise std: 2.32
          Mean value_function loss: 346.6843
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 49.0200
                       Mean reward: 639.09
               Mean episode length: 190.01
    Episode_Reward/reaching_object: 0.9459
     Episode_Reward/lifting_object: 124.9402
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 1.99s
                      Time elapsed: 00:33:22
                               ETA: 00:38:35

################################################################################
                     [1m Learning iteration 928/2000 [0m                      

                       Computation: 49004 steps/s (collection: 1.904s, learning 0.102s)
             Mean action noise std: 2.32
          Mean value_function loss: 328.7273
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 49.0203
                       Mean reward: 665.80
               Mean episode length: 197.73
    Episode_Reward/reaching_object: 0.9558
     Episode_Reward/lifting_object: 127.3484
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 2.01s
                      Time elapsed: 00:33:24
                               ETA: 00:38:33

################################################################################
                     [1m Learning iteration 929/2000 [0m                      

                       Computation: 49931 steps/s (collection: 1.875s, learning 0.094s)
             Mean action noise std: 2.32
          Mean value_function loss: 354.2738
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 49.0208
                       Mean reward: 643.99
               Mean episode length: 193.24
    Episode_Reward/reaching_object: 0.9348
     Episode_Reward/lifting_object: 124.3905
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.7083
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 1.97s
                      Time elapsed: 00:33:26
                               ETA: 00:38:30

################################################################################
                     [1m Learning iteration 930/2000 [0m                      

                       Computation: 49412 steps/s (collection: 1.879s, learning 0.111s)
             Mean action noise std: 2.32
          Mean value_function loss: 343.5219
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 49.0218
                       Mean reward: 643.06
               Mean episode length: 192.27
    Episode_Reward/reaching_object: 0.9526
     Episode_Reward/lifting_object: 126.7408
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 1.99s
                      Time elapsed: 00:33:28
                               ETA: 00:38:28

################################################################################
                     [1m Learning iteration 931/2000 [0m                      

                       Computation: 48781 steps/s (collection: 1.901s, learning 0.114s)
             Mean action noise std: 2.32
          Mean value_function loss: 345.3358
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 49.0243
                       Mean reward: 686.95
               Mean episode length: 199.09
    Episode_Reward/reaching_object: 0.9748
     Episode_Reward/lifting_object: 130.3508
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 2.02s
                      Time elapsed: 00:33:30
                               ETA: 00:38:26

################################################################################
                     [1m Learning iteration 932/2000 [0m                      

                       Computation: 48937 steps/s (collection: 1.885s, learning 0.124s)
             Mean action noise std: 2.32
          Mean value_function loss: 330.0772
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 49.0259
                       Mean reward: 647.98
               Mean episode length: 189.80
    Episode_Reward/reaching_object: 0.9652
     Episode_Reward/lifting_object: 129.8331
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 2.01s
                      Time elapsed: 00:33:32
                               ETA: 00:38:23

################################################################################
                     [1m Learning iteration 933/2000 [0m                      

                       Computation: 48489 steps/s (collection: 1.903s, learning 0.125s)
             Mean action noise std: 2.32
          Mean value_function loss: 324.3589
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 49.0253
                       Mean reward: 710.28
               Mean episode length: 201.62
    Episode_Reward/reaching_object: 0.9434
     Episode_Reward/lifting_object: 125.8873
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 2.03s
                      Time elapsed: 00:33:34
                               ETA: 00:38:21

################################################################################
                     [1m Learning iteration 934/2000 [0m                      

                       Computation: 48078 steps/s (collection: 1.914s, learning 0.131s)
             Mean action noise std: 2.32
          Mean value_function loss: 345.6538
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 49.0262
                       Mean reward: 652.26
               Mean episode length: 189.97
    Episode_Reward/reaching_object: 0.9663
     Episode_Reward/lifting_object: 129.2394
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 2.04s
                      Time elapsed: 00:33:36
                               ETA: 00:38:19

################################################################################
                     [1m Learning iteration 935/2000 [0m                      

                       Computation: 49282 steps/s (collection: 1.875s, learning 0.120s)
             Mean action noise std: 2.32
          Mean value_function loss: 335.8694
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 49.0272
                       Mean reward: 642.58
               Mean episode length: 185.03
    Episode_Reward/reaching_object: 0.9714
     Episode_Reward/lifting_object: 130.7600
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 1.99s
                      Time elapsed: 00:33:38
                               ETA: 00:38:16

################################################################################
                     [1m Learning iteration 936/2000 [0m                      

                       Computation: 49455 steps/s (collection: 1.877s, learning 0.110s)
             Mean action noise std: 2.32
          Mean value_function loss: 391.6095
               Mean surrogate loss: 0.0124
                 Mean entropy loss: 49.0276
                       Mean reward: 686.95
               Mean episode length: 199.62
    Episode_Reward/reaching_object: 1.0007
     Episode_Reward/lifting_object: 135.1059
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 1.99s
                      Time elapsed: 00:33:40
                               ETA: 00:38:14

################################################################################
                     [1m Learning iteration 937/2000 [0m                      

                       Computation: 49174 steps/s (collection: 1.881s, learning 0.118s)
             Mean action noise std: 2.32
          Mean value_function loss: 358.8525
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 49.0286
                       Mean reward: 654.70
               Mean episode length: 188.40
    Episode_Reward/reaching_object: 0.9975
     Episode_Reward/lifting_object: 135.5580
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 2.00s
                      Time elapsed: 00:33:42
                               ETA: 00:38:12

################################################################################
                     [1m Learning iteration 938/2000 [0m                      

                       Computation: 47529 steps/s (collection: 1.943s, learning 0.125s)
             Mean action noise std: 2.32
          Mean value_function loss: 334.5726
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 49.0308
                       Mean reward: 702.23
               Mean episode length: 200.84
    Episode_Reward/reaching_object: 1.0194
     Episode_Reward/lifting_object: 138.0605
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 2.07s
                      Time elapsed: 00:33:44
                               ETA: 00:38:09

################################################################################
                     [1m Learning iteration 939/2000 [0m                      

                       Computation: 48586 steps/s (collection: 1.899s, learning 0.125s)
             Mean action noise std: 2.32
          Mean value_function loss: 307.9056
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 49.0334
                       Mean reward: 721.12
               Mean episode length: 205.25
    Episode_Reward/reaching_object: 1.0195
     Episode_Reward/lifting_object: 138.6275
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 2.02s
                      Time elapsed: 00:33:46
                               ETA: 00:38:07

################################################################################
                     [1m Learning iteration 940/2000 [0m                      

                       Computation: 48368 steps/s (collection: 1.911s, learning 0.122s)
             Mean action noise std: 2.32
          Mean value_function loss: 313.8401
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 49.0412
                       Mean reward: 750.16
               Mean episode length: 211.11
    Episode_Reward/reaching_object: 1.0358
     Episode_Reward/lifting_object: 140.8606
      Episode_Reward/object_height: 0.0676
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 2.03s
                      Time elapsed: 00:33:48
                               ETA: 00:38:05

################################################################################
                     [1m Learning iteration 941/2000 [0m                      

                       Computation: 49105 steps/s (collection: 1.882s, learning 0.120s)
             Mean action noise std: 2.32
          Mean value_function loss: 278.3414
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 49.0450
                       Mean reward: 695.39
               Mean episode length: 201.29
    Episode_Reward/reaching_object: 1.0229
     Episode_Reward/lifting_object: 138.4401
      Episode_Reward/object_height: 0.0667
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 2.00s
                      Time elapsed: 00:33:50
                               ETA: 00:38:02

################################################################################
                     [1m Learning iteration 942/2000 [0m                      

                       Computation: 48523 steps/s (collection: 1.903s, learning 0.123s)
             Mean action noise std: 2.32
          Mean value_function loss: 322.5738
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 49.0470
                       Mean reward: 663.81
               Mean episode length: 194.07
    Episode_Reward/reaching_object: 0.9987
     Episode_Reward/lifting_object: 135.0104
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 2.03s
                      Time elapsed: 00:33:52
                               ETA: 00:38:00

################################################################################
                     [1m Learning iteration 943/2000 [0m                      

                       Computation: 48947 steps/s (collection: 1.887s, learning 0.122s)
             Mean action noise std: 2.32
          Mean value_function loss: 315.9388
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 49.0497
                       Mean reward: 674.33
               Mean episode length: 191.64
    Episode_Reward/reaching_object: 0.9912
     Episode_Reward/lifting_object: 133.9404
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 2.01s
                      Time elapsed: 00:33:54
                               ETA: 00:37:58

################################################################################
                     [1m Learning iteration 944/2000 [0m                      

                       Computation: 48424 steps/s (collection: 1.912s, learning 0.118s)
             Mean action noise std: 2.32
          Mean value_function loss: 295.4102
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 49.0520
                       Mean reward: 707.92
               Mean episode length: 203.97
    Episode_Reward/reaching_object: 1.0225
     Episode_Reward/lifting_object: 137.8998
      Episode_Reward/object_height: 0.0667
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 2.03s
                      Time elapsed: 00:33:56
                               ETA: 00:37:56

################################################################################
                     [1m Learning iteration 945/2000 [0m                      

                       Computation: 48755 steps/s (collection: 1.890s, learning 0.127s)
             Mean action noise std: 2.32
          Mean value_function loss: 283.3543
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 49.0551
                       Mean reward: 702.19
               Mean episode length: 202.15
    Episode_Reward/reaching_object: 1.0450
     Episode_Reward/lifting_object: 141.9690
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 2.02s
                      Time elapsed: 00:33:58
                               ETA: 00:37:53

################################################################################
                     [1m Learning iteration 946/2000 [0m                      

                       Computation: 48300 steps/s (collection: 1.911s, learning 0.125s)
             Mean action noise std: 2.32
          Mean value_function loss: 308.8943
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 49.0597
                       Mean reward: 749.50
               Mean episode length: 210.29
    Episode_Reward/reaching_object: 1.0535
     Episode_Reward/lifting_object: 143.4594
      Episode_Reward/object_height: 0.0700
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 2.04s
                      Time elapsed: 00:34:00
                               ETA: 00:37:51

################################################################################
                     [1m Learning iteration 947/2000 [0m                      

                       Computation: 49341 steps/s (collection: 1.872s, learning 0.120s)
             Mean action noise std: 2.33
          Mean value_function loss: 320.3769
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 49.0708
                       Mean reward: 740.39
               Mean episode length: 208.26
    Episode_Reward/reaching_object: 1.0372
     Episode_Reward/lifting_object: 140.7208
      Episode_Reward/object_height: 0.0678
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 1.99s
                      Time elapsed: 00:34:02
                               ETA: 00:37:49

################################################################################
                     [1m Learning iteration 948/2000 [0m                      

                       Computation: 48458 steps/s (collection: 1.900s, learning 0.129s)
             Mean action noise std: 2.33
          Mean value_function loss: 314.9459
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 49.0757
                       Mean reward: 684.89
               Mean episode length: 194.26
    Episode_Reward/reaching_object: 1.0154
     Episode_Reward/lifting_object: 137.8133
      Episode_Reward/object_height: 0.0672
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 2.03s
                      Time elapsed: 00:34:04
                               ETA: 00:37:46

################################################################################
                     [1m Learning iteration 949/2000 [0m                      

                       Computation: 48908 steps/s (collection: 1.877s, learning 0.133s)
             Mean action noise std: 2.33
          Mean value_function loss: 304.7400
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 49.0761
                       Mean reward: 654.89
               Mean episode length: 188.26
    Episode_Reward/reaching_object: 0.9808
     Episode_Reward/lifting_object: 133.3266
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 2.01s
                      Time elapsed: 00:34:06
                               ETA: 00:37:44

################################################################################
                     [1m Learning iteration 950/2000 [0m                      

                       Computation: 48387 steps/s (collection: 1.904s, learning 0.127s)
             Mean action noise std: 2.33
          Mean value_function loss: 297.3060
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 49.0748
                       Mean reward: 709.13
               Mean episode length: 201.25
    Episode_Reward/reaching_object: 1.0435
     Episode_Reward/lifting_object: 141.8262
      Episode_Reward/object_height: 0.0688
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 2.03s
                      Time elapsed: 00:34:08
                               ETA: 00:37:42

################################################################################
                     [1m Learning iteration 951/2000 [0m                      

                       Computation: 48179 steps/s (collection: 1.918s, learning 0.123s)
             Mean action noise std: 2.33
          Mean value_function loss: 284.2877
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 49.0751
                       Mean reward: 754.38
               Mean episode length: 211.42
    Episode_Reward/reaching_object: 1.0259
     Episode_Reward/lifting_object: 139.8366
      Episode_Reward/object_height: 0.0678
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 2.04s
                      Time elapsed: 00:34:10
                               ETA: 00:37:39

################################################################################
                     [1m Learning iteration 952/2000 [0m                      

                       Computation: 48148 steps/s (collection: 1.914s, learning 0.128s)
             Mean action noise std: 2.33
          Mean value_function loss: 311.2598
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 49.0756
                       Mean reward: 754.29
               Mean episode length: 211.09
    Episode_Reward/reaching_object: 1.0592
     Episode_Reward/lifting_object: 144.4829
      Episode_Reward/object_height: 0.0707
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 2.04s
                      Time elapsed: 00:34:13
                               ETA: 00:37:37

################################################################################
                     [1m Learning iteration 953/2000 [0m                      

                       Computation: 47357 steps/s (collection: 1.940s, learning 0.136s)
             Mean action noise std: 2.33
          Mean value_function loss: 283.9130
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 49.0752
                       Mean reward: 688.43
               Mean episode length: 199.21
    Episode_Reward/reaching_object: 1.0190
     Episode_Reward/lifting_object: 137.3929
      Episode_Reward/object_height: 0.0666
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 2.08s
                      Time elapsed: 00:34:15
                               ETA: 00:37:35

################################################################################
                     [1m Learning iteration 954/2000 [0m                      

                       Computation: 47794 steps/s (collection: 1.925s, learning 0.132s)
             Mean action noise std: 2.33
          Mean value_function loss: 304.1971
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 49.0747
                       Mean reward: 665.53
               Mean episode length: 194.43
    Episode_Reward/reaching_object: 1.0076
     Episode_Reward/lifting_object: 135.6244
      Episode_Reward/object_height: 0.0651
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 2.06s
                      Time elapsed: 00:34:17
                               ETA: 00:37:33

################################################################################
                     [1m Learning iteration 955/2000 [0m                      

                       Computation: 48946 steps/s (collection: 1.878s, learning 0.130s)
             Mean action noise std: 2.33
          Mean value_function loss: 296.5177
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 49.0742
                       Mean reward: 662.31
               Mean episode length: 191.34
    Episode_Reward/reaching_object: 1.0087
     Episode_Reward/lifting_object: 135.8182
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 2.01s
                      Time elapsed: 00:34:19
                               ETA: 00:37:30

################################################################################
                     [1m Learning iteration 956/2000 [0m                      

                       Computation: 48415 steps/s (collection: 1.907s, learning 0.123s)
             Mean action noise std: 2.33
          Mean value_function loss: 283.5338
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 49.0741
                       Mean reward: 690.70
               Mean episode length: 197.85
    Episode_Reward/reaching_object: 1.0297
     Episode_Reward/lifting_object: 138.7786
      Episode_Reward/object_height: 0.0661
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 2.03s
                      Time elapsed: 00:34:21
                               ETA: 00:37:28

################################################################################
                     [1m Learning iteration 957/2000 [0m                      

                       Computation: 48298 steps/s (collection: 1.911s, learning 0.125s)
             Mean action noise std: 2.33
          Mean value_function loss: 290.8759
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 49.0733
                       Mean reward: 647.20
               Mean episode length: 189.64
    Episode_Reward/reaching_object: 1.0489
     Episode_Reward/lifting_object: 141.4825
      Episode_Reward/object_height: 0.0670
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 2.04s
                      Time elapsed: 00:34:23
                               ETA: 00:37:26

################################################################################
                     [1m Learning iteration 958/2000 [0m                      

                       Computation: 47231 steps/s (collection: 1.959s, learning 0.123s)
             Mean action noise std: 2.33
          Mean value_function loss: 288.1207
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 49.0765
                       Mean reward: 748.76
               Mean episode length: 210.95
    Episode_Reward/reaching_object: 1.0502
     Episode_Reward/lifting_object: 142.2214
      Episode_Reward/object_height: 0.0666
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 2.08s
                      Time elapsed: 00:34:25
                               ETA: 00:37:24

################################################################################
                     [1m Learning iteration 959/2000 [0m                      

                       Computation: 48627 steps/s (collection: 1.903s, learning 0.119s)
             Mean action noise std: 2.33
          Mean value_function loss: 266.2225
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 49.0833
                       Mean reward: 735.94
               Mean episode length: 207.31
    Episode_Reward/reaching_object: 1.0589
     Episode_Reward/lifting_object: 143.5676
      Episode_Reward/object_height: 0.0689
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 2.02s
                      Time elapsed: 00:34:27
                               ETA: 00:37:21

################################################################################
                     [1m Learning iteration 960/2000 [0m                      

                       Computation: 49202 steps/s (collection: 1.890s, learning 0.108s)
             Mean action noise std: 2.33
          Mean value_function loss: 275.5424
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 49.0863
                       Mean reward: 690.60
               Mean episode length: 199.76
    Episode_Reward/reaching_object: 1.0627
     Episode_Reward/lifting_object: 144.0911
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 2.00s
                      Time elapsed: 00:34:29
                               ETA: 00:37:19

################################################################################
                     [1m Learning iteration 961/2000 [0m                      

                       Computation: 48481 steps/s (collection: 1.904s, learning 0.124s)
             Mean action noise std: 2.33
          Mean value_function loss: 274.5018
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 49.0880
                       Mean reward: 770.13
               Mean episode length: 214.28
    Episode_Reward/reaching_object: 1.0795
     Episode_Reward/lifting_object: 147.3046
      Episode_Reward/object_height: 0.0703
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 2.03s
                      Time elapsed: 00:34:31
                               ETA: 00:37:17

################################################################################
                     [1m Learning iteration 962/2000 [0m                      

                       Computation: 48342 steps/s (collection: 1.920s, learning 0.113s)
             Mean action noise std: 2.33
          Mean value_function loss: 293.8179
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 49.0890
                       Mean reward: 757.84
               Mean episode length: 213.78
    Episode_Reward/reaching_object: 1.0694
     Episode_Reward/lifting_object: 145.2560
      Episode_Reward/object_height: 0.0703
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 2.03s
                      Time elapsed: 00:34:33
                               ETA: 00:37:14

################################################################################
                     [1m Learning iteration 963/2000 [0m                      

                       Computation: 48533 steps/s (collection: 1.914s, learning 0.111s)
             Mean action noise std: 2.33
          Mean value_function loss: 266.5916
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 49.0868
                       Mean reward: 735.47
               Mean episode length: 210.66
    Episode_Reward/reaching_object: 1.0825
     Episode_Reward/lifting_object: 146.4445
      Episode_Reward/object_height: 0.0707
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 2.03s
                      Time elapsed: 00:34:35
                               ETA: 00:37:12

################################################################################
                     [1m Learning iteration 964/2000 [0m                      

                       Computation: 47589 steps/s (collection: 1.944s, learning 0.122s)
             Mean action noise std: 2.33
          Mean value_function loss: 277.7312
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 49.0856
                       Mean reward: 669.79
               Mean episode length: 193.70
    Episode_Reward/reaching_object: 1.0412
     Episode_Reward/lifting_object: 140.8026
      Episode_Reward/object_height: 0.0678
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 2.07s
                      Time elapsed: 00:34:37
                               ETA: 00:37:10

################################################################################
                     [1m Learning iteration 965/2000 [0m                      

                       Computation: 48113 steps/s (collection: 1.921s, learning 0.122s)
             Mean action noise std: 2.33
          Mean value_function loss: 289.2872
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 49.0871
                       Mean reward: 716.57
               Mean episode length: 203.79
    Episode_Reward/reaching_object: 1.0211
     Episode_Reward/lifting_object: 136.8583
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 2.04s
                      Time elapsed: 00:34:39
                               ETA: 00:37:08

################################################################################
                     [1m Learning iteration 966/2000 [0m                      

                       Computation: 48370 steps/s (collection: 1.918s, learning 0.114s)
             Mean action noise std: 2.33
          Mean value_function loss: 278.6552
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 49.0885
                       Mean reward: 658.74
               Mean episode length: 193.18
    Episode_Reward/reaching_object: 1.0010
     Episode_Reward/lifting_object: 133.4183
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 2.03s
                      Time elapsed: 00:34:41
                               ETA: 00:37:05

################################################################################
                     [1m Learning iteration 967/2000 [0m                      

                       Computation: 48711 steps/s (collection: 1.888s, learning 0.130s)
             Mean action noise std: 2.33
          Mean value_function loss: 301.8462
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 49.0898
                       Mean reward: 645.31
               Mean episode length: 191.13
    Episode_Reward/reaching_object: 1.0034
     Episode_Reward/lifting_object: 133.1799
      Episode_Reward/object_height: 0.0623
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 2.02s
                      Time elapsed: 00:34:43
                               ETA: 00:37:03

################################################################################
                     [1m Learning iteration 968/2000 [0m                      

                       Computation: 47919 steps/s (collection: 1.927s, learning 0.124s)
             Mean action noise std: 2.33
          Mean value_function loss: 280.7063
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 49.0908
                       Mean reward: 658.26
               Mean episode length: 190.17
    Episode_Reward/reaching_object: 0.9784
     Episode_Reward/lifting_object: 129.6958
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 2.05s
                      Time elapsed: 00:34:45
                               ETA: 00:37:01

################################################################################
                     [1m Learning iteration 969/2000 [0m                      

                       Computation: 49073 steps/s (collection: 1.876s, learning 0.128s)
             Mean action noise std: 2.33
          Mean value_function loss: 278.0600
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 49.0932
                       Mean reward: 682.91
               Mean episode length: 198.72
    Episode_Reward/reaching_object: 1.0022
     Episode_Reward/lifting_object: 132.9412
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 2.00s
                      Time elapsed: 00:34:47
                               ETA: 00:36:58

################################################################################
                     [1m Learning iteration 970/2000 [0m                      

                       Computation: 48691 steps/s (collection: 1.891s, learning 0.128s)
             Mean action noise std: 2.33
          Mean value_function loss: 260.9531
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 49.0945
                       Mean reward: 709.29
               Mean episode length: 204.70
    Episode_Reward/reaching_object: 1.0124
     Episode_Reward/lifting_object: 134.4260
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 2.02s
                      Time elapsed: 00:34:49
                               ETA: 00:36:56

################################################################################
                     [1m Learning iteration 971/2000 [0m                      

                       Computation: 48381 steps/s (collection: 1.911s, learning 0.121s)
             Mean action noise std: 2.33
          Mean value_function loss: 291.1885
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 49.0961
                       Mean reward: 711.88
               Mean episode length: 206.10
    Episode_Reward/reaching_object: 0.9965
     Episode_Reward/lifting_object: 132.3060
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 2.03s
                      Time elapsed: 00:34:51
                               ETA: 00:36:54

################################################################################
                     [1m Learning iteration 972/2000 [0m                      

                       Computation: 48629 steps/s (collection: 1.896s, learning 0.126s)
             Mean action noise std: 2.33
          Mean value_function loss: 267.6296
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 49.0983
                       Mean reward: 679.00
               Mean episode length: 195.60
    Episode_Reward/reaching_object: 1.0083
     Episode_Reward/lifting_object: 134.1582
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 2.02s
                      Time elapsed: 00:34:53
                               ETA: 00:36:52

################################################################################
                     [1m Learning iteration 973/2000 [0m                      

                       Computation: 48814 steps/s (collection: 1.898s, learning 0.116s)
             Mean action noise std: 2.33
          Mean value_function loss: 266.1965
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 49.1003
                       Mean reward: 730.35
               Mean episode length: 208.13
    Episode_Reward/reaching_object: 1.0333
     Episode_Reward/lifting_object: 136.9020
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 2.01s
                      Time elapsed: 00:34:55
                               ETA: 00:36:49

################################################################################
                     [1m Learning iteration 974/2000 [0m                      

                       Computation: 49167 steps/s (collection: 1.892s, learning 0.108s)
             Mean action noise std: 2.33
          Mean value_function loss: 249.1552
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 49.1025
                       Mean reward: 745.75
               Mean episode length: 211.00
    Episode_Reward/reaching_object: 1.0428
     Episode_Reward/lifting_object: 139.3014
      Episode_Reward/object_height: 0.0664
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 2.00s
                      Time elapsed: 00:34:57
                               ETA: 00:36:47

################################################################################
                     [1m Learning iteration 975/2000 [0m                      

                       Computation: 49869 steps/s (collection: 1.872s, learning 0.100s)
             Mean action noise std: 2.33
          Mean value_function loss: 278.4112
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 49.1053
                       Mean reward: 748.94
               Mean episode length: 208.74
    Episode_Reward/reaching_object: 1.0614
     Episode_Reward/lifting_object: 142.5313
      Episode_Reward/object_height: 0.0678
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 1.97s
                      Time elapsed: 00:34:59
                               ETA: 00:36:45

################################################################################
                     [1m Learning iteration 976/2000 [0m                      

                       Computation: 49167 steps/s (collection: 1.888s, learning 0.111s)
             Mean action noise std: 2.33
          Mean value_function loss: 275.2556
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 49.1069
                       Mean reward: 772.46
               Mean episode length: 213.41
    Episode_Reward/reaching_object: 1.1041
     Episode_Reward/lifting_object: 149.8917
      Episode_Reward/object_height: 0.0721
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 2.00s
                      Time elapsed: 00:35:01
                               ETA: 00:36:42

################################################################################
                     [1m Learning iteration 977/2000 [0m                      

                       Computation: 48596 steps/s (collection: 1.913s, learning 0.110s)
             Mean action noise std: 2.33
          Mean value_function loss: 294.2709
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 49.1073
                       Mean reward: 727.46
               Mean episode length: 206.07
    Episode_Reward/reaching_object: 1.1017
     Episode_Reward/lifting_object: 149.4974
      Episode_Reward/object_height: 0.0723
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 2.02s
                      Time elapsed: 00:35:03
                               ETA: 00:36:40

################################################################################
                     [1m Learning iteration 978/2000 [0m                      

                       Computation: 48994 steps/s (collection: 1.874s, learning 0.132s)
             Mean action noise std: 2.33
          Mean value_function loss: 287.6166
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 49.1077
                       Mean reward: 782.26
               Mean episode length: 216.92
    Episode_Reward/reaching_object: 1.1128
     Episode_Reward/lifting_object: 151.2582
      Episode_Reward/object_height: 0.0733
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 2.01s
                      Time elapsed: 00:35:05
                               ETA: 00:36:38

################################################################################
                     [1m Learning iteration 979/2000 [0m                      

                       Computation: 47813 steps/s (collection: 1.953s, learning 0.103s)
             Mean action noise std: 2.33
          Mean value_function loss: 279.8058
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 49.1077
                       Mean reward: 719.37
               Mean episode length: 202.45
    Episode_Reward/reaching_object: 1.0941
     Episode_Reward/lifting_object: 147.7886
      Episode_Reward/object_height: 0.0713
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 2.06s
                      Time elapsed: 00:35:07
                               ETA: 00:36:35

################################################################################
                     [1m Learning iteration 980/2000 [0m                      

                       Computation: 46609 steps/s (collection: 1.996s, learning 0.114s)
             Mean action noise std: 2.33
          Mean value_function loss: 293.6822
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 49.1072
                       Mean reward: 709.18
               Mean episode length: 199.05
    Episode_Reward/reaching_object: 1.0570
     Episode_Reward/lifting_object: 142.1957
      Episode_Reward/object_height: 0.0676
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 2.11s
                      Time elapsed: 00:35:09
                               ETA: 00:36:33

################################################################################
                     [1m Learning iteration 981/2000 [0m                      

                       Computation: 47287 steps/s (collection: 1.983s, learning 0.096s)
             Mean action noise std: 2.33
          Mean value_function loss: 293.2165
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 49.1088
                       Mean reward: 718.21
               Mean episode length: 201.84
    Episode_Reward/reaching_object: 1.0291
     Episode_Reward/lifting_object: 139.1911
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 2.08s
                      Time elapsed: 00:35:11
                               ETA: 00:36:31

################################################################################
                     [1m Learning iteration 982/2000 [0m                      

                       Computation: 46878 steps/s (collection: 1.991s, learning 0.106s)
             Mean action noise std: 2.33
          Mean value_function loss: 314.2346
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 49.1088
                       Mean reward: 659.37
               Mean episode length: 192.68
    Episode_Reward/reaching_object: 1.0214
     Episode_Reward/lifting_object: 136.2543
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 2.10s
                      Time elapsed: 00:35:14
                               ETA: 00:36:29

################################################################################
                     [1m Learning iteration 983/2000 [0m                      

                       Computation: 47969 steps/s (collection: 1.935s, learning 0.115s)
             Mean action noise std: 2.33
          Mean value_function loss: 309.4018
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 49.1101
                       Mean reward: 728.15
               Mean episode length: 204.85
    Episode_Reward/reaching_object: 1.0354
     Episode_Reward/lifting_object: 138.9852
      Episode_Reward/object_height: 0.0659
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 2.05s
                      Time elapsed: 00:35:16
                               ETA: 00:36:27

################################################################################
                     [1m Learning iteration 984/2000 [0m                      

                       Computation: 44109 steps/s (collection: 2.106s, learning 0.123s)
             Mean action noise std: 2.33
          Mean value_function loss: 291.3293
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 49.1099
                       Mean reward: 690.79
               Mean episode length: 197.44
    Episode_Reward/reaching_object: 1.0265
     Episode_Reward/lifting_object: 136.8454
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 2.23s
                      Time elapsed: 00:35:18
                               ETA: 00:36:25

################################################################################
                     [1m Learning iteration 985/2000 [0m                      

                       Computation: 48013 steps/s (collection: 1.952s, learning 0.095s)
             Mean action noise std: 2.33
          Mean value_function loss: 297.5075
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 49.1149
                       Mean reward: 784.20
               Mean episode length: 218.78
    Episode_Reward/reaching_object: 1.0747
     Episode_Reward/lifting_object: 144.9863
      Episode_Reward/object_height: 0.0688
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 2.05s
                      Time elapsed: 00:35:20
                               ETA: 00:36:22

################################################################################
                     [1m Learning iteration 986/2000 [0m                      

                       Computation: 47447 steps/s (collection: 1.958s, learning 0.114s)
             Mean action noise std: 2.34
          Mean value_function loss: 273.6403
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 49.1237
                       Mean reward: 688.03
               Mean episode length: 197.87
    Episode_Reward/reaching_object: 1.0645
     Episode_Reward/lifting_object: 143.2708
      Episode_Reward/object_height: 0.0680
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 2.07s
                      Time elapsed: 00:35:22
                               ETA: 00:36:20

################################################################################
                     [1m Learning iteration 987/2000 [0m                      

                       Computation: 46078 steps/s (collection: 2.016s, learning 0.117s)
             Mean action noise std: 2.34
          Mean value_function loss: 268.9772
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 49.1307
                       Mean reward: 712.14
               Mean episode length: 200.73
    Episode_Reward/reaching_object: 1.0874
     Episode_Reward/lifting_object: 146.4423
      Episode_Reward/object_height: 0.0705
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 2.13s
                      Time elapsed: 00:35:24
                               ETA: 00:36:18

################################################################################
                     [1m Learning iteration 988/2000 [0m                      

                       Computation: 46559 steps/s (collection: 1.991s, learning 0.120s)
             Mean action noise std: 2.34
          Mean value_function loss: 232.1903
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 49.1362
                       Mean reward: 750.01
               Mean episode length: 209.86
    Episode_Reward/reaching_object: 1.0487
     Episode_Reward/lifting_object: 141.0580
      Episode_Reward/object_height: 0.0674
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 2.11s
                      Time elapsed: 00:35:26
                               ETA: 00:36:16

################################################################################
                     [1m Learning iteration 989/2000 [0m                      

                       Computation: 44911 steps/s (collection: 2.039s, learning 0.150s)
             Mean action noise std: 2.34
          Mean value_function loss: 260.8464
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 49.1368
                       Mean reward: 742.03
               Mean episode length: 205.17
    Episode_Reward/reaching_object: 1.0777
     Episode_Reward/lifting_object: 145.4692
      Episode_Reward/object_height: 0.0707
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 2.19s
                      Time elapsed: 00:35:28
                               ETA: 00:36:14

################################################################################
                     [1m Learning iteration 990/2000 [0m                      

                       Computation: 45867 steps/s (collection: 2.051s, learning 0.092s)
             Mean action noise std: 2.34
          Mean value_function loss: 255.7049
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 49.1359
                       Mean reward: 759.99
               Mean episode length: 212.07
    Episode_Reward/reaching_object: 1.0961
     Episode_Reward/lifting_object: 148.4958
      Episode_Reward/object_height: 0.0715
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 2.14s
                      Time elapsed: 00:35:31
                               ETA: 00:36:11

################################################################################
                     [1m Learning iteration 991/2000 [0m                      

                       Computation: 43857 steps/s (collection: 2.076s, learning 0.165s)
             Mean action noise std: 2.34
          Mean value_function loss: 256.6166
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 49.1362
                       Mean reward: 759.63
               Mean episode length: 210.70
    Episode_Reward/reaching_object: 1.0815
     Episode_Reward/lifting_object: 146.7318
      Episode_Reward/object_height: 0.0711
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 2.24s
                      Time elapsed: 00:35:33
                               ETA: 00:36:09

################################################################################
                     [1m Learning iteration 992/2000 [0m                      

                       Computation: 48584 steps/s (collection: 1.922s, learning 0.101s)
             Mean action noise std: 2.34
          Mean value_function loss: 257.4085
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 49.1368
                       Mean reward: 781.71
               Mean episode length: 217.43
    Episode_Reward/reaching_object: 1.1024
     Episode_Reward/lifting_object: 149.8598
      Episode_Reward/object_height: 0.0730
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 2.02s
                      Time elapsed: 00:35:35
                               ETA: 00:36:07

################################################################################
                     [1m Learning iteration 993/2000 [0m                      

                       Computation: 43406 steps/s (collection: 2.118s, learning 0.147s)
             Mean action noise std: 2.34
          Mean value_function loss: 250.3533
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 49.1371
                       Mean reward: 743.09
               Mean episode length: 209.28
    Episode_Reward/reaching_object: 1.1045
     Episode_Reward/lifting_object: 150.5021
      Episode_Reward/object_height: 0.0728
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 2.26s
                      Time elapsed: 00:35:37
                               ETA: 00:36:05

################################################################################
                     [1m Learning iteration 994/2000 [0m                      

                       Computation: 47538 steps/s (collection: 1.976s, learning 0.092s)
             Mean action noise std: 2.34
          Mean value_function loss: 254.7993
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 49.1388
                       Mean reward: 745.34
               Mean episode length: 209.99
    Episode_Reward/reaching_object: 1.1239
     Episode_Reward/lifting_object: 153.7473
      Episode_Reward/object_height: 0.0745
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 2.07s
                      Time elapsed: 00:35:39
                               ETA: 00:36:03

################################################################################
                     [1m Learning iteration 995/2000 [0m                      

                       Computation: 48508 steps/s (collection: 1.912s, learning 0.114s)
             Mean action noise std: 2.34
          Mean value_function loss: 246.9802
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 49.1403
                       Mean reward: 750.44
               Mean episode length: 213.00
    Episode_Reward/reaching_object: 1.0924
     Episode_Reward/lifting_object: 148.4561
      Episode_Reward/object_height: 0.0714
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 2.03s
                      Time elapsed: 00:35:41
                               ETA: 00:36:01

################################################################################
                     [1m Learning iteration 996/2000 [0m                      

                       Computation: 44761 steps/s (collection: 2.101s, learning 0.095s)
             Mean action noise std: 2.34
          Mean value_function loss: 250.6854
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 49.1405
                       Mean reward: 700.16
               Mean episode length: 199.67
    Episode_Reward/reaching_object: 1.0948
     Episode_Reward/lifting_object: 149.2776
      Episode_Reward/object_height: 0.0724
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 2.20s
                      Time elapsed: 00:35:43
                               ETA: 00:35:58

################################################################################
                     [1m Learning iteration 997/2000 [0m                      

                       Computation: 48013 steps/s (collection: 1.938s, learning 0.110s)
             Mean action noise std: 2.34
          Mean value_function loss: 262.1145
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 49.1406
                       Mean reward: 746.78
               Mean episode length: 210.06
    Episode_Reward/reaching_object: 1.1204
     Episode_Reward/lifting_object: 152.7798
      Episode_Reward/object_height: 0.0736
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 2.05s
                      Time elapsed: 00:35:45
                               ETA: 00:35:56

################################################################################
                     [1m Learning iteration 998/2000 [0m                      

                       Computation: 48109 steps/s (collection: 1.923s, learning 0.121s)
             Mean action noise std: 2.34
          Mean value_function loss: 242.7133
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 49.1408
                       Mean reward: 752.85
               Mean episode length: 209.71
    Episode_Reward/reaching_object: 1.0772
     Episode_Reward/lifting_object: 146.5697
      Episode_Reward/object_height: 0.0703
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 2.04s
                      Time elapsed: 00:35:47
                               ETA: 00:35:54

################################################################################
                     [1m Learning iteration 999/2000 [0m                      

                       Computation: 47702 steps/s (collection: 1.935s, learning 0.126s)
             Mean action noise std: 2.34
          Mean value_function loss: 231.1205
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 49.1413
                       Mean reward: 778.52
               Mean episode length: 217.91
    Episode_Reward/reaching_object: 1.0927
     Episode_Reward/lifting_object: 149.5159
      Episode_Reward/object_height: 0.0718
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 2.06s
                      Time elapsed: 00:35:50
                               ETA: 00:35:52

################################################################################
                     [1m Learning iteration 1000/2000 [0m                     

                       Computation: 14718 steps/s (collection: 6.532s, learning 0.147s)
             Mean action noise std: 2.34
          Mean value_function loss: 237.2415
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 49.1433
                       Mean reward: 743.37
               Mean episode length: 209.93
    Episode_Reward/reaching_object: 1.1009
     Episode_Reward/lifting_object: 150.7175
      Episode_Reward/object_height: 0.0722
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 6.68s
                      Time elapsed: 00:35:56
                               ETA: 00:35:54

################################################################################
                     [1m Learning iteration 1001/2000 [0m                     

                       Computation: 14027 steps/s (collection: 6.881s, learning 0.127s)
             Mean action noise std: 2.34
          Mean value_function loss: 271.1434
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 49.1453
                       Mean reward: 750.53
               Mean episode length: 210.20
    Episode_Reward/reaching_object: 1.0841
     Episode_Reward/lifting_object: 149.2525
      Episode_Reward/object_height: 0.0707
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 7.01s
                      Time elapsed: 00:36:03
                               ETA: 00:35:57

################################################################################
                     [1m Learning iteration 1002/2000 [0m                     

                       Computation: 14155 steps/s (collection: 6.800s, learning 0.145s)
             Mean action noise std: 2.34
          Mean value_function loss: 1137348293165056.0000
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 49.1464
                       Mean reward: -40118934.18
               Mean episode length: 217.17
    Episode_Reward/reaching_object: 1.0981
     Episode_Reward/lifting_object: 150.9332
      Episode_Reward/object_height: 0.0723
        Episode_Reward/action_rate: -6884.0684
          Episode_Reward/joint_vel: -1959769.0000
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 6.94s
                      Time elapsed: 00:36:10
                               ETA: 00:35:59

################################################################################
                     [1m Learning iteration 1003/2000 [0m                     

                       Computation: 14666 steps/s (collection: 6.563s, learning 0.140s)
             Mean action noise std: 2.34
          Mean value_function loss: 233.8147
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 49.1476
                       Mean reward: 808.13
               Mean episode length: 222.32
    Episode_Reward/reaching_object: 1.1333
     Episode_Reward/lifting_object: 156.7321
      Episode_Reward/object_height: 0.0757
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 6.70s
                      Time elapsed: 00:36:17
                               ETA: 00:36:02

################################################################################
                     [1m Learning iteration 1004/2000 [0m                     

                       Computation: 14554 steps/s (collection: 6.645s, learning 0.109s)
             Mean action noise std: 2.34
          Mean value_function loss: 255.7084
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 49.1534
                       Mean reward: 779.88
               Mean episode length: 215.45
    Episode_Reward/reaching_object: 1.1398
     Episode_Reward/lifting_object: 157.8541
      Episode_Reward/object_height: 0.0762
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 6.75s
                      Time elapsed: 00:36:24
                               ETA: 00:36:04

################################################################################
                     [1m Learning iteration 1005/2000 [0m                     

                       Computation: 14396 steps/s (collection: 6.680s, learning 0.148s)
             Mean action noise std: 2.34
          Mean value_function loss: 255.3646
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 49.1568
                       Mean reward: 794.00
               Mean episode length: 219.28
    Episode_Reward/reaching_object: 1.1053
     Episode_Reward/lifting_object: 152.4184
      Episode_Reward/object_height: 0.0738
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 6.83s
                      Time elapsed: 00:36:30
                               ETA: 00:36:06

################################################################################
                     [1m Learning iteration 1006/2000 [0m                     

                       Computation: 14236 steps/s (collection: 6.791s, learning 0.115s)
             Mean action noise std: 2.34
          Mean value_function loss: 250.2015
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 49.1617
                       Mean reward: 764.96
               Mean episode length: 214.32
    Episode_Reward/reaching_object: 1.1340
     Episode_Reward/lifting_object: 156.2241
      Episode_Reward/object_height: 0.0761
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 6.91s
                      Time elapsed: 00:36:37
                               ETA: 00:36:09

################################################################################
                     [1m Learning iteration 1007/2000 [0m                     

                       Computation: 14633 steps/s (collection: 6.599s, learning 0.119s)
             Mean action noise std: 2.34
          Mean value_function loss: 248.5730
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 49.1666
                       Mean reward: 768.51
               Mean episode length: 215.17
    Episode_Reward/reaching_object: 1.1035
     Episode_Reward/lifting_object: 152.1192
      Episode_Reward/object_height: 0.0742
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 6.72s
                      Time elapsed: 00:36:44
                               ETA: 00:36:11

################################################################################
                     [1m Learning iteration 1008/2000 [0m                     

                       Computation: 16251 steps/s (collection: 5.941s, learning 0.108s)
             Mean action noise std: 2.34
          Mean value_function loss: 230.2074
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 49.1686
                       Mean reward: 799.79
               Mean episode length: 221.54
    Episode_Reward/reaching_object: 1.1416
     Episode_Reward/lifting_object: 157.8055
      Episode_Reward/object_height: 0.0767
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 6.05s
                      Time elapsed: 00:36:50
                               ETA: 00:36:13

################################################################################
                     [1m Learning iteration 1009/2000 [0m                     

                       Computation: 52152 steps/s (collection: 1.786s, learning 0.099s)
             Mean action noise std: 2.34
          Mean value_function loss: 230.1954
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 49.1708
                       Mean reward: 709.21
               Mean episode length: 200.96
    Episode_Reward/reaching_object: 1.1180
     Episode_Reward/lifting_object: 153.3388
      Episode_Reward/object_height: 0.0749
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 1.88s
                      Time elapsed: 00:36:52
                               ETA: 00:36:10

################################################################################
                     [1m Learning iteration 1010/2000 [0m                     

                       Computation: 51948 steps/s (collection: 1.807s, learning 0.085s)
             Mean action noise std: 2.34
          Mean value_function loss: 210.2179
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 49.1725
                       Mean reward: 779.29
               Mean episode length: 216.49
    Episode_Reward/reaching_object: 1.1187
     Episode_Reward/lifting_object: 153.5509
      Episode_Reward/object_height: 0.0751
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 1.89s
                      Time elapsed: 00:36:54
                               ETA: 00:36:08

################################################################################
                     [1m Learning iteration 1011/2000 [0m                     

                       Computation: 51502 steps/s (collection: 1.818s, learning 0.090s)
             Mean action noise std: 2.34
          Mean value_function loss: 217.6444
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 49.1729
                       Mean reward: 766.91
               Mean episode length: 213.28
    Episode_Reward/reaching_object: 1.1129
     Episode_Reward/lifting_object: 152.3722
      Episode_Reward/object_height: 0.0735
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 1.91s
                      Time elapsed: 00:36:56
                               ETA: 00:36:05

################################################################################
                     [1m Learning iteration 1012/2000 [0m                     

                       Computation: 48289 steps/s (collection: 1.906s, learning 0.130s)
             Mean action noise std: 2.34
          Mean value_function loss: 241.5218
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 49.1733
                       Mean reward: 805.84
               Mean episode length: 222.49
    Episode_Reward/reaching_object: 1.1308
     Episode_Reward/lifting_object: 154.6839
      Episode_Reward/object_height: 0.0753
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 2.04s
                      Time elapsed: 00:36:58
                               ETA: 00:36:03

################################################################################
                     [1m Learning iteration 1013/2000 [0m                     

                       Computation: 52165 steps/s (collection: 1.784s, learning 0.101s)
             Mean action noise std: 2.34
          Mean value_function loss: 222.2275
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 49.1736
                       Mean reward: 780.67
               Mean episode length: 217.71
    Episode_Reward/reaching_object: 1.1264
     Episode_Reward/lifting_object: 153.7480
      Episode_Reward/object_height: 0.0743
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 1.88s
                      Time elapsed: 00:37:00
                               ETA: 00:36:01

################################################################################
                     [1m Learning iteration 1014/2000 [0m                     

                       Computation: 52067 steps/s (collection: 1.768s, learning 0.120s)
             Mean action noise std: 2.34
          Mean value_function loss: 256.4085
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 49.1739
                       Mean reward: 734.00
               Mean episode length: 204.22
    Episode_Reward/reaching_object: 1.1413
     Episode_Reward/lifting_object: 156.4432
      Episode_Reward/object_height: 0.0767
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 1.89s
                      Time elapsed: 00:37:02
                               ETA: 00:35:58

################################################################################
                     [1m Learning iteration 1015/2000 [0m                     

                       Computation: 52368 steps/s (collection: 1.779s, learning 0.098s)
             Mean action noise std: 2.34
          Mean value_function loss: 237.6910
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 49.1747
                       Mean reward: 776.63
               Mean episode length: 215.00
    Episode_Reward/reaching_object: 1.1286
     Episode_Reward/lifting_object: 154.3063
      Episode_Reward/object_height: 0.0752
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 1.88s
                      Time elapsed: 00:37:03
                               ETA: 00:35:56

################################################################################
                     [1m Learning iteration 1016/2000 [0m                     

                       Computation: 52697 steps/s (collection: 1.764s, learning 0.101s)
             Mean action noise std: 2.34
          Mean value_function loss: 229.9513
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 49.1753
                       Mean reward: 818.95
               Mean episode length: 226.30
    Episode_Reward/reaching_object: 1.1351
     Episode_Reward/lifting_object: 155.9076
      Episode_Reward/object_height: 0.0756
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 1.87s
                      Time elapsed: 00:37:05
                               ETA: 00:35:53

################################################################################
                     [1m Learning iteration 1017/2000 [0m                     

                       Computation: 53317 steps/s (collection: 1.756s, learning 0.088s)
             Mean action noise std: 2.35
          Mean value_function loss: 231.1821
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 49.1759
                       Mean reward: 768.68
               Mean episode length: 212.32
    Episode_Reward/reaching_object: 1.1355
     Episode_Reward/lifting_object: 156.4735
      Episode_Reward/object_height: 0.0771
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 1.84s
                      Time elapsed: 00:37:07
                               ETA: 00:35:51

################################################################################
                     [1m Learning iteration 1018/2000 [0m                     

                       Computation: 52047 steps/s (collection: 1.799s, learning 0.090s)
             Mean action noise std: 2.35
          Mean value_function loss: 238.4504
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 49.1765
                       Mean reward: 762.07
               Mean episode length: 211.00
    Episode_Reward/reaching_object: 1.1366
     Episode_Reward/lifting_object: 157.2690
      Episode_Reward/object_height: 0.0774
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 1.89s
                      Time elapsed: 00:37:09
                               ETA: 00:35:48

################################################################################
                     [1m Learning iteration 1019/2000 [0m                     

                       Computation: 52513 steps/s (collection: 1.765s, learning 0.107s)
             Mean action noise std: 2.35
          Mean value_function loss: 272.9708
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 49.1776
                       Mean reward: 764.45
               Mean episode length: 211.43
    Episode_Reward/reaching_object: 1.0779
     Episode_Reward/lifting_object: 148.9688
      Episode_Reward/object_height: 0.0723
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 1.87s
                      Time elapsed: 00:37:11
                               ETA: 00:35:46

################################################################################
                     [1m Learning iteration 1020/2000 [0m                     

                       Computation: 52334 steps/s (collection: 1.776s, learning 0.103s)
             Mean action noise std: 2.35
          Mean value_function loss: 300.2540
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 49.1819
                       Mean reward: 756.32
               Mean episode length: 210.60
    Episode_Reward/reaching_object: 1.0515
     Episode_Reward/lifting_object: 145.0442
      Episode_Reward/object_height: 0.0705
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 1.88s
                      Time elapsed: 00:37:13
                               ETA: 00:35:43

################################################################################
                     [1m Learning iteration 1021/2000 [0m                     

                       Computation: 52084 steps/s (collection: 1.787s, learning 0.101s)
             Mean action noise std: 2.35
          Mean value_function loss: 257.2866
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 49.1891
                       Mean reward: 748.29
               Mean episode length: 208.85
    Episode_Reward/reaching_object: 1.0773
     Episode_Reward/lifting_object: 150.0690
      Episode_Reward/object_height: 0.0732
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 1.89s
                      Time elapsed: 00:37:15
                               ETA: 00:35:41

################################################################################
                     [1m Learning iteration 1022/2000 [0m                     

                       Computation: 53025 steps/s (collection: 1.740s, learning 0.114s)
             Mean action noise std: 2.35
          Mean value_function loss: 276.9593
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 49.2025
                       Mean reward: 766.63
               Mean episode length: 211.52
    Episode_Reward/reaching_object: 1.0750
     Episode_Reward/lifting_object: 149.6983
      Episode_Reward/object_height: 0.0738
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 1.85s
                      Time elapsed: 00:37:17
                               ETA: 00:35:38

################################################################################
                     [1m Learning iteration 1023/2000 [0m                     

                       Computation: 52399 steps/s (collection: 1.769s, learning 0.107s)
             Mean action noise std: 2.35
          Mean value_function loss: 286.1376
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 49.2175
                       Mean reward: 792.75
               Mean episode length: 217.19
    Episode_Reward/reaching_object: 1.1187
     Episode_Reward/lifting_object: 155.7648
      Episode_Reward/object_height: 0.0771
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 1.88s
                      Time elapsed: 00:37:18
                               ETA: 00:35:36

################################################################################
                     [1m Learning iteration 1024/2000 [0m                     

                       Computation: 51752 steps/s (collection: 1.800s, learning 0.100s)
             Mean action noise std: 2.35
          Mean value_function loss: 259.1670
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 49.2297
                       Mean reward: 778.24
               Mean episode length: 212.56
    Episode_Reward/reaching_object: 1.1087
     Episode_Reward/lifting_object: 155.3998
      Episode_Reward/object_height: 0.0774
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 1.90s
                      Time elapsed: 00:37:20
                               ETA: 00:35:33

################################################################################
                     [1m Learning iteration 1025/2000 [0m                     

                       Computation: 52737 steps/s (collection: 1.760s, learning 0.104s)
             Mean action noise std: 2.35
          Mean value_function loss: 243.8821
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 49.2341
                       Mean reward: 787.95
               Mean episode length: 219.03
    Episode_Reward/reaching_object: 1.1127
     Episode_Reward/lifting_object: 155.7215
      Episode_Reward/object_height: 0.0774
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 1.86s
                      Time elapsed: 00:37:22
                               ETA: 00:35:31

################################################################################
                     [1m Learning iteration 1026/2000 [0m                     

                       Computation: 50721 steps/s (collection: 1.841s, learning 0.097s)
             Mean action noise std: 2.35
          Mean value_function loss: 252.6460
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 49.2410
                       Mean reward: 815.24
               Mean episode length: 221.94
    Episode_Reward/reaching_object: 1.1393
     Episode_Reward/lifting_object: 159.6161
      Episode_Reward/object_height: 0.0796
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 1.94s
                      Time elapsed: 00:37:24
                               ETA: 00:35:28

################################################################################
                     [1m Learning iteration 1027/2000 [0m                     

                       Computation: 52781 steps/s (collection: 1.777s, learning 0.085s)
             Mean action noise std: 2.35
          Mean value_function loss: 247.5379
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 49.2508
                       Mean reward: 778.62
               Mean episode length: 215.11
    Episode_Reward/reaching_object: 1.1136
     Episode_Reward/lifting_object: 156.0365
      Episode_Reward/object_height: 0.0778
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 1.86s
                      Time elapsed: 00:37:26
                               ETA: 00:35:26

################################################################################
                     [1m Learning iteration 1028/2000 [0m                     

                       Computation: 51455 steps/s (collection: 1.799s, learning 0.112s)
             Mean action noise std: 2.36
          Mean value_function loss: 228.3466
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 49.2573
                       Mean reward: 836.26
               Mean episode length: 230.19
    Episode_Reward/reaching_object: 1.1382
     Episode_Reward/lifting_object: 158.1482
      Episode_Reward/object_height: 0.0785
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 1.91s
                      Time elapsed: 00:37:28
                               ETA: 00:35:23

################################################################################
                     [1m Learning iteration 1029/2000 [0m                     

                       Computation: 51672 steps/s (collection: 1.783s, learning 0.119s)
             Mean action noise std: 2.36
          Mean value_function loss: 195.5560
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 49.2668
                       Mean reward: 832.10
               Mean episode length: 227.70
    Episode_Reward/reaching_object: 1.1583
     Episode_Reward/lifting_object: 161.3186
      Episode_Reward/object_height: 0.0800
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 1.90s
                      Time elapsed: 00:37:30
                               ETA: 00:35:21

################################################################################
                     [1m Learning iteration 1030/2000 [0m                     

                       Computation: 51820 steps/s (collection: 1.796s, learning 0.101s)
             Mean action noise std: 2.36
          Mean value_function loss: 194.6824
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 49.2732
                       Mean reward: 819.43
               Mean episode length: 224.33
    Episode_Reward/reaching_object: 1.1794
     Episode_Reward/lifting_object: 164.6013
      Episode_Reward/object_height: 0.0817
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 1.90s
                      Time elapsed: 00:37:32
                               ETA: 00:35:18

################################################################################
                     [1m Learning iteration 1031/2000 [0m                     

                       Computation: 52299 steps/s (collection: 1.786s, learning 0.094s)
             Mean action noise std: 2.36
          Mean value_function loss: 229.8257
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 49.2811
                       Mean reward: 745.87
               Mean episode length: 208.53
    Episode_Reward/reaching_object: 1.1328
     Episode_Reward/lifting_object: 157.7350
      Episode_Reward/object_height: 0.0773
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 1.88s
                      Time elapsed: 00:37:34
                               ETA: 00:35:16

################################################################################
                     [1m Learning iteration 1032/2000 [0m                     

                       Computation: 52486 steps/s (collection: 1.782s, learning 0.091s)
             Mean action noise std: 2.36
          Mean value_function loss: 214.0111
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 49.2885
                       Mean reward: 801.41
               Mean episode length: 217.99
    Episode_Reward/reaching_object: 1.1304
     Episode_Reward/lifting_object: 157.4532
      Episode_Reward/object_height: 0.0766
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 1.87s
                      Time elapsed: 00:37:35
                               ETA: 00:35:14

################################################################################
                     [1m Learning iteration 1033/2000 [0m                     

                       Computation: 52938 steps/s (collection: 1.753s, learning 0.104s)
             Mean action noise std: 2.36
          Mean value_function loss: 210.1573
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.2957
                       Mean reward: 814.76
               Mean episode length: 223.92
    Episode_Reward/reaching_object: 1.1703
     Episode_Reward/lifting_object: 163.8736
      Episode_Reward/object_height: 0.0797
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 1.86s
                      Time elapsed: 00:37:37
                               ETA: 00:35:11

################################################################################
                     [1m Learning iteration 1034/2000 [0m                     

                       Computation: 51997 steps/s (collection: 1.779s, learning 0.112s)
             Mean action noise std: 2.36
          Mean value_function loss: 326.7673
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 49.3026
                       Mean reward: 833.95
               Mean episode length: 229.93
    Episode_Reward/reaching_object: 1.1835
     Episode_Reward/lifting_object: 165.8821
      Episode_Reward/object_height: 0.0809
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 1.89s
                      Time elapsed: 00:37:39
                               ETA: 00:35:09

################################################################################
                     [1m Learning iteration 1035/2000 [0m                     

                       Computation: 52179 steps/s (collection: 1.770s, learning 0.114s)
             Mean action noise std: 2.36
          Mean value_function loss: 211.0149
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 49.3086
                       Mean reward: 851.03
               Mean episode length: 234.54
    Episode_Reward/reaching_object: 1.1764
     Episode_Reward/lifting_object: 164.3659
      Episode_Reward/object_height: 0.0797
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 1.88s
                      Time elapsed: 00:37:41
                               ETA: 00:35:06

################################################################################
                     [1m Learning iteration 1036/2000 [0m                     

                       Computation: 51095 steps/s (collection: 1.813s, learning 0.111s)
             Mean action noise std: 2.36
          Mean value_function loss: 216.3121
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 49.3109
                       Mean reward: 858.82
               Mean episode length: 233.90
    Episode_Reward/reaching_object: 1.1531
     Episode_Reward/lifting_object: 160.8817
      Episode_Reward/object_height: 0.0777
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 1.92s
                      Time elapsed: 00:37:43
                               ETA: 00:35:04

################################################################################
                     [1m Learning iteration 1037/2000 [0m                     

                       Computation: 52584 steps/s (collection: 1.781s, learning 0.089s)
             Mean action noise std: 2.36
          Mean value_function loss: 201.4875
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 49.3135
                       Mean reward: 764.82
               Mean episode length: 214.02
    Episode_Reward/reaching_object: 1.1356
     Episode_Reward/lifting_object: 157.4442
      Episode_Reward/object_height: 0.0761
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 1.87s
                      Time elapsed: 00:37:45
                               ETA: 00:35:01

################################################################################
                     [1m Learning iteration 1038/2000 [0m                     

                       Computation: 51554 steps/s (collection: 1.776s, learning 0.131s)
             Mean action noise std: 2.36
          Mean value_function loss: 171.1947
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 49.3205
                       Mean reward: 788.07
               Mean episode length: 216.72
    Episode_Reward/reaching_object: 1.1529
     Episode_Reward/lifting_object: 161.5080
      Episode_Reward/object_height: 0.0774
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 1.91s
                      Time elapsed: 00:37:47
                               ETA: 00:34:59

################################################################################
                     [1m Learning iteration 1039/2000 [0m                     

                       Computation: 51269 steps/s (collection: 1.762s, learning 0.156s)
             Mean action noise std: 2.36
          Mean value_function loss: 189.0301
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 49.3276
                       Mean reward: 759.10
               Mean episode length: 209.29
    Episode_Reward/reaching_object: 1.1431
     Episode_Reward/lifting_object: 160.3660
      Episode_Reward/object_height: 0.0766
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 1.92s
                      Time elapsed: 00:37:49
                               ETA: 00:34:56

################################################################################
                     [1m Learning iteration 1040/2000 [0m                     

                       Computation: 51969 steps/s (collection: 1.785s, learning 0.107s)
             Mean action noise std: 2.37
          Mean value_function loss: 183.4676
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 49.3330
                       Mean reward: 796.99
               Mean episode length: 220.15
    Episode_Reward/reaching_object: 1.1364
     Episode_Reward/lifting_object: 159.2537
      Episode_Reward/object_height: 0.0760
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 1.89s
                      Time elapsed: 00:37:51
                               ETA: 00:34:54

################################################################################
                     [1m Learning iteration 1041/2000 [0m                     

                       Computation: 52291 steps/s (collection: 1.775s, learning 0.105s)
             Mean action noise std: 2.37
          Mean value_function loss: 212.5496
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 49.3347
                       Mean reward: 816.02
               Mean episode length: 223.26
    Episode_Reward/reaching_object: 1.1505
     Episode_Reward/lifting_object: 162.0253
      Episode_Reward/object_height: 0.0774
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 1.88s
                      Time elapsed: 00:37:52
                               ETA: 00:34:51

################################################################################
                     [1m Learning iteration 1042/2000 [0m                     

                       Computation: 50373 steps/s (collection: 1.802s, learning 0.149s)
             Mean action noise std: 2.37
          Mean value_function loss: 191.0220
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 49.3350
                       Mean reward: 833.06
               Mean episode length: 228.26
    Episode_Reward/reaching_object: 1.1597
     Episode_Reward/lifting_object: 163.1117
      Episode_Reward/object_height: 0.0775
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 1.95s
                      Time elapsed: 00:37:54
                               ETA: 00:34:49

################################################################################
                     [1m Learning iteration 1043/2000 [0m                     

                       Computation: 50006 steps/s (collection: 1.867s, learning 0.099s)
             Mean action noise std: 2.37
          Mean value_function loss: 166.5117
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 49.3411
                       Mean reward: 850.97
               Mean episode length: 231.08
    Episode_Reward/reaching_object: 1.1617
     Episode_Reward/lifting_object: 163.5462
      Episode_Reward/object_height: 0.0783
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 1.97s
                      Time elapsed: 00:37:56
                               ETA: 00:34:47

################################################################################
                     [1m Learning iteration 1044/2000 [0m                     

                       Computation: 51435 steps/s (collection: 1.826s, learning 0.086s)
             Mean action noise std: 2.37
          Mean value_function loss: 177.2502
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 49.3437
                       Mean reward: 835.64
               Mean episode length: 229.16
    Episode_Reward/reaching_object: 1.1708
     Episode_Reward/lifting_object: 164.4895
      Episode_Reward/object_height: 0.0785
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 1.91s
                      Time elapsed: 00:37:58
                               ETA: 00:34:44

################################################################################
                     [1m Learning iteration 1045/2000 [0m                     

                       Computation: 50715 steps/s (collection: 1.836s, learning 0.103s)
             Mean action noise std: 2.37
          Mean value_function loss: 152.5542
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 49.3478
                       Mean reward: 830.56
               Mean episode length: 226.71
    Episode_Reward/reaching_object: 1.1856
     Episode_Reward/lifting_object: 166.4977
      Episode_Reward/object_height: 0.0789
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 1.94s
                      Time elapsed: 00:38:00
                               ETA: 00:34:42

################################################################################
                     [1m Learning iteration 1046/2000 [0m                     

                       Computation: 50558 steps/s (collection: 1.844s, learning 0.101s)
             Mean action noise std: 2.37
          Mean value_function loss: 173.8236
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 49.3503
                       Mean reward: 838.80
               Mean episode length: 228.64
    Episode_Reward/reaching_object: 1.1688
     Episode_Reward/lifting_object: 163.9678
      Episode_Reward/object_height: 0.0772
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 1.94s
                      Time elapsed: 00:38:02
                               ETA: 00:34:39

################################################################################
                     [1m Learning iteration 1047/2000 [0m                     

                       Computation: 49840 steps/s (collection: 1.862s, learning 0.111s)
             Mean action noise std: 2.37
          Mean value_function loss: 168.2430
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 49.3511
                       Mean reward: 811.91
               Mean episode length: 221.29
    Episode_Reward/reaching_object: 1.1792
     Episode_Reward/lifting_object: 166.0703
      Episode_Reward/object_height: 0.0783
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 1.97s
                      Time elapsed: 00:38:04
                               ETA: 00:34:37

################################################################################
                     [1m Learning iteration 1048/2000 [0m                     

                       Computation: 47794 steps/s (collection: 1.929s, learning 0.128s)
             Mean action noise std: 2.37
          Mean value_function loss: 162.0550
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 49.3518
                       Mean reward: 810.49
               Mean episode length: 222.22
    Episode_Reward/reaching_object: 1.1799
     Episode_Reward/lifting_object: 165.5001
      Episode_Reward/object_height: 0.0778
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 2.06s
                      Time elapsed: 00:38:06
                               ETA: 00:34:35

################################################################################
                     [1m Learning iteration 1049/2000 [0m                     

                       Computation: 49413 steps/s (collection: 1.901s, learning 0.089s)
             Mean action noise std: 2.37
          Mean value_function loss: 142.9408
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 49.3527
                       Mean reward: 837.30
               Mean episode length: 230.93
    Episode_Reward/reaching_object: 1.2186
     Episode_Reward/lifting_object: 171.1832
      Episode_Reward/object_height: 0.0806
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 1.99s
                      Time elapsed: 00:38:08
                               ETA: 00:34:32

################################################################################
                     [1m Learning iteration 1050/2000 [0m                     

                       Computation: 49845 steps/s (collection: 1.866s, learning 0.106s)
             Mean action noise std: 2.37
          Mean value_function loss: 179.2618
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 49.3535
                       Mean reward: 801.65
               Mean episode length: 220.23
    Episode_Reward/reaching_object: 1.1399
     Episode_Reward/lifting_object: 159.6623
      Episode_Reward/object_height: 0.0740
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 1.97s
                      Time elapsed: 00:38:10
                               ETA: 00:34:30

################################################################################
                     [1m Learning iteration 1051/2000 [0m                     

                       Computation: 49778 steps/s (collection: 1.850s, learning 0.125s)
             Mean action noise std: 2.37
          Mean value_function loss: 198.7156
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 49.3545
                       Mean reward: 818.09
               Mean episode length: 224.14
    Episode_Reward/reaching_object: 1.1523
     Episode_Reward/lifting_object: 161.4957
      Episode_Reward/object_height: 0.0751
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 1.97s
                      Time elapsed: 00:38:12
                               ETA: 00:34:28

################################################################################
                     [1m Learning iteration 1052/2000 [0m                     

                       Computation: 49253 steps/s (collection: 1.880s, learning 0.116s)
             Mean action noise std: 2.37
          Mean value_function loss: 223.8318
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 49.3571
                       Mean reward: 780.34
               Mean episode length: 215.19
    Episode_Reward/reaching_object: 1.0976
     Episode_Reward/lifting_object: 153.2758
      Episode_Reward/object_height: 0.0705
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 2.00s
                      Time elapsed: 00:38:14
                               ETA: 00:34:25

################################################################################
                     [1m Learning iteration 1053/2000 [0m                     

                       Computation: 49840 steps/s (collection: 1.874s, learning 0.098s)
             Mean action noise std: 2.37
          Mean value_function loss: 187.9066
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 49.3668
                       Mean reward: 789.13
               Mean episode length: 218.11
    Episode_Reward/reaching_object: 1.1386
     Episode_Reward/lifting_object: 159.4424
      Episode_Reward/object_height: 0.0733
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 1.97s
                      Time elapsed: 00:38:16
                               ETA: 00:34:23

################################################################################
                     [1m Learning iteration 1054/2000 [0m                     

                       Computation: 49931 steps/s (collection: 1.867s, learning 0.102s)
             Mean action noise std: 2.37
          Mean value_function loss: 177.3680
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 49.3756
                       Mean reward: 824.10
               Mean episode length: 225.60
    Episode_Reward/reaching_object: 1.1590
     Episode_Reward/lifting_object: 162.9129
      Episode_Reward/object_height: 0.0755
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 1.97s
                      Time elapsed: 00:38:18
                               ETA: 00:34:21

################################################################################
                     [1m Learning iteration 1055/2000 [0m                     

                       Computation: 50915 steps/s (collection: 1.835s, learning 0.096s)
             Mean action noise std: 2.37
          Mean value_function loss: 172.8605
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 49.3821
                       Mean reward: 830.93
               Mean episode length: 226.49
    Episode_Reward/reaching_object: 1.1650
     Episode_Reward/lifting_object: 164.3565
      Episode_Reward/object_height: 0.0760
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 1.93s
                      Time elapsed: 00:38:20
                               ETA: 00:34:18

################################################################################
                     [1m Learning iteration 1056/2000 [0m                     

                       Computation: 47711 steps/s (collection: 1.899s, learning 0.162s)
             Mean action noise std: 2.37
          Mean value_function loss: 171.6947
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 49.3900
                       Mean reward: 807.64
               Mean episode length: 221.90
    Episode_Reward/reaching_object: 1.1318
     Episode_Reward/lifting_object: 158.8522
      Episode_Reward/object_height: 0.0735
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 2.06s
                      Time elapsed: 00:38:22
                               ETA: 00:34:16

################################################################################
                     [1m Learning iteration 1057/2000 [0m                     

                       Computation: 49686 steps/s (collection: 1.879s, learning 0.099s)
             Mean action noise std: 2.37
          Mean value_function loss: 183.3870
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 49.3967
                       Mean reward: 819.21
               Mean episode length: 224.69
    Episode_Reward/reaching_object: 1.1953
     Episode_Reward/lifting_object: 168.4809
      Episode_Reward/object_height: 0.0776
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 1.98s
                      Time elapsed: 00:38:24
                               ETA: 00:34:14

################################################################################
                     [1m Learning iteration 1058/2000 [0m                     

                       Computation: 49625 steps/s (collection: 1.867s, learning 0.114s)
             Mean action noise std: 2.37
          Mean value_function loss: 169.9404
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 49.3994
                       Mean reward: 819.73
               Mean episode length: 226.93
    Episode_Reward/reaching_object: 1.1507
     Episode_Reward/lifting_object: 161.0549
      Episode_Reward/object_height: 0.0746
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 1.98s
                      Time elapsed: 00:38:26
                               ETA: 00:34:11

################################################################################
                     [1m Learning iteration 1059/2000 [0m                     

                       Computation: 47678 steps/s (collection: 1.929s, learning 0.133s)
             Mean action noise std: 2.37
          Mean value_function loss: 158.1311
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 49.4003
                       Mean reward: 797.52
               Mean episode length: 219.00
    Episode_Reward/reaching_object: 1.1623
     Episode_Reward/lifting_object: 163.1956
      Episode_Reward/object_height: 0.0749
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 2.06s
                      Time elapsed: 00:38:28
                               ETA: 00:34:09

################################################################################
                     [1m Learning iteration 1060/2000 [0m                     

                       Computation: 49014 steps/s (collection: 1.889s, learning 0.117s)
             Mean action noise std: 2.38
          Mean value_function loss: 170.6048
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 49.4006
                       Mean reward: 789.92
               Mean episode length: 216.98
    Episode_Reward/reaching_object: 1.1307
     Episode_Reward/lifting_object: 158.6201
      Episode_Reward/object_height: 0.0726
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 2.01s
                      Time elapsed: 00:38:30
                               ETA: 00:34:07

################################################################################
                     [1m Learning iteration 1061/2000 [0m                     

                       Computation: 49836 steps/s (collection: 1.819s, learning 0.153s)
             Mean action noise std: 2.38
          Mean value_function loss: 152.8701
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 49.4020
                       Mean reward: 773.24
               Mean episode length: 212.30
    Episode_Reward/reaching_object: 1.1598
     Episode_Reward/lifting_object: 162.3933
      Episode_Reward/object_height: 0.0749
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 1.97s
                      Time elapsed: 00:38:32
                               ETA: 00:34:04

################################################################################
                     [1m Learning iteration 1062/2000 [0m                     

                       Computation: 46732 steps/s (collection: 1.976s, learning 0.128s)
             Mean action noise std: 2.38
          Mean value_function loss: 146.4377
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 49.4076
                       Mean reward: 801.00
               Mean episode length: 219.72
    Episode_Reward/reaching_object: 1.1925
     Episode_Reward/lifting_object: 167.6760
      Episode_Reward/object_height: 0.0763
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 18.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 2.10s
                      Time elapsed: 00:38:34
                               ETA: 00:34:02

################################################################################
                     [1m Learning iteration 1063/2000 [0m                     

                       Computation: 45981 steps/s (collection: 2.004s, learning 0.134s)
             Mean action noise std: 2.38
          Mean value_function loss: 142.8450
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 49.4111
                       Mean reward: 821.74
               Mean episode length: 224.16
    Episode_Reward/reaching_object: 1.1936
     Episode_Reward/lifting_object: 168.0340
      Episode_Reward/object_height: 0.0767
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 2.14s
                      Time elapsed: 00:38:36
                               ETA: 00:34:00

################################################################################
                     [1m Learning iteration 1064/2000 [0m                     

                       Computation: 48273 steps/s (collection: 1.942s, learning 0.095s)
             Mean action noise std: 2.38
          Mean value_function loss: 152.3970
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 49.4124
                       Mean reward: 788.00
               Mean episode length: 219.45
    Episode_Reward/reaching_object: 1.1512
     Episode_Reward/lifting_object: 160.8954
      Episode_Reward/object_height: 0.0732
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 2.04s
                      Time elapsed: 00:38:38
                               ETA: 00:33:57

################################################################################
                     [1m Learning iteration 1065/2000 [0m                     

                       Computation: 47188 steps/s (collection: 1.982s, learning 0.102s)
             Mean action noise std: 2.38
          Mean value_function loss: 173.2158
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 49.4129
                       Mean reward: 844.31
               Mean episode length: 229.42
    Episode_Reward/reaching_object: 1.1658
     Episode_Reward/lifting_object: 163.8829
      Episode_Reward/object_height: 0.0738
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 2.08s
                      Time elapsed: 00:38:40
                               ETA: 00:33:55

################################################################################
                     [1m Learning iteration 1066/2000 [0m                     

                       Computation: 48150 steps/s (collection: 1.936s, learning 0.106s)
             Mean action noise std: 2.38
          Mean value_function loss: 167.9550
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 49.4134
                       Mean reward: 845.95
               Mean episode length: 229.59
    Episode_Reward/reaching_object: 1.1737
     Episode_Reward/lifting_object: 165.3022
      Episode_Reward/object_height: 0.0747
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 2.04s
                      Time elapsed: 00:38:42
                               ETA: 00:33:53

################################################################################
                     [1m Learning iteration 1067/2000 [0m                     

                       Computation: 50280 steps/s (collection: 1.869s, learning 0.087s)
             Mean action noise std: 2.38
          Mean value_function loss: 154.8167
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 49.4143
                       Mean reward: 840.43
               Mean episode length: 228.94
    Episode_Reward/reaching_object: 1.1639
     Episode_Reward/lifting_object: 164.1447
      Episode_Reward/object_height: 0.0739
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 1.96s
                      Time elapsed: 00:38:44
                               ETA: 00:33:51

################################################################################
                     [1m Learning iteration 1068/2000 [0m                     

                       Computation: 52023 steps/s (collection: 1.801s, learning 0.089s)
             Mean action noise std: 2.38
          Mean value_function loss: 149.1892
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 49.4183
                       Mean reward: 825.35
               Mean episode length: 226.15
    Episode_Reward/reaching_object: 1.1674
     Episode_Reward/lifting_object: 164.0170
      Episode_Reward/object_height: 0.0739
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 1.89s
                      Time elapsed: 00:38:46
                               ETA: 00:33:48

################################################################################
                     [1m Learning iteration 1069/2000 [0m                     

                       Computation: 51304 steps/s (collection: 1.819s, learning 0.098s)
             Mean action noise std: 2.38
          Mean value_function loss: 158.2903
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 49.4258
                       Mean reward: 849.77
               Mean episode length: 231.32
    Episode_Reward/reaching_object: 1.1702
     Episode_Reward/lifting_object: 164.1945
      Episode_Reward/object_height: 0.0738
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 1.92s
                      Time elapsed: 00:38:48
                               ETA: 00:33:46

################################################################################
                     [1m Learning iteration 1070/2000 [0m                     

                       Computation: 50710 steps/s (collection: 1.814s, learning 0.125s)
             Mean action noise std: 2.38
          Mean value_function loss: 138.9084
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 49.4301
                       Mean reward: 832.38
               Mean episode length: 225.07
    Episode_Reward/reaching_object: 1.1603
     Episode_Reward/lifting_object: 162.8281
      Episode_Reward/object_height: 0.0723
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 1.94s
                      Time elapsed: 00:38:50
                               ETA: 00:33:43

################################################################################
                     [1m Learning iteration 1071/2000 [0m                     

                       Computation: 50601 steps/s (collection: 1.843s, learning 0.100s)
             Mean action noise std: 2.38
          Mean value_function loss: 125.5948
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 49.4363
                       Mean reward: 873.67
               Mean episode length: 237.21
    Episode_Reward/reaching_object: 1.2069
     Episode_Reward/lifting_object: 169.4760
      Episode_Reward/object_height: 0.0765
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 1.94s
                      Time elapsed: 00:38:52
                               ETA: 00:33:41

################################################################################
                     [1m Learning iteration 1072/2000 [0m                     

                       Computation: 51652 steps/s (collection: 1.801s, learning 0.103s)
             Mean action noise std: 2.38
          Mean value_function loss: 133.6689
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 49.4423
                       Mean reward: 794.47
               Mean episode length: 219.58
    Episode_Reward/reaching_object: 1.1840
     Episode_Reward/lifting_object: 165.1801
      Episode_Reward/object_height: 0.0738
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 1.90s
                      Time elapsed: 00:38:54
                               ETA: 00:33:39

################################################################################
                     [1m Learning iteration 1073/2000 [0m                     

                       Computation: 51224 steps/s (collection: 1.780s, learning 0.140s)
             Mean action noise std: 2.38
          Mean value_function loss: 124.4504
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 49.4512
                       Mean reward: 834.92
               Mean episode length: 230.63
    Episode_Reward/reaching_object: 1.2052
     Episode_Reward/lifting_object: 169.3442
      Episode_Reward/object_height: 0.0758
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 1.92s
                      Time elapsed: 00:38:56
                               ETA: 00:33:36

################################################################################
                     [1m Learning iteration 1074/2000 [0m                     

                       Computation: 52880 steps/s (collection: 1.769s, learning 0.090s)
             Mean action noise std: 2.38
          Mean value_function loss: 130.9819
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 49.4586
                       Mean reward: 904.13
               Mean episode length: 243.00
    Episode_Reward/reaching_object: 1.2051
     Episode_Reward/lifting_object: 168.4010
      Episode_Reward/object_height: 0.0751
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 1.86s
                      Time elapsed: 00:38:58
                               ETA: 00:33:34

################################################################################
                     [1m Learning iteration 1075/2000 [0m                     

                       Computation: 50874 steps/s (collection: 1.814s, learning 0.119s)
             Mean action noise std: 2.38
          Mean value_function loss: 138.8930
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 49.4633
                       Mean reward: 867.92
               Mean episode length: 235.69
    Episode_Reward/reaching_object: 1.2012
     Episode_Reward/lifting_object: 168.0417
      Episode_Reward/object_height: 0.0744
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 1.93s
                      Time elapsed: 00:39:00
                               ETA: 00:33:31

################################################################################
                     [1m Learning iteration 1076/2000 [0m                     

                       Computation: 50878 steps/s (collection: 1.816s, learning 0.117s)
             Mean action noise std: 2.39
          Mean value_function loss: 163.1672
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 49.4680
                       Mean reward: 849.04
               Mean episode length: 229.22
    Episode_Reward/reaching_object: 1.1946
     Episode_Reward/lifting_object: 167.7033
      Episode_Reward/object_height: 0.0737
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 1.93s
                      Time elapsed: 00:39:02
                               ETA: 00:33:29

################################################################################
                     [1m Learning iteration 1077/2000 [0m                     

                       Computation: 51532 steps/s (collection: 1.793s, learning 0.115s)
             Mean action noise std: 2.39
          Mean value_function loss: 182.2671
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 49.4731
                       Mean reward: 804.33
               Mean episode length: 218.72
    Episode_Reward/reaching_object: 1.1706
     Episode_Reward/lifting_object: 163.7957
      Episode_Reward/object_height: 0.0718
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 1.91s
                      Time elapsed: 00:39:04
                               ETA: 00:33:27

################################################################################
                     [1m Learning iteration 1078/2000 [0m                     

                       Computation: 51645 steps/s (collection: 1.783s, learning 0.121s)
             Mean action noise std: 2.39
          Mean value_function loss: 165.0198
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 49.4856
                       Mean reward: 846.65
               Mean episode length: 229.55
    Episode_Reward/reaching_object: 1.1922
     Episode_Reward/lifting_object: 166.3298
      Episode_Reward/object_height: 0.0730
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 1.90s
                      Time elapsed: 00:39:05
                               ETA: 00:33:24

################################################################################
                     [1m Learning iteration 1079/2000 [0m                     

                       Computation: 52653 steps/s (collection: 1.769s, learning 0.098s)
             Mean action noise std: 2.39
          Mean value_function loss: 133.4782
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 49.4974
                       Mean reward: 840.28
               Mean episode length: 227.89
    Episode_Reward/reaching_object: 1.1713
     Episode_Reward/lifting_object: 162.8739
      Episode_Reward/object_height: 0.0718
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 1.87s
                      Time elapsed: 00:39:07
                               ETA: 00:33:22

################################################################################
                     [1m Learning iteration 1080/2000 [0m                     

                       Computation: 51955 steps/s (collection: 1.789s, learning 0.103s)
             Mean action noise std: 2.39
          Mean value_function loss: 136.3171
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 49.5053
                       Mean reward: 820.08
               Mean episode length: 223.01
    Episode_Reward/reaching_object: 1.1748
     Episode_Reward/lifting_object: 164.0420
      Episode_Reward/object_height: 0.0721
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 1.89s
                      Time elapsed: 00:39:09
                               ETA: 00:33:19

################################################################################
                     [1m Learning iteration 1081/2000 [0m                     

                       Computation: 52040 steps/s (collection: 1.771s, learning 0.118s)
             Mean action noise std: 2.39
          Mean value_function loss: 145.5080
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 49.5156
                       Mean reward: 869.85
               Mean episode length: 233.97
    Episode_Reward/reaching_object: 1.1833
     Episode_Reward/lifting_object: 165.2677
      Episode_Reward/object_height: 0.0717
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 1.89s
                      Time elapsed: 00:39:11
                               ETA: 00:33:17

################################################################################
                     [1m Learning iteration 1082/2000 [0m                     

                       Computation: 51957 steps/s (collection: 1.777s, learning 0.115s)
             Mean action noise std: 2.39
          Mean value_function loss: 172.9799
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 49.5223
                       Mean reward: 816.10
               Mean episode length: 222.56
    Episode_Reward/reaching_object: 1.1764
     Episode_Reward/lifting_object: 164.1452
      Episode_Reward/object_height: 0.0708
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 1.89s
                      Time elapsed: 00:39:13
                               ETA: 00:33:14

################################################################################
                     [1m Learning iteration 1083/2000 [0m                     

                       Computation: 51607 steps/s (collection: 1.785s, learning 0.120s)
             Mean action noise std: 2.39
          Mean value_function loss: 129.3748
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 49.5266
                       Mean reward: 831.54
               Mean episode length: 226.36
    Episode_Reward/reaching_object: 1.1781
     Episode_Reward/lifting_object: 164.4374
      Episode_Reward/object_height: 0.0707
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 1.90s
                      Time elapsed: 00:39:15
                               ETA: 00:33:12

################################################################################
                     [1m Learning iteration 1084/2000 [0m                     

                       Computation: 51743 steps/s (collection: 1.792s, learning 0.108s)
             Mean action noise std: 2.39
          Mean value_function loss: 136.8738
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 49.5289
                       Mean reward: 872.21
               Mean episode length: 235.32
    Episode_Reward/reaching_object: 1.2162
     Episode_Reward/lifting_object: 170.2566
      Episode_Reward/object_height: 0.0734
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 1.90s
                      Time elapsed: 00:39:17
                               ETA: 00:33:10

################################################################################
                     [1m Learning iteration 1085/2000 [0m                     

                       Computation: 52057 steps/s (collection: 1.785s, learning 0.104s)
             Mean action noise std: 2.39
          Mean value_function loss: 148.5833
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 49.5303
                       Mean reward: 854.89
               Mean episode length: 230.49
    Episode_Reward/reaching_object: 1.1883
     Episode_Reward/lifting_object: 165.6434
      Episode_Reward/object_height: 0.0707
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 1.89s
                      Time elapsed: 00:39:19
                               ETA: 00:33:07

################################################################################
                     [1m Learning iteration 1086/2000 [0m                     

                       Computation: 52188 steps/s (collection: 1.794s, learning 0.090s)
             Mean action noise std: 2.39
          Mean value_function loss: 134.2315
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 49.5366
                       Mean reward: 823.67
               Mean episode length: 223.55
    Episode_Reward/reaching_object: 1.2233
     Episode_Reward/lifting_object: 171.3939
      Episode_Reward/object_height: 0.0730
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 1.88s
                      Time elapsed: 00:39:21
                               ETA: 00:33:05

################################################################################
                     [1m Learning iteration 1087/2000 [0m                     

                       Computation: 50827 steps/s (collection: 1.812s, learning 0.122s)
             Mean action noise std: 2.40
          Mean value_function loss: 134.3807
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 49.5419
                       Mean reward: 874.65
               Mean episode length: 235.56
    Episode_Reward/reaching_object: 1.2211
     Episode_Reward/lifting_object: 171.2463
      Episode_Reward/object_height: 0.0724
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 1.93s
                      Time elapsed: 00:39:23
                               ETA: 00:33:02

################################################################################
                     [1m Learning iteration 1088/2000 [0m                     

                       Computation: 52171 steps/s (collection: 1.798s, learning 0.086s)
             Mean action noise std: 2.40
          Mean value_function loss: 171.0129
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 49.5536
                       Mean reward: 836.16
               Mean episode length: 226.17
    Episode_Reward/reaching_object: 1.1955
     Episode_Reward/lifting_object: 167.0433
      Episode_Reward/object_height: 0.0708
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 1.88s
                      Time elapsed: 00:39:24
                               ETA: 00:33:00

################################################################################
                     [1m Learning iteration 1089/2000 [0m                     

                       Computation: 51921 steps/s (collection: 1.784s, learning 0.109s)
             Mean action noise std: 2.40
          Mean value_function loss: 128.4670
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 49.5677
                       Mean reward: 861.69
               Mean episode length: 232.88
    Episode_Reward/reaching_object: 1.2107
     Episode_Reward/lifting_object: 169.7432
      Episode_Reward/object_height: 0.0715
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 1.89s
                      Time elapsed: 00:39:26
                               ETA: 00:32:58

################################################################################
                     [1m Learning iteration 1090/2000 [0m                     

                       Computation: 52072 steps/s (collection: 1.776s, learning 0.112s)
             Mean action noise std: 2.40
          Mean value_function loss: 167.1115
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 49.5785
                       Mean reward: 858.08
               Mean episode length: 233.93
    Episode_Reward/reaching_object: 1.1818
     Episode_Reward/lifting_object: 164.6078
      Episode_Reward/object_height: 0.0691
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 1.89s
                      Time elapsed: 00:39:28
                               ETA: 00:32:55

################################################################################
                     [1m Learning iteration 1091/2000 [0m                     

                       Computation: 51479 steps/s (collection: 1.793s, learning 0.116s)
             Mean action noise std: 2.40
          Mean value_function loss: 141.5665
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 49.5934
                       Mean reward: 830.09
               Mean episode length: 225.47
    Episode_Reward/reaching_object: 1.1836
     Episode_Reward/lifting_object: 165.1535
      Episode_Reward/object_height: 0.0691
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 1.91s
                      Time elapsed: 00:39:30
                               ETA: 00:32:53

################################################################################
                     [1m Learning iteration 1092/2000 [0m                     

                       Computation: 51456 steps/s (collection: 1.797s, learning 0.113s)
             Mean action noise std: 2.40
          Mean value_function loss: 122.0135
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 49.6020
                       Mean reward: 837.17
               Mean episode length: 227.58
    Episode_Reward/reaching_object: 1.1737
     Episode_Reward/lifting_object: 162.9562
      Episode_Reward/object_height: 0.0686
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 1.91s
                      Time elapsed: 00:39:32
                               ETA: 00:32:50

################################################################################
                     [1m Learning iteration 1093/2000 [0m                     

                       Computation: 51782 steps/s (collection: 1.789s, learning 0.110s)
             Mean action noise std: 2.41
          Mean value_function loss: 119.5695
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 49.6126
                       Mean reward: 868.80
               Mean episode length: 234.44
    Episode_Reward/reaching_object: 1.2342
     Episode_Reward/lifting_object: 172.8492
      Episode_Reward/object_height: 0.0725
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 1.90s
                      Time elapsed: 00:39:34
                               ETA: 00:32:48

################################################################################
                     [1m Learning iteration 1094/2000 [0m                     

                       Computation: 51708 steps/s (collection: 1.782s, learning 0.119s)
             Mean action noise std: 2.41
          Mean value_function loss: 130.0376
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 49.6246
                       Mean reward: 856.05
               Mean episode length: 233.20
    Episode_Reward/reaching_object: 1.2228
     Episode_Reward/lifting_object: 170.8518
      Episode_Reward/object_height: 0.0716
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 1.90s
                      Time elapsed: 00:39:36
                               ETA: 00:32:46

################################################################################
                     [1m Learning iteration 1095/2000 [0m                     

                       Computation: 52352 steps/s (collection: 1.765s, learning 0.113s)
             Mean action noise std: 2.41
          Mean value_function loss: 129.0070
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 49.6373
                       Mean reward: 893.88
               Mean episode length: 240.07
    Episode_Reward/reaching_object: 1.2327
     Episode_Reward/lifting_object: 172.3990
      Episode_Reward/object_height: 0.0724
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 1.88s
                      Time elapsed: 00:39:38
                               ETA: 00:32:43

################################################################################
                     [1m Learning iteration 1096/2000 [0m                     

                       Computation: 51232 steps/s (collection: 1.801s, learning 0.118s)
             Mean action noise std: 2.41
          Mean value_function loss: 120.5576
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 49.6451
                       Mean reward: 830.51
               Mean episode length: 226.90
    Episode_Reward/reaching_object: 1.2182
     Episode_Reward/lifting_object: 169.3254
      Episode_Reward/object_height: 0.0710
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 1.92s
                      Time elapsed: 00:39:40
                               ETA: 00:32:41

################################################################################
                     [1m Learning iteration 1097/2000 [0m                     

                       Computation: 52138 steps/s (collection: 1.786s, learning 0.100s)
             Mean action noise std: 2.41
          Mean value_function loss: 128.1539
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 49.6498
                       Mean reward: 808.84
               Mean episode length: 219.65
    Episode_Reward/reaching_object: 1.1850
     Episode_Reward/lifting_object: 164.6053
      Episode_Reward/object_height: 0.0688
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 1.89s
                      Time elapsed: 00:39:41
                               ETA: 00:32:38

################################################################################
                     [1m Learning iteration 1098/2000 [0m                     

                       Computation: 51333 steps/s (collection: 1.799s, learning 0.116s)
             Mean action noise std: 2.41
          Mean value_function loss: 113.5394
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 49.6574
                       Mean reward: 887.18
               Mean episode length: 238.45
    Episode_Reward/reaching_object: 1.2154
     Episode_Reward/lifting_object: 169.0390
      Episode_Reward/object_height: 0.0709
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 1.92s
                      Time elapsed: 00:39:43
                               ETA: 00:32:36

################################################################################
                     [1m Learning iteration 1099/2000 [0m                     

                       Computation: 51743 steps/s (collection: 1.793s, learning 0.107s)
             Mean action noise std: 2.41
          Mean value_function loss: 139.8686
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 49.6651
                       Mean reward: 805.44
               Mean episode length: 221.30
    Episode_Reward/reaching_object: 1.2059
     Episode_Reward/lifting_object: 167.3365
      Episode_Reward/object_height: 0.0701
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 1.90s
                      Time elapsed: 00:39:45
                               ETA: 00:32:34

################################################################################
                     [1m Learning iteration 1100/2000 [0m                     

                       Computation: 52165 steps/s (collection: 1.792s, learning 0.092s)
             Mean action noise std: 2.41
          Mean value_function loss: 163.5498
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 49.6741
                       Mean reward: 827.92
               Mean episode length: 226.63
    Episode_Reward/reaching_object: 1.1894
     Episode_Reward/lifting_object: 164.5354
      Episode_Reward/object_height: 0.0692
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 1.88s
                      Time elapsed: 00:39:47
                               ETA: 00:32:31

################################################################################
                     [1m Learning iteration 1101/2000 [0m                     

                       Computation: 51709 steps/s (collection: 1.787s, learning 0.114s)
             Mean action noise std: 2.41
          Mean value_function loss: 154.2715
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 49.6830
                       Mean reward: 842.09
               Mean episode length: 227.88
    Episode_Reward/reaching_object: 1.1953
     Episode_Reward/lifting_object: 165.9712
      Episode_Reward/object_height: 0.0694
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 1.90s
                      Time elapsed: 00:39:49
                               ETA: 00:32:29

################################################################################
                     [1m Learning iteration 1102/2000 [0m                     

                       Computation: 52296 steps/s (collection: 1.773s, learning 0.107s)
             Mean action noise std: 2.42
          Mean value_function loss: 117.4435
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 49.6941
                       Mean reward: 864.13
               Mean episode length: 233.16
    Episode_Reward/reaching_object: 1.2142
     Episode_Reward/lifting_object: 169.0541
      Episode_Reward/object_height: 0.0711
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 1.88s
                      Time elapsed: 00:39:51
                               ETA: 00:32:27

################################################################################
                     [1m Learning iteration 1103/2000 [0m                     

                       Computation: 51984 steps/s (collection: 1.774s, learning 0.117s)
             Mean action noise std: 2.42
          Mean value_function loss: 111.9665
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 49.7058
                       Mean reward: 847.33
               Mean episode length: 229.96
    Episode_Reward/reaching_object: 1.2443
     Episode_Reward/lifting_object: 173.1805
      Episode_Reward/object_height: 0.0728
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 1.89s
                      Time elapsed: 00:39:53
                               ETA: 00:32:24

################################################################################
                     [1m Learning iteration 1104/2000 [0m                     

                       Computation: 51998 steps/s (collection: 1.782s, learning 0.108s)
             Mean action noise std: 2.42
          Mean value_function loss: 119.6206
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 49.7136
                       Mean reward: 874.31
               Mean episode length: 236.10
    Episode_Reward/reaching_object: 1.2061
     Episode_Reward/lifting_object: 167.8285
      Episode_Reward/object_height: 0.0704
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 1.89s
                      Time elapsed: 00:39:55
                               ETA: 00:32:22

################################################################################
                     [1m Learning iteration 1105/2000 [0m                     

                       Computation: 51082 steps/s (collection: 1.812s, learning 0.113s)
             Mean action noise std: 2.42
          Mean value_function loss: 130.3228
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 49.7178
                       Mean reward: 841.39
               Mean episode length: 227.94
    Episode_Reward/reaching_object: 1.2186
     Episode_Reward/lifting_object: 169.3607
      Episode_Reward/object_height: 0.0710
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 1.92s
                      Time elapsed: 00:39:57
                               ETA: 00:32:19

################################################################################
                     [1m Learning iteration 1106/2000 [0m                     

                       Computation: 52341 steps/s (collection: 1.789s, learning 0.089s)
             Mean action noise std: 2.42
          Mean value_function loss: 129.3851
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 49.7236
                       Mean reward: 872.59
               Mean episode length: 235.54
    Episode_Reward/reaching_object: 1.2081
     Episode_Reward/lifting_object: 167.8564
      Episode_Reward/object_height: 0.0703
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 1.88s
                      Time elapsed: 00:39:59
                               ETA: 00:32:17

################################################################################
                     [1m Learning iteration 1107/2000 [0m                     

                       Computation: 51743 steps/s (collection: 1.798s, learning 0.102s)
             Mean action noise std: 2.42
          Mean value_function loss: 106.1328
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 49.7309
                       Mean reward: 859.96
               Mean episode length: 233.69
    Episode_Reward/reaching_object: 1.1957
     Episode_Reward/lifting_object: 165.5454
      Episode_Reward/object_height: 0.0696
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 1.90s
                      Time elapsed: 00:40:00
                               ETA: 00:32:15

################################################################################
                     [1m Learning iteration 1108/2000 [0m                     

                       Computation: 51787 steps/s (collection: 1.799s, learning 0.100s)
             Mean action noise std: 2.42
          Mean value_function loss: 116.4743
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 49.7371
                       Mean reward: 875.48
               Mean episode length: 236.83
    Episode_Reward/reaching_object: 1.2195
     Episode_Reward/lifting_object: 169.8735
      Episode_Reward/object_height: 0.0707
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 1.90s
                      Time elapsed: 00:40:02
                               ETA: 00:32:12

################################################################################
                     [1m Learning iteration 1109/2000 [0m                     

                       Computation: 51958 steps/s (collection: 1.772s, learning 0.120s)
             Mean action noise std: 2.42
          Mean value_function loss: 170.2814
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 49.7419
                       Mean reward: 855.18
               Mean episode length: 230.93
    Episode_Reward/reaching_object: 1.2236
     Episode_Reward/lifting_object: 170.6064
      Episode_Reward/object_height: 0.0706
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 1.89s
                      Time elapsed: 00:40:04
                               ETA: 00:32:10

################################################################################
                     [1m Learning iteration 1110/2000 [0m                     

                       Computation: 52007 steps/s (collection: 1.781s, learning 0.110s)
             Mean action noise std: 2.42
          Mean value_function loss: 121.1543
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 49.7509
                       Mean reward: 883.30
               Mean episode length: 237.97
    Episode_Reward/reaching_object: 1.2403
     Episode_Reward/lifting_object: 172.3412
      Episode_Reward/object_height: 0.0717
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 1.89s
                      Time elapsed: 00:40:06
                               ETA: 00:32:07

################################################################################
                     [1m Learning iteration 1111/2000 [0m                     

                       Computation: 51465 steps/s (collection: 1.789s, learning 0.121s)
             Mean action noise std: 2.42
          Mean value_function loss: 106.5313
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 49.7578
                       Mean reward: 854.01
               Mean episode length: 229.83
    Episode_Reward/reaching_object: 1.2150
     Episode_Reward/lifting_object: 169.4613
      Episode_Reward/object_height: 0.0705
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 1.91s
                      Time elapsed: 00:40:08
                               ETA: 00:32:05

################################################################################
                     [1m Learning iteration 1112/2000 [0m                     

                       Computation: 51803 steps/s (collection: 1.791s, learning 0.107s)
             Mean action noise std: 2.42
          Mean value_function loss: 100.9251
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 49.7614
                       Mean reward: 821.10
               Mean episode length: 222.90
    Episode_Reward/reaching_object: 1.2239
     Episode_Reward/lifting_object: 170.9805
      Episode_Reward/object_height: 0.0707
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 1.90s
                      Time elapsed: 00:40:10
                               ETA: 00:32:03

################################################################################
                     [1m Learning iteration 1113/2000 [0m                     

                       Computation: 52565 steps/s (collection: 1.767s, learning 0.104s)
             Mean action noise std: 2.43
          Mean value_function loss: 106.5613
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 49.7636
                       Mean reward: 892.68
               Mean episode length: 239.56
    Episode_Reward/reaching_object: 1.2325
     Episode_Reward/lifting_object: 171.8189
      Episode_Reward/object_height: 0.0712
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 1.87s
                      Time elapsed: 00:40:12
                               ETA: 00:32:00

################################################################################
                     [1m Learning iteration 1114/2000 [0m                     

                       Computation: 51938 steps/s (collection: 1.794s, learning 0.099s)
             Mean action noise std: 2.43
          Mean value_function loss: 111.0743
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 49.7711
                       Mean reward: 873.39
               Mean episode length: 235.07
    Episode_Reward/reaching_object: 1.2491
     Episode_Reward/lifting_object: 174.6493
      Episode_Reward/object_height: 0.0730
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 1.89s
                      Time elapsed: 00:40:14
                               ETA: 00:31:58

################################################################################
                     [1m Learning iteration 1115/2000 [0m                     

                       Computation: 51610 steps/s (collection: 1.799s, learning 0.106s)
             Mean action noise std: 2.43
          Mean value_function loss: 110.2996
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 49.7870
                       Mean reward: 852.68
               Mean episode length: 230.88
    Episode_Reward/reaching_object: 1.2252
     Episode_Reward/lifting_object: 170.4547
      Episode_Reward/object_height: 0.0708
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 1.90s
                      Time elapsed: 00:40:16
                               ETA: 00:31:56

################################################################################
                     [1m Learning iteration 1116/2000 [0m                     

                       Computation: 51521 steps/s (collection: 1.794s, learning 0.114s)
             Mean action noise std: 2.43
          Mean value_function loss: 93.0237
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 49.7941
                       Mean reward: 912.49
               Mean episode length: 243.37
    Episode_Reward/reaching_object: 1.2431
     Episode_Reward/lifting_object: 173.0943
      Episode_Reward/object_height: 0.0723
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 1.91s
                      Time elapsed: 00:40:18
                               ETA: 00:31:53

################################################################################
                     [1m Learning iteration 1117/2000 [0m                     

                       Computation: 51901 steps/s (collection: 1.781s, learning 0.113s)
             Mean action noise std: 2.43
          Mean value_function loss: 123.2122
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 49.8044
                       Mean reward: 860.01
               Mean episode length: 231.80
    Episode_Reward/reaching_object: 1.2218
     Episode_Reward/lifting_object: 170.1444
      Episode_Reward/object_height: 0.0709
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 1.89s
                      Time elapsed: 00:40:19
                               ETA: 00:31:51

################################################################################
                     [1m Learning iteration 1118/2000 [0m                     

                       Computation: 51536 steps/s (collection: 1.794s, learning 0.113s)
             Mean action noise std: 2.43
          Mean value_function loss: 113.9136
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 49.8238
                       Mean reward: 861.23
               Mean episode length: 233.12
    Episode_Reward/reaching_object: 1.2328
     Episode_Reward/lifting_object: 171.1689
      Episode_Reward/object_height: 0.0715
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 1.91s
                      Time elapsed: 00:40:21
                               ETA: 00:31:48

################################################################################
                     [1m Learning iteration 1119/2000 [0m                     

                       Computation: 51302 steps/s (collection: 1.800s, learning 0.116s)
             Mean action noise std: 2.43
          Mean value_function loss: 133.3315
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 49.8320
                       Mean reward: 857.27
               Mean episode length: 231.32
    Episode_Reward/reaching_object: 1.2316
     Episode_Reward/lifting_object: 171.3902
      Episode_Reward/object_height: 0.0714
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 1.92s
                      Time elapsed: 00:40:23
                               ETA: 00:31:46

################################################################################
                     [1m Learning iteration 1120/2000 [0m                     

                       Computation: 51250 steps/s (collection: 1.802s, learning 0.117s)
             Mean action noise std: 2.43
          Mean value_function loss: 114.4010
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 49.8351
                       Mean reward: 804.04
               Mean episode length: 221.95
    Episode_Reward/reaching_object: 1.2177
     Episode_Reward/lifting_object: 168.3519
      Episode_Reward/object_height: 0.0707
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 1.92s
                      Time elapsed: 00:40:25
                               ETA: 00:31:44

################################################################################
                     [1m Learning iteration 1121/2000 [0m                     

                       Computation: 51585 steps/s (collection: 1.806s, learning 0.100s)
             Mean action noise std: 2.44
          Mean value_function loss: 115.4234
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 49.8422
                       Mean reward: 899.74
               Mean episode length: 242.09
    Episode_Reward/reaching_object: 1.2438
     Episode_Reward/lifting_object: 172.4693
      Episode_Reward/object_height: 0.0719
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 1.91s
                      Time elapsed: 00:40:27
                               ETA: 00:31:41

################################################################################
                     [1m Learning iteration 1122/2000 [0m                     

                       Computation: 51104 steps/s (collection: 1.808s, learning 0.116s)
             Mean action noise std: 2.44
          Mean value_function loss: 124.4302
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 49.8533
                       Mean reward: 859.69
               Mean episode length: 233.53
    Episode_Reward/reaching_object: 1.2027
     Episode_Reward/lifting_object: 166.1149
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 1.92s
                      Time elapsed: 00:40:29
                               ETA: 00:31:39

################################################################################
                     [1m Learning iteration 1123/2000 [0m                     

                       Computation: 51320 steps/s (collection: 1.807s, learning 0.108s)
             Mean action noise std: 2.44
          Mean value_function loss: 119.1205
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 49.8595
                       Mean reward: 835.99
               Mean episode length: 227.38
    Episode_Reward/reaching_object: 1.1983
     Episode_Reward/lifting_object: 165.2024
      Episode_Reward/object_height: 0.0689
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 1.92s
                      Time elapsed: 00:40:31
                               ETA: 00:31:37

################################################################################
                     [1m Learning iteration 1124/2000 [0m                     

                       Computation: 51506 steps/s (collection: 1.803s, learning 0.106s)
             Mean action noise std: 2.44
          Mean value_function loss: 126.6220
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 49.8644
                       Mean reward: 860.65
               Mean episode length: 231.07
    Episode_Reward/reaching_object: 1.2267
     Episode_Reward/lifting_object: 170.4092
      Episode_Reward/object_height: 0.0707
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 1.91s
                      Time elapsed: 00:40:33
                               ETA: 00:31:34

################################################################################
                     [1m Learning iteration 1125/2000 [0m                     

                       Computation: 51810 steps/s (collection: 1.785s, learning 0.113s)
             Mean action noise std: 2.44
          Mean value_function loss: 144.8596
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 49.8715
                       Mean reward: 800.08
               Mean episode length: 218.16
    Episode_Reward/reaching_object: 1.2273
     Episode_Reward/lifting_object: 170.0781
      Episode_Reward/object_height: 0.0704
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 1.90s
                      Time elapsed: 00:40:35
                               ETA: 00:31:32

################################################################################
                     [1m Learning iteration 1126/2000 [0m                     

                       Computation: 51522 steps/s (collection: 1.794s, learning 0.114s)
             Mean action noise std: 2.44
          Mean value_function loss: 126.9714
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 49.8782
                       Mean reward: 870.63
               Mean episode length: 232.72
    Episode_Reward/reaching_object: 1.2174
     Episode_Reward/lifting_object: 168.8804
      Episode_Reward/object_height: 0.0701
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 1.91s
                      Time elapsed: 00:40:37
                               ETA: 00:31:30

################################################################################
                     [1m Learning iteration 1127/2000 [0m                     

                       Computation: 51905 steps/s (collection: 1.783s, learning 0.111s)
             Mean action noise std: 2.44
          Mean value_function loss: 153.7610
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 49.8855
                       Mean reward: 843.88
               Mean episode length: 227.93
    Episode_Reward/reaching_object: 1.1993
     Episode_Reward/lifting_object: 166.0493
      Episode_Reward/object_height: 0.0686
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 1.89s
                      Time elapsed: 00:40:39
                               ETA: 00:31:27

################################################################################
                     [1m Learning iteration 1128/2000 [0m                     

                       Computation: 52390 steps/s (collection: 1.753s, learning 0.123s)
             Mean action noise std: 2.44
          Mean value_function loss: 119.5106
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 49.8947
                       Mean reward: 887.79
               Mean episode length: 238.36
    Episode_Reward/reaching_object: 1.2532
     Episode_Reward/lifting_object: 173.3907
      Episode_Reward/object_height: 0.0726
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 1.88s
                      Time elapsed: 00:40:40
                               ETA: 00:31:25

################################################################################
                     [1m Learning iteration 1129/2000 [0m                     

                       Computation: 51764 steps/s (collection: 1.775s, learning 0.124s)
             Mean action noise std: 2.44
          Mean value_function loss: 167.9685
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 49.9059
                       Mean reward: 807.44
               Mean episode length: 220.66
    Episode_Reward/reaching_object: 1.2109
     Episode_Reward/lifting_object: 166.6306
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 1.90s
                      Time elapsed: 00:40:42
                               ETA: 00:31:22

################################################################################
                     [1m Learning iteration 1130/2000 [0m                     

                       Computation: 52259 steps/s (collection: 1.767s, learning 0.114s)
             Mean action noise std: 2.45
          Mean value_function loss: 136.3415
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 49.9184
                       Mean reward: 847.65
               Mean episode length: 229.47
    Episode_Reward/reaching_object: 1.2287
     Episode_Reward/lifting_object: 169.1459
      Episode_Reward/object_height: 0.0705
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 1.88s
                      Time elapsed: 00:40:44
                               ETA: 00:31:20

################################################################################
                     [1m Learning iteration 1131/2000 [0m                     

                       Computation: 51438 steps/s (collection: 1.803s, learning 0.109s)
             Mean action noise std: 2.45
          Mean value_function loss: 121.1886
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 49.9344
                       Mean reward: 886.18
               Mean episode length: 237.92
    Episode_Reward/reaching_object: 1.2386
     Episode_Reward/lifting_object: 170.9552
      Episode_Reward/object_height: 0.0713
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 1.91s
                      Time elapsed: 00:40:46
                               ETA: 00:31:18

################################################################################
                     [1m Learning iteration 1132/2000 [0m                     

                       Computation: 51986 steps/s (collection: 1.785s, learning 0.106s)
             Mean action noise std: 2.45
          Mean value_function loss: 124.0672
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 49.9478
                       Mean reward: 881.92
               Mean episode length: 237.95
    Episode_Reward/reaching_object: 1.2394
     Episode_Reward/lifting_object: 171.4947
      Episode_Reward/object_height: 0.0715
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 1.89s
                      Time elapsed: 00:40:48
                               ETA: 00:31:15

################################################################################
                     [1m Learning iteration 1133/2000 [0m                     

                       Computation: 52011 steps/s (collection: 1.779s, learning 0.111s)
             Mean action noise std: 2.45
          Mean value_function loss: 125.5395
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 49.9648
                       Mean reward: 868.25
               Mean episode length: 234.12
    Episode_Reward/reaching_object: 1.2121
     Episode_Reward/lifting_object: 166.9189
      Episode_Reward/object_height: 0.0692
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 1.89s
                      Time elapsed: 00:40:50
                               ETA: 00:31:13

################################################################################
                     [1m Learning iteration 1134/2000 [0m                     

                       Computation: 52380 steps/s (collection: 1.775s, learning 0.102s)
             Mean action noise std: 2.45
          Mean value_function loss: 120.7362
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 49.9764
                       Mean reward: 809.65
               Mean episode length: 220.15
    Episode_Reward/reaching_object: 1.2249
     Episode_Reward/lifting_object: 168.9888
      Episode_Reward/object_height: 0.0706
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 1.88s
                      Time elapsed: 00:40:52
                               ETA: 00:31:11

################################################################################
                     [1m Learning iteration 1135/2000 [0m                     

                       Computation: 51601 steps/s (collection: 1.785s, learning 0.120s)
             Mean action noise std: 2.45
          Mean value_function loss: 97.3803
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 49.9872
                       Mean reward: 851.73
               Mean episode length: 229.40
    Episode_Reward/reaching_object: 1.2529
     Episode_Reward/lifting_object: 173.6955
      Episode_Reward/object_height: 0.0731
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 1.91s
                      Time elapsed: 00:40:54
                               ETA: 00:31:08

################################################################################
                     [1m Learning iteration 1136/2000 [0m                     

                       Computation: 52019 steps/s (collection: 1.777s, learning 0.113s)
             Mean action noise std: 2.45
          Mean value_function loss: 89.4298
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 49.9910
                       Mean reward: 878.20
               Mean episode length: 236.28
    Episode_Reward/reaching_object: 1.2642
     Episode_Reward/lifting_object: 175.2337
      Episode_Reward/object_height: 0.0740
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 1.89s
                      Time elapsed: 00:40:56
                               ETA: 00:31:06

################################################################################
                     [1m Learning iteration 1137/2000 [0m                     

                       Computation: 51593 steps/s (collection: 1.789s, learning 0.117s)
             Mean action noise std: 2.45
          Mean value_function loss: 110.2660
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 49.9940
                       Mean reward: 892.62
               Mean episode length: 239.14
    Episode_Reward/reaching_object: 1.2360
     Episode_Reward/lifting_object: 171.3871
      Episode_Reward/object_height: 0.0723
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 1.91s
                      Time elapsed: 00:40:57
                               ETA: 00:31:03

################################################################################
                     [1m Learning iteration 1138/2000 [0m                     

                       Computation: 52020 steps/s (collection: 1.774s, learning 0.116s)
             Mean action noise std: 2.46
          Mean value_function loss: 99.0622
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 50.0035
                       Mean reward: 880.79
               Mean episode length: 238.00
    Episode_Reward/reaching_object: 1.2517
     Episode_Reward/lifting_object: 173.7843
      Episode_Reward/object_height: 0.0738
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 1.89s
                      Time elapsed: 00:40:59
                               ETA: 00:31:01

################################################################################
                     [1m Learning iteration 1139/2000 [0m                     

                       Computation: 50864 steps/s (collection: 1.819s, learning 0.114s)
             Mean action noise std: 2.46
          Mean value_function loss: 123.3496
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 50.0139
                       Mean reward: 846.42
               Mean episode length: 228.25
    Episode_Reward/reaching_object: 1.1987
     Episode_Reward/lifting_object: 166.0166
      Episode_Reward/object_height: 0.0700
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 1.93s
                      Time elapsed: 00:41:01
                               ETA: 00:30:59

################################################################################
                     [1m Learning iteration 1140/2000 [0m                     

                       Computation: 52009 steps/s (collection: 1.780s, learning 0.110s)
             Mean action noise std: 2.46
          Mean value_function loss: 134.2727
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 50.0259
                       Mean reward: 854.49
               Mean episode length: 232.03
    Episode_Reward/reaching_object: 1.2352
     Episode_Reward/lifting_object: 171.3445
      Episode_Reward/object_height: 0.0726
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 1.89s
                      Time elapsed: 00:41:03
                               ETA: 00:30:56

################################################################################
                     [1m Learning iteration 1141/2000 [0m                     

                       Computation: 52357 steps/s (collection: 1.781s, learning 0.096s)
             Mean action noise std: 2.46
          Mean value_function loss: 121.8103
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 50.0402
                       Mean reward: 860.21
               Mean episode length: 231.97
    Episode_Reward/reaching_object: 1.1942
     Episode_Reward/lifting_object: 165.7035
      Episode_Reward/object_height: 0.0700
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 1.88s
                      Time elapsed: 00:41:05
                               ETA: 00:30:54

################################################################################
                     [1m Learning iteration 1142/2000 [0m                     

                       Computation: 52134 steps/s (collection: 1.777s, learning 0.109s)
             Mean action noise std: 2.46
          Mean value_function loss: 119.1220
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 50.0479
                       Mean reward: 864.46
               Mean episode length: 232.18
    Episode_Reward/reaching_object: 1.2098
     Episode_Reward/lifting_object: 168.0276
      Episode_Reward/object_height: 0.0711
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 1.89s
                      Time elapsed: 00:41:07
                               ETA: 00:30:52

################################################################################
                     [1m Learning iteration 1143/2000 [0m                     

                       Computation: 51993 steps/s (collection: 1.776s, learning 0.115s)
             Mean action noise std: 2.46
          Mean value_function loss: 101.3215
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 50.0604
                       Mean reward: 886.24
               Mean episode length: 238.53
    Episode_Reward/reaching_object: 1.2223
     Episode_Reward/lifting_object: 169.7077
      Episode_Reward/object_height: 0.0721
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 1.89s
                      Time elapsed: 00:41:09
                               ETA: 00:30:49

################################################################################
                     [1m Learning iteration 1144/2000 [0m                     

                       Computation: 51670 steps/s (collection: 1.785s, learning 0.118s)
             Mean action noise std: 2.46
          Mean value_function loss: 96.4222
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 50.0686
                       Mean reward: 868.34
               Mean episode length: 232.90
    Episode_Reward/reaching_object: 1.2534
     Episode_Reward/lifting_object: 174.4586
      Episode_Reward/object_height: 0.0743
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 1.90s
                      Time elapsed: 00:41:11
                               ETA: 00:30:47

################################################################################
                     [1m Learning iteration 1145/2000 [0m                     

                       Computation: 51544 steps/s (collection: 1.793s, learning 0.115s)
             Mean action noise std: 2.47
          Mean value_function loss: 121.5362
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 50.0820
                       Mean reward: 874.58
               Mean episode length: 236.59
    Episode_Reward/reaching_object: 1.2404
     Episode_Reward/lifting_object: 172.6884
      Episode_Reward/object_height: 0.0735
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 1.91s
                      Time elapsed: 00:41:13
                               ETA: 00:30:45

################################################################################
                     [1m Learning iteration 1146/2000 [0m                     

                       Computation: 51504 steps/s (collection: 1.793s, learning 0.116s)
             Mean action noise std: 2.47
          Mean value_function loss: 119.7074
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 50.0975
                       Mean reward: 899.15
               Mean episode length: 240.53
    Episode_Reward/reaching_object: 1.2167
     Episode_Reward/lifting_object: 169.3642
      Episode_Reward/object_height: 0.0720
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 1.91s
                      Time elapsed: 00:41:15
                               ETA: 00:30:42

################################################################################
                     [1m Learning iteration 1147/2000 [0m                     

                       Computation: 51321 steps/s (collection: 1.804s, learning 0.112s)
             Mean action noise std: 2.47
          Mean value_function loss: 157.7377
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 50.1049
                       Mean reward: 802.82
               Mean episode length: 218.24
    Episode_Reward/reaching_object: 1.1943
     Episode_Reward/lifting_object: 165.9051
      Episode_Reward/object_height: 0.0703
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 1.92s
                      Time elapsed: 00:41:16
                               ETA: 00:30:40

################################################################################
                     [1m Learning iteration 1148/2000 [0m                     

                       Computation: 52768 steps/s (collection: 1.768s, learning 0.095s)
             Mean action noise std: 2.47
          Mean value_function loss: 96.7271
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 50.1165
                       Mean reward: 896.70
               Mean episode length: 239.92
    Episode_Reward/reaching_object: 1.2566
     Episode_Reward/lifting_object: 174.9711
      Episode_Reward/object_height: 0.0745
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 1.86s
                      Time elapsed: 00:41:18
                               ETA: 00:30:38

################################################################################
                     [1m Learning iteration 1149/2000 [0m                     

                       Computation: 49388 steps/s (collection: 1.876s, learning 0.114s)
             Mean action noise std: 2.47
          Mean value_function loss: 101.3479
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 50.1309
                       Mean reward: 888.06
               Mean episode length: 238.44
    Episode_Reward/reaching_object: 1.2357
     Episode_Reward/lifting_object: 171.6696
      Episode_Reward/object_height: 0.0732
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 1.99s
                      Time elapsed: 00:41:20
                               ETA: 00:30:35

################################################################################
                     [1m Learning iteration 1150/2000 [0m                     

                       Computation: 51778 steps/s (collection: 1.791s, learning 0.108s)
             Mean action noise std: 2.47
          Mean value_function loss: 98.4158
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 50.1428
                       Mean reward: 886.75
               Mean episode length: 237.36
    Episode_Reward/reaching_object: 1.2490
     Episode_Reward/lifting_object: 173.4604
      Episode_Reward/object_height: 0.0736
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 1.90s
                      Time elapsed: 00:41:22
                               ETA: 00:30:33

################################################################################
                     [1m Learning iteration 1151/2000 [0m                     

                       Computation: 50823 steps/s (collection: 1.827s, learning 0.108s)
             Mean action noise std: 2.47
          Mean value_function loss: 105.1505
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 50.1486
                       Mean reward: 839.23
               Mean episode length: 226.01
    Episode_Reward/reaching_object: 1.2216
     Episode_Reward/lifting_object: 169.8914
      Episode_Reward/object_height: 0.0716
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 1.93s
                      Time elapsed: 00:41:24
                               ETA: 00:30:31

################################################################################
                     [1m Learning iteration 1152/2000 [0m                     

                       Computation: 52092 steps/s (collection: 1.781s, learning 0.106s)
             Mean action noise std: 2.47
          Mean value_function loss: 131.2946
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 50.1549
                       Mean reward: 829.33
               Mean episode length: 223.27
    Episode_Reward/reaching_object: 1.2386
     Episode_Reward/lifting_object: 171.7834
      Episode_Reward/object_height: 0.0720
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 1.89s
                      Time elapsed: 00:41:26
                               ETA: 00:30:28

################################################################################
                     [1m Learning iteration 1153/2000 [0m                     

                       Computation: 52494 steps/s (collection: 1.761s, learning 0.112s)
             Mean action noise std: 2.48
          Mean value_function loss: 120.5523
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 50.1626
                       Mean reward: 903.22
               Mean episode length: 241.20
    Episode_Reward/reaching_object: 1.2256
     Episode_Reward/lifting_object: 169.5331
      Episode_Reward/object_height: 0.0711
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 1.87s
                      Time elapsed: 00:41:28
                               ETA: 00:30:26

################################################################################
                     [1m Learning iteration 1154/2000 [0m                     

                       Computation: 52136 steps/s (collection: 1.774s, learning 0.112s)
             Mean action noise std: 2.48
          Mean value_function loss: 120.9687
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 50.1746
                       Mean reward: 872.57
               Mean episode length: 234.47
    Episode_Reward/reaching_object: 1.2372
     Episode_Reward/lifting_object: 171.4666
      Episode_Reward/object_height: 0.0713
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 1.89s
                      Time elapsed: 00:41:30
                               ETA: 00:30:24

################################################################################
                     [1m Learning iteration 1155/2000 [0m                     

                       Computation: 52559 steps/s (collection: 1.781s, learning 0.090s)
             Mean action noise std: 2.48
          Mean value_function loss: 135.6018
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 50.1886
                       Mean reward: 818.92
               Mean episode length: 220.89
    Episode_Reward/reaching_object: 1.2058
     Episode_Reward/lifting_object: 166.5805
      Episode_Reward/object_height: 0.0687
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 1.87s
                      Time elapsed: 00:41:32
                               ETA: 00:30:21

################################################################################
                     [1m Learning iteration 1156/2000 [0m                     

                       Computation: 51322 steps/s (collection: 1.790s, learning 0.126s)
             Mean action noise std: 2.48
          Mean value_function loss: 129.7534
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 50.2023
                       Mean reward: 828.90
               Mean episode length: 224.38
    Episode_Reward/reaching_object: 1.2421
     Episode_Reward/lifting_object: 171.4717
      Episode_Reward/object_height: 0.0711
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 1.92s
                      Time elapsed: 00:41:34
                               ETA: 00:30:19

################################################################################
                     [1m Learning iteration 1157/2000 [0m                     

                       Computation: 52085 steps/s (collection: 1.775s, learning 0.112s)
             Mean action noise std: 2.48
          Mean value_function loss: 99.1294
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 50.2156
                       Mean reward: 865.34
               Mean episode length: 231.91
    Episode_Reward/reaching_object: 1.2258
     Episode_Reward/lifting_object: 169.2504
      Episode_Reward/object_height: 0.0703
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 1.89s
                      Time elapsed: 00:41:35
                               ETA: 00:30:16

################################################################################
                     [1m Learning iteration 1158/2000 [0m                     

                       Computation: 52358 steps/s (collection: 1.769s, learning 0.108s)
             Mean action noise std: 2.48
          Mean value_function loss: 111.0924
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 50.2213
                       Mean reward: 886.02
               Mean episode length: 237.40
    Episode_Reward/reaching_object: 1.2585
     Episode_Reward/lifting_object: 174.3564
      Episode_Reward/object_height: 0.0721
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 1.88s
                      Time elapsed: 00:41:37
                               ETA: 00:30:14

################################################################################
                     [1m Learning iteration 1159/2000 [0m                     

                       Computation: 52148 steps/s (collection: 1.777s, learning 0.108s)
             Mean action noise std: 2.48
          Mean value_function loss: 93.8794
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 50.2288
                       Mean reward: 867.70
               Mean episode length: 233.21
    Episode_Reward/reaching_object: 1.2680
     Episode_Reward/lifting_object: 175.9020
      Episode_Reward/object_height: 0.0729
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 1.89s
                      Time elapsed: 00:41:39
                               ETA: 00:30:12

################################################################################
                     [1m Learning iteration 1160/2000 [0m                     

                       Computation: 52368 steps/s (collection: 1.767s, learning 0.111s)
             Mean action noise std: 2.48
          Mean value_function loss: 96.0914
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 50.2370
                       Mean reward: 874.39
               Mean episode length: 233.79
    Episode_Reward/reaching_object: 1.2415
     Episode_Reward/lifting_object: 171.9073
      Episode_Reward/object_height: 0.0715
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 1.88s
                      Time elapsed: 00:41:41
                               ETA: 00:30:09

################################################################################
                     [1m Learning iteration 1161/2000 [0m                     

                       Computation: 52725 steps/s (collection: 1.767s, learning 0.098s)
             Mean action noise std: 2.49
          Mean value_function loss: 93.9387
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 50.2460
                       Mean reward: 884.60
               Mean episode length: 236.76
    Episode_Reward/reaching_object: 1.2308
     Episode_Reward/lifting_object: 170.3625
      Episode_Reward/object_height: 0.0705
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 1.86s
                      Time elapsed: 00:41:43
                               ETA: 00:30:07

################################################################################
                     [1m Learning iteration 1162/2000 [0m                     

                       Computation: 50462 steps/s (collection: 1.838s, learning 0.110s)
             Mean action noise std: 2.49
          Mean value_function loss: 99.4022
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.2530
                       Mean reward: 880.34
               Mean episode length: 235.52
    Episode_Reward/reaching_object: 1.2396
     Episode_Reward/lifting_object: 172.2253
      Episode_Reward/object_height: 0.0713
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 1.95s
                      Time elapsed: 00:41:45
                               ETA: 00:30:05

################################################################################
                     [1m Learning iteration 1163/2000 [0m                     

                       Computation: 51471 steps/s (collection: 1.793s, learning 0.117s)
             Mean action noise std: 2.49
          Mean value_function loss: 95.8245
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 50.2680
                       Mean reward: 895.71
               Mean episode length: 239.09
    Episode_Reward/reaching_object: 1.2264
     Episode_Reward/lifting_object: 169.9215
      Episode_Reward/object_height: 0.0702
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 1.91s
                      Time elapsed: 00:41:47
                               ETA: 00:30:02

################################################################################
                     [1m Learning iteration 1164/2000 [0m                     

                       Computation: 51029 steps/s (collection: 1.819s, learning 0.107s)
             Mean action noise std: 2.49
          Mean value_function loss: 169.5759
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 50.2781
                       Mean reward: 828.91
               Mean episode length: 224.88
    Episode_Reward/reaching_object: 1.2117
     Episode_Reward/lifting_object: 167.5032
      Episode_Reward/object_height: 0.0691
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 1.93s
                      Time elapsed: 00:41:49
                               ETA: 00:30:00

################################################################################
                     [1m Learning iteration 1165/2000 [0m                     

                       Computation: 51942 steps/s (collection: 1.780s, learning 0.113s)
             Mean action noise std: 2.49
          Mean value_function loss: 130.6256
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 50.2864
                       Mean reward: 894.14
               Mean episode length: 238.73
    Episode_Reward/reaching_object: 1.2624
     Episode_Reward/lifting_object: 174.2262
      Episode_Reward/object_height: 0.0722
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 1.89s
                      Time elapsed: 00:41:51
                               ETA: 00:29:58

################################################################################
                     [1m Learning iteration 1166/2000 [0m                     

                       Computation: 51697 steps/s (collection: 1.787s, learning 0.115s)
             Mean action noise std: 2.49
          Mean value_function loss: 157.4435
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 50.2958
                       Mean reward: 840.40
               Mean episode length: 228.10
    Episode_Reward/reaching_object: 1.2197
     Episode_Reward/lifting_object: 168.7169
      Episode_Reward/object_height: 0.0694
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 1.90s
                      Time elapsed: 00:41:53
                               ETA: 00:29:55

################################################################################
                     [1m Learning iteration 1167/2000 [0m                     

                       Computation: 51628 steps/s (collection: 1.783s, learning 0.121s)
             Mean action noise std: 2.49
          Mean value_function loss: 130.9598
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 50.3058
                       Mean reward: 856.58
               Mean episode length: 230.64
    Episode_Reward/reaching_object: 1.2252
     Episode_Reward/lifting_object: 169.6515
      Episode_Reward/object_height: 0.0702
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 1.90s
                      Time elapsed: 00:41:54
                               ETA: 00:29:53

################################################################################
                     [1m Learning iteration 1168/2000 [0m                     

                       Computation: 52009 steps/s (collection: 1.787s, learning 0.103s)
             Mean action noise std: 2.49
          Mean value_function loss: 108.7432
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 50.3143
                       Mean reward: 847.01
               Mean episode length: 228.04
    Episode_Reward/reaching_object: 1.2567
     Episode_Reward/lifting_object: 174.2288
      Episode_Reward/object_height: 0.0724
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 1.89s
                      Time elapsed: 00:41:56
                               ETA: 00:29:51

################################################################################
                     [1m Learning iteration 1169/2000 [0m                     

                       Computation: 52189 steps/s (collection: 1.786s, learning 0.098s)
             Mean action noise std: 2.50
          Mean value_function loss: 114.4566
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 50.3244
                       Mean reward: 864.13
               Mean episode length: 232.39
    Episode_Reward/reaching_object: 1.2322
     Episode_Reward/lifting_object: 170.8351
      Episode_Reward/object_height: 0.0712
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 1.88s
                      Time elapsed: 00:41:58
                               ETA: 00:29:48

################################################################################
                     [1m Learning iteration 1170/2000 [0m                     

                       Computation: 51640 steps/s (collection: 1.778s, learning 0.126s)
             Mean action noise std: 2.50
          Mean value_function loss: 102.7269
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 50.3379
                       Mean reward: 891.03
               Mean episode length: 238.04
    Episode_Reward/reaching_object: 1.2422
     Episode_Reward/lifting_object: 172.6402
      Episode_Reward/object_height: 0.0722
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 1.90s
                      Time elapsed: 00:42:00
                               ETA: 00:29:46

################################################################################
                     [1m Learning iteration 1171/2000 [0m                     

                       Computation: 51091 steps/s (collection: 1.814s, learning 0.110s)
             Mean action noise std: 2.50
          Mean value_function loss: 93.4854
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 50.3587
                       Mean reward: 889.17
               Mean episode length: 236.75
    Episode_Reward/reaching_object: 1.2560
     Episode_Reward/lifting_object: 174.9141
      Episode_Reward/object_height: 0.0735
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 1.92s
                      Time elapsed: 00:42:02
                               ETA: 00:29:44

################################################################################
                     [1m Learning iteration 1172/2000 [0m                     

                       Computation: 50861 steps/s (collection: 1.823s, learning 0.110s)
             Mean action noise std: 2.50
          Mean value_function loss: 102.5267
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 50.3745
                       Mean reward: 882.44
               Mean episode length: 236.53
    Episode_Reward/reaching_object: 1.2409
     Episode_Reward/lifting_object: 172.7425
      Episode_Reward/object_height: 0.0726
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 1.93s
                      Time elapsed: 00:42:04
                               ETA: 00:29:41

################################################################################
                     [1m Learning iteration 1173/2000 [0m                     

                       Computation: 51320 steps/s (collection: 1.802s, learning 0.113s)
             Mean action noise std: 2.50
          Mean value_function loss: 140.6054
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 50.3863
                       Mean reward: 899.90
               Mean episode length: 239.58
    Episode_Reward/reaching_object: 1.2367
     Episode_Reward/lifting_object: 172.2579
      Episode_Reward/object_height: 0.0721
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 1.92s
                      Time elapsed: 00:42:06
                               ETA: 00:29:39

################################################################################
                     [1m Learning iteration 1174/2000 [0m                     

                       Computation: 52313 steps/s (collection: 1.774s, learning 0.105s)
             Mean action noise std: 2.50
          Mean value_function loss: 122.6675
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 50.3949
                       Mean reward: 839.40
               Mean episode length: 225.60
    Episode_Reward/reaching_object: 1.2303
     Episode_Reward/lifting_object: 171.4034
      Episode_Reward/object_height: 0.0721
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 1.88s
                      Time elapsed: 00:42:08
                               ETA: 00:29:37

################################################################################
                     [1m Learning iteration 1175/2000 [0m                     

                       Computation: 51128 steps/s (collection: 1.797s, learning 0.126s)
             Mean action noise std: 2.50
          Mean value_function loss: 123.8438
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 50.4065
                       Mean reward: 888.87
               Mean episode length: 238.42
    Episode_Reward/reaching_object: 1.2424
     Episode_Reward/lifting_object: 173.3048
      Episode_Reward/object_height: 0.0728
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 1.92s
                      Time elapsed: 00:42:10
                               ETA: 00:29:35

################################################################################
                     [1m Learning iteration 1176/2000 [0m                     

                       Computation: 51940 steps/s (collection: 1.792s, learning 0.101s)
             Mean action noise std: 2.51
          Mean value_function loss: 145.8773
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 50.4197
                       Mean reward: 858.13
               Mean episode length: 230.46
    Episode_Reward/reaching_object: 1.2330
     Episode_Reward/lifting_object: 172.0309
      Episode_Reward/object_height: 0.0723
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 1.89s
                      Time elapsed: 00:42:12
                               ETA: 00:29:32

################################################################################
                     [1m Learning iteration 1177/2000 [0m                     

                       Computation: 51142 steps/s (collection: 1.799s, learning 0.123s)
             Mean action noise std: 2.51
          Mean value_function loss: 150.9426
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 50.4338
                       Mean reward: 888.05
               Mean episode length: 237.71
    Episode_Reward/reaching_object: 1.2396
     Episode_Reward/lifting_object: 172.7986
      Episode_Reward/object_height: 0.0729
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 1.92s
                      Time elapsed: 00:42:13
                               ETA: 00:29:30

################################################################################
                     [1m Learning iteration 1178/2000 [0m                     

                       Computation: 51610 steps/s (collection: 1.794s, learning 0.111s)
             Mean action noise std: 2.51
          Mean value_function loss: 98.7912
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 50.4545
                       Mean reward: 901.25
               Mean episode length: 240.27
    Episode_Reward/reaching_object: 1.2347
     Episode_Reward/lifting_object: 172.2734
      Episode_Reward/object_height: 0.0729
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 1.90s
                      Time elapsed: 00:42:15
                               ETA: 00:29:28

################################################################################
                     [1m Learning iteration 1179/2000 [0m                     

                       Computation: 51631 steps/s (collection: 1.796s, learning 0.108s)
             Mean action noise std: 2.51
          Mean value_function loss: 120.5458
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 50.4689
                       Mean reward: 849.52
               Mean episode length: 228.85
    Episode_Reward/reaching_object: 1.2464
     Episode_Reward/lifting_object: 174.1487
      Episode_Reward/object_height: 0.0735
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 1.90s
                      Time elapsed: 00:42:17
                               ETA: 00:29:25

################################################################################
                     [1m Learning iteration 1180/2000 [0m                     

                       Computation: 51482 steps/s (collection: 1.800s, learning 0.109s)
             Mean action noise std: 2.51
          Mean value_function loss: 134.8991
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 50.4763
                       Mean reward: 877.55
               Mean episode length: 235.02
    Episode_Reward/reaching_object: 1.2313
     Episode_Reward/lifting_object: 171.7114
      Episode_Reward/object_height: 0.0727
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 1.91s
                      Time elapsed: 00:42:19
                               ETA: 00:29:23

################################################################################
                     [1m Learning iteration 1181/2000 [0m                     

                       Computation: 50994 steps/s (collection: 1.813s, learning 0.115s)
             Mean action noise std: 2.51
          Mean value_function loss: 101.3596
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 50.4853
                       Mean reward: 870.65
               Mean episode length: 233.87
    Episode_Reward/reaching_object: 1.2184
     Episode_Reward/lifting_object: 169.3956
      Episode_Reward/object_height: 0.0715
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 1.93s
                      Time elapsed: 00:42:21
                               ETA: 00:29:21

################################################################################
                     [1m Learning iteration 1182/2000 [0m                     

                       Computation: 51242 steps/s (collection: 1.802s, learning 0.116s)
             Mean action noise std: 2.51
          Mean value_function loss: 132.1285
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 50.4989
                       Mean reward: 830.98
               Mean episode length: 224.97
    Episode_Reward/reaching_object: 1.2278
     Episode_Reward/lifting_object: 170.6313
      Episode_Reward/object_height: 0.0723
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 1.92s
                      Time elapsed: 00:42:23
                               ETA: 00:29:18

################################################################################
                     [1m Learning iteration 1183/2000 [0m                     

                       Computation: 51252 steps/s (collection: 1.815s, learning 0.103s)
             Mean action noise std: 2.52
          Mean value_function loss: 127.2809
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 50.5114
                       Mean reward: 859.03
               Mean episode length: 231.10
    Episode_Reward/reaching_object: 1.2451
     Episode_Reward/lifting_object: 173.4410
      Episode_Reward/object_height: 0.0733
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 1.92s
                      Time elapsed: 00:42:25
                               ETA: 00:29:16

################################################################################
                     [1m Learning iteration 1184/2000 [0m                     

                       Computation: 52264 steps/s (collection: 1.788s, learning 0.093s)
             Mean action noise std: 2.52
          Mean value_function loss: 114.1144
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 50.5233
                       Mean reward: 834.40
               Mean episode length: 225.78
    Episode_Reward/reaching_object: 1.2356
     Episode_Reward/lifting_object: 171.9187
      Episode_Reward/object_height: 0.0728
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 1.88s
                      Time elapsed: 00:42:27
                               ETA: 00:29:14

################################################################################
                     [1m Learning iteration 1185/2000 [0m                     

                       Computation: 51958 steps/s (collection: 1.778s, learning 0.114s)
             Mean action noise std: 2.52
          Mean value_function loss: 116.7672
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 50.5307
                       Mean reward: 896.38
               Mean episode length: 239.06
    Episode_Reward/reaching_object: 1.2367
     Episode_Reward/lifting_object: 171.6039
      Episode_Reward/object_height: 0.0725
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 1.89s
                      Time elapsed: 00:42:29
                               ETA: 00:29:11

################################################################################
                     [1m Learning iteration 1186/2000 [0m                     

                       Computation: 51831 steps/s (collection: 1.781s, learning 0.116s)
             Mean action noise std: 2.52
          Mean value_function loss: 104.4807
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 50.5412
                       Mean reward: 895.24
               Mean episode length: 239.06
    Episode_Reward/reaching_object: 1.2318
     Episode_Reward/lifting_object: 171.1216
      Episode_Reward/object_height: 0.0723
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 1.90s
                      Time elapsed: 00:42:31
                               ETA: 00:29:09

################################################################################
                     [1m Learning iteration 1187/2000 [0m                     

                       Computation: 51328 steps/s (collection: 1.801s, learning 0.115s)
             Mean action noise std: 2.52
          Mean value_function loss: 99.3645
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 50.5488
                       Mean reward: 883.13
               Mean episode length: 235.86
    Episode_Reward/reaching_object: 1.2478
     Episode_Reward/lifting_object: 173.9277
      Episode_Reward/object_height: 0.0730
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 1.92s
                      Time elapsed: 00:42:33
                               ETA: 00:29:07

################################################################################
                     [1m Learning iteration 1188/2000 [0m                     

                       Computation: 51589 steps/s (collection: 1.796s, learning 0.110s)
             Mean action noise std: 2.52
          Mean value_function loss: 88.4409
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 50.5560
                       Mean reward: 838.00
               Mean episode length: 226.10
    Episode_Reward/reaching_object: 1.2604
     Episode_Reward/lifting_object: 175.9019
      Episode_Reward/object_height: 0.0739
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 1.91s
                      Time elapsed: 00:42:34
                               ETA: 00:29:04

################################################################################
                     [1m Learning iteration 1189/2000 [0m                     

                       Computation: 52339 steps/s (collection: 1.775s, learning 0.104s)
             Mean action noise std: 2.52
          Mean value_function loss: 98.7683
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 50.5617
                       Mean reward: 861.83
               Mean episode length: 231.83
    Episode_Reward/reaching_object: 1.2427
     Episode_Reward/lifting_object: 173.5986
      Episode_Reward/object_height: 0.0723
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 1.88s
                      Time elapsed: 00:42:36
                               ETA: 00:29:02

################################################################################
                     [1m Learning iteration 1190/2000 [0m                     

                       Computation: 51845 steps/s (collection: 1.802s, learning 0.094s)
             Mean action noise std: 2.52
          Mean value_function loss: 102.0495
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 50.5683
                       Mean reward: 862.73
               Mean episode length: 232.34
    Episode_Reward/reaching_object: 1.2295
     Episode_Reward/lifting_object: 171.6407
      Episode_Reward/object_height: 0.0714
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 1.90s
                      Time elapsed: 00:42:38
                               ETA: 00:29:00

################################################################################
                     [1m Learning iteration 1191/2000 [0m                     

                       Computation: 52190 steps/s (collection: 1.779s, learning 0.104s)
             Mean action noise std: 2.52
          Mean value_function loss: 100.0781
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 50.5756
                       Mean reward: 842.20
               Mean episode length: 226.70
    Episode_Reward/reaching_object: 1.2253
     Episode_Reward/lifting_object: 171.2881
      Episode_Reward/object_height: 0.0707
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 1.88s
                      Time elapsed: 00:42:40
                               ETA: 00:28:57

################################################################################
                     [1m Learning iteration 1192/2000 [0m                     

                       Computation: 52077 steps/s (collection: 1.794s, learning 0.094s)
             Mean action noise std: 2.53
          Mean value_function loss: 91.8277
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.5850
                       Mean reward: 868.01
               Mean episode length: 232.57
    Episode_Reward/reaching_object: 1.2211
     Episode_Reward/lifting_object: 170.3405
      Episode_Reward/object_height: 0.0699
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 1.89s
                      Time elapsed: 00:42:42
                               ETA: 00:28:55

################################################################################
                     [1m Learning iteration 1193/2000 [0m                     

                       Computation: 50864 steps/s (collection: 1.815s, learning 0.118s)
             Mean action noise std: 2.53
          Mean value_function loss: 97.4240
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 50.5989
                       Mean reward: 902.65
               Mean episode length: 241.47
    Episode_Reward/reaching_object: 1.2339
     Episode_Reward/lifting_object: 172.7689
      Episode_Reward/object_height: 0.0708
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 1.93s
                      Time elapsed: 00:42:44
                               ETA: 00:28:53

################################################################################
                     [1m Learning iteration 1194/2000 [0m                     

                       Computation: 51189 steps/s (collection: 1.806s, learning 0.115s)
             Mean action noise std: 2.53
          Mean value_function loss: 126.9193
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.6108
                       Mean reward: 834.44
               Mean episode length: 225.35
    Episode_Reward/reaching_object: 1.2256
     Episode_Reward/lifting_object: 171.2367
      Episode_Reward/object_height: 0.0695
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 1.92s
                      Time elapsed: 00:42:46
                               ETA: 00:28:50

################################################################################
                     [1m Learning iteration 1195/2000 [0m                     

                       Computation: 50743 steps/s (collection: 1.825s, learning 0.113s)
             Mean action noise std: 2.53
          Mean value_function loss: 102.8303
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 50.6184
                       Mean reward: 833.78
               Mean episode length: 224.14
    Episode_Reward/reaching_object: 1.2301
     Episode_Reward/lifting_object: 171.5457
      Episode_Reward/object_height: 0.0697
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 1.94s
                      Time elapsed: 00:42:48
                               ETA: 00:28:48

################################################################################
                     [1m Learning iteration 1196/2000 [0m                     

                       Computation: 51388 steps/s (collection: 1.806s, learning 0.107s)
             Mean action noise std: 2.53
          Mean value_function loss: 104.3761
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 50.6341
                       Mean reward: 878.65
               Mean episode length: 235.54
    Episode_Reward/reaching_object: 1.2301
     Episode_Reward/lifting_object: 171.8180
      Episode_Reward/object_height: 0.0693
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 1.91s
                      Time elapsed: 00:42:50
                               ETA: 00:28:46

################################################################################
                     [1m Learning iteration 1197/2000 [0m                     

                       Computation: 52307 steps/s (collection: 1.782s, learning 0.098s)
             Mean action noise std: 2.53
          Mean value_function loss: 108.0695
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 50.6425
                       Mean reward: 849.40
               Mean episode length: 228.79
    Episode_Reward/reaching_object: 1.2556
     Episode_Reward/lifting_object: 175.5726
      Episode_Reward/object_height: 0.0710
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 1.88s
                      Time elapsed: 00:42:52
                               ETA: 00:28:44

################################################################################
                     [1m Learning iteration 1198/2000 [0m                     

                       Computation: 50721 steps/s (collection: 1.820s, learning 0.118s)
             Mean action noise std: 2.53
          Mean value_function loss: 144.1817
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.6514
                       Mean reward: 840.23
               Mean episode length: 226.30
    Episode_Reward/reaching_object: 1.2488
     Episode_Reward/lifting_object: 174.6008
      Episode_Reward/object_height: 0.0705
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 1.94s
                      Time elapsed: 00:42:54
                               ETA: 00:28:41

################################################################################
                     [1m Learning iteration 1199/2000 [0m                     

                       Computation: 51314 steps/s (collection: 1.801s, learning 0.114s)
             Mean action noise std: 2.53
          Mean value_function loss: 105.2570
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 50.6660
                       Mean reward: 871.02
               Mean episode length: 234.00
    Episode_Reward/reaching_object: 1.2259
     Episode_Reward/lifting_object: 170.8189
      Episode_Reward/object_height: 0.0694
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 1.92s
                      Time elapsed: 00:42:55
                               ETA: 00:28:39

################################################################################
                     [1m Learning iteration 1200/2000 [0m                     

                       Computation: 51107 steps/s (collection: 1.808s, learning 0.115s)
             Mean action noise std: 2.54
          Mean value_function loss: 120.3760
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 50.6768
                       Mean reward: 863.06
               Mean episode length: 232.48
    Episode_Reward/reaching_object: 1.2382
     Episode_Reward/lifting_object: 172.2310
      Episode_Reward/object_height: 0.0699
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 1.92s
                      Time elapsed: 00:42:57
                               ETA: 00:28:37

################################################################################
                     [1m Learning iteration 1201/2000 [0m                     

                       Computation: 50734 steps/s (collection: 1.829s, learning 0.109s)
             Mean action noise std: 2.54
          Mean value_function loss: 106.2451
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 50.6909
                       Mean reward: 882.60
               Mean episode length: 235.79
    Episode_Reward/reaching_object: 1.2345
     Episode_Reward/lifting_object: 171.5355
      Episode_Reward/object_height: 0.0696
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 1.94s
                      Time elapsed: 00:42:59
                               ETA: 00:28:34

################################################################################
                     [1m Learning iteration 1202/2000 [0m                     

                       Computation: 48494 steps/s (collection: 1.915s, learning 0.113s)
             Mean action noise std: 2.54
          Mean value_function loss: 117.4237
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 50.7115
                       Mean reward: 895.51
               Mean episode length: 239.44
    Episode_Reward/reaching_object: 1.2363
     Episode_Reward/lifting_object: 171.9291
      Episode_Reward/object_height: 0.0697
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 2.03s
                      Time elapsed: 00:43:01
                               ETA: 00:28:32

################################################################################
                     [1m Learning iteration 1203/2000 [0m                     

                       Computation: 50028 steps/s (collection: 1.859s, learning 0.106s)
             Mean action noise std: 2.54
          Mean value_function loss: 116.1277
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 50.7301
                       Mean reward: 818.48
               Mean episode length: 220.69
    Episode_Reward/reaching_object: 1.2114
     Episode_Reward/lifting_object: 168.4211
      Episode_Reward/object_height: 0.0685
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 1.96s
                      Time elapsed: 00:43:03
                               ETA: 00:28:30

################################################################################
                     [1m Learning iteration 1204/2000 [0m                     

                       Computation: 51315 steps/s (collection: 1.809s, learning 0.107s)
             Mean action noise std: 2.54
          Mean value_function loss: 103.4282
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 50.7458
                       Mean reward: 880.58
               Mean episode length: 235.41
    Episode_Reward/reaching_object: 1.2464
     Episode_Reward/lifting_object: 173.5525
      Episode_Reward/object_height: 0.0708
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 1.92s
                      Time elapsed: 00:43:05
                               ETA: 00:28:28

################################################################################
                     [1m Learning iteration 1205/2000 [0m                     

                       Computation: 52250 steps/s (collection: 1.794s, learning 0.088s)
             Mean action noise std: 2.54
          Mean value_function loss: 129.7713
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 50.7562
                       Mean reward: 888.28
               Mean episode length: 237.43
    Episode_Reward/reaching_object: 1.2357
     Episode_Reward/lifting_object: 172.5228
      Episode_Reward/object_height: 0.0703
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 1.88s
                      Time elapsed: 00:43:07
                               ETA: 00:28:25

################################################################################
                     [1m Learning iteration 1206/2000 [0m                     

                       Computation: 51537 steps/s (collection: 1.813s, learning 0.094s)
             Mean action noise std: 2.55
          Mean value_function loss: 112.5935
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 50.7651
                       Mean reward: 838.29
               Mean episode length: 224.97
    Episode_Reward/reaching_object: 1.2618
     Episode_Reward/lifting_object: 175.6875
      Episode_Reward/object_height: 0.0721
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 1.91s
                      Time elapsed: 00:43:09
                               ETA: 00:28:23

################################################################################
                     [1m Learning iteration 1207/2000 [0m                     

                       Computation: 50702 steps/s (collection: 1.823s, learning 0.116s)
             Mean action noise std: 2.55
          Mean value_function loss: 108.1626
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 50.7842
                       Mean reward: 882.17
               Mean episode length: 236.49
    Episode_Reward/reaching_object: 1.2295
     Episode_Reward/lifting_object: 170.6733
      Episode_Reward/object_height: 0.0697
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 1.94s
                      Time elapsed: 00:43:11
                               ETA: 00:28:21

################################################################################
                     [1m Learning iteration 1208/2000 [0m                     

                       Computation: 49108 steps/s (collection: 1.845s, learning 0.157s)
             Mean action noise std: 2.55
          Mean value_function loss: 119.2006
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 50.7989
                       Mean reward: 877.44
               Mean episode length: 235.31
    Episode_Reward/reaching_object: 1.2475
     Episode_Reward/lifting_object: 173.6761
      Episode_Reward/object_height: 0.0710
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 2.00s
                      Time elapsed: 00:43:13
                               ETA: 00:28:18

################################################################################
                     [1m Learning iteration 1209/2000 [0m                     

                       Computation: 50525 steps/s (collection: 1.787s, learning 0.158s)
             Mean action noise std: 2.55
          Mean value_function loss: 364.0480
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 50.8031
                       Mean reward: 855.07
               Mean episode length: 230.02
    Episode_Reward/reaching_object: 1.2365
     Episode_Reward/lifting_object: 172.4297
      Episode_Reward/object_height: 0.0705
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 1.95s
                      Time elapsed: 00:43:15
                               ETA: 00:28:16

################################################################################
                     [1m Learning iteration 1210/2000 [0m                     

                       Computation: 50699 steps/s (collection: 1.794s, learning 0.145s)
             Mean action noise std: 2.55
          Mean value_function loss: 165.2541
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 50.8076
                       Mean reward: 827.70
               Mean episode length: 222.49
    Episode_Reward/reaching_object: 1.2171
     Episode_Reward/lifting_object: 169.0521
      Episode_Reward/object_height: 0.0693
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 1.94s
                      Time elapsed: 00:43:17
                               ETA: 00:28:14

################################################################################
                     [1m Learning iteration 1211/2000 [0m                     

                       Computation: 50179 steps/s (collection: 1.835s, learning 0.125s)
             Mean action noise std: 2.55
          Mean value_function loss: 113.1888
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 50.8185
                       Mean reward: 863.57
               Mean episode length: 232.82
    Episode_Reward/reaching_object: 1.2169
     Episode_Reward/lifting_object: 169.4929
      Episode_Reward/object_height: 0.0693
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 1.96s
                      Time elapsed: 00:43:19
                               ETA: 00:28:12

################################################################################
                     [1m Learning iteration 1212/2000 [0m                     

                       Computation: 50026 steps/s (collection: 1.820s, learning 0.146s)
             Mean action noise std: 2.55
          Mean value_function loss: 114.1429
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 50.8308
                       Mean reward: 860.18
               Mean episode length: 231.22
    Episode_Reward/reaching_object: 1.2190
     Episode_Reward/lifting_object: 169.3851
      Episode_Reward/object_height: 0.0693
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 1.97s
                      Time elapsed: 00:43:21
                               ETA: 00:28:09

################################################################################
                     [1m Learning iteration 1213/2000 [0m                     

                       Computation: 51172 steps/s (collection: 1.821s, learning 0.100s)
             Mean action noise std: 2.55
          Mean value_function loss: 125.2907
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 50.8348
                       Mean reward: 862.23
               Mean episode length: 231.59
    Episode_Reward/reaching_object: 1.2425
     Episode_Reward/lifting_object: 173.6138
      Episode_Reward/object_height: 0.0710
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 1.92s
                      Time elapsed: 00:43:23
                               ETA: 00:28:07

################################################################################
                     [1m Learning iteration 1214/2000 [0m                     

                       Computation: 52289 steps/s (collection: 1.790s, learning 0.090s)
             Mean action noise std: 2.55
          Mean value_function loss: 145.7912
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 50.8439
                       Mean reward: 804.86
               Mean episode length: 217.49
    Episode_Reward/reaching_object: 1.2094
     Episode_Reward/lifting_object: 168.4923
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 1.88s
                      Time elapsed: 00:43:25
                               ETA: 00:28:05

################################################################################
                     [1m Learning iteration 1215/2000 [0m                     

                       Computation: 51573 steps/s (collection: 1.810s, learning 0.096s)
             Mean action noise std: 2.56
          Mean value_function loss: 102.1543
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 50.8576
                       Mean reward: 918.88
               Mean episode length: 245.27
    Episode_Reward/reaching_object: 1.2585
     Episode_Reward/lifting_object: 175.2236
      Episode_Reward/object_height: 0.0722
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 1.91s
                      Time elapsed: 00:43:26
                               ETA: 00:28:02

################################################################################
                     [1m Learning iteration 1216/2000 [0m                     

                       Computation: 51115 steps/s (collection: 1.835s, learning 0.088s)
             Mean action noise std: 2.56
          Mean value_function loss: 105.2656
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 50.8737
                       Mean reward: 911.06
               Mean episode length: 243.28
    Episode_Reward/reaching_object: 1.2524
     Episode_Reward/lifting_object: 174.7736
      Episode_Reward/object_height: 0.0717
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 1.92s
                      Time elapsed: 00:43:28
                               ETA: 00:28:00

################################################################################
                     [1m Learning iteration 1217/2000 [0m                     

                       Computation: 52129 steps/s (collection: 1.785s, learning 0.101s)
             Mean action noise std: 2.56
          Mean value_function loss: 152.8838
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 50.8838
                       Mean reward: 846.03
               Mean episode length: 227.15
    Episode_Reward/reaching_object: 1.2226
     Episode_Reward/lifting_object: 170.4270
      Episode_Reward/object_height: 0.0694
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 1.89s
                      Time elapsed: 00:43:30
                               ETA: 00:27:58

################################################################################
                     [1m Learning iteration 1218/2000 [0m                     

                       Computation: 51889 steps/s (collection: 1.801s, learning 0.094s)
             Mean action noise std: 2.56
          Mean value_function loss: 148.8498
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 50.8966
                       Mean reward: 862.40
               Mean episode length: 230.43
    Episode_Reward/reaching_object: 1.2319
     Episode_Reward/lifting_object: 172.1901
      Episode_Reward/object_height: 0.0703
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 1.89s
                      Time elapsed: 00:43:32
                               ETA: 00:27:56

################################################################################
                     [1m Learning iteration 1219/2000 [0m                     

                       Computation: 49945 steps/s (collection: 1.843s, learning 0.126s)
             Mean action noise std: 2.56
          Mean value_function loss: 140.8730
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 50.9065
                       Mean reward: 858.71
               Mean episode length: 229.99
    Episode_Reward/reaching_object: 1.2359
     Episode_Reward/lifting_object: 172.4355
      Episode_Reward/object_height: 0.0698
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 1.97s
                      Time elapsed: 00:43:34
                               ETA: 00:27:53

################################################################################
                     [1m Learning iteration 1220/2000 [0m                     

                       Computation: 51064 steps/s (collection: 1.837s, learning 0.088s)
             Mean action noise std: 2.56
          Mean value_function loss: 131.5272
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 50.9134
                       Mean reward: 842.58
               Mean episode length: 227.07
    Episode_Reward/reaching_object: 1.2317
     Episode_Reward/lifting_object: 171.5137
      Episode_Reward/object_height: 0.0699
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 1.93s
                      Time elapsed: 00:43:36
                               ETA: 00:27:51

################################################################################
                     [1m Learning iteration 1221/2000 [0m                     

                       Computation: 51286 steps/s (collection: 1.805s, learning 0.112s)
             Mean action noise std: 2.56
          Mean value_function loss: 98.4339
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 50.9217
                       Mean reward: 891.04
               Mean episode length: 238.93
    Episode_Reward/reaching_object: 1.2657
     Episode_Reward/lifting_object: 176.8578
      Episode_Reward/object_height: 0.0721
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 1.92s
                      Time elapsed: 00:43:38
                               ETA: 00:27:49

################################################################################
                     [1m Learning iteration 1222/2000 [0m                     

                       Computation: 51369 steps/s (collection: 1.811s, learning 0.103s)
             Mean action noise std: 2.57
          Mean value_function loss: 193.9228
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 50.9337
                       Mean reward: 880.70
               Mean episode length: 237.03
    Episode_Reward/reaching_object: 1.2386
     Episode_Reward/lifting_object: 172.6347
      Episode_Reward/object_height: 0.0705
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 1.91s
                      Time elapsed: 00:43:40
                               ETA: 00:27:46

################################################################################
                     [1m Learning iteration 1223/2000 [0m                     

                       Computation: 51276 steps/s (collection: 1.816s, learning 0.102s)
             Mean action noise std: 2.57
          Mean value_function loss: 135.3698
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 50.9454
                       Mean reward: 878.01
               Mean episode length: 235.40
    Episode_Reward/reaching_object: 1.2129
     Episode_Reward/lifting_object: 169.0013
      Episode_Reward/object_height: 0.0684
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 1.92s
                      Time elapsed: 00:43:42
                               ETA: 00:27:44

################################################################################
                     [1m Learning iteration 1224/2000 [0m                     

                       Computation: 50946 steps/s (collection: 1.824s, learning 0.106s)
             Mean action noise std: 2.57
          Mean value_function loss: 233.6317
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 50.9515
                       Mean reward: 866.01
               Mean episode length: 232.78
    Episode_Reward/reaching_object: 1.1932
     Episode_Reward/lifting_object: 165.2460
      Episode_Reward/object_height: 0.0675
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 1.93s
                      Time elapsed: 00:43:44
                               ETA: 00:27:42

################################################################################
                     [1m Learning iteration 1225/2000 [0m                     

                       Computation: 49975 steps/s (collection: 1.858s, learning 0.109s)
             Mean action noise std: 2.57
          Mean value_function loss: 166.0878
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 50.9590
                       Mean reward: 896.76
               Mean episode length: 239.27
    Episode_Reward/reaching_object: 1.2336
     Episode_Reward/lifting_object: 171.7520
      Episode_Reward/object_height: 0.0700
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 1.97s
                      Time elapsed: 00:43:46
                               ETA: 00:27:40

################################################################################
                     [1m Learning iteration 1226/2000 [0m                     

                       Computation: 50823 steps/s (collection: 1.836s, learning 0.099s)
             Mean action noise std: 2.57
          Mean value_function loss: 121.9546
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 50.9700
                       Mean reward: 852.74
               Mean episode length: 228.58
    Episode_Reward/reaching_object: 1.2329
     Episode_Reward/lifting_object: 172.1614
      Episode_Reward/object_height: 0.0697
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 1.93s
                      Time elapsed: 00:43:48
                               ETA: 00:27:37

################################################################################
                     [1m Learning iteration 1227/2000 [0m                     

                       Computation: 50362 steps/s (collection: 1.834s, learning 0.118s)
             Mean action noise std: 2.57
          Mean value_function loss: 145.0467
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 50.9837
                       Mean reward: 853.22
               Mean episode length: 228.85
    Episode_Reward/reaching_object: 1.2426
     Episode_Reward/lifting_object: 173.6696
      Episode_Reward/object_height: 0.0707
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 1.95s
                      Time elapsed: 00:43:50
                               ETA: 00:27:35

################################################################################
                     [1m Learning iteration 1228/2000 [0m                     

                       Computation: 49684 steps/s (collection: 1.858s, learning 0.121s)
             Mean action noise std: 2.57
          Mean value_function loss: 124.4756
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 50.9925
                       Mean reward: 886.10
               Mean episode length: 236.93
    Episode_Reward/reaching_object: 1.2159
     Episode_Reward/lifting_object: 169.4969
      Episode_Reward/object_height: 0.0684
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 1.98s
                      Time elapsed: 00:43:52
                               ETA: 00:27:33

################################################################################
                     [1m Learning iteration 1229/2000 [0m                     

                       Computation: 49522 steps/s (collection: 1.862s, learning 0.123s)
             Mean action noise std: 2.57
          Mean value_function loss: 147.2839
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 50.9969
                       Mean reward: 855.05
               Mean episode length: 229.96
    Episode_Reward/reaching_object: 1.2024
     Episode_Reward/lifting_object: 167.5636
      Episode_Reward/object_height: 0.0673
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 1.99s
                      Time elapsed: 00:43:54
                               ETA: 00:27:31

################################################################################
                     [1m Learning iteration 1230/2000 [0m                     

                       Computation: 50319 steps/s (collection: 1.833s, learning 0.120s)
             Mean action noise std: 2.57
          Mean value_function loss: 113.7244
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.0063
                       Mean reward: 879.43
               Mean episode length: 235.31
    Episode_Reward/reaching_object: 1.2396
     Episode_Reward/lifting_object: 173.4575
      Episode_Reward/object_height: 0.0697
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 1.95s
                      Time elapsed: 00:43:56
                               ETA: 00:27:28

################################################################################
                     [1m Learning iteration 1231/2000 [0m                     

                       Computation: 51092 steps/s (collection: 1.826s, learning 0.098s)
             Mean action noise std: 2.58
          Mean value_function loss: 129.0196
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 51.0188
                       Mean reward: 846.11
               Mean episode length: 228.37
    Episode_Reward/reaching_object: 1.2200
     Episode_Reward/lifting_object: 169.9355
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 1.92s
                      Time elapsed: 00:43:57
                               ETA: 00:27:26

################################################################################
                     [1m Learning iteration 1232/2000 [0m                     

                       Computation: 49526 steps/s (collection: 1.878s, learning 0.107s)
             Mean action noise std: 2.58
          Mean value_function loss: 125.8656
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 51.0277
                       Mean reward: 844.90
               Mean episode length: 226.93
    Episode_Reward/reaching_object: 1.2090
     Episode_Reward/lifting_object: 169.0508
      Episode_Reward/object_height: 0.0671
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 1.98s
                      Time elapsed: 00:43:59
                               ETA: 00:27:24

################################################################################
                     [1m Learning iteration 1233/2000 [0m                     

                       Computation: 50724 steps/s (collection: 1.830s, learning 0.108s)
             Mean action noise std: 2.58
          Mean value_function loss: 121.3082
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 51.0394
                       Mean reward: 851.81
               Mean episode length: 228.82
    Episode_Reward/reaching_object: 1.1993
     Episode_Reward/lifting_object: 167.5540
      Episode_Reward/object_height: 0.0670
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 1.94s
                      Time elapsed: 00:44:01
                               ETA: 00:27:22

################################################################################
                     [1m Learning iteration 1234/2000 [0m                     

                       Computation: 49690 steps/s (collection: 1.862s, learning 0.116s)
             Mean action noise std: 2.58
          Mean value_function loss: 109.3199
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 51.0551
                       Mean reward: 881.20
               Mean episode length: 236.29
    Episode_Reward/reaching_object: 1.2082
     Episode_Reward/lifting_object: 169.0342
      Episode_Reward/object_height: 0.0673
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 1.98s
                      Time elapsed: 00:44:03
                               ETA: 00:27:19

################################################################################
                     [1m Learning iteration 1235/2000 [0m                     

                       Computation: 50410 steps/s (collection: 1.825s, learning 0.125s)
             Mean action noise std: 2.58
          Mean value_function loss: 130.4747
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 51.0653
                       Mean reward: 855.52
               Mean episode length: 229.49
    Episode_Reward/reaching_object: 1.2166
     Episode_Reward/lifting_object: 170.3815
      Episode_Reward/object_height: 0.0680
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 1.95s
                      Time elapsed: 00:44:05
                               ETA: 00:27:17

################################################################################
                     [1m Learning iteration 1236/2000 [0m                     

                       Computation: 51130 steps/s (collection: 1.822s, learning 0.101s)
             Mean action noise std: 2.58
          Mean value_function loss: 123.7767
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 51.0724
                       Mean reward: 848.73
               Mean episode length: 227.47
    Episode_Reward/reaching_object: 1.2132
     Episode_Reward/lifting_object: 170.3156
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 1.92s
                      Time elapsed: 00:44:07
                               ETA: 00:27:15

################################################################################
                     [1m Learning iteration 1237/2000 [0m                     

                       Computation: 47099 steps/s (collection: 1.924s, learning 0.164s)
             Mean action noise std: 2.58
          Mean value_function loss: 144.7637
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 51.0788
                       Mean reward: 892.14
               Mean episode length: 238.72
    Episode_Reward/reaching_object: 1.2299
     Episode_Reward/lifting_object: 173.1750
      Episode_Reward/object_height: 0.0695
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 2.09s
                      Time elapsed: 00:44:09
                               ETA: 00:27:13

################################################################################
                     [1m Learning iteration 1238/2000 [0m                     

                       Computation: 50040 steps/s (collection: 1.860s, learning 0.104s)
             Mean action noise std: 2.58
          Mean value_function loss: 151.4937
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 51.0847
                       Mean reward: 863.38
               Mean episode length: 231.64
    Episode_Reward/reaching_object: 1.2147
     Episode_Reward/lifting_object: 170.4333
      Episode_Reward/object_height: 0.0688
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 1.96s
                      Time elapsed: 00:44:11
                               ETA: 00:27:10

################################################################################
                     [1m Learning iteration 1239/2000 [0m                     

                       Computation: 50656 steps/s (collection: 1.846s, learning 0.095s)
             Mean action noise std: 2.58
          Mean value_function loss: 148.0438
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 51.0895
                       Mean reward: 838.87
               Mean episode length: 226.57
    Episode_Reward/reaching_object: 1.2095
     Episode_Reward/lifting_object: 170.0357
      Episode_Reward/object_height: 0.0685
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 1.94s
                      Time elapsed: 00:44:13
                               ETA: 00:27:08

################################################################################
                     [1m Learning iteration 1240/2000 [0m                     

                       Computation: 50549 steps/s (collection: 1.843s, learning 0.102s)
             Mean action noise std: 2.58
          Mean value_function loss: 133.6470
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 51.0936
                       Mean reward: 824.65
               Mean episode length: 221.28
    Episode_Reward/reaching_object: 1.1856
     Episode_Reward/lifting_object: 166.6218
      Episode_Reward/object_height: 0.0670
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 1.94s
                      Time elapsed: 00:44:15
                               ETA: 00:27:06

################################################################################
                     [1m Learning iteration 1241/2000 [0m                     

                       Computation: 50169 steps/s (collection: 1.868s, learning 0.091s)
             Mean action noise std: 2.58
          Mean value_function loss: 117.9395
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 51.0973
                       Mean reward: 878.40
               Mean episode length: 234.85
    Episode_Reward/reaching_object: 1.2168
     Episode_Reward/lifting_object: 171.0669
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 1.96s
                      Time elapsed: 00:44:17
                               ETA: 00:27:04

################################################################################
                     [1m Learning iteration 1242/2000 [0m                     

                       Computation: 49613 steps/s (collection: 1.890s, learning 0.092s)
             Mean action noise std: 2.59
          Mean value_function loss: 166.2903
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.1018
                       Mean reward: 810.55
               Mean episode length: 217.61
    Episode_Reward/reaching_object: 1.2124
     Episode_Reward/lifting_object: 170.2188
      Episode_Reward/object_height: 0.0680
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 1.98s
                      Time elapsed: 00:44:19
                               ETA: 00:27:01

################################################################################
                     [1m Learning iteration 1243/2000 [0m                     

                       Computation: 47744 steps/s (collection: 1.906s, learning 0.153s)
             Mean action noise std: 2.59
          Mean value_function loss: 104.4552
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 51.1098
                       Mean reward: 889.64
               Mean episode length: 237.53
    Episode_Reward/reaching_object: 1.2303
     Episode_Reward/lifting_object: 172.1790
      Episode_Reward/object_height: 0.0697
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 2.06s
                      Time elapsed: 00:44:21
                               ETA: 00:26:59

################################################################################
                     [1m Learning iteration 1244/2000 [0m                     

                       Computation: 49288 steps/s (collection: 1.889s, learning 0.105s)
             Mean action noise std: 2.59
          Mean value_function loss: 108.7294
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 51.1212
                       Mean reward: 872.17
               Mean episode length: 233.47
    Episode_Reward/reaching_object: 1.2044
     Episode_Reward/lifting_object: 168.6072
      Episode_Reward/object_height: 0.0678
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 1.99s
                      Time elapsed: 00:44:23
                               ETA: 00:26:57

################################################################################
                     [1m Learning iteration 1245/2000 [0m                     

                       Computation: 49210 steps/s (collection: 1.889s, learning 0.108s)
             Mean action noise std: 2.59
          Mean value_function loss: 85.4646
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.1335
                       Mean reward: 845.95
               Mean episode length: 228.85
    Episode_Reward/reaching_object: 1.2326
     Episode_Reward/lifting_object: 172.5318
      Episode_Reward/object_height: 0.0693
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 2.00s
                      Time elapsed: 00:44:25
                               ETA: 00:26:55

################################################################################
                     [1m Learning iteration 1246/2000 [0m                     

                       Computation: 48785 steps/s (collection: 1.892s, learning 0.123s)
             Mean action noise std: 2.59
          Mean value_function loss: 108.3370
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 51.1469
                       Mean reward: 809.48
               Mean episode length: 218.37
    Episode_Reward/reaching_object: 1.2250
     Episode_Reward/lifting_object: 171.1335
      Episode_Reward/object_height: 0.0688
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 2.02s
                      Time elapsed: 00:44:27
                               ETA: 00:26:52

################################################################################
                     [1m Learning iteration 1247/2000 [0m                     

                       Computation: 48007 steps/s (collection: 1.941s, learning 0.107s)
             Mean action noise std: 2.59
          Mean value_function loss: 98.6024
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.1583
                       Mean reward: 845.81
               Mean episode length: 227.45
    Episode_Reward/reaching_object: 1.2478
     Episode_Reward/lifting_object: 174.2680
      Episode_Reward/object_height: 0.0699
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 2.05s
                      Time elapsed: 00:44:29
                               ETA: 00:26:50

################################################################################
                     [1m Learning iteration 1248/2000 [0m                     

                       Computation: 49149 steps/s (collection: 1.895s, learning 0.105s)
             Mean action noise std: 2.59
          Mean value_function loss: 102.5546
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 51.1664
                       Mean reward: 860.40
               Mean episode length: 231.25
    Episode_Reward/reaching_object: 1.2532
     Episode_Reward/lifting_object: 174.7795
      Episode_Reward/object_height: 0.0698
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 2.00s
                      Time elapsed: 00:44:31
                               ETA: 00:26:48

################################################################################
                     [1m Learning iteration 1249/2000 [0m                     

                       Computation: 50015 steps/s (collection: 1.868s, learning 0.097s)
             Mean action noise std: 2.59
          Mean value_function loss: 124.5982
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 51.1792
                       Mean reward: 899.48
               Mean episode length: 239.13
    Episode_Reward/reaching_object: 1.2376
     Episode_Reward/lifting_object: 172.5905
      Episode_Reward/object_height: 0.0686
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 1.97s
                      Time elapsed: 00:44:33
                               ETA: 00:26:46

################################################################################
                     [1m Learning iteration 1250/2000 [0m                     

                       Computation: 49409 steps/s (collection: 1.900s, learning 0.090s)
             Mean action noise std: 2.60
          Mean value_function loss: 137.3961
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 51.1991
                       Mean reward: 880.64
               Mean episode length: 234.81
    Episode_Reward/reaching_object: 1.2242
     Episode_Reward/lifting_object: 171.2967
      Episode_Reward/object_height: 0.0681
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 1.99s
                      Time elapsed: 00:44:35
                               ETA: 00:26:44

################################################################################
                     [1m Learning iteration 1251/2000 [0m                     

                       Computation: 47624 steps/s (collection: 1.963s, learning 0.101s)
             Mean action noise std: 2.60
          Mean value_function loss: 97.5142
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 51.2158
                       Mean reward: 889.05
               Mean episode length: 237.16
    Episode_Reward/reaching_object: 1.2348
     Episode_Reward/lifting_object: 172.8442
      Episode_Reward/object_height: 0.0688
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 2.06s
                      Time elapsed: 00:44:37
                               ETA: 00:26:41

################################################################################
                     [1m Learning iteration 1252/2000 [0m                     

                       Computation: 48899 steps/s (collection: 1.907s, learning 0.103s)
             Mean action noise std: 2.60
          Mean value_function loss: 123.3869
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 51.2315
                       Mean reward: 827.34
               Mean episode length: 222.22
    Episode_Reward/reaching_object: 1.1961
     Episode_Reward/lifting_object: 167.0744
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 2.01s
                      Time elapsed: 00:44:39
                               ETA: 00:26:39

################################################################################
                     [1m Learning iteration 1253/2000 [0m                     

                       Computation: 47474 steps/s (collection: 1.956s, learning 0.115s)
             Mean action noise std: 2.60
          Mean value_function loss: 108.4793
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 51.2448
                       Mean reward: 809.78
               Mean episode length: 218.95
    Episode_Reward/reaching_object: 1.2219
     Episode_Reward/lifting_object: 170.9451
      Episode_Reward/object_height: 0.0681
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 2.07s
                      Time elapsed: 00:44:41
                               ETA: 00:26:37

################################################################################
                     [1m Learning iteration 1254/2000 [0m                     

                       Computation: 50408 steps/s (collection: 1.856s, learning 0.095s)
             Mean action noise std: 2.60
          Mean value_function loss: 123.0918
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 51.2553
                       Mean reward: 882.02
               Mean episode length: 234.91
    Episode_Reward/reaching_object: 1.2025
     Episode_Reward/lifting_object: 168.2071
      Episode_Reward/object_height: 0.0667
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 1.95s
                      Time elapsed: 00:44:43
                               ETA: 00:26:35

################################################################################
                     [1m Learning iteration 1255/2000 [0m                     

                       Computation: 49610 steps/s (collection: 1.876s, learning 0.106s)
             Mean action noise std: 2.60
          Mean value_function loss: 101.0641
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 51.2664
                       Mean reward: 881.72
               Mean episode length: 235.14
    Episode_Reward/reaching_object: 1.2220
     Episode_Reward/lifting_object: 171.0942
      Episode_Reward/object_height: 0.0686
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 1.98s
                      Time elapsed: 00:44:45
                               ETA: 00:26:33

################################################################################
                     [1m Learning iteration 1256/2000 [0m                     

                       Computation: 47591 steps/s (collection: 1.927s, learning 0.139s)
             Mean action noise std: 2.60
          Mean value_function loss: 146.6031
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 51.2754
                       Mean reward: 839.39
               Mean episode length: 226.04
    Episode_Reward/reaching_object: 1.2125
     Episode_Reward/lifting_object: 169.8278
      Episode_Reward/object_height: 0.0680
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 2.07s
                      Time elapsed: 00:44:47
                               ETA: 00:26:30

################################################################################
                     [1m Learning iteration 1257/2000 [0m                     

                       Computation: 49085 steps/s (collection: 1.898s, learning 0.105s)
             Mean action noise std: 2.61
          Mean value_function loss: 154.9752
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 51.2863
                       Mean reward: 867.90
               Mean episode length: 232.26
    Episode_Reward/reaching_object: 1.1607
     Episode_Reward/lifting_object: 161.6089
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 2.00s
                      Time elapsed: 00:44:49
                               ETA: 00:26:28

################################################################################
                     [1m Learning iteration 1258/2000 [0m                     

                       Computation: 50105 steps/s (collection: 1.845s, learning 0.117s)
             Mean action noise std: 2.61
          Mean value_function loss: 74.7672
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.2981
                       Mean reward: 883.35
               Mean episode length: 236.90
    Episode_Reward/reaching_object: 1.2610
     Episode_Reward/lifting_object: 177.1235
      Episode_Reward/object_height: 0.0714
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 1.96s
                      Time elapsed: 00:44:51
                               ETA: 00:26:26

################################################################################
                     [1m Learning iteration 1259/2000 [0m                     

                       Computation: 50079 steps/s (collection: 1.862s, learning 0.101s)
             Mean action noise std: 2.61
          Mean value_function loss: 101.8360
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 51.3117
                       Mean reward: 864.67
               Mean episode length: 232.63
    Episode_Reward/reaching_object: 1.2426
     Episode_Reward/lifting_object: 173.7350
      Episode_Reward/object_height: 0.0699
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 1.96s
                      Time elapsed: 00:44:53
                               ETA: 00:26:24

################################################################################
                     [1m Learning iteration 1260/2000 [0m                     

                       Computation: 49090 steps/s (collection: 1.900s, learning 0.102s)
             Mean action noise std: 2.61
          Mean value_function loss: 99.2426
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.3195
                       Mean reward: 900.71
               Mean episode length: 239.91
    Episode_Reward/reaching_object: 1.2579
     Episode_Reward/lifting_object: 176.0813
      Episode_Reward/object_height: 0.0710
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 2.00s
                      Time elapsed: 00:44:55
                               ETA: 00:26:21

################################################################################
                     [1m Learning iteration 1261/2000 [0m                     

                       Computation: 50366 steps/s (collection: 1.858s, learning 0.094s)
             Mean action noise std: 2.61
          Mean value_function loss: 147.0119
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 51.3278
                       Mean reward: 858.25
               Mean episode length: 229.72
    Episode_Reward/reaching_object: 1.2024
     Episode_Reward/lifting_object: 167.5015
      Episode_Reward/object_height: 0.0671
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 1.95s
                      Time elapsed: 00:44:57
                               ETA: 00:26:19

################################################################################
                     [1m Learning iteration 1262/2000 [0m                     

                       Computation: 49697 steps/s (collection: 1.873s, learning 0.105s)
             Mean action noise std: 2.61
          Mean value_function loss: 136.5818
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.3363
                       Mean reward: 870.75
               Mean episode length: 232.53
    Episode_Reward/reaching_object: 1.2174
     Episode_Reward/lifting_object: 169.8484
      Episode_Reward/object_height: 0.0685
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 1.98s
                      Time elapsed: 00:44:59
                               ETA: 00:26:17

################################################################################
                     [1m Learning iteration 1263/2000 [0m                     

                       Computation: 49560 steps/s (collection: 1.893s, learning 0.091s)
             Mean action noise std: 2.61
          Mean value_function loss: 142.3260
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 51.3475
                       Mean reward: 847.62
               Mean episode length: 228.08
    Episode_Reward/reaching_object: 1.2207
     Episode_Reward/lifting_object: 169.7484
      Episode_Reward/object_height: 0.0685
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 1.98s
                      Time elapsed: 00:45:01
                               ETA: 00:26:15

################################################################################
                     [1m Learning iteration 1264/2000 [0m                     

                       Computation: 50318 steps/s (collection: 1.848s, learning 0.106s)
             Mean action noise std: 2.61
          Mean value_function loss: 167.6889
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 51.3563
                       Mean reward: 841.94
               Mean episode length: 226.78
    Episode_Reward/reaching_object: 1.1931
     Episode_Reward/lifting_object: 165.4604
      Episode_Reward/object_height: 0.0664
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 1.95s
                      Time elapsed: 00:45:03
                               ETA: 00:26:13

################################################################################
                     [1m Learning iteration 1265/2000 [0m                     

                       Computation: 49485 steps/s (collection: 1.860s, learning 0.126s)
             Mean action noise std: 2.62
          Mean value_function loss: 191.9324
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 51.3636
                       Mean reward: 871.01
               Mean episode length: 234.66
    Episode_Reward/reaching_object: 1.2566
     Episode_Reward/lifting_object: 174.6642
      Episode_Reward/object_height: 0.0710
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 1.99s
                      Time elapsed: 00:45:05
                               ETA: 00:26:10

################################################################################
                     [1m Learning iteration 1266/2000 [0m                     

                       Computation: 49172 steps/s (collection: 1.881s, learning 0.118s)
             Mean action noise std: 2.62
          Mean value_function loss: 113.2731
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 51.3715
                       Mean reward: 855.87
               Mean episode length: 229.69
    Episode_Reward/reaching_object: 1.2304
     Episode_Reward/lifting_object: 170.5788
      Episode_Reward/object_height: 0.0692
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 2.00s
                      Time elapsed: 00:45:07
                               ETA: 00:26:08

################################################################################
                     [1m Learning iteration 1267/2000 [0m                     

                       Computation: 50395 steps/s (collection: 1.847s, learning 0.104s)
             Mean action noise std: 2.62
          Mean value_function loss: 116.3349
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 51.3835
                       Mean reward: 834.07
               Mean episode length: 224.97
    Episode_Reward/reaching_object: 1.2324
     Episode_Reward/lifting_object: 170.9355
      Episode_Reward/object_height: 0.0699
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 1.95s
                      Time elapsed: 00:45:09
                               ETA: 00:26:06

################################################################################
                     [1m Learning iteration 1268/2000 [0m                     

                       Computation: 49248 steps/s (collection: 1.889s, learning 0.107s)
             Mean action noise std: 2.62
          Mean value_function loss: 121.5118
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.3937
                       Mean reward: 865.06
               Mean episode length: 231.18
    Episode_Reward/reaching_object: 1.2188
     Episode_Reward/lifting_object: 168.9791
      Episode_Reward/object_height: 0.0692
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 2.00s
                      Time elapsed: 00:45:11
                               ETA: 00:26:04

################################################################################
                     [1m Learning iteration 1269/2000 [0m                     

                       Computation: 49409 steps/s (collection: 1.887s, learning 0.103s)
             Mean action noise std: 2.62
          Mean value_function loss: 121.2111
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 51.4049
                       Mean reward: 851.20
               Mean episode length: 229.19
    Episode_Reward/reaching_object: 1.2450
     Episode_Reward/lifting_object: 172.5197
      Episode_Reward/object_height: 0.0708
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 1.99s
                      Time elapsed: 00:45:13
                               ETA: 00:26:01

################################################################################
                     [1m Learning iteration 1270/2000 [0m                     

                       Computation: 49357 steps/s (collection: 1.893s, learning 0.099s)
             Mean action noise std: 2.62
          Mean value_function loss: 114.0798
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 51.4120
                       Mean reward: 913.03
               Mean episode length: 243.72
    Episode_Reward/reaching_object: 1.2438
     Episode_Reward/lifting_object: 172.5856
      Episode_Reward/object_height: 0.0712
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 1.99s
                      Time elapsed: 00:45:15
                               ETA: 00:25:59

################################################################################
                     [1m Learning iteration 1271/2000 [0m                     

                       Computation: 49909 steps/s (collection: 1.875s, learning 0.095s)
             Mean action noise std: 2.62
          Mean value_function loss: 131.5131
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 51.4197
                       Mean reward: 828.36
               Mean episode length: 222.30
    Episode_Reward/reaching_object: 1.2226
     Episode_Reward/lifting_object: 169.7273
      Episode_Reward/object_height: 0.0698
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 1.97s
                      Time elapsed: 00:45:17
                               ETA: 00:25:57

################################################################################
                     [1m Learning iteration 1272/2000 [0m                     

                       Computation: 48944 steps/s (collection: 1.907s, learning 0.101s)
             Mean action noise std: 2.62
          Mean value_function loss: 152.4813
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 51.4295
                       Mean reward: 825.69
               Mean episode length: 221.77
    Episode_Reward/reaching_object: 1.2045
     Episode_Reward/lifting_object: 166.7637
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 2.01s
                      Time elapsed: 00:45:19
                               ETA: 00:25:55

################################################################################
                     [1m Learning iteration 1273/2000 [0m                     

                       Computation: 48195 steps/s (collection: 1.914s, learning 0.125s)
             Mean action noise std: 2.62
          Mean value_function loss: 113.2821
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 51.4357
                       Mean reward: 852.45
               Mean episode length: 228.75
    Episode_Reward/reaching_object: 1.2190
     Episode_Reward/lifting_object: 168.8120
      Episode_Reward/object_height: 0.0695
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 2.04s
                      Time elapsed: 00:45:21
                               ETA: 00:25:53

################################################################################
                     [1m Learning iteration 1274/2000 [0m                     

                       Computation: 49265 steps/s (collection: 1.904s, learning 0.092s)
             Mean action noise std: 2.62
          Mean value_function loss: 115.7154
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 51.4456
                       Mean reward: 871.99
               Mean episode length: 233.24
    Episode_Reward/reaching_object: 1.2341
     Episode_Reward/lifting_object: 171.2369
      Episode_Reward/object_height: 0.0709
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 2.00s
                      Time elapsed: 00:45:23
                               ETA: 00:25:50

################################################################################
                     [1m Learning iteration 1275/2000 [0m                     

                       Computation: 49713 steps/s (collection: 1.877s, learning 0.100s)
             Mean action noise std: 2.63
          Mean value_function loss: 150.1055
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.4550
                       Mean reward: 805.45
               Mean episode length: 217.22
    Episode_Reward/reaching_object: 1.1878
     Episode_Reward/lifting_object: 164.3058
      Episode_Reward/object_height: 0.0680
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 1.98s
                      Time elapsed: 00:45:25
                               ETA: 00:25:48

################################################################################
                     [1m Learning iteration 1276/2000 [0m                     

                       Computation: 48890 steps/s (collection: 1.890s, learning 0.121s)
             Mean action noise std: 2.63
          Mean value_function loss: 88.1379
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 51.4644
                       Mean reward: 894.68
               Mean episode length: 238.61
    Episode_Reward/reaching_object: 1.2392
     Episode_Reward/lifting_object: 171.7603
      Episode_Reward/object_height: 0.0715
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 2.01s
                      Time elapsed: 00:45:27
                               ETA: 00:25:46

################################################################################
                     [1m Learning iteration 1277/2000 [0m                     

                       Computation: 49223 steps/s (collection: 1.882s, learning 0.115s)
             Mean action noise std: 2.63
          Mean value_function loss: 124.8722
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 51.4670
                       Mean reward: 858.49
               Mean episode length: 230.63
    Episode_Reward/reaching_object: 1.2133
     Episode_Reward/lifting_object: 167.8005
      Episode_Reward/object_height: 0.0698
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 2.00s
                      Time elapsed: 00:45:29
                               ETA: 00:25:44

################################################################################
                     [1m Learning iteration 1278/2000 [0m                     

                       Computation: 50475 steps/s (collection: 1.848s, learning 0.100s)
             Mean action noise std: 2.63
          Mean value_function loss: 92.5540
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 51.4740
                       Mean reward: 874.65
               Mean episode length: 234.83
    Episode_Reward/reaching_object: 1.2572
     Episode_Reward/lifting_object: 174.3273
      Episode_Reward/object_height: 0.0729
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 1.95s
                      Time elapsed: 00:45:31
                               ETA: 00:25:41

################################################################################
                     [1m Learning iteration 1279/2000 [0m                     

                       Computation: 49501 steps/s (collection: 1.882s, learning 0.104s)
             Mean action noise std: 2.63
          Mean value_function loss: 126.3925
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 51.4881
                       Mean reward: 853.05
               Mean episode length: 229.51
    Episode_Reward/reaching_object: 1.2203
     Episode_Reward/lifting_object: 168.8721
      Episode_Reward/object_height: 0.0706
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 1.99s
                      Time elapsed: 00:45:33
                               ETA: 00:25:39

################################################################################
                     [1m Learning iteration 1280/2000 [0m                     

                       Computation: 49321 steps/s (collection: 1.869s, learning 0.124s)
             Mean action noise std: 2.63
          Mean value_function loss: 147.2905
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 51.5015
                       Mean reward: 840.33
               Mean episode length: 226.97
    Episode_Reward/reaching_object: 1.2112
     Episode_Reward/lifting_object: 167.4645
      Episode_Reward/object_height: 0.0701
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 1.99s
                      Time elapsed: 00:45:35
                               ETA: 00:25:37

################################################################################
                     [1m Learning iteration 1281/2000 [0m                     

                       Computation: 48728 steps/s (collection: 1.893s, learning 0.125s)
             Mean action noise std: 2.63
          Mean value_function loss: 137.0635
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 51.5146
                       Mean reward: 836.15
               Mean episode length: 224.72
    Episode_Reward/reaching_object: 1.2247
     Episode_Reward/lifting_object: 169.9297
      Episode_Reward/object_height: 0.0712
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 2.02s
                      Time elapsed: 00:45:37
                               ETA: 00:25:35

################################################################################
                     [1m Learning iteration 1282/2000 [0m                     

                       Computation: 48904 steps/s (collection: 1.893s, learning 0.117s)
             Mean action noise std: 2.63
          Mean value_function loss: 103.2783
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 51.5284
                       Mean reward: 864.54
               Mean episode length: 231.27
    Episode_Reward/reaching_object: 1.2418
     Episode_Reward/lifting_object: 172.3632
      Episode_Reward/object_height: 0.0725
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 2.01s
                      Time elapsed: 00:45:39
                               ETA: 00:25:33

################################################################################
                     [1m Learning iteration 1283/2000 [0m                     

                       Computation: 49251 steps/s (collection: 1.875s, learning 0.121s)
             Mean action noise std: 2.64
          Mean value_function loss: 124.9718
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 51.5422
                       Mean reward: 882.24
               Mean episode length: 236.09
    Episode_Reward/reaching_object: 1.2535
     Episode_Reward/lifting_object: 174.0692
      Episode_Reward/object_height: 0.0730
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 2.00s
                      Time elapsed: 00:45:41
                               ETA: 00:25:30

################################################################################
                     [1m Learning iteration 1284/2000 [0m                     

                       Computation: 49384 steps/s (collection: 1.866s, learning 0.125s)
             Mean action noise std: 2.64
          Mean value_function loss: 183.2997
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.5504
                       Mean reward: 886.30
               Mean episode length: 238.05
    Episode_Reward/reaching_object: 1.2513
     Episode_Reward/lifting_object: 173.5287
      Episode_Reward/object_height: 0.0731
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 1.99s
                      Time elapsed: 00:45:43
                               ETA: 00:25:28

################################################################################
                     [1m Learning iteration 1285/2000 [0m                     

                       Computation: 49263 steps/s (collection: 1.871s, learning 0.124s)
             Mean action noise std: 2.64
          Mean value_function loss: 182.7838
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 51.5592
                       Mean reward: 876.54
               Mean episode length: 234.61
    Episode_Reward/reaching_object: 1.2426
     Episode_Reward/lifting_object: 172.8938
      Episode_Reward/object_height: 0.0732
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 2.00s
                      Time elapsed: 00:45:45
                               ETA: 00:25:26

################################################################################
                     [1m Learning iteration 1286/2000 [0m                     

                       Computation: 49893 steps/s (collection: 1.849s, learning 0.122s)
             Mean action noise std: 2.64
          Mean value_function loss: 98.5210
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 51.5692
                       Mean reward: 855.84
               Mean episode length: 229.95
    Episode_Reward/reaching_object: 1.2336
     Episode_Reward/lifting_object: 171.3210
      Episode_Reward/object_height: 0.0725
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 1.97s
                      Time elapsed: 00:45:47
                               ETA: 00:25:24

################################################################################
                     [1m Learning iteration 1287/2000 [0m                     

                       Computation: 49771 steps/s (collection: 1.860s, learning 0.116s)
             Mean action noise std: 2.64
          Mean value_function loss: 90.3581
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 51.5800
                       Mean reward: 871.84
               Mean episode length: 232.83
    Episode_Reward/reaching_object: 1.2613
     Episode_Reward/lifting_object: 175.8995
      Episode_Reward/object_height: 0.0745
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 1.98s
                      Time elapsed: 00:45:49
                               ETA: 00:25:21

################################################################################
                     [1m Learning iteration 1288/2000 [0m                     

                       Computation: 49851 steps/s (collection: 1.849s, learning 0.123s)
             Mean action noise std: 2.64
          Mean value_function loss: 82.2917
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 51.5859
                       Mean reward: 893.45
               Mean episode length: 238.16
    Episode_Reward/reaching_object: 1.2658
     Episode_Reward/lifting_object: 176.7003
      Episode_Reward/object_height: 0.0745
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 1.97s
                      Time elapsed: 00:45:51
                               ETA: 00:25:19

################################################################################
                     [1m Learning iteration 1289/2000 [0m                     

                       Computation: 48955 steps/s (collection: 1.892s, learning 0.116s)
             Mean action noise std: 2.64
          Mean value_function loss: 100.2672
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.5943
                       Mean reward: 891.62
               Mean episode length: 237.78
    Episode_Reward/reaching_object: 1.2488
     Episode_Reward/lifting_object: 174.5547
      Episode_Reward/object_height: 0.0734
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 2.01s
                      Time elapsed: 00:45:53
                               ETA: 00:25:17

################################################################################
                     [1m Learning iteration 1290/2000 [0m                     

                       Computation: 49570 steps/s (collection: 1.875s, learning 0.108s)
             Mean action noise std: 2.64
          Mean value_function loss: 114.3549
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 51.6050
                       Mean reward: 836.96
               Mean episode length: 225.10
    Episode_Reward/reaching_object: 1.2228
     Episode_Reward/lifting_object: 170.3833
      Episode_Reward/object_height: 0.0716
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 1.98s
                      Time elapsed: 00:45:55
                               ETA: 00:25:15

################################################################################
                     [1m Learning iteration 1291/2000 [0m                     

                       Computation: 49730 steps/s (collection: 1.882s, learning 0.095s)
             Mean action noise std: 2.64
          Mean value_function loss: 96.2108
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 51.6095
                       Mean reward: 897.73
               Mean episode length: 239.71
    Episode_Reward/reaching_object: 1.2756
     Episode_Reward/lifting_object: 178.1827
      Episode_Reward/object_height: 0.0750
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 1.98s
                      Time elapsed: 00:45:57
                               ETA: 00:25:13

################################################################################
                     [1m Learning iteration 1292/2000 [0m                     

                       Computation: 49312 steps/s (collection: 1.878s, learning 0.115s)
             Mean action noise std: 2.65
          Mean value_function loss: 97.0087
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 51.6177
                       Mean reward: 891.32
               Mean episode length: 238.07
    Episode_Reward/reaching_object: 1.2393
     Episode_Reward/lifting_object: 172.0531
      Episode_Reward/object_height: 0.0729
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 1.99s
                      Time elapsed: 00:45:59
                               ETA: 00:25:10

################################################################################
                     [1m Learning iteration 1293/2000 [0m                     

                       Computation: 49560 steps/s (collection: 1.859s, learning 0.125s)
             Mean action noise std: 2.65
          Mean value_function loss: 117.0693
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 51.6272
                       Mean reward: 892.85
               Mean episode length: 238.28
    Episode_Reward/reaching_object: 1.2612
     Episode_Reward/lifting_object: 175.9886
      Episode_Reward/object_height: 0.0745
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 1.98s
                      Time elapsed: 00:46:01
                               ETA: 00:25:08

################################################################################
                     [1m Learning iteration 1294/2000 [0m                     

                       Computation: 49744 steps/s (collection: 1.867s, learning 0.109s)
             Mean action noise std: 2.65
          Mean value_function loss: 126.2714
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 51.6347
                       Mean reward: 871.47
               Mean episode length: 234.44
    Episode_Reward/reaching_object: 1.2333
     Episode_Reward/lifting_object: 171.6506
      Episode_Reward/object_height: 0.0726
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 1.98s
                      Time elapsed: 00:46:03
                               ETA: 00:25:06

################################################################################
                     [1m Learning iteration 1295/2000 [0m                     

                       Computation: 49135 steps/s (collection: 1.881s, learning 0.120s)
             Mean action noise std: 2.65
          Mean value_function loss: 111.5975
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 51.6421
                       Mean reward: 867.44
               Mean episode length: 233.13
    Episode_Reward/reaching_object: 1.2324
     Episode_Reward/lifting_object: 170.7344
      Episode_Reward/object_height: 0.0726
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 2.00s
                      Time elapsed: 00:46:05
                               ETA: 00:25:04

################################################################################
                     [1m Learning iteration 1296/2000 [0m                     

                       Computation: 49460 steps/s (collection: 1.866s, learning 0.121s)
             Mean action noise std: 2.65
          Mean value_function loss: 131.2690
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 51.6499
                       Mean reward: 867.31
               Mean episode length: 232.25
    Episode_Reward/reaching_object: 1.2326
     Episode_Reward/lifting_object: 171.4468
      Episode_Reward/object_height: 0.0726
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 1.99s
                      Time elapsed: 00:46:07
                               ETA: 00:25:02

################################################################################
                     [1m Learning iteration 1297/2000 [0m                     

                       Computation: 50223 steps/s (collection: 1.839s, learning 0.119s)
             Mean action noise std: 2.65
          Mean value_function loss: 107.3552
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 51.6592
                       Mean reward: 888.58
               Mean episode length: 237.77
    Episode_Reward/reaching_object: 1.2347
     Episode_Reward/lifting_object: 171.8915
      Episode_Reward/object_height: 0.0730
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 1.96s
                      Time elapsed: 00:46:09
                               ETA: 00:24:59

################################################################################
                     [1m Learning iteration 1298/2000 [0m                     

                       Computation: 49418 steps/s (collection: 1.851s, learning 0.138s)
             Mean action noise std: 2.65
          Mean value_function loss: 115.0295
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.6708
                       Mean reward: 901.40
               Mean episode length: 240.22
    Episode_Reward/reaching_object: 1.2514
     Episode_Reward/lifting_object: 174.0643
      Episode_Reward/object_height: 0.0740
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 1.99s
                      Time elapsed: 00:46:11
                               ETA: 00:24:57

################################################################################
                     [1m Learning iteration 1299/2000 [0m                     

                       Computation: 49371 steps/s (collection: 1.859s, learning 0.133s)
             Mean action noise std: 2.65
          Mean value_function loss: 136.0237
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.6864
                       Mean reward: 865.38
               Mean episode length: 231.02
    Episode_Reward/reaching_object: 1.2445
     Episode_Reward/lifting_object: 173.2677
      Episode_Reward/object_height: 0.0734
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 1.99s
                      Time elapsed: 00:46:13
                               ETA: 00:24:55

################################################################################
                     [1m Learning iteration 1300/2000 [0m                     

                       Computation: 49349 steps/s (collection: 1.861s, learning 0.131s)
             Mean action noise std: 2.65
          Mean value_function loss: 120.8324
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 51.6982
                       Mean reward: 907.69
               Mean episode length: 241.48
    Episode_Reward/reaching_object: 1.2558
     Episode_Reward/lifting_object: 175.2552
      Episode_Reward/object_height: 0.0745
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 1.99s
                      Time elapsed: 00:46:15
                               ETA: 00:24:53

################################################################################
                     [1m Learning iteration 1301/2000 [0m                     

                       Computation: 48376 steps/s (collection: 1.905s, learning 0.127s)
             Mean action noise std: 2.66
          Mean value_function loss: 117.5139
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 51.7040
                       Mean reward: 815.47
               Mean episode length: 220.02
    Episode_Reward/reaching_object: 1.2361
     Episode_Reward/lifting_object: 171.9692
      Episode_Reward/object_height: 0.0733
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 2.03s
                      Time elapsed: 00:46:17
                               ETA: 00:24:51

################################################################################
                     [1m Learning iteration 1302/2000 [0m                     

                       Computation: 48855 steps/s (collection: 1.882s, learning 0.131s)
             Mean action noise std: 2.66
          Mean value_function loss: 135.6321
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 51.7085
                       Mean reward: 862.14
               Mean episode length: 231.16
    Episode_Reward/reaching_object: 1.2384
     Episode_Reward/lifting_object: 172.2599
      Episode_Reward/object_height: 0.0737
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 2.01s
                      Time elapsed: 00:46:19
                               ETA: 00:24:48

################################################################################
                     [1m Learning iteration 1303/2000 [0m                     

                       Computation: 47872 steps/s (collection: 1.923s, learning 0.130s)
             Mean action noise std: 2.66
          Mean value_function loss: 128.8200
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 51.7188
                       Mean reward: 875.50
               Mean episode length: 233.28
    Episode_Reward/reaching_object: 1.2474
     Episode_Reward/lifting_object: 173.8042
      Episode_Reward/object_height: 0.0746
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 2.05s
                      Time elapsed: 00:46:21
                               ETA: 00:24:46

################################################################################
                     [1m Learning iteration 1304/2000 [0m                     

                       Computation: 48710 steps/s (collection: 1.889s, learning 0.129s)
             Mean action noise std: 2.66
          Mean value_function loss: 151.7413
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 51.7294
                       Mean reward: 778.18
               Mean episode length: 211.95
    Episode_Reward/reaching_object: 1.1952
     Episode_Reward/lifting_object: 166.5525
      Episode_Reward/object_height: 0.0715
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 2.02s
                      Time elapsed: 00:46:23
                               ETA: 00:24:44

################################################################################
                     [1m Learning iteration 1305/2000 [0m                     

                       Computation: 49695 steps/s (collection: 1.859s, learning 0.119s)
             Mean action noise std: 2.66
          Mean value_function loss: 132.1810
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 51.7372
                       Mean reward: 863.65
               Mean episode length: 233.17
    Episode_Reward/reaching_object: 1.2330
     Episode_Reward/lifting_object: 171.0199
      Episode_Reward/object_height: 0.0740
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 1.98s
                      Time elapsed: 00:46:25
                               ETA: 00:24:42

################################################################################
                     [1m Learning iteration 1306/2000 [0m                     

                       Computation: 48725 steps/s (collection: 1.893s, learning 0.125s)
             Mean action noise std: 2.66
          Mean value_function loss: 141.4947
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 51.7488
                       Mean reward: 858.19
               Mean episode length: 230.54
    Episode_Reward/reaching_object: 1.2292
     Episode_Reward/lifting_object: 170.9638
      Episode_Reward/object_height: 0.0743
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 2.02s
                      Time elapsed: 00:46:27
                               ETA: 00:24:40

################################################################################
                     [1m Learning iteration 1307/2000 [0m                     

                       Computation: 49433 steps/s (collection: 1.860s, learning 0.129s)
             Mean action noise std: 2.66
          Mean value_function loss: 139.9369
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 51.7557
                       Mean reward: 784.83
               Mean episode length: 214.32
    Episode_Reward/reaching_object: 1.2083
     Episode_Reward/lifting_object: 167.6814
      Episode_Reward/object_height: 0.0732
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 1.99s
                      Time elapsed: 00:46:29
                               ETA: 00:24:37

################################################################################
                     [1m Learning iteration 1308/2000 [0m                     

                       Computation: 48806 steps/s (collection: 1.880s, learning 0.134s)
             Mean action noise std: 2.66
          Mean value_function loss: 120.8165
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 51.7616
                       Mean reward: 903.38
               Mean episode length: 240.02
    Episode_Reward/reaching_object: 1.2334
     Episode_Reward/lifting_object: 171.5936
      Episode_Reward/object_height: 0.0751
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 2.01s
                      Time elapsed: 00:46:31
                               ETA: 00:24:35

################################################################################
                     [1m Learning iteration 1309/2000 [0m                     

                       Computation: 49076 steps/s (collection: 1.889s, learning 0.115s)
             Mean action noise std: 2.66
          Mean value_function loss: 86.3765
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 51.7718
                       Mean reward: 906.96
               Mean episode length: 241.56
    Episode_Reward/reaching_object: 1.2641
     Episode_Reward/lifting_object: 175.8099
      Episode_Reward/object_height: 0.0772
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 2.00s
                      Time elapsed: 00:46:33
                               ETA: 00:24:33

################################################################################
                     [1m Learning iteration 1310/2000 [0m                     

                       Computation: 48902 steps/s (collection: 1.894s, learning 0.116s)
             Mean action noise std: 2.66
          Mean value_function loss: 122.9057
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 51.7771
                       Mean reward: 873.78
               Mean episode length: 235.17
    Episode_Reward/reaching_object: 1.2437
     Episode_Reward/lifting_object: 172.8531
      Episode_Reward/object_height: 0.0758
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 2.01s
                      Time elapsed: 00:46:35
                               ETA: 00:24:31

################################################################################
                     [1m Learning iteration 1311/2000 [0m                     

                       Computation: 49489 steps/s (collection: 1.887s, learning 0.100s)
             Mean action noise std: 2.67
          Mean value_function loss: 114.7240
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.7878
                       Mean reward: 898.48
               Mean episode length: 238.95
    Episode_Reward/reaching_object: 1.2429
     Episode_Reward/lifting_object: 173.3307
      Episode_Reward/object_height: 0.0762
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 1.99s
                      Time elapsed: 00:46:37
                               ETA: 00:24:29

################################################################################
                     [1m Learning iteration 1312/2000 [0m                     

                       Computation: 49402 steps/s (collection: 1.879s, learning 0.111s)
             Mean action noise std: 2.67
          Mean value_function loss: 115.3359
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 51.7995
                       Mean reward: 853.14
               Mean episode length: 228.95
    Episode_Reward/reaching_object: 1.2494
     Episode_Reward/lifting_object: 174.1494
      Episode_Reward/object_height: 0.0769
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 1.99s
                      Time elapsed: 00:46:39
                               ETA: 00:24:26

################################################################################
                     [1m Learning iteration 1313/2000 [0m                     

                       Computation: 49651 steps/s (collection: 1.846s, learning 0.134s)
             Mean action noise std: 2.67
          Mean value_function loss: 134.4345
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 51.8089
                       Mean reward: 875.56
               Mean episode length: 234.53
    Episode_Reward/reaching_object: 1.2145
     Episode_Reward/lifting_object: 168.8704
      Episode_Reward/object_height: 0.0738
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 1.98s
                      Time elapsed: 00:46:41
                               ETA: 00:24:24

################################################################################
                     [1m Learning iteration 1314/2000 [0m                     

                       Computation: 49541 steps/s (collection: 1.864s, learning 0.120s)
             Mean action noise std: 2.67
          Mean value_function loss: 147.6426
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 51.8207
                       Mean reward: 835.71
               Mean episode length: 224.65
    Episode_Reward/reaching_object: 1.2335
     Episode_Reward/lifting_object: 172.1678
      Episode_Reward/object_height: 0.0752
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 1.98s
                      Time elapsed: 00:46:43
                               ETA: 00:24:22

################################################################################
                     [1m Learning iteration 1315/2000 [0m                     

                       Computation: 49458 steps/s (collection: 1.853s, learning 0.134s)
             Mean action noise std: 2.67
          Mean value_function loss: 125.9897
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.8265
                       Mean reward: 846.15
               Mean episode length: 227.08
    Episode_Reward/reaching_object: 1.2336
     Episode_Reward/lifting_object: 172.3637
      Episode_Reward/object_height: 0.0750
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 1.99s
                      Time elapsed: 00:46:45
                               ETA: 00:24:20

################################################################################
                     [1m Learning iteration 1316/2000 [0m                     

                       Computation: 49633 steps/s (collection: 1.865s, learning 0.116s)
             Mean action noise std: 2.67
          Mean value_function loss: 119.4718
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 51.8348
                       Mean reward: 873.60
               Mean episode length: 233.39
    Episode_Reward/reaching_object: 1.2265
     Episode_Reward/lifting_object: 171.6223
      Episode_Reward/object_height: 0.0745
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 1.98s
                      Time elapsed: 00:46:47
                               ETA: 00:24:17

################################################################################
                     [1m Learning iteration 1317/2000 [0m                     

                       Computation: 49237 steps/s (collection: 1.878s, learning 0.119s)
             Mean action noise std: 2.67
          Mean value_function loss: 93.1043
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.8392
                       Mean reward: 849.26
               Mean episode length: 228.38
    Episode_Reward/reaching_object: 1.2195
     Episode_Reward/lifting_object: 170.7262
      Episode_Reward/object_height: 0.0745
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 2.00s
                      Time elapsed: 00:46:49
                               ETA: 00:24:15

################################################################################
                     [1m Learning iteration 1318/2000 [0m                     

                       Computation: 49459 steps/s (collection: 1.858s, learning 0.130s)
             Mean action noise std: 2.67
          Mean value_function loss: 77.9441
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 51.8442
                       Mean reward: 879.35
               Mean episode length: 235.30
    Episode_Reward/reaching_object: 1.2415
     Episode_Reward/lifting_object: 174.2554
      Episode_Reward/object_height: 0.0763
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 1.99s
                      Time elapsed: 00:46:51
                               ETA: 00:24:13

################################################################################
                     [1m Learning iteration 1319/2000 [0m                     

                       Computation: 49439 steps/s (collection: 1.864s, learning 0.125s)
             Mean action noise std: 2.67
          Mean value_function loss: 98.5283
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.8519
                       Mean reward: 835.29
               Mean episode length: 224.94
    Episode_Reward/reaching_object: 1.2505
     Episode_Reward/lifting_object: 175.6576
      Episode_Reward/object_height: 0.0767
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 1.99s
                      Time elapsed: 00:46:53
                               ETA: 00:24:11

################################################################################
                     [1m Learning iteration 1320/2000 [0m                     

                       Computation: 49210 steps/s (collection: 1.874s, learning 0.124s)
             Mean action noise std: 2.68
          Mean value_function loss: 99.1689
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 51.8632
                       Mean reward: 881.71
               Mean episode length: 235.93
    Episode_Reward/reaching_object: 1.2215
     Episode_Reward/lifting_object: 171.2174
      Episode_Reward/object_height: 0.0749
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 2.00s
                      Time elapsed: 00:46:55
                               ETA: 00:24:09

################################################################################
                     [1m Learning iteration 1321/2000 [0m                     

                       Computation: 49059 steps/s (collection: 1.877s, learning 0.127s)
             Mean action noise std: 2.68
          Mean value_function loss: 98.9010
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 51.8751
                       Mean reward: 873.59
               Mean episode length: 233.82
    Episode_Reward/reaching_object: 1.2365
     Episode_Reward/lifting_object: 173.1613
      Episode_Reward/object_height: 0.0762
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 2.00s
                      Time elapsed: 00:46:57
                               ETA: 00:24:06

################################################################################
                     [1m Learning iteration 1322/2000 [0m                     

                       Computation: 49219 steps/s (collection: 1.891s, learning 0.106s)
             Mean action noise std: 2.68
          Mean value_function loss: 117.1304
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 51.8857
                       Mean reward: 903.10
               Mean episode length: 239.97
    Episode_Reward/reaching_object: 1.2407
     Episode_Reward/lifting_object: 174.3187
      Episode_Reward/object_height: 0.0768
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 2.00s
                      Time elapsed: 00:46:59
                               ETA: 00:24:04

################################################################################
                     [1m Learning iteration 1323/2000 [0m                     

                       Computation: 48851 steps/s (collection: 1.886s, learning 0.126s)
             Mean action noise std: 2.68
          Mean value_function loss: 173.3502
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 51.9004
                       Mean reward: 894.72
               Mean episode length: 238.09
    Episode_Reward/reaching_object: 1.2129
     Episode_Reward/lifting_object: 170.0225
      Episode_Reward/object_height: 0.0750
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 2.01s
                      Time elapsed: 00:47:01
                               ETA: 00:24:02

################################################################################
                     [1m Learning iteration 1324/2000 [0m                     

                       Computation: 49197 steps/s (collection: 1.872s, learning 0.127s)
             Mean action noise std: 2.68
          Mean value_function loss: 137.8533
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 51.9102
                       Mean reward: 882.05
               Mean episode length: 235.58
    Episode_Reward/reaching_object: 1.2470
     Episode_Reward/lifting_object: 175.3022
      Episode_Reward/object_height: 0.0774
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 2.00s
                      Time elapsed: 00:47:03
                               ETA: 00:24:00

################################################################################
                     [1m Learning iteration 1325/2000 [0m                     

                       Computation: 48711 steps/s (collection: 1.892s, learning 0.126s)
             Mean action noise std: 2.68
          Mean value_function loss: 114.5291
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.9185
                       Mean reward: 903.11
               Mean episode length: 240.51
    Episode_Reward/reaching_object: 1.2554
     Episode_Reward/lifting_object: 176.9286
      Episode_Reward/object_height: 0.0784
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 2.02s
                      Time elapsed: 00:47:05
                               ETA: 00:23:58

################################################################################
                     [1m Learning iteration 1326/2000 [0m                     

                       Computation: 48859 steps/s (collection: 1.886s, learning 0.126s)
             Mean action noise std: 2.68
          Mean value_function loss: 98.0565
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 51.9280
                       Mean reward: 896.62
               Mean episode length: 238.34
    Episode_Reward/reaching_object: 1.2464
     Episode_Reward/lifting_object: 175.0705
      Episode_Reward/object_height: 0.0781
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 2.01s
                      Time elapsed: 00:47:07
                               ETA: 00:23:55

################################################################################
                     [1m Learning iteration 1327/2000 [0m                     

                       Computation: 49114 steps/s (collection: 1.872s, learning 0.130s)
             Mean action noise std: 2.68
          Mean value_function loss: 132.7381
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 51.9370
                       Mean reward: 864.28
               Mean episode length: 231.61
    Episode_Reward/reaching_object: 1.1940
     Episode_Reward/lifting_object: 167.5206
      Episode_Reward/object_height: 0.0738
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 2.00s
                      Time elapsed: 00:47:09
                               ETA: 00:23:53

################################################################################
                     [1m Learning iteration 1328/2000 [0m                     

                       Computation: 49593 steps/s (collection: 1.864s, learning 0.119s)
             Mean action noise std: 2.68
          Mean value_function loss: 156.7182
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 51.9423
                       Mean reward: 838.30
               Mean episode length: 225.16
    Episode_Reward/reaching_object: 1.1886
     Episode_Reward/lifting_object: 166.7130
      Episode_Reward/object_height: 0.0738
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 1.98s
                      Time elapsed: 00:47:11
                               ETA: 00:23:51

################################################################################
                     [1m Learning iteration 1329/2000 [0m                     

                       Computation: 48940 steps/s (collection: 1.886s, learning 0.122s)
             Mean action noise std: 2.69
          Mean value_function loss: 114.9754
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 51.9471
                       Mean reward: 890.52
               Mean episode length: 236.56
    Episode_Reward/reaching_object: 1.2202
     Episode_Reward/lifting_object: 171.7568
      Episode_Reward/object_height: 0.0763
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 2.01s
                      Time elapsed: 00:47:13
                               ETA: 00:23:49

################################################################################
                     [1m Learning iteration 1330/2000 [0m                     

                       Computation: 49271 steps/s (collection: 1.881s, learning 0.115s)
             Mean action noise std: 2.69
          Mean value_function loss: 228.6133
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 51.9572
                       Mean reward: 850.45
               Mean episode length: 228.15
    Episode_Reward/reaching_object: 1.2092
     Episode_Reward/lifting_object: 169.4688
      Episode_Reward/object_height: 0.0753
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 2.00s
                      Time elapsed: 00:47:15
                               ETA: 00:23:47

################################################################################
                     [1m Learning iteration 1331/2000 [0m                     

                       Computation: 49985 steps/s (collection: 1.865s, learning 0.102s)
             Mean action noise std: 2.69
          Mean value_function loss: 102.8423
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 51.9699
                       Mean reward: 880.64
               Mean episode length: 234.85
    Episode_Reward/reaching_object: 1.2484
     Episode_Reward/lifting_object: 175.3717
      Episode_Reward/object_height: 0.0785
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 1.97s
                      Time elapsed: 00:47:17
                               ETA: 00:23:44

################################################################################
                     [1m Learning iteration 1332/2000 [0m                     

                       Computation: 49409 steps/s (collection: 1.865s, learning 0.125s)
             Mean action noise std: 2.69
          Mean value_function loss: 116.5401
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 51.9812
                       Mean reward: 854.32
               Mean episode length: 228.93
    Episode_Reward/reaching_object: 1.2444
     Episode_Reward/lifting_object: 174.3999
      Episode_Reward/object_height: 0.0782
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 1.99s
                      Time elapsed: 00:47:19
                               ETA: 00:23:42

################################################################################
                     [1m Learning iteration 1333/2000 [0m                     

                       Computation: 20180 steps/s (collection: 4.741s, learning 0.130s)
             Mean action noise std: 2.69
          Mean value_function loss: 150.7962
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 51.9847
                       Mean reward: 850.41
               Mean episode length: 228.14
    Episode_Reward/reaching_object: 1.2273
     Episode_Reward/lifting_object: 171.7983
      Episode_Reward/object_height: 0.0768
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 4.87s
                      Time elapsed: 00:47:24
                               ETA: 00:23:42

################################################################################
                     [1m Learning iteration 1334/2000 [0m                     

                       Computation: 14639 steps/s (collection: 6.593s, learning 0.122s)
             Mean action noise std: 2.69
          Mean value_function loss: 180.7179
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 51.9885
                       Mean reward: 836.94
               Mean episode length: 225.16
    Episode_Reward/reaching_object: 1.1908
     Episode_Reward/lifting_object: 166.7380
      Episode_Reward/object_height: 0.0742
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 6.71s
                      Time elapsed: 00:47:30
                               ETA: 00:23:42

################################################################################
                     [1m Learning iteration 1335/2000 [0m                     

                       Computation: 14660 steps/s (collection: 6.573s, learning 0.132s)
             Mean action noise std: 2.69
          Mean value_function loss: 167.7476
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 51.9959
                       Mean reward: 831.85
               Mean episode length: 225.37
    Episode_Reward/reaching_object: 1.2117
     Episode_Reward/lifting_object: 169.1332
      Episode_Reward/object_height: 0.0760
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 6.71s
                      Time elapsed: 00:47:37
                               ETA: 00:23:42

################################################################################
                     [1m Learning iteration 1336/2000 [0m                     

                       Computation: 14559 steps/s (collection: 6.629s, learning 0.123s)
             Mean action noise std: 2.69
          Mean value_function loss: 140.4445
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 52.0062
                       Mean reward: 883.92
               Mean episode length: 235.29
    Episode_Reward/reaching_object: 1.2166
     Episode_Reward/lifting_object: 170.0163
      Episode_Reward/object_height: 0.0757
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 6.75s
                      Time elapsed: 00:47:44
                               ETA: 00:23:42

################################################################################
                     [1m Learning iteration 1337/2000 [0m                     

                       Computation: 14731 steps/s (collection: 6.551s, learning 0.122s)
             Mean action noise std: 2.69
          Mean value_function loss: 596.8895
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 52.0139
                       Mean reward: 843.50
               Mean episode length: 226.84
    Episode_Reward/reaching_object: 1.2264
     Episode_Reward/lifting_object: 171.5101
      Episode_Reward/object_height: 0.0766
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 6.67s
                      Time elapsed: 00:47:50
                               ETA: 00:23:42

################################################################################
                     [1m Learning iteration 1338/2000 [0m                     

                       Computation: 14740 steps/s (collection: 6.512s, learning 0.157s)
             Mean action noise std: 2.69
          Mean value_function loss: 211.5521
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 52.0220
                       Mean reward: 883.13
               Mean episode length: 235.34
    Episode_Reward/reaching_object: 1.2498
     Episode_Reward/lifting_object: 174.7201
      Episode_Reward/object_height: 0.0779
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 6.67s
                      Time elapsed: 00:47:57
                               ETA: 00:23:42

################################################################################
                     [1m Learning iteration 1339/2000 [0m                     

                       Computation: 14974 steps/s (collection: 6.443s, learning 0.122s)
             Mean action noise std: 2.70
          Mean value_function loss: 119.0955
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.0268
                       Mean reward: 863.25
               Mean episode length: 231.94
    Episode_Reward/reaching_object: 1.2282
     Episode_Reward/lifting_object: 171.4629
      Episode_Reward/object_height: 0.0765
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 6.56s
                      Time elapsed: 00:48:04
                               ETA: 00:23:42

################################################################################
                     [1m Learning iteration 1340/2000 [0m                     

                       Computation: 14795 steps/s (collection: 6.503s, learning 0.142s)
             Mean action noise std: 2.70
          Mean value_function loss: 98.4655
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 52.0369
                       Mean reward: 894.69
               Mean episode length: 238.78
    Episode_Reward/reaching_object: 1.2613
     Episode_Reward/lifting_object: 176.1751
      Episode_Reward/object_height: 0.0787
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 6.64s
                      Time elapsed: 00:48:10
                               ETA: 00:23:42

################################################################################
                     [1m Learning iteration 1341/2000 [0m                     

                       Computation: 14039 steps/s (collection: 6.876s, learning 0.126s)
             Mean action noise std: 2.70
          Mean value_function loss: 123.3316
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 52.0496
                       Mean reward: 859.69
               Mean episode length: 230.46
    Episode_Reward/reaching_object: 1.2506
     Episode_Reward/lifting_object: 174.7614
      Episode_Reward/object_height: 0.0778
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 7.00s
                      Time elapsed: 00:48:17
                               ETA: 00:23:42

################################################################################
                     [1m Learning iteration 1342/2000 [0m                     

                       Computation: 51989 steps/s (collection: 1.782s, learning 0.109s)
             Mean action noise std: 2.70
          Mean value_function loss: 113.7271
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 52.0569
                       Mean reward: 866.90
               Mean episode length: 231.72
    Episode_Reward/reaching_object: 1.2099
     Episode_Reward/lifting_object: 168.1796
      Episode_Reward/object_height: 0.0751
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 1.89s
                      Time elapsed: 00:48:19
                               ETA: 00:23:40

################################################################################
                     [1m Learning iteration 1343/2000 [0m                     

                       Computation: 52277 steps/s (collection: 1.771s, learning 0.109s)
             Mean action noise std: 2.70
          Mean value_function loss: 123.7941
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 52.0631
                       Mean reward: 868.53
               Mean episode length: 232.46
    Episode_Reward/reaching_object: 1.2365
     Episode_Reward/lifting_object: 172.4369
      Episode_Reward/object_height: 0.0771
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 1.88s
                      Time elapsed: 00:48:21
                               ETA: 00:23:38

################################################################################
                     [1m Learning iteration 1344/2000 [0m                     

                       Computation: 51877 steps/s (collection: 1.789s, learning 0.106s)
             Mean action noise std: 2.70
          Mean value_function loss: 115.8386
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 52.0704
                       Mean reward: 878.51
               Mean episode length: 233.68
    Episode_Reward/reaching_object: 1.2689
     Episode_Reward/lifting_object: 177.3300
      Episode_Reward/object_height: 0.0791
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 1.89s
                      Time elapsed: 00:48:23
                               ETA: 00:23:36

################################################################################
                     [1m Learning iteration 1345/2000 [0m                     

                       Computation: 50319 steps/s (collection: 1.844s, learning 0.110s)
             Mean action noise std: 2.70
          Mean value_function loss: 121.8387
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 52.0788
                       Mean reward: 919.43
               Mean episode length: 243.52
    Episode_Reward/reaching_object: 1.2473
     Episode_Reward/lifting_object: 174.0146
      Episode_Reward/object_height: 0.0773
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 1.95s
                      Time elapsed: 00:48:25
                               ETA: 00:23:33

################################################################################
                     [1m Learning iteration 1346/2000 [0m                     

                       Computation: 51822 steps/s (collection: 1.782s, learning 0.115s)
             Mean action noise std: 2.70
          Mean value_function loss: 123.6015
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 52.0874
                       Mean reward: 874.70
               Mean episode length: 233.15
    Episode_Reward/reaching_object: 1.2518
     Episode_Reward/lifting_object: 173.8920
      Episode_Reward/object_height: 0.0772
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 1.90s
                      Time elapsed: 00:48:27
                               ETA: 00:23:31

################################################################################
                     [1m Learning iteration 1347/2000 [0m                     

                       Computation: 52233 steps/s (collection: 1.770s, learning 0.112s)
             Mean action noise std: 2.70
          Mean value_function loss: 137.3668
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 52.0991
                       Mean reward: 863.87
               Mean episode length: 231.12
    Episode_Reward/reaching_object: 1.2344
     Episode_Reward/lifting_object: 171.4999
      Episode_Reward/object_height: 0.0759
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 1.88s
                      Time elapsed: 00:48:29
                               ETA: 00:23:29

################################################################################
                     [1m Learning iteration 1348/2000 [0m                     

                       Computation: 52832 steps/s (collection: 1.771s, learning 0.090s)
             Mean action noise std: 2.71
          Mean value_function loss: 128.4514
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 52.1110
                       Mean reward: 862.46
               Mean episode length: 230.86
    Episode_Reward/reaching_object: 1.2251
     Episode_Reward/lifting_object: 170.0488
      Episode_Reward/object_height: 0.0749
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 1.86s
                      Time elapsed: 00:48:31
                               ETA: 00:23:26

################################################################################
                     [1m Learning iteration 1349/2000 [0m                     

                       Computation: 53155 steps/s (collection: 1.747s, learning 0.102s)
             Mean action noise std: 2.71
          Mean value_function loss: 120.6444
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 52.1210
                       Mean reward: 862.82
               Mean episode length: 230.49
    Episode_Reward/reaching_object: 1.2395
     Episode_Reward/lifting_object: 172.3082
      Episode_Reward/object_height: 0.0762
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 1.85s
                      Time elapsed: 00:48:32
                               ETA: 00:23:24

################################################################################
                     [1m Learning iteration 1350/2000 [0m                     

                       Computation: 52259 steps/s (collection: 1.758s, learning 0.123s)
             Mean action noise std: 2.71
          Mean value_function loss: 150.7810
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 52.1343
                       Mean reward: 854.81
               Mean episode length: 228.96
    Episode_Reward/reaching_object: 1.2279
     Episode_Reward/lifting_object: 170.3994
      Episode_Reward/object_height: 0.0748
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 1.88s
                      Time elapsed: 00:48:34
                               ETA: 00:23:22

################################################################################
                     [1m Learning iteration 1351/2000 [0m                     

                       Computation: 48257 steps/s (collection: 1.927s, learning 0.110s)
             Mean action noise std: 2.71
          Mean value_function loss: 171.6393
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 52.1426
                       Mean reward: 814.90
               Mean episode length: 220.26
    Episode_Reward/reaching_object: 1.2028
     Episode_Reward/lifting_object: 166.3326
      Episode_Reward/object_height: 0.0736
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 2.04s
                      Time elapsed: 00:48:36
                               ETA: 00:23:20

################################################################################
                     [1m Learning iteration 1352/2000 [0m                     

                       Computation: 51624 steps/s (collection: 1.804s, learning 0.101s)
             Mean action noise std: 2.71
          Mean value_function loss: 105.2494
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.1518
                       Mean reward: 870.18
               Mean episode length: 232.47
    Episode_Reward/reaching_object: 1.2477
     Episode_Reward/lifting_object: 173.4493
      Episode_Reward/object_height: 0.0768
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 1.90s
                      Time elapsed: 00:48:38
                               ETA: 00:23:17

################################################################################
                     [1m Learning iteration 1353/2000 [0m                     

                       Computation: 48987 steps/s (collection: 1.907s, learning 0.100s)
             Mean action noise std: 2.71
          Mean value_function loss: 97.8848
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 52.1658
                       Mean reward: 883.19
               Mean episode length: 235.26
    Episode_Reward/reaching_object: 1.2444
     Episode_Reward/lifting_object: 173.0916
      Episode_Reward/object_height: 0.0766
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 2.01s
                      Time elapsed: 00:48:40
                               ETA: 00:23:15

################################################################################
                     [1m Learning iteration 1354/2000 [0m                     

                       Computation: 51562 steps/s (collection: 1.793s, learning 0.114s)
             Mean action noise std: 2.71
          Mean value_function loss: 113.7152
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 52.1746
                       Mean reward: 868.23
               Mean episode length: 231.70
    Episode_Reward/reaching_object: 1.2363
     Episode_Reward/lifting_object: 172.5043
      Episode_Reward/object_height: 0.0762
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 1.91s
                      Time elapsed: 00:48:42
                               ETA: 00:23:13

################################################################################
                     [1m Learning iteration 1355/2000 [0m                     

                       Computation: 51735 steps/s (collection: 1.789s, learning 0.112s)
             Mean action noise std: 2.71
          Mean value_function loss: 125.3169
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 52.1820
                       Mean reward: 882.52
               Mean episode length: 235.36
    Episode_Reward/reaching_object: 1.2091
     Episode_Reward/lifting_object: 168.6519
      Episode_Reward/object_height: 0.0744
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 1.90s
                      Time elapsed: 00:48:44
                               ETA: 00:23:11

################################################################################
                     [1m Learning iteration 1356/2000 [0m                     

                       Computation: 50919 steps/s (collection: 1.814s, learning 0.117s)
             Mean action noise std: 2.71
          Mean value_function loss: 139.8880
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 52.1908
                       Mean reward: 885.88
               Mean episode length: 236.58
    Episode_Reward/reaching_object: 1.2327
     Episode_Reward/lifting_object: 172.1792
      Episode_Reward/object_height: 0.0759
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 1.93s
                      Time elapsed: 00:48:46
                               ETA: 00:23:08

################################################################################
                     [1m Learning iteration 1357/2000 [0m                     

                       Computation: 51075 steps/s (collection: 1.811s, learning 0.114s)
             Mean action noise std: 2.72
          Mean value_function loss: 122.5522
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 52.1959
                       Mean reward: 872.76
               Mean episode length: 233.09
    Episode_Reward/reaching_object: 1.2331
     Episode_Reward/lifting_object: 172.4559
      Episode_Reward/object_height: 0.0760
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 1.92s
                      Time elapsed: 00:48:48
                               ETA: 00:23:06

################################################################################
                     [1m Learning iteration 1358/2000 [0m                     

                       Computation: 51805 steps/s (collection: 1.787s, learning 0.111s)
             Mean action noise std: 2.72
          Mean value_function loss: 139.8750
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 52.2040
                       Mean reward: 846.67
               Mean episode length: 226.49
    Episode_Reward/reaching_object: 1.1995
     Episode_Reward/lifting_object: 167.2632
      Episode_Reward/object_height: 0.0735
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 1.90s
                      Time elapsed: 00:48:50
                               ETA: 00:23:04

################################################################################
                     [1m Learning iteration 1359/2000 [0m                     

                       Computation: 49953 steps/s (collection: 1.859s, learning 0.109s)
             Mean action noise std: 2.72
          Mean value_function loss: 105.7253
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 52.2148
                       Mean reward: 865.12
               Mean episode length: 231.17
    Episode_Reward/reaching_object: 1.2100
     Episode_Reward/lifting_object: 168.6484
      Episode_Reward/object_height: 0.0745
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 1.97s
                      Time elapsed: 00:48:52
                               ETA: 00:23:02

################################################################################
                     [1m Learning iteration 1360/2000 [0m                     

                       Computation: 51375 steps/s (collection: 1.797s, learning 0.116s)
             Mean action noise std: 2.72
          Mean value_function loss: 109.2035
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 52.2199
                       Mean reward: 842.93
               Mean episode length: 225.60
    Episode_Reward/reaching_object: 1.2163
     Episode_Reward/lifting_object: 169.9693
      Episode_Reward/object_height: 0.0748
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 1.91s
                      Time elapsed: 00:48:54
                               ETA: 00:22:59

################################################################################
                     [1m Learning iteration 1361/2000 [0m                     

                       Computation: 51654 steps/s (collection: 1.814s, learning 0.090s)
             Mean action noise std: 2.72
          Mean value_function loss: 111.9121
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 52.2295
                       Mean reward: 832.63
               Mean episode length: 224.45
    Episode_Reward/reaching_object: 1.1939
     Episode_Reward/lifting_object: 167.0200
      Episode_Reward/object_height: 0.0731
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 1.90s
                      Time elapsed: 00:48:56
                               ETA: 00:22:57

################################################################################
                     [1m Learning iteration 1362/2000 [0m                     

                       Computation: 52429 steps/s (collection: 1.753s, learning 0.122s)
             Mean action noise std: 2.72
          Mean value_function loss: 92.7696
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.2467
                       Mean reward: 875.30
               Mean episode length: 232.68
    Episode_Reward/reaching_object: 1.2443
     Episode_Reward/lifting_object: 174.9787
      Episode_Reward/object_height: 0.0773
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 1.87s
                      Time elapsed: 00:48:57
                               ETA: 00:22:55

################################################################################
                     [1m Learning iteration 1363/2000 [0m                     

                       Computation: 53012 steps/s (collection: 1.742s, learning 0.112s)
             Mean action noise std: 2.72
          Mean value_function loss: 137.9616
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 52.2608
                       Mean reward: 856.90
               Mean episode length: 229.22
    Episode_Reward/reaching_object: 1.2122
     Episode_Reward/lifting_object: 170.5579
      Episode_Reward/object_height: 0.0752
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 1.85s
                      Time elapsed: 00:48:59
                               ETA: 00:22:52

################################################################################
                     [1m Learning iteration 1364/2000 [0m                     

                       Computation: 52753 steps/s (collection: 1.746s, learning 0.117s)
             Mean action noise std: 2.73
          Mean value_function loss: 151.5249
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 52.2730
                       Mean reward: 853.05
               Mean episode length: 227.74
    Episode_Reward/reaching_object: 1.2111
     Episode_Reward/lifting_object: 170.4089
      Episode_Reward/object_height: 0.0752
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 1.86s
                      Time elapsed: 00:49:01
                               ETA: 00:22:50

################################################################################
                     [1m Learning iteration 1365/2000 [0m                     

                       Computation: 52666 steps/s (collection: 1.755s, learning 0.112s)
             Mean action noise std: 2.73
          Mean value_function loss: 114.3124
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 52.2856
                       Mean reward: 896.80
               Mean episode length: 239.86
    Episode_Reward/reaching_object: 1.2398
     Episode_Reward/lifting_object: 174.7621
      Episode_Reward/object_height: 0.0773
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 1.87s
                      Time elapsed: 00:49:03
                               ETA: 00:22:48

################################################################################
                     [1m Learning iteration 1366/2000 [0m                     

                       Computation: 52152 steps/s (collection: 1.773s, learning 0.112s)
             Mean action noise std: 2.73
          Mean value_function loss: 125.4955
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 52.2942
                       Mean reward: 879.08
               Mean episode length: 234.79
    Episode_Reward/reaching_object: 1.2241
     Episode_Reward/lifting_object: 172.4450
      Episode_Reward/object_height: 0.0761
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 1.88s
                      Time elapsed: 00:49:05
                               ETA: 00:22:46

################################################################################
                     [1m Learning iteration 1367/2000 [0m                     

                       Computation: 52949 steps/s (collection: 1.750s, learning 0.107s)
             Mean action noise std: 2.73
          Mean value_function loss: 129.3962
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 52.3015
                       Mean reward: 856.95
               Mean episode length: 229.17
    Episode_Reward/reaching_object: 1.2137
     Episode_Reward/lifting_object: 171.5879
      Episode_Reward/object_height: 0.0755
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 1.86s
                      Time elapsed: 00:49:07
                               ETA: 00:22:43

################################################################################
                     [1m Learning iteration 1368/2000 [0m                     

                       Computation: 52908 steps/s (collection: 1.743s, learning 0.115s)
             Mean action noise std: 2.73
          Mean value_function loss: 107.0138
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.3122
                       Mean reward: 874.10
               Mean episode length: 233.64
    Episode_Reward/reaching_object: 1.2148
     Episode_Reward/lifting_object: 171.7811
      Episode_Reward/object_height: 0.0757
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 1.86s
                      Time elapsed: 00:49:09
                               ETA: 00:22:41

################################################################################
                     [1m Learning iteration 1369/2000 [0m                     

                       Computation: 52737 steps/s (collection: 1.754s, learning 0.110s)
             Mean action noise std: 2.73
          Mean value_function loss: 133.6769
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 52.3202
                       Mean reward: 834.22
               Mean episode length: 224.16
    Episode_Reward/reaching_object: 1.2064
     Episode_Reward/lifting_object: 170.8579
      Episode_Reward/object_height: 0.0753
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 1.86s
                      Time elapsed: 00:49:10
                               ETA: 00:22:39

################################################################################
                     [1m Learning iteration 1370/2000 [0m                     

                       Computation: 53301 steps/s (collection: 1.734s, learning 0.110s)
             Mean action noise std: 2.73
          Mean value_function loss: 88.4115
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 52.3250
                       Mean reward: 839.84
               Mean episode length: 225.32
    Episode_Reward/reaching_object: 1.2015
     Episode_Reward/lifting_object: 170.3814
      Episode_Reward/object_height: 0.0744
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 1.84s
                      Time elapsed: 00:49:12
                               ETA: 00:22:36

################################################################################
                     [1m Learning iteration 1371/2000 [0m                     

                       Computation: 52643 steps/s (collection: 1.760s, learning 0.108s)
             Mean action noise std: 2.73
          Mean value_function loss: 112.4523
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 52.3354
                       Mean reward: 838.63
               Mean episode length: 226.62
    Episode_Reward/reaching_object: 1.2076
     Episode_Reward/lifting_object: 171.2122
      Episode_Reward/object_height: 0.0749
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 1.87s
                      Time elapsed: 00:49:14
                               ETA: 00:22:34

################################################################################
                     [1m Learning iteration 1372/2000 [0m                     

                       Computation: 53403 steps/s (collection: 1.754s, learning 0.087s)
             Mean action noise std: 2.73
          Mean value_function loss: 130.2179
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 52.3444
                       Mean reward: 811.86
               Mean episode length: 218.64
    Episode_Reward/reaching_object: 1.1883
     Episode_Reward/lifting_object: 168.7290
      Episode_Reward/object_height: 0.0737
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 1.84s
                      Time elapsed: 00:49:16
                               ETA: 00:22:32

################################################################################
                     [1m Learning iteration 1373/2000 [0m                     

                       Computation: 52168 steps/s (collection: 1.775s, learning 0.109s)
             Mean action noise std: 2.73
          Mean value_function loss: 118.6413
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 52.3524
                       Mean reward: 836.92
               Mean episode length: 225.50
    Episode_Reward/reaching_object: 1.2088
     Episode_Reward/lifting_object: 171.4085
      Episode_Reward/object_height: 0.0748
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 1.88s
                      Time elapsed: 00:49:18
                               ETA: 00:22:30

################################################################################
                     [1m Learning iteration 1374/2000 [0m                     

                       Computation: 51040 steps/s (collection: 1.816s, learning 0.110s)
             Mean action noise std: 2.74
          Mean value_function loss: 116.7703
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.3607
                       Mean reward: 890.53
               Mean episode length: 236.83
    Episode_Reward/reaching_object: 1.2090
     Episode_Reward/lifting_object: 171.5693
      Episode_Reward/object_height: 0.0747
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 1.93s
                      Time elapsed: 00:49:20
                               ETA: 00:22:27

################################################################################
                     [1m Learning iteration 1375/2000 [0m                     

                       Computation: 51806 steps/s (collection: 1.789s, learning 0.109s)
             Mean action noise std: 2.74
          Mean value_function loss: 100.5668
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 52.3674
                       Mean reward: 875.03
               Mean episode length: 233.82
    Episode_Reward/reaching_object: 1.2333
     Episode_Reward/lifting_object: 175.3700
      Episode_Reward/object_height: 0.0761
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 1.90s
                      Time elapsed: 00:49:22
                               ETA: 00:22:25

################################################################################
                     [1m Learning iteration 1376/2000 [0m                     

                       Computation: 52497 steps/s (collection: 1.759s, learning 0.113s)
             Mean action noise std: 2.74
          Mean value_function loss: 118.3546
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.3738
                       Mean reward: 875.55
               Mean episode length: 233.75
    Episode_Reward/reaching_object: 1.2091
     Episode_Reward/lifting_object: 171.7041
      Episode_Reward/object_height: 0.0744
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 1.87s
                      Time elapsed: 00:49:24
                               ETA: 00:22:23

################################################################################
                     [1m Learning iteration 1377/2000 [0m                     

                       Computation: 52372 steps/s (collection: 1.763s, learning 0.114s)
             Mean action noise std: 2.74
          Mean value_function loss: 130.9976
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 52.3853
                       Mean reward: 862.84
               Mean episode length: 230.61
    Episode_Reward/reaching_object: 1.1905
     Episode_Reward/lifting_object: 168.9990
      Episode_Reward/object_height: 0.0735
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 1.88s
                      Time elapsed: 00:49:26
                               ETA: 00:22:20

################################################################################
                     [1m Learning iteration 1378/2000 [0m                     

                       Computation: 52873 steps/s (collection: 1.756s, learning 0.103s)
             Mean action noise std: 2.74
          Mean value_function loss: 142.5528
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 52.3982
                       Mean reward: 812.93
               Mean episode length: 218.26
    Episode_Reward/reaching_object: 1.1775
     Episode_Reward/lifting_object: 167.0633
      Episode_Reward/object_height: 0.0725
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 1.86s
                      Time elapsed: 00:49:27
                               ETA: 00:22:18

################################################################################
                     [1m Learning iteration 1379/2000 [0m                     

                       Computation: 51998 steps/s (collection: 1.770s, learning 0.120s)
             Mean action noise std: 2.74
          Mean value_function loss: 113.0025
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 52.4089
                       Mean reward: 862.91
               Mean episode length: 230.19
    Episode_Reward/reaching_object: 1.2211
     Episode_Reward/lifting_object: 173.5240
      Episode_Reward/object_height: 0.0759
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 1.89s
                      Time elapsed: 00:49:29
                               ETA: 00:22:16

################################################################################
                     [1m Learning iteration 1380/2000 [0m                     

                       Computation: 52380 steps/s (collection: 1.765s, learning 0.112s)
             Mean action noise std: 2.74
          Mean value_function loss: 116.3587
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 52.4237
                       Mean reward: 828.03
               Mean episode length: 221.90
    Episode_Reward/reaching_object: 1.2089
     Episode_Reward/lifting_object: 171.7198
      Episode_Reward/object_height: 0.0752
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 1.88s
                      Time elapsed: 00:49:31
                               ETA: 00:22:14

################################################################################
                     [1m Learning iteration 1381/2000 [0m                     

                       Computation: 51990 steps/s (collection: 1.778s, learning 0.113s)
             Mean action noise std: 2.74
          Mean value_function loss: 120.5875
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 52.4314
                       Mean reward: 839.59
               Mean episode length: 224.53
    Episode_Reward/reaching_object: 1.1925
     Episode_Reward/lifting_object: 169.5518
      Episode_Reward/object_height: 0.0743
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 1.89s
                      Time elapsed: 00:49:33
                               ETA: 00:22:11

################################################################################
                     [1m Learning iteration 1382/2000 [0m                     

                       Computation: 52482 steps/s (collection: 1.767s, learning 0.107s)
             Mean action noise std: 2.74
          Mean value_function loss: 118.4728
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 52.4384
                       Mean reward: 853.89
               Mean episode length: 229.29
    Episode_Reward/reaching_object: 1.2044
     Episode_Reward/lifting_object: 170.4166
      Episode_Reward/object_height: 0.0754
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 1.87s
                      Time elapsed: 00:49:35
                               ETA: 00:22:09

################################################################################
                     [1m Learning iteration 1383/2000 [0m                     

                       Computation: 48346 steps/s (collection: 1.936s, learning 0.097s)
             Mean action noise std: 2.75
          Mean value_function loss: 123.4919
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 52.4460
                       Mean reward: 836.78
               Mean episode length: 224.24
    Episode_Reward/reaching_object: 1.2195
     Episode_Reward/lifting_object: 172.9552
      Episode_Reward/object_height: 0.0768
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 2.03s
                      Time elapsed: 00:49:37
                               ETA: 00:22:07

################################################################################
                     [1m Learning iteration 1384/2000 [0m                     

                       Computation: 50411 steps/s (collection: 1.858s, learning 0.092s)
             Mean action noise std: 2.75
          Mean value_function loss: 124.1409
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 52.4510
                       Mean reward: 882.50
               Mean episode length: 234.41
    Episode_Reward/reaching_object: 1.2207
     Episode_Reward/lifting_object: 172.9501
      Episode_Reward/object_height: 0.0773
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 1.95s
                      Time elapsed: 00:49:39
                               ETA: 00:22:05

################################################################################
                     [1m Learning iteration 1385/2000 [0m                     

                       Computation: 51132 steps/s (collection: 1.790s, learning 0.133s)
             Mean action noise std: 2.75
          Mean value_function loss: 98.5660
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.4564
                       Mean reward: 889.22
               Mean episode length: 237.38
    Episode_Reward/reaching_object: 1.2263
     Episode_Reward/lifting_object: 173.2803
      Episode_Reward/object_height: 0.0783
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 1.92s
                      Time elapsed: 00:49:41
                               ETA: 00:22:02

################################################################################
                     [1m Learning iteration 1386/2000 [0m                     

                       Computation: 49522 steps/s (collection: 1.825s, learning 0.160s)
             Mean action noise std: 2.75
          Mean value_function loss: 97.1820
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 52.4665
                       Mean reward: 859.74
               Mean episode length: 230.02
    Episode_Reward/reaching_object: 1.2316
     Episode_Reward/lifting_object: 174.2133
      Episode_Reward/object_height: 0.0787
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 1.99s
                      Time elapsed: 00:49:43
                               ETA: 00:22:00

################################################################################
                     [1m Learning iteration 1387/2000 [0m                     

                       Computation: 51091 steps/s (collection: 1.816s, learning 0.108s)
             Mean action noise std: 2.75
          Mean value_function loss: 113.2582
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 52.4753
                       Mean reward: 862.99
               Mean episode length: 231.91
    Episode_Reward/reaching_object: 1.2212
     Episode_Reward/lifting_object: 172.6764
      Episode_Reward/object_height: 0.0781
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 1.92s
                      Time elapsed: 00:49:45
                               ETA: 00:21:58

################################################################################
                     [1m Learning iteration 1388/2000 [0m                     

                       Computation: 49161 steps/s (collection: 1.826s, learning 0.174s)
             Mean action noise std: 2.75
          Mean value_function loss: 114.6159
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.4862
                       Mean reward: 916.12
               Mean episode length: 242.72
    Episode_Reward/reaching_object: 1.2206
     Episode_Reward/lifting_object: 172.0525
      Episode_Reward/object_height: 0.0775
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 2.00s
                      Time elapsed: 00:49:47
                               ETA: 00:21:56

################################################################################
                     [1m Learning iteration 1389/2000 [0m                     

                       Computation: 49467 steps/s (collection: 1.828s, learning 0.160s)
             Mean action noise std: 2.75
          Mean value_function loss: 107.7436
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 52.4990
                       Mean reward: 851.78
               Mean episode length: 228.62
    Episode_Reward/reaching_object: 1.2210
     Episode_Reward/lifting_object: 172.0779
      Episode_Reward/object_height: 0.0773
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 1.99s
                      Time elapsed: 00:49:49
                               ETA: 00:21:53

################################################################################
                     [1m Learning iteration 1390/2000 [0m                     

                       Computation: 51789 steps/s (collection: 1.779s, learning 0.119s)
             Mean action noise std: 2.75
          Mean value_function loss: 120.8707
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 52.5077
                       Mean reward: 894.11
               Mean episode length: 238.52
    Episode_Reward/reaching_object: 1.2118
     Episode_Reward/lifting_object: 170.2841
      Episode_Reward/object_height: 0.0769
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 1.90s
                      Time elapsed: 00:49:51
                               ETA: 00:21:51

################################################################################
                     [1m Learning iteration 1391/2000 [0m                     

                       Computation: 52183 steps/s (collection: 1.787s, learning 0.097s)
             Mean action noise std: 2.75
          Mean value_function loss: 121.5609
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.5139
                       Mean reward: 878.39
               Mean episode length: 234.03
    Episode_Reward/reaching_object: 1.2182
     Episode_Reward/lifting_object: 171.9037
      Episode_Reward/object_height: 0.0764
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 1.88s
                      Time elapsed: 00:49:52
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 1392/2000 [0m                     

                       Computation: 50540 steps/s (collection: 1.840s, learning 0.106s)
             Mean action noise std: 2.75
          Mean value_function loss: 150.8676
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 52.5231
                       Mean reward: 812.33
               Mean episode length: 217.91
    Episode_Reward/reaching_object: 1.1895
     Episode_Reward/lifting_object: 167.2196
      Episode_Reward/object_height: 0.0742
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 1.95s
                      Time elapsed: 00:49:54
                               ETA: 00:21:47

################################################################################
                     [1m Learning iteration 1393/2000 [0m                     

                       Computation: 50289 steps/s (collection: 1.846s, learning 0.109s)
             Mean action noise std: 2.76
          Mean value_function loss: 122.9544
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.5333
                       Mean reward: 870.91
               Mean episode length: 232.81
    Episode_Reward/reaching_object: 1.2255
     Episode_Reward/lifting_object: 172.5372
      Episode_Reward/object_height: 0.0768
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 1.95s
                      Time elapsed: 00:49:56
                               ETA: 00:21:44

################################################################################
                     [1m Learning iteration 1394/2000 [0m                     

                       Computation: 51977 steps/s (collection: 1.790s, learning 0.102s)
             Mean action noise std: 2.76
          Mean value_function loss: 121.2683
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 52.5443
                       Mean reward: 835.82
               Mean episode length: 226.07
    Episode_Reward/reaching_object: 1.2131
     Episode_Reward/lifting_object: 170.8975
      Episode_Reward/object_height: 0.0758
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 1.89s
                      Time elapsed: 00:49:58
                               ETA: 00:21:42

################################################################################
                     [1m Learning iteration 1395/2000 [0m                     

                       Computation: 51405 steps/s (collection: 1.813s, learning 0.100s)
             Mean action noise std: 2.76
          Mean value_function loss: 113.9767
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 52.5564
                       Mean reward: 872.81
               Mean episode length: 233.20
    Episode_Reward/reaching_object: 1.2292
     Episode_Reward/lifting_object: 173.3252
      Episode_Reward/object_height: 0.0776
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 1.91s
                      Time elapsed: 00:50:00
                               ETA: 00:21:40

################################################################################
                     [1m Learning iteration 1396/2000 [0m                     

                       Computation: 50533 steps/s (collection: 1.832s, learning 0.113s)
             Mean action noise std: 2.76
          Mean value_function loss: 111.7249
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.5631
                       Mean reward: 886.12
               Mean episode length: 235.46
    Episode_Reward/reaching_object: 1.2571
     Episode_Reward/lifting_object: 177.6176
      Episode_Reward/object_height: 0.0801
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 1.95s
                      Time elapsed: 00:50:02
                               ETA: 00:21:38

################################################################################
                     [1m Learning iteration 1397/2000 [0m                     

                       Computation: 51846 steps/s (collection: 1.803s, learning 0.093s)
             Mean action noise std: 2.76
          Mean value_function loss: 135.7163
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 52.5693
                       Mean reward: 850.52
               Mean episode length: 227.15
    Episode_Reward/reaching_object: 1.2192
     Episode_Reward/lifting_object: 171.9874
      Episode_Reward/object_height: 0.0777
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 1.90s
                      Time elapsed: 00:50:04
                               ETA: 00:21:35

################################################################################
                     [1m Learning iteration 1398/2000 [0m                     

                       Computation: 50852 steps/s (collection: 1.814s, learning 0.119s)
             Mean action noise std: 2.76
          Mean value_function loss: 138.6193
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 52.5788
                       Mean reward: 861.47
               Mean episode length: 229.97
    Episode_Reward/reaching_object: 1.2132
     Episode_Reward/lifting_object: 171.0703
      Episode_Reward/object_height: 0.0776
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 1.93s
                      Time elapsed: 00:50:06
                               ETA: 00:21:33

################################################################################
                     [1m Learning iteration 1399/2000 [0m                     

                       Computation: 50919 steps/s (collection: 1.794s, learning 0.136s)
             Mean action noise std: 2.76
          Mean value_function loss: 143.9894
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 52.5922
                       Mean reward: 833.44
               Mean episode length: 223.24
    Episode_Reward/reaching_object: 1.2237
     Episode_Reward/lifting_object: 172.3904
      Episode_Reward/object_height: 0.0781
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 1.93s
                      Time elapsed: 00:50:08
                               ETA: 00:21:31

################################################################################
                     [1m Learning iteration 1400/2000 [0m                     

                       Computation: 51266 steps/s (collection: 1.798s, learning 0.120s)
             Mean action noise std: 2.76
          Mean value_function loss: 143.5021
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 52.6017
                       Mean reward: 886.93
               Mean episode length: 236.27
    Episode_Reward/reaching_object: 1.2169
     Episode_Reward/lifting_object: 171.4930
      Episode_Reward/object_height: 0.0777
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 1.92s
                      Time elapsed: 00:50:10
                               ETA: 00:21:29

################################################################################
                     [1m Learning iteration 1401/2000 [0m                     

                       Computation: 47939 steps/s (collection: 1.953s, learning 0.098s)
             Mean action noise std: 2.76
          Mean value_function loss: 221.4550
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.6050
                       Mean reward: 864.16
               Mean episode length: 230.10
    Episode_Reward/reaching_object: 1.1966
     Episode_Reward/lifting_object: 168.5655
      Episode_Reward/object_height: 0.0762
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 2.05s
                      Time elapsed: 00:50:12
                               ETA: 00:21:27

################################################################################
                     [1m Learning iteration 1402/2000 [0m                     

                       Computation: 51110 steps/s (collection: 1.814s, learning 0.110s)
             Mean action noise std: 2.77
          Mean value_function loss: 136.6584
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 52.6117
                       Mean reward: 826.58
               Mean episode length: 221.39
    Episode_Reward/reaching_object: 1.2255
     Episode_Reward/lifting_object: 172.8663
      Episode_Reward/object_height: 0.0788
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 1.92s
                      Time elapsed: 00:50:14
                               ETA: 00:21:24

################################################################################
                     [1m Learning iteration 1403/2000 [0m                     

                       Computation: 51428 steps/s (collection: 1.822s, learning 0.089s)
             Mean action noise std: 2.77
          Mean value_function loss: 144.2329
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.6233
                       Mean reward: 847.02
               Mean episode length: 226.94
    Episode_Reward/reaching_object: 1.1904
     Episode_Reward/lifting_object: 167.6628
      Episode_Reward/object_height: 0.0757
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 1.91s
                      Time elapsed: 00:50:16
                               ETA: 00:21:22

################################################################################
                     [1m Learning iteration 1404/2000 [0m                     

                       Computation: 51038 steps/s (collection: 1.808s, learning 0.118s)
             Mean action noise std: 2.77
          Mean value_function loss: 155.7751
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 52.6362
                       Mean reward: 829.44
               Mean episode length: 223.20
    Episode_Reward/reaching_object: 1.1886
     Episode_Reward/lifting_object: 167.4514
      Episode_Reward/object_height: 0.0754
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 1.93s
                      Time elapsed: 00:50:18
                               ETA: 00:21:20

################################################################################
                     [1m Learning iteration 1405/2000 [0m                     

                       Computation: 48910 steps/s (collection: 1.859s, learning 0.151s)
             Mean action noise std: 2.77
          Mean value_function loss: 124.1708
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.6460
                       Mean reward: 892.46
               Mean episode length: 237.11
    Episode_Reward/reaching_object: 1.2315
     Episode_Reward/lifting_object: 173.8120
      Episode_Reward/object_height: 0.0785
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 2.01s
                      Time elapsed: 00:50:20
                               ETA: 00:21:18

################################################################################
                     [1m Learning iteration 1406/2000 [0m                     

                       Computation: 48897 steps/s (collection: 1.919s, learning 0.091s)
             Mean action noise std: 2.77
          Mean value_function loss: 146.8920
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 52.6552
                       Mean reward: 839.65
               Mean episode length: 225.77
    Episode_Reward/reaching_object: 1.2343
     Episode_Reward/lifting_object: 174.2273
      Episode_Reward/object_height: 0.0784
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 2.01s
                      Time elapsed: 00:50:22
                               ETA: 00:21:15

################################################################################
                     [1m Learning iteration 1407/2000 [0m                     

                       Computation: 51618 steps/s (collection: 1.803s, learning 0.101s)
             Mean action noise std: 2.77
          Mean value_function loss: 150.2300
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.6625
                       Mean reward: 818.21
               Mean episode length: 219.71
    Episode_Reward/reaching_object: 1.1955
     Episode_Reward/lifting_object: 168.2933
      Episode_Reward/object_height: 0.0755
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 1.90s
                      Time elapsed: 00:50:24
                               ETA: 00:21:13

################################################################################
                     [1m Learning iteration 1408/2000 [0m                     

                       Computation: 50323 steps/s (collection: 1.851s, learning 0.102s)
             Mean action noise std: 2.77
          Mean value_function loss: 131.2994
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.6789
                       Mean reward: 857.80
               Mean episode length: 229.19
    Episode_Reward/reaching_object: 1.2190
     Episode_Reward/lifting_object: 171.8123
      Episode_Reward/object_height: 0.0768
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 1.95s
                      Time elapsed: 00:50:25
                               ETA: 00:21:11

################################################################################
                     [1m Learning iteration 1409/2000 [0m                     

                       Computation: 48854 steps/s (collection: 1.897s, learning 0.115s)
             Mean action noise std: 2.78
          Mean value_function loss: 130.2618
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 52.6904
                       Mean reward: 869.16
               Mean episode length: 231.76
    Episode_Reward/reaching_object: 1.2059
     Episode_Reward/lifting_object: 169.9456
      Episode_Reward/object_height: 0.0759
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 2.01s
                      Time elapsed: 00:50:28
                               ETA: 00:21:09

################################################################################
                     [1m Learning iteration 1410/2000 [0m                     

                       Computation: 49810 steps/s (collection: 1.884s, learning 0.090s)
             Mean action noise std: 2.78
          Mean value_function loss: 110.8544
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.6933
                       Mean reward: 878.76
               Mean episode length: 234.80
    Episode_Reward/reaching_object: 1.2366
     Episode_Reward/lifting_object: 174.6454
      Episode_Reward/object_height: 0.0784
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 1.97s
                      Time elapsed: 00:50:29
                               ETA: 00:21:06

################################################################################
                     [1m Learning iteration 1411/2000 [0m                     

                       Computation: 49276 steps/s (collection: 1.880s, learning 0.115s)
             Mean action noise std: 2.78
          Mean value_function loss: 102.6389
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 52.6990
                       Mean reward: 877.97
               Mean episode length: 234.08
    Episode_Reward/reaching_object: 1.2238
     Episode_Reward/lifting_object: 173.0209
      Episode_Reward/object_height: 0.0776
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 1.99s
                      Time elapsed: 00:50:31
                               ETA: 00:21:04

################################################################################
                     [1m Learning iteration 1412/2000 [0m                     

                       Computation: 47826 steps/s (collection: 1.919s, learning 0.136s)
             Mean action noise std: 2.78
          Mean value_function loss: 108.6605
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 52.7033
                       Mean reward: 863.66
               Mean episode length: 230.85
    Episode_Reward/reaching_object: 1.2066
     Episode_Reward/lifting_object: 170.3159
      Episode_Reward/object_height: 0.0763
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 2.06s
                      Time elapsed: 00:50:34
                               ETA: 00:21:02

################################################################################
                     [1m Learning iteration 1413/2000 [0m                     

                       Computation: 49212 steps/s (collection: 1.914s, learning 0.084s)
             Mean action noise std: 2.78
          Mean value_function loss: 107.6365
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 52.7074
                       Mean reward: 889.72
               Mean episode length: 237.03
    Episode_Reward/reaching_object: 1.2483
     Episode_Reward/lifting_object: 176.4062
      Episode_Reward/object_height: 0.0786
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 2.00s
                      Time elapsed: 00:50:36
                               ETA: 00:21:00

################################################################################
                     [1m Learning iteration 1414/2000 [0m                     

                       Computation: 50907 steps/s (collection: 1.839s, learning 0.092s)
             Mean action noise std: 2.78
          Mean value_function loss: 116.0340
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 52.7195
                       Mean reward: 864.86
               Mean episode length: 230.79
    Episode_Reward/reaching_object: 1.2087
     Episode_Reward/lifting_object: 170.8662
      Episode_Reward/object_height: 0.0764
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 1.93s
                      Time elapsed: 00:50:37
                               ETA: 00:20:58

################################################################################
                     [1m Learning iteration 1415/2000 [0m                     

                       Computation: 51652 steps/s (collection: 1.809s, learning 0.095s)
             Mean action noise std: 2.78
          Mean value_function loss: 121.4122
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.7333
                       Mean reward: 846.47
               Mean episode length: 226.56
    Episode_Reward/reaching_object: 1.2272
     Episode_Reward/lifting_object: 173.5445
      Episode_Reward/object_height: 0.0778
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 1.90s
                      Time elapsed: 00:50:39
                               ETA: 00:20:55

################################################################################
                     [1m Learning iteration 1416/2000 [0m                     

                       Computation: 52426 steps/s (collection: 1.785s, learning 0.091s)
             Mean action noise std: 2.78
          Mean value_function loss: 109.0843
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.7419
                       Mean reward: 826.76
               Mean episode length: 221.43
    Episode_Reward/reaching_object: 1.2077
     Episode_Reward/lifting_object: 170.9398
      Episode_Reward/object_height: 0.0771
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 1.88s
                      Time elapsed: 00:50:41
                               ETA: 00:20:53

################################################################################
                     [1m Learning iteration 1417/2000 [0m                     

                       Computation: 50439 steps/s (collection: 1.839s, learning 0.110s)
             Mean action noise std: 2.78
          Mean value_function loss: 147.4549
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 52.7506
                       Mean reward: 837.75
               Mean episode length: 224.11
    Episode_Reward/reaching_object: 1.1822
     Episode_Reward/lifting_object: 166.9739
      Episode_Reward/object_height: 0.0747
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 1.95s
                      Time elapsed: 00:50:43
                               ETA: 00:20:51

################################################################################
                     [1m Learning iteration 1418/2000 [0m                     

                       Computation: 51875 steps/s (collection: 1.803s, learning 0.092s)
             Mean action noise std: 2.78
          Mean value_function loss: 108.7281
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 52.7577
                       Mean reward: 864.39
               Mean episode length: 231.80
    Episode_Reward/reaching_object: 1.2269
     Episode_Reward/lifting_object: 173.3112
      Episode_Reward/object_height: 0.0780
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 1.90s
                      Time elapsed: 00:50:45
                               ETA: 00:20:49

################################################################################
                     [1m Learning iteration 1419/2000 [0m                     

                       Computation: 51525 steps/s (collection: 1.795s, learning 0.113s)
             Mean action noise std: 2.78
          Mean value_function loss: 111.9500
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 52.7634
                       Mean reward: 876.32
               Mean episode length: 234.57
    Episode_Reward/reaching_object: 1.2220
     Episode_Reward/lifting_object: 172.9933
      Episode_Reward/object_height: 0.0778
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 1.91s
                      Time elapsed: 00:50:47
                               ETA: 00:20:46

################################################################################
                     [1m Learning iteration 1420/2000 [0m                     

                       Computation: 50715 steps/s (collection: 1.822s, learning 0.117s)
             Mean action noise std: 2.79
          Mean value_function loss: 141.2396
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.7665
                       Mean reward: 893.08
               Mean episode length: 237.11
    Episode_Reward/reaching_object: 1.1964
     Episode_Reward/lifting_object: 169.4073
      Episode_Reward/object_height: 0.0763
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 1.94s
                      Time elapsed: 00:50:49
                               ETA: 00:20:44

################################################################################
                     [1m Learning iteration 1421/2000 [0m                     

                       Computation: 50336 steps/s (collection: 1.810s, learning 0.143s)
             Mean action noise std: 2.79
          Mean value_function loss: 120.0894
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 52.7763
                       Mean reward: 875.74
               Mean episode length: 234.08
    Episode_Reward/reaching_object: 1.2021
     Episode_Reward/lifting_object: 170.6597
      Episode_Reward/object_height: 0.0765
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 1.95s
                      Time elapsed: 00:50:51
                               ETA: 00:20:42

################################################################################
                     [1m Learning iteration 1422/2000 [0m                     

                       Computation: 52344 steps/s (collection: 1.779s, learning 0.099s)
             Mean action noise std: 2.79
          Mean value_function loss: 123.6673
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 52.7837
                       Mean reward: 840.33
               Mean episode length: 225.86
    Episode_Reward/reaching_object: 1.1890
     Episode_Reward/lifting_object: 168.8821
      Episode_Reward/object_height: 0.0759
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 1.88s
                      Time elapsed: 00:50:53
                               ETA: 00:20:40

################################################################################
                     [1m Learning iteration 1423/2000 [0m                     

                       Computation: 51533 steps/s (collection: 1.820s, learning 0.087s)
             Mean action noise std: 2.79
          Mean value_function loss: 110.8885
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.7903
                       Mean reward: 848.89
               Mean episode length: 227.41
    Episode_Reward/reaching_object: 1.2037
     Episode_Reward/lifting_object: 170.5651
      Episode_Reward/object_height: 0.0766
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 1.91s
                      Time elapsed: 00:50:55
                               ETA: 00:20:37

################################################################################
                     [1m Learning iteration 1424/2000 [0m                     

                       Computation: 50995 steps/s (collection: 1.822s, learning 0.106s)
             Mean action noise std: 2.79
          Mean value_function loss: 141.6017
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.7969
                       Mean reward: 830.56
               Mean episode length: 222.56
    Episode_Reward/reaching_object: 1.1856
     Episode_Reward/lifting_object: 168.1018
      Episode_Reward/object_height: 0.0755
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 1.93s
                      Time elapsed: 00:50:57
                               ETA: 00:20:35

################################################################################
                     [1m Learning iteration 1425/2000 [0m                     

                       Computation: 51518 steps/s (collection: 1.815s, learning 0.093s)
             Mean action noise std: 2.79
          Mean value_function loss: 146.0989
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.8077
                       Mean reward: 890.47
               Mean episode length: 237.07
    Episode_Reward/reaching_object: 1.2106
     Episode_Reward/lifting_object: 172.1640
      Episode_Reward/object_height: 0.0770
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 1.91s
                      Time elapsed: 00:50:58
                               ETA: 00:20:33

################################################################################
                     [1m Learning iteration 1426/2000 [0m                     

                       Computation: 52189 steps/s (collection: 1.798s, learning 0.086s)
             Mean action noise std: 2.79
          Mean value_function loss: 118.7822
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 52.8195
                       Mean reward: 870.14
               Mean episode length: 232.51
    Episode_Reward/reaching_object: 1.2129
     Episode_Reward/lifting_object: 172.0432
      Episode_Reward/object_height: 0.0775
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 1.88s
                      Time elapsed: 00:51:00
                               ETA: 00:20:31

################################################################################
                     [1m Learning iteration 1427/2000 [0m                     

                       Computation: 51278 steps/s (collection: 1.806s, learning 0.112s)
             Mean action noise std: 2.79
          Mean value_function loss: 106.6237
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 52.8259
                       Mean reward: 846.49
               Mean episode length: 227.05
    Episode_Reward/reaching_object: 1.2045
     Episode_Reward/lifting_object: 170.4672
      Episode_Reward/object_height: 0.0772
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 1.92s
                      Time elapsed: 00:51:02
                               ETA: 00:20:28

################################################################################
                     [1m Learning iteration 1428/2000 [0m                     

                       Computation: 51584 steps/s (collection: 1.812s, learning 0.094s)
             Mean action noise std: 2.79
          Mean value_function loss: 94.5077
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 52.8318
                       Mean reward: 873.71
               Mean episode length: 233.31
    Episode_Reward/reaching_object: 1.2380
     Episode_Reward/lifting_object: 175.1936
      Episode_Reward/object_height: 0.0796
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 1.91s
                      Time elapsed: 00:51:04
                               ETA: 00:20:26

################################################################################
                     [1m Learning iteration 1429/2000 [0m                     

                       Computation: 51901 steps/s (collection: 1.797s, learning 0.098s)
             Mean action noise std: 2.79
          Mean value_function loss: 100.0342
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.8394
                       Mean reward: 880.93
               Mean episode length: 235.42
    Episode_Reward/reaching_object: 1.2383
     Episode_Reward/lifting_object: 174.9526
      Episode_Reward/object_height: 0.0796
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 1.89s
                      Time elapsed: 00:51:06
                               ETA: 00:20:24

################################################################################
                     [1m Learning iteration 1430/2000 [0m                     

                       Computation: 50176 steps/s (collection: 1.816s, learning 0.143s)
             Mean action noise std: 2.80
          Mean value_function loss: 110.7351
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 52.8463
                       Mean reward: 908.32
               Mean episode length: 241.36
    Episode_Reward/reaching_object: 1.2441
     Episode_Reward/lifting_object: 175.9068
      Episode_Reward/object_height: 0.0800
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 1.96s
                      Time elapsed: 00:51:08
                               ETA: 00:20:22

################################################################################
                     [1m Learning iteration 1431/2000 [0m                     

                       Computation: 50944 steps/s (collection: 1.821s, learning 0.109s)
             Mean action noise std: 2.80
          Mean value_function loss: 107.4603
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 52.8496
                       Mean reward: 880.57
               Mean episode length: 234.91
    Episode_Reward/reaching_object: 1.2174
     Episode_Reward/lifting_object: 171.2344
      Episode_Reward/object_height: 0.0782
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 1.93s
                      Time elapsed: 00:51:10
                               ETA: 00:20:20

################################################################################
                     [1m Learning iteration 1432/2000 [0m                     

                       Computation: 50141 steps/s (collection: 1.832s, learning 0.128s)
             Mean action noise std: 2.80
          Mean value_function loss: 132.4764
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.8562
                       Mean reward: 874.46
               Mean episode length: 232.56
    Episode_Reward/reaching_object: 1.2168
     Episode_Reward/lifting_object: 171.4757
      Episode_Reward/object_height: 0.0775
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 1.96s
                      Time elapsed: 00:51:12
                               ETA: 00:20:17

################################################################################
                     [1m Learning iteration 1433/2000 [0m                     

                       Computation: 50766 steps/s (collection: 1.799s, learning 0.137s)
             Mean action noise std: 2.80
          Mean value_function loss: 149.9177
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 52.8665
                       Mean reward: 900.29
               Mean episode length: 238.97
    Episode_Reward/reaching_object: 1.2353
     Episode_Reward/lifting_object: 174.3852
      Episode_Reward/object_height: 0.0794
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 1.94s
                      Time elapsed: 00:51:14
                               ETA: 00:20:15

################################################################################
                     [1m Learning iteration 1434/2000 [0m                     

                       Computation: 51670 steps/s (collection: 1.810s, learning 0.093s)
             Mean action noise std: 2.80
          Mean value_function loss: 135.0820
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.8723
                       Mean reward: 873.27
               Mean episode length: 232.71
    Episode_Reward/reaching_object: 1.2292
     Episode_Reward/lifting_object: 172.9144
      Episode_Reward/object_height: 0.0788
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 1.90s
                      Time elapsed: 00:51:16
                               ETA: 00:20:13

################################################################################
                     [1m Learning iteration 1435/2000 [0m                     

                       Computation: 51638 steps/s (collection: 1.795s, learning 0.109s)
             Mean action noise std: 2.80
          Mean value_function loss: 125.7762
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.8827
                       Mean reward: 895.74
               Mean episode length: 239.57
    Episode_Reward/reaching_object: 1.2471
     Episode_Reward/lifting_object: 175.9524
      Episode_Reward/object_height: 0.0803
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 1.90s
                      Time elapsed: 00:51:18
                               ETA: 00:20:11

################################################################################
                     [1m Learning iteration 1436/2000 [0m                     

                       Computation: 49960 steps/s (collection: 1.858s, learning 0.109s)
             Mean action noise std: 2.80
          Mean value_function loss: 99.0806
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 52.8934
                       Mean reward: 858.61
               Mean episode length: 229.86
    Episode_Reward/reaching_object: 1.2136
     Episode_Reward/lifting_object: 171.4659
      Episode_Reward/object_height: 0.0786
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 1.97s
                      Time elapsed: 00:51:20
                               ETA: 00:20:08

################################################################################
                     [1m Learning iteration 1437/2000 [0m                     

                       Computation: 52088 steps/s (collection: 1.799s, learning 0.088s)
             Mean action noise std: 2.80
          Mean value_function loss: 117.9110
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 52.8971
                       Mean reward: 854.82
               Mean episode length: 228.35
    Episode_Reward/reaching_object: 1.2301
     Episode_Reward/lifting_object: 174.0328
      Episode_Reward/object_height: 0.0801
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 1.89s
                      Time elapsed: 00:51:22
                               ETA: 00:20:06

################################################################################
                     [1m Learning iteration 1438/2000 [0m                     

                       Computation: 51722 steps/s (collection: 1.785s, learning 0.115s)
             Mean action noise std: 2.80
          Mean value_function loss: 110.8129
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 52.9009
                       Mean reward: 827.31
               Mean episode length: 221.96
    Episode_Reward/reaching_object: 1.2214
     Episode_Reward/lifting_object: 172.7469
      Episode_Reward/object_height: 0.0795
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 1.90s
                      Time elapsed: 00:51:23
                               ETA: 00:20:04

################################################################################
                     [1m Learning iteration 1439/2000 [0m                     

                       Computation: 51418 steps/s (collection: 1.796s, learning 0.116s)
             Mean action noise std: 2.80
          Mean value_function loss: 97.2367
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.9126
                       Mean reward: 896.50
               Mean episode length: 238.01
    Episode_Reward/reaching_object: 1.2409
     Episode_Reward/lifting_object: 176.6669
      Episode_Reward/object_height: 0.0809
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 1.91s
                      Time elapsed: 00:51:25
                               ETA: 00:20:02

################################################################################
                     [1m Learning iteration 1440/2000 [0m                     

                       Computation: 50390 steps/s (collection: 1.844s, learning 0.107s)
             Mean action noise std: 2.81
          Mean value_function loss: 105.2495
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 52.9255
                       Mean reward: 874.72
               Mean episode length: 233.37
    Episode_Reward/reaching_object: 1.2352
     Episode_Reward/lifting_object: 175.7104
      Episode_Reward/object_height: 0.0802
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 1.95s
                      Time elapsed: 00:51:27
                               ETA: 00:19:59

################################################################################
                     [1m Learning iteration 1441/2000 [0m                     

                       Computation: 52065 steps/s (collection: 1.790s, learning 0.098s)
             Mean action noise std: 2.81
          Mean value_function loss: 115.0681
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 52.9414
                       Mean reward: 868.32
               Mean episode length: 232.64
    Episode_Reward/reaching_object: 1.2110
     Episode_Reward/lifting_object: 172.0448
      Episode_Reward/object_height: 0.0783
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 1.89s
                      Time elapsed: 00:51:29
                               ETA: 00:19:57

################################################################################
                     [1m Learning iteration 1442/2000 [0m                     

                       Computation: 50840 steps/s (collection: 1.832s, learning 0.102s)
             Mean action noise std: 2.81
          Mean value_function loss: 116.7816
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 52.9534
                       Mean reward: 868.34
               Mean episode length: 233.30
    Episode_Reward/reaching_object: 1.2125
     Episode_Reward/lifting_object: 172.2234
      Episode_Reward/object_height: 0.0785
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 1.93s
                      Time elapsed: 00:51:31
                               ETA: 00:19:55

################################################################################
                     [1m Learning iteration 1443/2000 [0m                     

                       Computation: 50242 steps/s (collection: 1.852s, learning 0.104s)
             Mean action noise std: 2.81
          Mean value_function loss: 129.8559
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 52.9596
                       Mean reward: 848.02
               Mean episode length: 226.01
    Episode_Reward/reaching_object: 1.2196
     Episode_Reward/lifting_object: 173.7397
      Episode_Reward/object_height: 0.0786
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 1.96s
                      Time elapsed: 00:51:33
                               ETA: 00:19:53

################################################################################
                     [1m Learning iteration 1444/2000 [0m                     

                       Computation: 51990 steps/s (collection: 1.801s, learning 0.090s)
             Mean action noise std: 2.81
          Mean value_function loss: 138.7584
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 52.9666
                       Mean reward: 838.26
               Mean episode length: 224.66
    Episode_Reward/reaching_object: 1.1882
     Episode_Reward/lifting_object: 168.6297
      Episode_Reward/object_height: 0.0761
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 1.89s
                      Time elapsed: 00:51:35
                               ETA: 00:19:51

################################################################################
                     [1m Learning iteration 1445/2000 [0m                     

                       Computation: 50649 steps/s (collection: 1.848s, learning 0.093s)
             Mean action noise std: 2.81
          Mean value_function loss: 118.6643
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 52.9707
                       Mean reward: 857.03
               Mean episode length: 229.59
    Episode_Reward/reaching_object: 1.2174
     Episode_Reward/lifting_object: 172.7802
      Episode_Reward/object_height: 0.0781
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 1.94s
                      Time elapsed: 00:51:37
                               ETA: 00:19:48

################################################################################
                     [1m Learning iteration 1446/2000 [0m                     

                       Computation: 50592 steps/s (collection: 1.826s, learning 0.117s)
             Mean action noise std: 2.81
          Mean value_function loss: 115.5161
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 52.9742
                       Mean reward: 843.42
               Mean episode length: 225.10
    Episode_Reward/reaching_object: 1.1948
     Episode_Reward/lifting_object: 169.9317
      Episode_Reward/object_height: 0.0766
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 1.94s
                      Time elapsed: 00:51:39
                               ETA: 00:19:46

################################################################################
                     [1m Learning iteration 1447/2000 [0m                     

                       Computation: 51493 steps/s (collection: 1.812s, learning 0.097s)
             Mean action noise std: 2.81
          Mean value_function loss: 89.9903
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 52.9819
                       Mean reward: 877.97
               Mean episode length: 233.94
    Episode_Reward/reaching_object: 1.2385
     Episode_Reward/lifting_object: 176.0363
      Episode_Reward/object_height: 0.0795
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 1.91s
                      Time elapsed: 00:51:41
                               ETA: 00:19:44

################################################################################
                     [1m Learning iteration 1448/2000 [0m                     

                       Computation: 51648 steps/s (collection: 1.807s, learning 0.097s)
             Mean action noise std: 2.81
          Mean value_function loss: 85.2632
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 52.9904
                       Mean reward: 893.62
               Mean episode length: 238.22
    Episode_Reward/reaching_object: 1.2168
     Episode_Reward/lifting_object: 172.7023
      Episode_Reward/object_height: 0.0777
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 1.90s
                      Time elapsed: 00:51:43
                               ETA: 00:19:42

################################################################################
                     [1m Learning iteration 1449/2000 [0m                     

                       Computation: 51120 steps/s (collection: 1.813s, learning 0.110s)
             Mean action noise std: 2.81
          Mean value_function loss: 178.7152
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.9979
                       Mean reward: 868.10
               Mean episode length: 232.45
    Episode_Reward/reaching_object: 1.2371
     Episode_Reward/lifting_object: 175.3166
      Episode_Reward/object_height: 0.0789
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 1.92s
                      Time elapsed: 00:51:45
                               ETA: 00:19:39

################################################################################
                     [1m Learning iteration 1450/2000 [0m                     

                       Computation: 50590 steps/s (collection: 1.839s, learning 0.105s)
             Mean action noise std: 2.82
          Mean value_function loss: 199.3660
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 53.0063
                       Mean reward: 887.99
               Mean episode length: 236.08
    Episode_Reward/reaching_object: 1.2102
     Episode_Reward/lifting_object: 170.8454
      Episode_Reward/object_height: 0.0764
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 1.94s
                      Time elapsed: 00:51:47
                               ETA: 00:19:37

################################################################################
                     [1m Learning iteration 1451/2000 [0m                     

                       Computation: 50765 steps/s (collection: 1.834s, learning 0.103s)
             Mean action noise std: 2.82
          Mean value_function loss: 125.2940
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.0163
                       Mean reward: 857.74
               Mean episode length: 230.30
    Episode_Reward/reaching_object: 1.1997
     Episode_Reward/lifting_object: 169.0792
      Episode_Reward/object_height: 0.0757
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 1.94s
                      Time elapsed: 00:51:48
                               ETA: 00:19:35

################################################################################
                     [1m Learning iteration 1452/2000 [0m                     

                       Computation: 50592 steps/s (collection: 1.851s, learning 0.093s)
             Mean action noise std: 2.82
          Mean value_function loss: 122.0380
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.0267
                       Mean reward: 903.84
               Mean episode length: 240.26
    Episode_Reward/reaching_object: 1.2161
     Episode_Reward/lifting_object: 171.3010
      Episode_Reward/object_height: 0.0759
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 1.94s
                      Time elapsed: 00:51:50
                               ETA: 00:19:33

################################################################################
                     [1m Learning iteration 1453/2000 [0m                     

                       Computation: 51325 steps/s (collection: 1.807s, learning 0.109s)
             Mean action noise std: 2.82
          Mean value_function loss: 109.7483
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 53.0368
                       Mean reward: 902.08
               Mean episode length: 239.97
    Episode_Reward/reaching_object: 1.2411
     Episode_Reward/lifting_object: 175.1413
      Episode_Reward/object_height: 0.0780
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 1.92s
                      Time elapsed: 00:51:52
                               ETA: 00:19:31

################################################################################
                     [1m Learning iteration 1454/2000 [0m                     

                       Computation: 52258 steps/s (collection: 1.770s, learning 0.111s)
             Mean action noise std: 2.82
          Mean value_function loss: 143.8383
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 53.0460
                       Mean reward: 782.02
               Mean episode length: 212.45
    Episode_Reward/reaching_object: 1.1980
     Episode_Reward/lifting_object: 168.0009
      Episode_Reward/object_height: 0.0751
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 1.88s
                      Time elapsed: 00:51:54
                               ETA: 00:19:28

################################################################################
                     [1m Learning iteration 1455/2000 [0m                     

                       Computation: 49138 steps/s (collection: 1.840s, learning 0.161s)
             Mean action noise std: 2.82
          Mean value_function loss: 125.3207
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.0560
                       Mean reward: 908.40
               Mean episode length: 241.27
    Episode_Reward/reaching_object: 1.2319
     Episode_Reward/lifting_object: 173.4233
      Episode_Reward/object_height: 0.0775
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 2.00s
                      Time elapsed: 00:51:56
                               ETA: 00:19:26

################################################################################
                     [1m Learning iteration 1456/2000 [0m                     

                       Computation: 50669 steps/s (collection: 1.822s, learning 0.119s)
             Mean action noise std: 2.82
          Mean value_function loss: 118.3173
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.0636
                       Mean reward: 887.68
               Mean episode length: 236.56
    Episode_Reward/reaching_object: 1.2247
     Episode_Reward/lifting_object: 172.5969
      Episode_Reward/object_height: 0.0772
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 1.94s
                      Time elapsed: 00:51:58
                               ETA: 00:19:24

################################################################################
                     [1m Learning iteration 1457/2000 [0m                     

                       Computation: 50686 steps/s (collection: 1.839s, learning 0.101s)
             Mean action noise std: 2.82
          Mean value_function loss: 151.4895
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.0711
                       Mean reward: 799.32
               Mean episode length: 214.82
    Episode_Reward/reaching_object: 1.2247
     Episode_Reward/lifting_object: 172.5079
      Episode_Reward/object_height: 0.0769
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 1.94s
                      Time elapsed: 00:52:00
                               ETA: 00:19:22

################################################################################
                     [1m Learning iteration 1458/2000 [0m                     

                       Computation: 49977 steps/s (collection: 1.828s, learning 0.139s)
             Mean action noise std: 2.83
          Mean value_function loss: 117.9082
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 53.0805
                       Mean reward: 874.97
               Mean episode length: 232.79
    Episode_Reward/reaching_object: 1.2235
     Episode_Reward/lifting_object: 172.3646
      Episode_Reward/object_height: 0.0769
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 1.97s
                      Time elapsed: 00:52:02
                               ETA: 00:19:19

################################################################################
                     [1m Learning iteration 1459/2000 [0m                     

                       Computation: 51133 steps/s (collection: 1.814s, learning 0.109s)
             Mean action noise std: 2.83
          Mean value_function loss: 138.5311
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 53.0908
                       Mean reward: 850.73
               Mean episode length: 227.97
    Episode_Reward/reaching_object: 1.2089
     Episode_Reward/lifting_object: 169.5349
      Episode_Reward/object_height: 0.0755
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 1.92s
                      Time elapsed: 00:52:04
                               ETA: 00:19:17

################################################################################
                     [1m Learning iteration 1460/2000 [0m                     

                       Computation: 50738 steps/s (collection: 1.840s, learning 0.097s)
             Mean action noise std: 2.83
          Mean value_function loss: 187.9579
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 53.1008
                       Mean reward: 863.72
               Mean episode length: 230.15
    Episode_Reward/reaching_object: 1.2169
     Episode_Reward/lifting_object: 171.2972
      Episode_Reward/object_height: 0.0760
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 1.94s
                      Time elapsed: 00:52:06
                               ETA: 00:19:15

################################################################################
                     [1m Learning iteration 1461/2000 [0m                     

                       Computation: 52130 steps/s (collection: 1.798s, learning 0.088s)
             Mean action noise std: 2.83
          Mean value_function loss: 246.9899
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 53.1124
                       Mean reward: 839.13
               Mean episode length: 224.91
    Episode_Reward/reaching_object: 1.2138
     Episode_Reward/lifting_object: 170.3154
      Episode_Reward/object_height: 0.0758
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 1.89s
                      Time elapsed: 00:52:08
                               ETA: 00:19:13

################################################################################
                     [1m Learning iteration 1462/2000 [0m                     

                       Computation: 51574 steps/s (collection: 1.796s, learning 0.110s)
             Mean action noise std: 2.83
          Mean value_function loss: 161.0986
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 53.1260
                       Mean reward: 856.12
               Mean episode length: 227.93
    Episode_Reward/reaching_object: 1.2043
     Episode_Reward/lifting_object: 168.7049
      Episode_Reward/object_height: 0.0755
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 1.91s
                      Time elapsed: 00:52:10
                               ETA: 00:19:11

################################################################################
                     [1m Learning iteration 1463/2000 [0m                     

                       Computation: 52313 steps/s (collection: 1.788s, learning 0.092s)
             Mean action noise std: 2.83
          Mean value_function loss: 107.6106
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 53.1389
                       Mean reward: 883.54
               Mean episode length: 235.47
    Episode_Reward/reaching_object: 1.2345
     Episode_Reward/lifting_object: 172.6256
      Episode_Reward/object_height: 0.0778
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 1.88s
                      Time elapsed: 00:52:12
                               ETA: 00:19:08

################################################################################
                     [1m Learning iteration 1464/2000 [0m                     

                       Computation: 50513 steps/s (collection: 1.860s, learning 0.087s)
             Mean action noise std: 2.83
          Mean value_function loss: 105.4030
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 53.1493
                       Mean reward: 886.91
               Mean episode length: 235.74
    Episode_Reward/reaching_object: 1.2514
     Episode_Reward/lifting_object: 175.6942
      Episode_Reward/object_height: 0.0785
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 1.95s
                      Time elapsed: 00:52:14
                               ETA: 00:19:06

################################################################################
                     [1m Learning iteration 1465/2000 [0m                     

                       Computation: 51425 steps/s (collection: 1.826s, learning 0.086s)
             Mean action noise std: 2.84
          Mean value_function loss: 103.0737
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.1601
                       Mean reward: 891.93
               Mean episode length: 236.92
    Episode_Reward/reaching_object: 1.2506
     Episode_Reward/lifting_object: 175.7220
      Episode_Reward/object_height: 0.0785
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 1.91s
                      Time elapsed: 00:52:15
                               ETA: 00:19:04

################################################################################
                     [1m Learning iteration 1466/2000 [0m                     

                       Computation: 52095 steps/s (collection: 1.792s, learning 0.095s)
             Mean action noise std: 2.84
          Mean value_function loss: 119.4079
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 53.1698
                       Mean reward: 851.85
               Mean episode length: 226.73
    Episode_Reward/reaching_object: 1.2429
     Episode_Reward/lifting_object: 174.1900
      Episode_Reward/object_height: 0.0782
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 1.89s
                      Time elapsed: 00:52:17
                               ETA: 00:19:02

################################################################################
                     [1m Learning iteration 1467/2000 [0m                     

                       Computation: 50998 steps/s (collection: 1.808s, learning 0.120s)
             Mean action noise std: 2.84
          Mean value_function loss: 105.8335
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.1767
                       Mean reward: 877.61
               Mean episode length: 233.95
    Episode_Reward/reaching_object: 1.2346
     Episode_Reward/lifting_object: 173.2383
      Episode_Reward/object_height: 0.0778
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 1.93s
                      Time elapsed: 00:52:19
                               ETA: 00:18:59

################################################################################
                     [1m Learning iteration 1468/2000 [0m                     

                       Computation: 49593 steps/s (collection: 1.853s, learning 0.130s)
             Mean action noise std: 2.84
          Mean value_function loss: 134.5840
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 53.1906
                       Mean reward: 859.51
               Mean episode length: 229.19
    Episode_Reward/reaching_object: 1.2505
     Episode_Reward/lifting_object: 175.4346
      Episode_Reward/object_height: 0.0784
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 1.98s
                      Time elapsed: 00:52:21
                               ETA: 00:18:57

################################################################################
                     [1m Learning iteration 1469/2000 [0m                     

                       Computation: 50267 steps/s (collection: 1.846s, learning 0.110s)
             Mean action noise std: 2.84
          Mean value_function loss: 103.5226
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 53.1989
                       Mean reward: 847.53
               Mean episode length: 228.47
    Episode_Reward/reaching_object: 1.2215
     Episode_Reward/lifting_object: 170.5355
      Episode_Reward/object_height: 0.0766
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 1.96s
                      Time elapsed: 00:52:23
                               ETA: 00:18:55

################################################################################
                     [1m Learning iteration 1470/2000 [0m                     

                       Computation: 51780 steps/s (collection: 1.801s, learning 0.097s)
             Mean action noise std: 2.84
          Mean value_function loss: 108.8457
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 53.2076
                       Mean reward: 886.60
               Mean episode length: 236.18
    Episode_Reward/reaching_object: 1.2569
     Episode_Reward/lifting_object: 176.1687
      Episode_Reward/object_height: 0.0791
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 1.90s
                      Time elapsed: 00:52:25
                               ETA: 00:18:53

################################################################################
                     [1m Learning iteration 1471/2000 [0m                     

                       Computation: 50295 steps/s (collection: 1.839s, learning 0.115s)
             Mean action noise std: 2.84
          Mean value_function loss: 115.5894
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 53.2129
                       Mean reward: 893.85
               Mean episode length: 238.09
    Episode_Reward/reaching_object: 1.2686
     Episode_Reward/lifting_object: 177.7585
      Episode_Reward/object_height: 0.0803
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 1.95s
                      Time elapsed: 00:52:27
                               ETA: 00:18:51

################################################################################
                     [1m Learning iteration 1472/2000 [0m                     

                       Computation: 51499 steps/s (collection: 1.798s, learning 0.111s)
             Mean action noise std: 2.84
          Mean value_function loss: 183.3415
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 53.2200
                       Mean reward: 880.59
               Mean episode length: 234.19
    Episode_Reward/reaching_object: 1.2484
     Episode_Reward/lifting_object: 174.8729
      Episode_Reward/object_height: 0.0788
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 1.91s
                      Time elapsed: 00:52:29
                               ETA: 00:18:48

################################################################################
                     [1m Learning iteration 1473/2000 [0m                     

                       Computation: 50260 steps/s (collection: 1.848s, learning 0.108s)
             Mean action noise std: 2.84
          Mean value_function loss: 179.8478
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.2313
                       Mean reward: 858.17
               Mean episode length: 229.11
    Episode_Reward/reaching_object: 1.2116
     Episode_Reward/lifting_object: 169.3802
      Episode_Reward/object_height: 0.0765
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 1.96s
                      Time elapsed: 00:52:31
                               ETA: 00:18:46

################################################################################
                     [1m Learning iteration 1474/2000 [0m                     

                       Computation: 51052 steps/s (collection: 1.819s, learning 0.106s)
             Mean action noise std: 2.85
          Mean value_function loss: 135.2340
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 53.2415
                       Mean reward: 862.94
               Mean episode length: 230.50
    Episode_Reward/reaching_object: 1.2490
     Episode_Reward/lifting_object: 174.6930
      Episode_Reward/object_height: 0.0792
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 1.93s
                      Time elapsed: 00:52:33
                               ETA: 00:18:44

################################################################################
                     [1m Learning iteration 1475/2000 [0m                     

                       Computation: 51577 steps/s (collection: 1.818s, learning 0.088s)
             Mean action noise std: 2.85
          Mean value_function loss: 133.3865
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 53.2534
                       Mean reward: 882.07
               Mean episode length: 233.66
    Episode_Reward/reaching_object: 1.2164
     Episode_Reward/lifting_object: 170.0880
      Episode_Reward/object_height: 0.0769
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 1.91s
                      Time elapsed: 00:52:35
                               ETA: 00:18:42

################################################################################
                     [1m Learning iteration 1476/2000 [0m                     

                       Computation: 51430 steps/s (collection: 1.809s, learning 0.102s)
             Mean action noise std: 2.85
          Mean value_function loss: 117.2100
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 53.2613
                       Mean reward: 912.23
               Mean episode length: 241.73
    Episode_Reward/reaching_object: 1.2447
     Episode_Reward/lifting_object: 174.1386
      Episode_Reward/object_height: 0.0792
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 1.91s
                      Time elapsed: 00:52:37
                               ETA: 00:18:40

################################################################################
                     [1m Learning iteration 1477/2000 [0m                     

                       Computation: 51215 steps/s (collection: 1.834s, learning 0.085s)
             Mean action noise std: 2.85
          Mean value_function loss: 129.9663
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 53.2675
                       Mean reward: 881.54
               Mean episode length: 235.89
    Episode_Reward/reaching_object: 1.2454
     Episode_Reward/lifting_object: 173.8313
      Episode_Reward/object_height: 0.0792
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 1.92s
                      Time elapsed: 00:52:39
                               ETA: 00:18:37

################################################################################
                     [1m Learning iteration 1478/2000 [0m                     

                       Computation: 51857 steps/s (collection: 1.798s, learning 0.098s)
             Mean action noise std: 2.85
          Mean value_function loss: 133.6822
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 53.2733
                       Mean reward: 878.42
               Mean episode length: 234.17
    Episode_Reward/reaching_object: 1.2437
     Episode_Reward/lifting_object: 174.0372
      Episode_Reward/object_height: 0.0790
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 1.90s
                      Time elapsed: 00:52:40
                               ETA: 00:18:35

################################################################################
                     [1m Learning iteration 1479/2000 [0m                     

                       Computation: 51136 steps/s (collection: 1.782s, learning 0.141s)
             Mean action noise std: 2.85
          Mean value_function loss: 116.8190
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.2780
                       Mean reward: 889.14
               Mean episode length: 236.94
    Episode_Reward/reaching_object: 1.2422
     Episode_Reward/lifting_object: 173.7177
      Episode_Reward/object_height: 0.0790
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 1.92s
                      Time elapsed: 00:52:42
                               ETA: 00:18:33

################################################################################
                     [1m Learning iteration 1480/2000 [0m                     

                       Computation: 49900 steps/s (collection: 1.804s, learning 0.166s)
             Mean action noise std: 2.85
          Mean value_function loss: 123.2561
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 53.2869
                       Mean reward: 897.90
               Mean episode length: 238.42
    Episode_Reward/reaching_object: 1.2515
     Episode_Reward/lifting_object: 175.6712
      Episode_Reward/object_height: 0.0796
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 1.97s
                      Time elapsed: 00:52:44
                               ETA: 00:18:31

################################################################################
                     [1m Learning iteration 1481/2000 [0m                     

                       Computation: 51263 steps/s (collection: 1.811s, learning 0.107s)
             Mean action noise std: 2.85
          Mean value_function loss: 142.3851
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 53.2928
                       Mean reward: 912.31
               Mean episode length: 242.23
    Episode_Reward/reaching_object: 1.2512
     Episode_Reward/lifting_object: 175.8777
      Episode_Reward/object_height: 0.0794
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 1.92s
                      Time elapsed: 00:52:46
                               ETA: 00:18:29

################################################################################
                     [1m Learning iteration 1482/2000 [0m                     

                       Computation: 51201 steps/s (collection: 1.822s, learning 0.098s)
             Mean action noise std: 2.85
          Mean value_function loss: 136.7232
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 53.3010
                       Mean reward: 890.37
               Mean episode length: 237.28
    Episode_Reward/reaching_object: 1.2348
     Episode_Reward/lifting_object: 173.3686
      Episode_Reward/object_height: 0.0785
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 1.92s
                      Time elapsed: 00:52:48
                               ETA: 00:18:26

################################################################################
                     [1m Learning iteration 1483/2000 [0m                     

                       Computation: 49437 steps/s (collection: 1.846s, learning 0.142s)
             Mean action noise std: 2.85
          Mean value_function loss: 115.9292
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.3061
                       Mean reward: 879.37
               Mean episode length: 234.24
    Episode_Reward/reaching_object: 1.2233
     Episode_Reward/lifting_object: 171.9993
      Episode_Reward/object_height: 0.0778
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 1.99s
                      Time elapsed: 00:52:50
                               ETA: 00:18:24

################################################################################
                     [1m Learning iteration 1484/2000 [0m                     

                       Computation: 51091 steps/s (collection: 1.830s, learning 0.094s)
             Mean action noise std: 2.85
          Mean value_function loss: 113.6831
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.3111
                       Mean reward: 879.63
               Mean episode length: 234.34
    Episode_Reward/reaching_object: 1.2367
     Episode_Reward/lifting_object: 174.0099
      Episode_Reward/object_height: 0.0790
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 1.92s
                      Time elapsed: 00:52:52
                               ETA: 00:18:22

################################################################################
                     [1m Learning iteration 1485/2000 [0m                     

                       Computation: 50974 steps/s (collection: 1.820s, learning 0.108s)
             Mean action noise std: 2.86
          Mean value_function loss: 111.0759
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 53.3181
                       Mean reward: 876.65
               Mean episode length: 233.13
    Episode_Reward/reaching_object: 1.2418
     Episode_Reward/lifting_object: 174.5882
      Episode_Reward/object_height: 0.0792
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 1.93s
                      Time elapsed: 00:52:54
                               ETA: 00:18:20

################################################################################
                     [1m Learning iteration 1486/2000 [0m                     

                       Computation: 51821 steps/s (collection: 1.807s, learning 0.090s)
             Mean action noise std: 2.86
          Mean value_function loss: 129.2079
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 53.3250
                       Mean reward: 888.41
               Mean episode length: 237.00
    Episode_Reward/reaching_object: 1.2562
     Episode_Reward/lifting_object: 176.7192
      Episode_Reward/object_height: 0.0803
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 1.90s
                      Time elapsed: 00:52:56
                               ETA: 00:18:17

################################################################################
                     [1m Learning iteration 1487/2000 [0m                     

                       Computation: 51849 steps/s (collection: 1.805s, learning 0.091s)
             Mean action noise std: 2.86
          Mean value_function loss: 143.3378
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 53.3352
                       Mean reward: 863.45
               Mean episode length: 231.58
    Episode_Reward/reaching_object: 1.2130
     Episode_Reward/lifting_object: 169.5891
      Episode_Reward/object_height: 0.0768
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 1.90s
                      Time elapsed: 00:52:58
                               ETA: 00:18:15

################################################################################
                     [1m Learning iteration 1488/2000 [0m                     

                       Computation: 51572 steps/s (collection: 1.820s, learning 0.086s)
             Mean action noise std: 2.86
          Mean value_function loss: 175.5807
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 53.3475
                       Mean reward: 890.45
               Mean episode length: 236.82
    Episode_Reward/reaching_object: 1.2314
     Episode_Reward/lifting_object: 172.9033
      Episode_Reward/object_height: 0.0778
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 1.91s
                      Time elapsed: 00:53:00
                               ETA: 00:18:13

################################################################################
                     [1m Learning iteration 1489/2000 [0m                     

                       Computation: 51338 steps/s (collection: 1.808s, learning 0.107s)
             Mean action noise std: 2.86
          Mean value_function loss: 106.7450
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 53.3582
                       Mean reward: 862.03
               Mean episode length: 232.87
    Episode_Reward/reaching_object: 1.2256
     Episode_Reward/lifting_object: 171.1969
      Episode_Reward/object_height: 0.0766
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 1.91s
                      Time elapsed: 00:53:02
                               ETA: 00:18:11

################################################################################
                     [1m Learning iteration 1490/2000 [0m                     

                       Computation: 50764 steps/s (collection: 1.832s, learning 0.105s)
             Mean action noise std: 2.86
          Mean value_function loss: 127.0987
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.3647
                       Mean reward: 856.51
               Mean episode length: 228.86
    Episode_Reward/reaching_object: 1.2310
     Episode_Reward/lifting_object: 172.3042
      Episode_Reward/object_height: 0.0768
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 1.94s
                      Time elapsed: 00:53:04
                               ETA: 00:18:09

################################################################################
                     [1m Learning iteration 1491/2000 [0m                     

                       Computation: 51318 steps/s (collection: 1.824s, learning 0.092s)
             Mean action noise std: 2.86
          Mean value_function loss: 106.0794
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 53.3743
                       Mean reward: 846.08
               Mean episode length: 227.49
    Episode_Reward/reaching_object: 1.2211
     Episode_Reward/lifting_object: 169.9604
      Episode_Reward/object_height: 0.0755
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 1.92s
                      Time elapsed: 00:53:06
                               ETA: 00:18:06

################################################################################
                     [1m Learning iteration 1492/2000 [0m                     

                       Computation: 50451 steps/s (collection: 1.814s, learning 0.134s)
             Mean action noise std: 2.86
          Mean value_function loss: 117.3294
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.3886
                       Mean reward: 809.15
               Mean episode length: 219.19
    Episode_Reward/reaching_object: 1.2286
     Episode_Reward/lifting_object: 170.9296
      Episode_Reward/object_height: 0.0747
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 1.95s
                      Time elapsed: 00:53:07
                               ETA: 00:18:04

################################################################################
                     [1m Learning iteration 1493/2000 [0m                     

                       Computation: 50816 steps/s (collection: 1.828s, learning 0.106s)
             Mean action noise std: 2.87
          Mean value_function loss: 110.7792
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 53.4006
                       Mean reward: 846.95
               Mean episode length: 226.37
    Episode_Reward/reaching_object: 1.2647
     Episode_Reward/lifting_object: 175.7591
      Episode_Reward/object_height: 0.0771
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 1.93s
                      Time elapsed: 00:53:09
                               ETA: 00:18:02

################################################################################
                     [1m Learning iteration 1494/2000 [0m                     

                       Computation: 49862 steps/s (collection: 1.846s, learning 0.126s)
             Mean action noise std: 2.87
          Mean value_function loss: 151.9832
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.4085
                       Mean reward: 863.61
               Mean episode length: 229.79
    Episode_Reward/reaching_object: 1.2406
     Episode_Reward/lifting_object: 172.0594
      Episode_Reward/object_height: 0.0757
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 1.97s
                      Time elapsed: 00:53:11
                               ETA: 00:18:00

################################################################################
                     [1m Learning iteration 1495/2000 [0m                     

                       Computation: 50543 steps/s (collection: 1.849s, learning 0.096s)
             Mean action noise std: 2.87
          Mean value_function loss: 113.8096
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.4155
                       Mean reward: 895.02
               Mean episode length: 237.52
    Episode_Reward/reaching_object: 1.2579
     Episode_Reward/lifting_object: 174.4764
      Episode_Reward/object_height: 0.0764
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 1.94s
                      Time elapsed: 00:53:13
                               ETA: 00:17:58

################################################################################
                     [1m Learning iteration 1496/2000 [0m                     

                       Computation: 50263 steps/s (collection: 1.838s, learning 0.118s)
             Mean action noise std: 2.87
          Mean value_function loss: 132.8485
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 53.4207
                       Mean reward: 859.71
               Mean episode length: 228.73
    Episode_Reward/reaching_object: 1.2260
     Episode_Reward/lifting_object: 169.9928
      Episode_Reward/object_height: 0.0741
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 1.96s
                      Time elapsed: 00:53:15
                               ETA: 00:17:55

################################################################################
                     [1m Learning iteration 1497/2000 [0m                     

                       Computation: 51344 steps/s (collection: 1.828s, learning 0.087s)
             Mean action noise std: 2.87
          Mean value_function loss: 164.6667
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 53.4242
                       Mean reward: 833.69
               Mean episode length: 224.14
    Episode_Reward/reaching_object: 1.2401
     Episode_Reward/lifting_object: 171.8802
      Episode_Reward/object_height: 0.0750
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 1.91s
                      Time elapsed: 00:53:17
                               ETA: 00:17:53

################################################################################
                     [1m Learning iteration 1498/2000 [0m                     

                       Computation: 49446 steps/s (collection: 1.860s, learning 0.128s)
             Mean action noise std: 2.87
          Mean value_function loss: 158.4552
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 53.4306
                       Mean reward: 862.53
               Mean episode length: 230.37
    Episode_Reward/reaching_object: 1.2178
     Episode_Reward/lifting_object: 168.6385
      Episode_Reward/object_height: 0.0736
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 1.99s
                      Time elapsed: 00:53:19
                               ETA: 00:17:51

################################################################################
                     [1m Learning iteration 1499/2000 [0m                     

                       Computation: 51436 steps/s (collection: 1.789s, learning 0.123s)
             Mean action noise std: 2.87
          Mean value_function loss: 138.0890
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 53.4385
                       Mean reward: 837.35
               Mean episode length: 224.41
    Episode_Reward/reaching_object: 1.2235
     Episode_Reward/lifting_object: 169.3889
      Episode_Reward/object_height: 0.0745
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 1.91s
                      Time elapsed: 00:53:21
                               ETA: 00:17:49

################################################################################
                     [1m Learning iteration 1500/2000 [0m                     

                       Computation: 51499 steps/s (collection: 1.822s, learning 0.087s)
             Mean action noise std: 2.87
          Mean value_function loss: 130.3769
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.4438
                       Mean reward: 853.65
               Mean episode length: 227.78
    Episode_Reward/reaching_object: 1.2606
     Episode_Reward/lifting_object: 174.8352
      Episode_Reward/object_height: 0.0772
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 1.91s
                      Time elapsed: 00:53:23
                               ETA: 00:17:47

################################################################################
                     [1m Learning iteration 1501/2000 [0m                     

                       Computation: 50867 steps/s (collection: 1.831s, learning 0.102s)
             Mean action noise std: 2.87
          Mean value_function loss: 152.2615
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.4492
                       Mean reward: 881.53
               Mean episode length: 235.96
    Episode_Reward/reaching_object: 1.2644
     Episode_Reward/lifting_object: 175.2664
      Episode_Reward/object_height: 0.0774
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 1.93s
                      Time elapsed: 00:53:25
                               ETA: 00:17:44

################################################################################
                     [1m Learning iteration 1502/2000 [0m                     

                       Computation: 50617 steps/s (collection: 1.835s, learning 0.108s)
             Mean action noise std: 2.87
          Mean value_function loss: 129.5021
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 53.4542
                       Mean reward: 870.96
               Mean episode length: 232.07
    Episode_Reward/reaching_object: 1.2430
     Episode_Reward/lifting_object: 172.5765
      Episode_Reward/object_height: 0.0761
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 1.94s
                      Time elapsed: 00:53:27
                               ETA: 00:17:42

################################################################################
                     [1m Learning iteration 1503/2000 [0m                     

                       Computation: 50308 steps/s (collection: 1.851s, learning 0.103s)
             Mean action noise std: 2.87
          Mean value_function loss: 105.6768
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 53.4621
                       Mean reward: 837.84
               Mean episode length: 226.42
    Episode_Reward/reaching_object: 1.2595
     Episode_Reward/lifting_object: 174.8541
      Episode_Reward/object_height: 0.0771
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 1.95s
                      Time elapsed: 00:53:29
                               ETA: 00:17:40

################################################################################
                     [1m Learning iteration 1504/2000 [0m                     

                       Computation: 49287 steps/s (collection: 1.890s, learning 0.105s)
             Mean action noise std: 2.87
          Mean value_function loss: 100.1312
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 53.4707
                       Mean reward: 856.65
               Mean episode length: 229.36
    Episode_Reward/reaching_object: 1.2440
     Episode_Reward/lifting_object: 172.9322
      Episode_Reward/object_height: 0.0761
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 1.99s
                      Time elapsed: 00:53:31
                               ETA: 00:17:38

################################################################################
                     [1m Learning iteration 1505/2000 [0m                     

                       Computation: 51357 steps/s (collection: 1.805s, learning 0.109s)
             Mean action noise std: 2.88
          Mean value_function loss: 105.1498
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 53.4784
                       Mean reward: 846.76
               Mean episode length: 227.51
    Episode_Reward/reaching_object: 1.2473
     Episode_Reward/lifting_object: 173.2184
      Episode_Reward/object_height: 0.0760
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 1.91s
                      Time elapsed: 00:53:33
                               ETA: 00:17:36

################################################################################
                     [1m Learning iteration 1506/2000 [0m                     

                       Computation: 50447 steps/s (collection: 1.859s, learning 0.090s)
             Mean action noise std: 2.88
          Mean value_function loss: 120.0824
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.4872
                       Mean reward: 887.87
               Mean episode length: 236.77
    Episode_Reward/reaching_object: 1.2083
     Episode_Reward/lifting_object: 167.8944
      Episode_Reward/object_height: 0.0733
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 1.95s
                      Time elapsed: 00:53:35
                               ETA: 00:17:33

################################################################################
                     [1m Learning iteration 1507/2000 [0m                     

                       Computation: 46043 steps/s (collection: 2.000s, learning 0.135s)
             Mean action noise std: 2.88
          Mean value_function loss: 122.7755
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 53.4975
                       Mean reward: 912.35
               Mean episode length: 243.08
    Episode_Reward/reaching_object: 1.2355
     Episode_Reward/lifting_object: 171.5581
      Episode_Reward/object_height: 0.0749
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 2.14s
                      Time elapsed: 00:53:37
                               ETA: 00:17:31

################################################################################
                     [1m Learning iteration 1508/2000 [0m                     

                       Computation: 45109 steps/s (collection: 2.044s, learning 0.136s)
             Mean action noise std: 2.88
          Mean value_function loss: 119.3605
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 53.5059
                       Mean reward: 850.78
               Mean episode length: 228.01
    Episode_Reward/reaching_object: 1.2340
     Episode_Reward/lifting_object: 171.8040
      Episode_Reward/object_height: 0.0744
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 2.18s
                      Time elapsed: 00:53:39
                               ETA: 00:17:29

################################################################################
                     [1m Learning iteration 1509/2000 [0m                     

                       Computation: 47467 steps/s (collection: 1.978s, learning 0.093s)
             Mean action noise std: 2.88
          Mean value_function loss: 137.2782
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 53.5102
                       Mean reward: 865.63
               Mean episode length: 231.80
    Episode_Reward/reaching_object: 1.2179
     Episode_Reward/lifting_object: 169.5503
      Episode_Reward/object_height: 0.0732
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 2.07s
                      Time elapsed: 00:53:41
                               ETA: 00:17:27

################################################################################
                     [1m Learning iteration 1510/2000 [0m                     

                       Computation: 50892 steps/s (collection: 1.827s, learning 0.105s)
             Mean action noise std: 2.88
          Mean value_function loss: 122.5183
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.5152
                       Mean reward: 883.35
               Mean episode length: 236.00
    Episode_Reward/reaching_object: 1.2214
     Episode_Reward/lifting_object: 169.8202
      Episode_Reward/object_height: 0.0735
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 1.93s
                      Time elapsed: 00:53:43
                               ETA: 00:17:25

################################################################################
                     [1m Learning iteration 1511/2000 [0m                     

                       Computation: 51040 steps/s (collection: 1.823s, learning 0.103s)
             Mean action noise std: 2.88
          Mean value_function loss: 105.9093
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 53.5247
                       Mean reward: 843.47
               Mean episode length: 226.52
    Episode_Reward/reaching_object: 1.2339
     Episode_Reward/lifting_object: 171.2678
      Episode_Reward/object_height: 0.0744
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 1.93s
                      Time elapsed: 00:53:45
                               ETA: 00:17:23

################################################################################
                     [1m Learning iteration 1512/2000 [0m                     

                       Computation: 51812 steps/s (collection: 1.808s, learning 0.089s)
             Mean action noise std: 2.88
          Mean value_function loss: 134.2045
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 53.5320
                       Mean reward: 838.85
               Mean episode length: 225.02
    Episode_Reward/reaching_object: 1.2325
     Episode_Reward/lifting_object: 171.3495
      Episode_Reward/object_height: 0.0733
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 1.90s
                      Time elapsed: 00:53:47
                               ETA: 00:17:20

################################################################################
                     [1m Learning iteration 1513/2000 [0m                     

                       Computation: 50781 steps/s (collection: 1.835s, learning 0.101s)
             Mean action noise std: 2.88
          Mean value_function loss: 139.3140
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 53.5377
                       Mean reward: 879.67
               Mean episode length: 234.79
    Episode_Reward/reaching_object: 1.2283
     Episode_Reward/lifting_object: 171.1500
      Episode_Reward/object_height: 0.0734
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 1.94s
                      Time elapsed: 00:53:49
                               ETA: 00:17:18

################################################################################
                     [1m Learning iteration 1514/2000 [0m                     

                       Computation: 50777 steps/s (collection: 1.842s, learning 0.094s)
             Mean action noise std: 2.88
          Mean value_function loss: 87.2773
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.5421
                       Mean reward: 886.73
               Mean episode length: 236.87
    Episode_Reward/reaching_object: 1.2515
     Episode_Reward/lifting_object: 173.7963
      Episode_Reward/object_height: 0.0751
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 1.94s
                      Time elapsed: 00:53:51
                               ETA: 00:17:16

################################################################################
                     [1m Learning iteration 1515/2000 [0m                     

                       Computation: 50580 steps/s (collection: 1.829s, learning 0.114s)
             Mean action noise std: 2.88
          Mean value_function loss: 139.4989
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 53.5479
                       Mean reward: 858.17
               Mean episode length: 230.71
    Episode_Reward/reaching_object: 1.2507
     Episode_Reward/lifting_object: 173.9892
      Episode_Reward/object_height: 0.0753
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 1.94s
                      Time elapsed: 00:53:53
                               ETA: 00:17:14

################################################################################
                     [1m Learning iteration 1516/2000 [0m                     

                       Computation: 50577 steps/s (collection: 1.842s, learning 0.102s)
             Mean action noise std: 2.89
          Mean value_function loss: 104.1071
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.5509
                       Mean reward: 869.97
               Mean episode length: 232.33
    Episode_Reward/reaching_object: 1.2425
     Episode_Reward/lifting_object: 172.8132
      Episode_Reward/object_height: 0.0746
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 1.94s
                      Time elapsed: 00:53:55
                               ETA: 00:17:12

################################################################################
                     [1m Learning iteration 1517/2000 [0m                     

                       Computation: 51214 steps/s (collection: 1.805s, learning 0.114s)
             Mean action noise std: 2.89
          Mean value_function loss: 141.9065
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.5575
                       Mean reward: 834.88
               Mean episode length: 225.81
    Episode_Reward/reaching_object: 1.2304
     Episode_Reward/lifting_object: 171.0563
      Episode_Reward/object_height: 0.0739
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 1.92s
                      Time elapsed: 00:53:56
                               ETA: 00:17:09

################################################################################
                     [1m Learning iteration 1518/2000 [0m                     

                       Computation: 50615 steps/s (collection: 1.820s, learning 0.123s)
             Mean action noise std: 2.89
          Mean value_function loss: 122.6751
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 53.5706
                       Mean reward: 877.20
               Mean episode length: 233.99
    Episode_Reward/reaching_object: 1.2357
     Episode_Reward/lifting_object: 171.6918
      Episode_Reward/object_height: 0.0737
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 1.94s
                      Time elapsed: 00:53:58
                               ETA: 00:17:07

################################################################################
                     [1m Learning iteration 1519/2000 [0m                     

                       Computation: 49976 steps/s (collection: 1.832s, learning 0.135s)
             Mean action noise std: 2.89
          Mean value_function loss: 134.4971
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.5774
                       Mean reward: 887.87
               Mean episode length: 236.42
    Episode_Reward/reaching_object: 1.2434
     Episode_Reward/lifting_object: 173.2155
      Episode_Reward/object_height: 0.0744
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 1.97s
                      Time elapsed: 00:54:00
                               ETA: 00:17:05

################################################################################
                     [1m Learning iteration 1520/2000 [0m                     

                       Computation: 51454 steps/s (collection: 1.816s, learning 0.094s)
             Mean action noise std: 2.89
          Mean value_function loss: 114.6267
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.5858
                       Mean reward: 894.82
               Mean episode length: 237.57
    Episode_Reward/reaching_object: 1.2606
     Episode_Reward/lifting_object: 175.4879
      Episode_Reward/object_height: 0.0754
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 1.91s
                      Time elapsed: 00:54:02
                               ETA: 00:17:03

################################################################################
                     [1m Learning iteration 1521/2000 [0m                     

                       Computation: 51566 steps/s (collection: 1.821s, learning 0.086s)
             Mean action noise std: 2.89
          Mean value_function loss: 133.7726
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 53.5977
                       Mean reward: 897.23
               Mean episode length: 238.60
    Episode_Reward/reaching_object: 1.2418
     Episode_Reward/lifting_object: 172.8961
      Episode_Reward/object_height: 0.0739
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 1.91s
                      Time elapsed: 00:54:04
                               ETA: 00:17:01

################################################################################
                     [1m Learning iteration 1522/2000 [0m                     

                       Computation: 51323 steps/s (collection: 1.825s, learning 0.091s)
             Mean action noise std: 2.89
          Mean value_function loss: 153.3507
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.6047
                       Mean reward: 813.52
               Mean episode length: 220.07
    Episode_Reward/reaching_object: 1.2194
     Episode_Reward/lifting_object: 169.4746
      Episode_Reward/object_height: 0.0726
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 1.92s
                      Time elapsed: 00:54:06
                               ETA: 00:16:58

################################################################################
                     [1m Learning iteration 1523/2000 [0m                     

                       Computation: 51915 steps/s (collection: 1.800s, learning 0.094s)
             Mean action noise std: 2.89
          Mean value_function loss: 132.1644
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 53.6146
                       Mean reward: 842.78
               Mean episode length: 225.23
    Episode_Reward/reaching_object: 1.2055
     Episode_Reward/lifting_object: 167.5200
      Episode_Reward/object_height: 0.0716
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 1.89s
                      Time elapsed: 00:54:08
                               ETA: 00:16:56

################################################################################
                     [1m Learning iteration 1524/2000 [0m                     

                       Computation: 50569 steps/s (collection: 1.850s, learning 0.094s)
             Mean action noise std: 2.89
          Mean value_function loss: 115.3143
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 53.6171
                       Mean reward: 863.92
               Mean episode length: 230.36
    Episode_Reward/reaching_object: 1.2477
     Episode_Reward/lifting_object: 173.8929
      Episode_Reward/object_height: 0.0744
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 1.94s
                      Time elapsed: 00:54:10
                               ETA: 00:16:54

################################################################################
                     [1m Learning iteration 1525/2000 [0m                     

                       Computation: 49737 steps/s (collection: 1.867s, learning 0.109s)
             Mean action noise std: 2.89
          Mean value_function loss: 134.3719
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.6213
                       Mean reward: 880.86
               Mean episode length: 235.22
    Episode_Reward/reaching_object: 1.2393
     Episode_Reward/lifting_object: 172.6539
      Episode_Reward/object_height: 0.0734
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 1.98s
                      Time elapsed: 00:54:12
                               ETA: 00:16:52

################################################################################
                     [1m Learning iteration 1526/2000 [0m                     

                       Computation: 50803 steps/s (collection: 1.842s, learning 0.093s)
             Mean action noise std: 2.90
          Mean value_function loss: 103.6762
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 53.6281
                       Mean reward: 871.93
               Mean episode length: 232.47
    Episode_Reward/reaching_object: 1.2587
     Episode_Reward/lifting_object: 175.8445
      Episode_Reward/object_height: 0.0753
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 1.93s
                      Time elapsed: 00:54:14
                               ETA: 00:16:50

################################################################################
                     [1m Learning iteration 1527/2000 [0m                     

                       Computation: 49710 steps/s (collection: 1.862s, learning 0.115s)
             Mean action noise std: 2.90
          Mean value_function loss: 122.0191
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 53.6397
                       Mean reward: 908.62
               Mean episode length: 242.66
    Episode_Reward/reaching_object: 1.2183
     Episode_Reward/lifting_object: 169.7153
      Episode_Reward/object_height: 0.0725
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 1.98s
                      Time elapsed: 00:54:16
                               ETA: 00:16:48

################################################################################
                     [1m Learning iteration 1528/2000 [0m                     

                       Computation: 50292 steps/s (collection: 1.834s, learning 0.121s)
             Mean action noise std: 2.90
          Mean value_function loss: 119.4786
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 53.6520
                       Mean reward: 875.12
               Mean episode length: 232.87
    Episode_Reward/reaching_object: 1.2457
     Episode_Reward/lifting_object: 173.7354
      Episode_Reward/object_height: 0.0742
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 1.95s
                      Time elapsed: 00:54:18
                               ETA: 00:16:45

################################################################################
                     [1m Learning iteration 1529/2000 [0m                     

                       Computation: 50802 steps/s (collection: 1.835s, learning 0.100s)
             Mean action noise std: 2.90
          Mean value_function loss: 97.7857
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 53.6576
                       Mean reward: 891.77
               Mean episode length: 237.89
    Episode_Reward/reaching_object: 1.2593
     Episode_Reward/lifting_object: 175.9261
      Episode_Reward/object_height: 0.0753
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 1.94s
                      Time elapsed: 00:54:20
                               ETA: 00:16:43

################################################################################
                     [1m Learning iteration 1530/2000 [0m                     

                       Computation: 50563 steps/s (collection: 1.813s, learning 0.131s)
             Mean action noise std: 2.90
          Mean value_function loss: 123.3928
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 53.6611
                       Mean reward: 844.73
               Mean episode length: 225.67
    Episode_Reward/reaching_object: 1.2303
     Episode_Reward/lifting_object: 171.7989
      Episode_Reward/object_height: 0.0734
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 1.94s
                      Time elapsed: 00:54:22
                               ETA: 00:16:41

################################################################################
                     [1m Learning iteration 1531/2000 [0m                     

                       Computation: 51187 steps/s (collection: 1.805s, learning 0.115s)
             Mean action noise std: 2.90
          Mean value_function loss: 111.6457
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 53.6644
                       Mean reward: 827.29
               Mean episode length: 221.64
    Episode_Reward/reaching_object: 1.2272
     Episode_Reward/lifting_object: 170.9035
      Episode_Reward/object_height: 0.0733
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 1.92s
                      Time elapsed: 00:54:24
                               ETA: 00:16:39

################################################################################
                     [1m Learning iteration 1532/2000 [0m                     

                       Computation: 51370 steps/s (collection: 1.808s, learning 0.106s)
             Mean action noise std: 2.90
          Mean value_function loss: 87.6960
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 53.6711
                       Mean reward: 908.18
               Mean episode length: 241.81
    Episode_Reward/reaching_object: 1.2367
     Episode_Reward/lifting_object: 172.4379
      Episode_Reward/object_height: 0.0736
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 1.91s
                      Time elapsed: 00:54:26
                               ETA: 00:16:37

################################################################################
                     [1m Learning iteration 1533/2000 [0m                     

                       Computation: 51726 steps/s (collection: 1.814s, learning 0.086s)
             Mean action noise std: 2.90
          Mean value_function loss: 103.9732
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 53.6783
                       Mean reward: 874.45
               Mean episode length: 233.26
    Episode_Reward/reaching_object: 1.2593
     Episode_Reward/lifting_object: 176.0521
      Episode_Reward/object_height: 0.0749
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 1.90s
                      Time elapsed: 00:54:27
                               ETA: 00:16:34

################################################################################
                     [1m Learning iteration 1534/2000 [0m                     

                       Computation: 51078 steps/s (collection: 1.834s, learning 0.090s)
             Mean action noise std: 2.90
          Mean value_function loss: 119.9618
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 53.6821
                       Mean reward: 878.61
               Mean episode length: 234.06
    Episode_Reward/reaching_object: 1.2531
     Episode_Reward/lifting_object: 175.6439
      Episode_Reward/object_height: 0.0746
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 1.92s
                      Time elapsed: 00:54:29
                               ETA: 00:16:32

################################################################################
                     [1m Learning iteration 1535/2000 [0m                     

                       Computation: 51761 steps/s (collection: 1.809s, learning 0.090s)
             Mean action noise std: 2.90
          Mean value_function loss: 123.4948
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 53.6873
                       Mean reward: 828.63
               Mean episode length: 222.07
    Episode_Reward/reaching_object: 1.2280
     Episode_Reward/lifting_object: 171.7921
      Episode_Reward/object_height: 0.0728
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 1.90s
                      Time elapsed: 00:54:31
                               ETA: 00:16:30

################################################################################
                     [1m Learning iteration 1536/2000 [0m                     

                       Computation: 51807 steps/s (collection: 1.808s, learning 0.090s)
             Mean action noise std: 2.90
          Mean value_function loss: 121.0027
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.6929
                       Mean reward: 874.01
               Mean episode length: 231.92
    Episode_Reward/reaching_object: 1.2188
     Episode_Reward/lifting_object: 170.8405
      Episode_Reward/object_height: 0.0723
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 1.90s
                      Time elapsed: 00:54:33
                               ETA: 00:16:28

################################################################################
                     [1m Learning iteration 1537/2000 [0m                     

                       Computation: 52056 steps/s (collection: 1.802s, learning 0.086s)
             Mean action noise std: 2.91
          Mean value_function loss: 97.1000
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 53.7004
                       Mean reward: 862.48
               Mean episode length: 232.09
    Episode_Reward/reaching_object: 1.2393
     Episode_Reward/lifting_object: 173.5887
      Episode_Reward/object_height: 0.0737
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 1.89s
                      Time elapsed: 00:54:35
                               ETA: 00:16:26

################################################################################
                     [1m Learning iteration 1538/2000 [0m                     

                       Computation: 50891 steps/s (collection: 1.842s, learning 0.090s)
             Mean action noise std: 2.91
          Mean value_function loss: 145.2023
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 53.7077
                       Mean reward: 865.11
               Mean episode length: 230.79
    Episode_Reward/reaching_object: 1.2215
     Episode_Reward/lifting_object: 171.2743
      Episode_Reward/object_height: 0.0729
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 1.93s
                      Time elapsed: 00:54:37
                               ETA: 00:16:23

################################################################################
                     [1m Learning iteration 1539/2000 [0m                     

                       Computation: 51014 steps/s (collection: 1.822s, learning 0.105s)
             Mean action noise std: 2.91
          Mean value_function loss: 152.7298
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 53.7165
                       Mean reward: 825.69
               Mean episode length: 221.33
    Episode_Reward/reaching_object: 1.2067
     Episode_Reward/lifting_object: 169.6347
      Episode_Reward/object_height: 0.0723
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 1.93s
                      Time elapsed: 00:54:39
                               ETA: 00:16:21

################################################################################
                     [1m Learning iteration 1540/2000 [0m                     

                       Computation: 51751 steps/s (collection: 1.800s, learning 0.100s)
             Mean action noise std: 2.91
          Mean value_function loss: 153.2737
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 53.7251
                       Mean reward: 828.63
               Mean episode length: 222.92
    Episode_Reward/reaching_object: 1.2037
     Episode_Reward/lifting_object: 169.1662
      Episode_Reward/object_height: 0.0722
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 1.90s
                      Time elapsed: 00:54:41
                               ETA: 00:16:19

################################################################################
                     [1m Learning iteration 1541/2000 [0m                     

                       Computation: 51600 steps/s (collection: 1.816s, learning 0.089s)
             Mean action noise std: 2.91
          Mean value_function loss: 152.9400
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 53.7340
                       Mean reward: 879.10
               Mean episode length: 235.58
    Episode_Reward/reaching_object: 1.2303
     Episode_Reward/lifting_object: 172.3565
      Episode_Reward/object_height: 0.0742
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 1.91s
                      Time elapsed: 00:54:43
                               ETA: 00:16:17

################################################################################
                     [1m Learning iteration 1542/2000 [0m                     

                       Computation: 50880 steps/s (collection: 1.810s, learning 0.122s)
             Mean action noise std: 2.91
          Mean value_function loss: 129.8755
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 53.7390
                       Mean reward: 878.49
               Mean episode length: 233.17
    Episode_Reward/reaching_object: 1.2117
     Episode_Reward/lifting_object: 169.9324
      Episode_Reward/object_height: 0.0734
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 1.93s
                      Time elapsed: 00:54:45
                               ETA: 00:16:15

################################################################################
                     [1m Learning iteration 1543/2000 [0m                     

                       Computation: 50821 steps/s (collection: 1.817s, learning 0.118s)
             Mean action noise std: 2.91
          Mean value_function loss: 102.3083
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.7422
                       Mean reward: 874.56
               Mean episode length: 233.00
    Episode_Reward/reaching_object: 1.2296
     Episode_Reward/lifting_object: 172.4774
      Episode_Reward/object_height: 0.0746
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 1.93s
                      Time elapsed: 00:54:47
                               ETA: 00:16:12

################################################################################
                     [1m Learning iteration 1544/2000 [0m                     

                       Computation: 50614 steps/s (collection: 1.827s, learning 0.115s)
             Mean action noise std: 2.91
          Mean value_function loss: 100.3457
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 53.7517
                       Mean reward: 889.89
               Mean episode length: 237.45
    Episode_Reward/reaching_object: 1.2635
     Episode_Reward/lifting_object: 177.3278
      Episode_Reward/object_height: 0.0773
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 1.94s
                      Time elapsed: 00:54:49
                               ETA: 00:16:10

################################################################################
                     [1m Learning iteration 1545/2000 [0m                     

                       Computation: 51516 steps/s (collection: 1.823s, learning 0.085s)
             Mean action noise std: 2.91
          Mean value_function loss: 132.7623
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 53.7613
                       Mean reward: 883.09
               Mean episode length: 235.73
    Episode_Reward/reaching_object: 1.2574
     Episode_Reward/lifting_object: 176.6034
      Episode_Reward/object_height: 0.0765
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 1.91s
                      Time elapsed: 00:54:50
                               ETA: 00:16:08

################################################################################
                     [1m Learning iteration 1546/2000 [0m                     

                       Computation: 50845 steps/s (collection: 1.831s, learning 0.102s)
             Mean action noise std: 2.91
          Mean value_function loss: 183.1255
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 53.7677
                       Mean reward: 838.20
               Mean episode length: 224.42
    Episode_Reward/reaching_object: 1.1918
     Episode_Reward/lifting_object: 167.1759
      Episode_Reward/object_height: 0.0724
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 1.93s
                      Time elapsed: 00:54:52
                               ETA: 00:16:06

################################################################################
                     [1m Learning iteration 1547/2000 [0m                     

                       Computation: 51141 steps/s (collection: 1.825s, learning 0.097s)
             Mean action noise std: 2.92
          Mean value_function loss: 111.1209
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.7739
                       Mean reward: 875.52
               Mean episode length: 234.60
    Episode_Reward/reaching_object: 1.2394
     Episode_Reward/lifting_object: 173.5193
      Episode_Reward/object_height: 0.0755
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 1.92s
                      Time elapsed: 00:54:54
                               ETA: 00:16:04

################################################################################
                     [1m Learning iteration 1548/2000 [0m                     

                       Computation: 51312 steps/s (collection: 1.801s, learning 0.115s)
             Mean action noise std: 2.92
          Mean value_function loss: 138.4525
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 53.7829
                       Mean reward: 855.29
               Mean episode length: 228.96
    Episode_Reward/reaching_object: 1.2012
     Episode_Reward/lifting_object: 168.3185
      Episode_Reward/object_height: 0.0731
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 1.92s
                      Time elapsed: 00:54:56
                               ETA: 00:16:01

################################################################################
                     [1m Learning iteration 1549/2000 [0m                     

                       Computation: 50938 steps/s (collection: 1.829s, learning 0.101s)
             Mean action noise std: 2.92
          Mean value_function loss: 131.6918
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.7921
                       Mean reward: 878.82
               Mean episode length: 233.49
    Episode_Reward/reaching_object: 1.2403
     Episode_Reward/lifting_object: 174.1497
      Episode_Reward/object_height: 0.0759
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 1.93s
                      Time elapsed: 00:54:58
                               ETA: 00:15:59

################################################################################
                     [1m Learning iteration 1550/2000 [0m                     

                       Computation: 51275 steps/s (collection: 1.811s, learning 0.106s)
             Mean action noise std: 2.92
          Mean value_function loss: 142.3716
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 53.8051
                       Mean reward: 857.04
               Mean episode length: 228.96
    Episode_Reward/reaching_object: 1.2172
     Episode_Reward/lifting_object: 170.3792
      Episode_Reward/object_height: 0.0746
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 1.92s
                      Time elapsed: 00:55:00
                               ETA: 00:15:57

################################################################################
                     [1m Learning iteration 1551/2000 [0m                     

                       Computation: 50111 steps/s (collection: 1.870s, learning 0.092s)
             Mean action noise std: 2.92
          Mean value_function loss: 103.5919
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 53.8148
                       Mean reward: 855.14
               Mean episode length: 228.30
    Episode_Reward/reaching_object: 1.2224
     Episode_Reward/lifting_object: 170.8802
      Episode_Reward/object_height: 0.0752
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 1.96s
                      Time elapsed: 00:55:02
                               ETA: 00:15:55

################################################################################
                     [1m Learning iteration 1552/2000 [0m                     

                       Computation: 50856 steps/s (collection: 1.821s, learning 0.112s)
             Mean action noise std: 2.92
          Mean value_function loss: 116.9807
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 53.8224
                       Mean reward: 884.92
               Mean episode length: 235.78
    Episode_Reward/reaching_object: 1.2398
     Episode_Reward/lifting_object: 174.0801
      Episode_Reward/object_height: 0.0762
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 1.93s
                      Time elapsed: 00:55:04
                               ETA: 00:15:53

################################################################################
                     [1m Learning iteration 1553/2000 [0m                     

                       Computation: 51995 steps/s (collection: 1.799s, learning 0.092s)
             Mean action noise std: 2.92
          Mean value_function loss: 125.9656
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 53.8325
                       Mean reward: 865.41
               Mean episode length: 229.85
    Episode_Reward/reaching_object: 1.2430
     Episode_Reward/lifting_object: 175.0032
      Episode_Reward/object_height: 0.0762
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 1.89s
                      Time elapsed: 00:55:06
                               ETA: 00:15:51

################################################################################
                     [1m Learning iteration 1554/2000 [0m                     

                       Computation: 51710 steps/s (collection: 1.817s, learning 0.085s)
             Mean action noise std: 2.92
          Mean value_function loss: 103.8643
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 53.8404
                       Mean reward: 883.52
               Mean episode length: 235.12
    Episode_Reward/reaching_object: 1.2522
     Episode_Reward/lifting_object: 176.1618
      Episode_Reward/object_height: 0.0768
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 1.90s
                      Time elapsed: 00:55:08
                               ETA: 00:15:48

################################################################################
                     [1m Learning iteration 1555/2000 [0m                     

                       Computation: 51306 steps/s (collection: 1.814s, learning 0.102s)
             Mean action noise std: 2.92
          Mean value_function loss: 123.8936
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 53.8456
                       Mean reward: 877.26
               Mean episode length: 233.71
    Episode_Reward/reaching_object: 1.2446
     Episode_Reward/lifting_object: 174.6852
      Episode_Reward/object_height: 0.0760
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 1.92s
                      Time elapsed: 00:55:10
                               ETA: 00:15:46

################################################################################
                     [1m Learning iteration 1556/2000 [0m                     

                       Computation: 49957 steps/s (collection: 1.861s, learning 0.107s)
             Mean action noise std: 2.93
          Mean value_function loss: 94.9526
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.8506
                       Mean reward: 899.47
               Mean episode length: 238.82
    Episode_Reward/reaching_object: 1.2195
     Episode_Reward/lifting_object: 171.0025
      Episode_Reward/object_height: 0.0736
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 1.97s
                      Time elapsed: 00:55:12
                               ETA: 00:15:44

################################################################################
                     [1m Learning iteration 1557/2000 [0m                     

                       Computation: 50692 steps/s (collection: 1.853s, learning 0.087s)
             Mean action noise std: 2.93
          Mean value_function loss: 129.1721
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 53.8592
                       Mean reward: 862.60
               Mean episode length: 229.85
    Episode_Reward/reaching_object: 1.2125
     Episode_Reward/lifting_object: 170.3916
      Episode_Reward/object_height: 0.0726
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 1.94s
                      Time elapsed: 00:55:14
                               ETA: 00:15:42

################################################################################
                     [1m Learning iteration 1558/2000 [0m                     

                       Computation: 49710 steps/s (collection: 1.863s, learning 0.115s)
             Mean action noise std: 2.93
          Mean value_function loss: 121.2488
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 53.8679
                       Mean reward: 834.45
               Mean episode length: 223.22
    Episode_Reward/reaching_object: 1.2478
     Episode_Reward/lifting_object: 175.4625
      Episode_Reward/object_height: 0.0749
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 1.98s
                      Time elapsed: 00:55:16
                               ETA: 00:15:40

################################################################################
                     [1m Learning iteration 1559/2000 [0m                     

                       Computation: 50951 steps/s (collection: 1.838s, learning 0.091s)
             Mean action noise std: 2.93
          Mean value_function loss: 123.8994
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.8742
                       Mean reward: 898.95
               Mean episode length: 239.00
    Episode_Reward/reaching_object: 1.2430
     Episode_Reward/lifting_object: 174.6625
      Episode_Reward/object_height: 0.0743
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 1.93s
                      Time elapsed: 00:55:17
                               ETA: 00:15:37

################################################################################
                     [1m Learning iteration 1560/2000 [0m                     

                       Computation: 49459 steps/s (collection: 1.898s, learning 0.090s)
             Mean action noise std: 2.93
          Mean value_function loss: 154.0320
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 53.8823
                       Mean reward: 819.09
               Mean episode length: 220.08
    Episode_Reward/reaching_object: 1.2145
     Episode_Reward/lifting_object: 170.1972
      Episode_Reward/object_height: 0.0725
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 1.99s
                      Time elapsed: 00:55:19
                               ETA: 00:15:35

################################################################################
                     [1m Learning iteration 1561/2000 [0m                     

                       Computation: 50091 steps/s (collection: 1.859s, learning 0.104s)
             Mean action noise std: 2.93
          Mean value_function loss: 126.1854
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 53.8882
                       Mean reward: 889.60
               Mean episode length: 236.00
    Episode_Reward/reaching_object: 1.2258
     Episode_Reward/lifting_object: 172.0812
      Episode_Reward/object_height: 0.0729
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 1.96s
                      Time elapsed: 00:55:21
                               ETA: 00:15:33

################################################################################
                     [1m Learning iteration 1562/2000 [0m                     

                       Computation: 49937 steps/s (collection: 1.860s, learning 0.109s)
             Mean action noise std: 2.93
          Mean value_function loss: 137.9454
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.8966
                       Mean reward: 892.03
               Mean episode length: 236.93
    Episode_Reward/reaching_object: 1.2082
     Episode_Reward/lifting_object: 169.6409
      Episode_Reward/object_height: 0.0720
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 1.97s
                      Time elapsed: 00:55:23
                               ETA: 00:15:31

################################################################################
                     [1m Learning iteration 1563/2000 [0m                     

                       Computation: 51046 steps/s (collection: 1.826s, learning 0.100s)
             Mean action noise std: 2.93
          Mean value_function loss: 116.8514
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 53.9047
                       Mean reward: 900.70
               Mean episode length: 240.07
    Episode_Reward/reaching_object: 1.2035
     Episode_Reward/lifting_object: 168.5946
      Episode_Reward/object_height: 0.0717
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 1.93s
                      Time elapsed: 00:55:25
                               ETA: 00:15:29

################################################################################
                     [1m Learning iteration 1564/2000 [0m                     

                       Computation: 51015 steps/s (collection: 1.824s, learning 0.103s)
             Mean action noise std: 2.93
          Mean value_function loss: 158.6770
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 53.9126
                       Mean reward: 841.93
               Mean episode length: 225.18
    Episode_Reward/reaching_object: 1.2122
     Episode_Reward/lifting_object: 170.1434
      Episode_Reward/object_height: 0.0722
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 1.93s
                      Time elapsed: 00:55:27
                               ETA: 00:15:27

################################################################################
                     [1m Learning iteration 1565/2000 [0m                     

                       Computation: 50021 steps/s (collection: 1.868s, learning 0.098s)
             Mean action noise std: 2.94
          Mean value_function loss: 119.9557
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 53.9212
                       Mean reward: 860.60
               Mean episode length: 229.14
    Episode_Reward/reaching_object: 1.2263
     Episode_Reward/lifting_object: 172.3130
      Episode_Reward/object_height: 0.0735
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 1.97s
                      Time elapsed: 00:55:29
                               ETA: 00:15:24

################################################################################
                     [1m Learning iteration 1566/2000 [0m                     

                       Computation: 50789 steps/s (collection: 1.835s, learning 0.101s)
             Mean action noise std: 2.94
          Mean value_function loss: 138.3408
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 53.9308
                       Mean reward: 858.76
               Mean episode length: 229.62
    Episode_Reward/reaching_object: 1.2245
     Episode_Reward/lifting_object: 171.5921
      Episode_Reward/object_height: 0.0733
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 1.94s
                      Time elapsed: 00:55:31
                               ETA: 00:15:22

################################################################################
                     [1m Learning iteration 1567/2000 [0m                     

                       Computation: 50142 steps/s (collection: 1.824s, learning 0.136s)
             Mean action noise std: 2.94
          Mean value_function loss: 166.1167
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.9391
                       Mean reward: 877.16
               Mean episode length: 233.63
    Episode_Reward/reaching_object: 1.2232
     Episode_Reward/lifting_object: 172.1225
      Episode_Reward/object_height: 0.0736
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 1.96s
                      Time elapsed: 00:55:33
                               ETA: 00:15:20

################################################################################
                     [1m Learning iteration 1568/2000 [0m                     

                       Computation: 49374 steps/s (collection: 1.881s, learning 0.110s)
             Mean action noise std: 2.94
          Mean value_function loss: 137.5398
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 53.9506
                       Mean reward: 863.73
               Mean episode length: 230.45
    Episode_Reward/reaching_object: 1.2025
     Episode_Reward/lifting_object: 169.0700
      Episode_Reward/object_height: 0.0723
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 1.99s
                      Time elapsed: 00:55:35
                               ETA: 00:15:18

################################################################################
                     [1m Learning iteration 1569/2000 [0m                     

                       Computation: 49649 steps/s (collection: 1.832s, learning 0.148s)
             Mean action noise std: 2.94
          Mean value_function loss: 123.0586
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 53.9584
                       Mean reward: 897.89
               Mean episode length: 238.57
    Episode_Reward/reaching_object: 1.2267
     Episode_Reward/lifting_object: 172.6357
      Episode_Reward/object_height: 0.0740
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 1.98s
                      Time elapsed: 00:55:37
                               ETA: 00:15:16

################################################################################
                     [1m Learning iteration 1570/2000 [0m                     

                       Computation: 49029 steps/s (collection: 1.861s, learning 0.144s)
             Mean action noise std: 2.94
          Mean value_function loss: 170.2302
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 53.9643
                       Mean reward: 887.25
               Mean episode length: 236.57
    Episode_Reward/reaching_object: 1.2063
     Episode_Reward/lifting_object: 169.5252
      Episode_Reward/object_height: 0.0720
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 2.01s
                      Time elapsed: 00:55:39
                               ETA: 00:15:14

################################################################################
                     [1m Learning iteration 1571/2000 [0m                     

                       Computation: 51053 steps/s (collection: 1.830s, learning 0.096s)
             Mean action noise std: 2.94
          Mean value_function loss: 128.3700
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 53.9737
                       Mean reward: 878.33
               Mean episode length: 234.97
    Episode_Reward/reaching_object: 1.2284
     Episode_Reward/lifting_object: 172.8370
      Episode_Reward/object_height: 0.0735
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 1.93s
                      Time elapsed: 00:55:41
                               ETA: 00:15:11

################################################################################
                     [1m Learning iteration 1572/2000 [0m                     

                       Computation: 49949 steps/s (collection: 1.870s, learning 0.098s)
             Mean action noise std: 2.94
          Mean value_function loss: 149.1557
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.9799
                       Mean reward: 863.71
               Mean episode length: 231.07
    Episode_Reward/reaching_object: 1.2224
     Episode_Reward/lifting_object: 171.8348
      Episode_Reward/object_height: 0.0732
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 1.97s
                      Time elapsed: 00:55:43
                               ETA: 00:15:09

################################################################################
                     [1m Learning iteration 1573/2000 [0m                     

                       Computation: 49823 steps/s (collection: 1.855s, learning 0.118s)
             Mean action noise std: 2.94
          Mean value_function loss: 109.5548
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.9875
                       Mean reward: 905.69
               Mean episode length: 240.12
    Episode_Reward/reaching_object: 1.2581
     Episode_Reward/lifting_object: 176.9415
      Episode_Reward/object_height: 0.0752
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 1.97s
                      Time elapsed: 00:55:45
                               ETA: 00:15:07

################################################################################
                     [1m Learning iteration 1574/2000 [0m                     

                       Computation: 48353 steps/s (collection: 1.910s, learning 0.123s)
             Mean action noise std: 2.94
          Mean value_function loss: 103.1679
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 53.9958
                       Mean reward: 880.79
               Mean episode length: 233.81
    Episode_Reward/reaching_object: 1.2213
     Episode_Reward/lifting_object: 172.0181
      Episode_Reward/object_height: 0.0726
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 2.03s
                      Time elapsed: 00:55:47
                               ETA: 00:15:05

################################################################################
                     [1m Learning iteration 1575/2000 [0m                     

                       Computation: 49658 steps/s (collection: 1.891s, learning 0.089s)
             Mean action noise std: 2.95
          Mean value_function loss: 100.7620
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 54.0033
                       Mean reward: 881.69
               Mean episode length: 235.22
    Episode_Reward/reaching_object: 1.2400
     Episode_Reward/lifting_object: 174.4374
      Episode_Reward/object_height: 0.0734
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 1.98s
                      Time elapsed: 00:55:49
                               ETA: 00:15:03

################################################################################
                     [1m Learning iteration 1576/2000 [0m                     

                       Computation: 49265 steps/s (collection: 1.902s, learning 0.093s)
             Mean action noise std: 2.95
          Mean value_function loss: 75.9614
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.0103
                       Mean reward: 886.99
               Mean episode length: 236.41
    Episode_Reward/reaching_object: 1.2558
     Episode_Reward/lifting_object: 176.8784
      Episode_Reward/object_height: 0.0745
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 2.00s
                      Time elapsed: 00:55:51
                               ETA: 00:15:01

################################################################################
                     [1m Learning iteration 1577/2000 [0m                     

                       Computation: 49991 steps/s (collection: 1.860s, learning 0.107s)
             Mean action noise std: 2.95
          Mean value_function loss: 98.7841
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 54.0200
                       Mean reward: 895.49
               Mean episode length: 237.47
    Episode_Reward/reaching_object: 1.2330
     Episode_Reward/lifting_object: 173.8957
      Episode_Reward/object_height: 0.0722
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 1.97s
                      Time elapsed: 00:55:53
                               ETA: 00:14:58

################################################################################
                     [1m Learning iteration 1578/2000 [0m                     

                       Computation: 49975 steps/s (collection: 1.874s, learning 0.093s)
             Mean action noise std: 2.95
          Mean value_function loss: 113.5437
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.0264
                       Mean reward: 861.14
               Mean episode length: 228.93
    Episode_Reward/reaching_object: 1.2294
     Episode_Reward/lifting_object: 172.9060
      Episode_Reward/object_height: 0.0715
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 1.97s
                      Time elapsed: 00:55:55
                               ETA: 00:14:56

################################################################################
                     [1m Learning iteration 1579/2000 [0m                     

                       Computation: 48449 steps/s (collection: 1.929s, learning 0.100s)
             Mean action noise std: 2.95
          Mean value_function loss: 118.4445
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.0330
                       Mean reward: 870.94
               Mean episode length: 232.15
    Episode_Reward/reaching_object: 1.2347
     Episode_Reward/lifting_object: 173.8932
      Episode_Reward/object_height: 0.0715
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 2.03s
                      Time elapsed: 00:55:57
                               ETA: 00:14:54

################################################################################
                     [1m Learning iteration 1580/2000 [0m                     

                       Computation: 47870 steps/s (collection: 1.921s, learning 0.132s)
             Mean action noise std: 2.95
          Mean value_function loss: 111.3893
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 54.0409
                       Mean reward: 848.76
               Mean episode length: 227.73
    Episode_Reward/reaching_object: 1.2494
     Episode_Reward/lifting_object: 175.5067
      Episode_Reward/object_height: 0.0721
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 2.05s
                      Time elapsed: 00:55:59
                               ETA: 00:14:52

################################################################################
                     [1m Learning iteration 1581/2000 [0m                     

                       Computation: 49511 steps/s (collection: 1.893s, learning 0.093s)
             Mean action noise std: 2.95
          Mean value_function loss: 138.2671
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.0456
                       Mean reward: 881.76
               Mean episode length: 234.10
    Episode_Reward/reaching_object: 1.2450
     Episode_Reward/lifting_object: 175.2974
      Episode_Reward/object_height: 0.0712
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 155516928
                    Iteration time: 1.99s
                      Time elapsed: 00:56:01
                               ETA: 00:14:50

################################################################################
                     [1m Learning iteration 1582/2000 [0m                     

                       Computation: 50793 steps/s (collection: 1.845s, learning 0.091s)
             Mean action noise std: 2.95
          Mean value_function loss: 117.6969
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 54.0524
                       Mean reward: 867.01
               Mean episode length: 231.75
    Episode_Reward/reaching_object: 1.2419
     Episode_Reward/lifting_object: 174.7221
      Episode_Reward/object_height: 0.0714
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 155615232
                    Iteration time: 1.94s
                      Time elapsed: 00:56:03
                               ETA: 00:14:48

################################################################################
                     [1m Learning iteration 1583/2000 [0m                     

                       Computation: 48246 steps/s (collection: 1.903s, learning 0.135s)
             Mean action noise std: 2.95
          Mean value_function loss: 153.5770
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.0645
                       Mean reward: 841.14
               Mean episode length: 225.17
    Episode_Reward/reaching_object: 1.2158
     Episode_Reward/lifting_object: 170.2925
      Episode_Reward/object_height: 0.0694
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 155713536
                    Iteration time: 2.04s
                      Time elapsed: 00:56:05
                               ETA: 00:14:45

################################################################################
                     [1m Learning iteration 1584/2000 [0m                     

                       Computation: 49276 steps/s (collection: 1.879s, learning 0.116s)
             Mean action noise std: 2.96
          Mean value_function loss: 147.9224
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.0708
                       Mean reward: 858.92
               Mean episode length: 228.73
    Episode_Reward/reaching_object: 1.2200
     Episode_Reward/lifting_object: 171.6597
      Episode_Reward/object_height: 0.0695
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 155811840
                    Iteration time: 1.99s
                      Time elapsed: 00:56:07
                               ETA: 00:14:43

################################################################################
                     [1m Learning iteration 1585/2000 [0m                     

                       Computation: 49862 steps/s (collection: 1.882s, learning 0.089s)
             Mean action noise std: 2.96
          Mean value_function loss: 154.3553
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 54.0768
                       Mean reward: 917.96
               Mean episode length: 244.34
    Episode_Reward/reaching_object: 1.2476
     Episode_Reward/lifting_object: 175.1428
      Episode_Reward/object_height: 0.0715
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 155910144
                    Iteration time: 1.97s
                      Time elapsed: 00:56:09
                               ETA: 00:14:41

################################################################################
                     [1m Learning iteration 1586/2000 [0m                     

                       Computation: 49828 steps/s (collection: 1.868s, learning 0.105s)
             Mean action noise std: 2.96
          Mean value_function loss: 170.7890
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.0843
                       Mean reward: 881.31
               Mean episode length: 233.98
    Episode_Reward/reaching_object: 1.2412
     Episode_Reward/lifting_object: 174.5496
      Episode_Reward/object_height: 0.0712
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 156008448
                    Iteration time: 1.97s
                      Time elapsed: 00:56:11
                               ETA: 00:14:39

################################################################################
                     [1m Learning iteration 1587/2000 [0m                     

                       Computation: 49864 steps/s (collection: 1.877s, learning 0.094s)
             Mean action noise std: 2.96
          Mean value_function loss: 127.0685
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 54.0939
                       Mean reward: 848.73
               Mean episode length: 225.87
    Episode_Reward/reaching_object: 1.2237
     Episode_Reward/lifting_object: 172.0407
      Episode_Reward/object_height: 0.0708
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 156106752
                    Iteration time: 1.97s
                      Time elapsed: 00:56:13
                               ETA: 00:14:37

################################################################################
                     [1m Learning iteration 1588/2000 [0m                     

                       Computation: 49624 steps/s (collection: 1.890s, learning 0.091s)
             Mean action noise std: 2.96
          Mean value_function loss: 109.4572
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.1036
                       Mean reward: 878.55
               Mean episode length: 234.20
    Episode_Reward/reaching_object: 1.2276
     Episode_Reward/lifting_object: 172.5364
      Episode_Reward/object_height: 0.0708
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 156205056
                    Iteration time: 1.98s
                      Time elapsed: 00:56:15
                               ETA: 00:14:35

################################################################################
                     [1m Learning iteration 1589/2000 [0m                     

                       Computation: 48886 steps/s (collection: 1.915s, learning 0.095s)
             Mean action noise std: 2.96
          Mean value_function loss: 112.4855
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 54.1169
                       Mean reward: 913.34
               Mean episode length: 242.18
    Episode_Reward/reaching_object: 1.2524
     Episode_Reward/lifting_object: 176.5901
      Episode_Reward/object_height: 0.0729
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 156303360
                    Iteration time: 2.01s
                      Time elapsed: 00:56:17
                               ETA: 00:14:33

################################################################################
                     [1m Learning iteration 1590/2000 [0m                     

                       Computation: 49225 steps/s (collection: 1.905s, learning 0.092s)
             Mean action noise std: 2.96
          Mean value_function loss: 137.3303
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 54.1264
                       Mean reward: 887.61
               Mean episode length: 236.05
    Episode_Reward/reaching_object: 1.2345
     Episode_Reward/lifting_object: 173.7021
      Episode_Reward/object_height: 0.0719
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 156401664
                    Iteration time: 2.00s
                      Time elapsed: 00:56:19
                               ETA: 00:14:30

################################################################################
                     [1m Learning iteration 1591/2000 [0m                     

                       Computation: 49268 steps/s (collection: 1.906s, learning 0.090s)
             Mean action noise std: 2.96
          Mean value_function loss: 102.6611
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.1323
                       Mean reward: 873.39
               Mean episode length: 233.96
    Episode_Reward/reaching_object: 1.2551
     Episode_Reward/lifting_object: 176.6136
      Episode_Reward/object_height: 0.0732
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 156499968
                    Iteration time: 2.00s
                      Time elapsed: 00:56:21
                               ETA: 00:14:28

################################################################################
                     [1m Learning iteration 1592/2000 [0m                     

                       Computation: 48038 steps/s (collection: 1.948s, learning 0.099s)
             Mean action noise std: 2.97
          Mean value_function loss: 113.8401
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.1407
                       Mean reward: 856.03
               Mean episode length: 228.79
    Episode_Reward/reaching_object: 1.2270
     Episode_Reward/lifting_object: 172.6920
      Episode_Reward/object_height: 0.0712
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 156598272
                    Iteration time: 2.05s
                      Time elapsed: 00:56:23
                               ETA: 00:14:26

################################################################################
                     [1m Learning iteration 1593/2000 [0m                     

                       Computation: 48430 steps/s (collection: 1.904s, learning 0.126s)
             Mean action noise std: 2.97
          Mean value_function loss: 124.1481
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 54.1516
                       Mean reward: 907.69
               Mean episode length: 240.78
    Episode_Reward/reaching_object: 1.2199
     Episode_Reward/lifting_object: 171.7153
      Episode_Reward/object_height: 0.0702
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 156696576
                    Iteration time: 2.03s
                      Time elapsed: 00:56:25
                               ETA: 00:14:24

################################################################################
                     [1m Learning iteration 1594/2000 [0m                     

                       Computation: 49364 steps/s (collection: 1.902s, learning 0.090s)
             Mean action noise std: 2.97
          Mean value_function loss: 146.0633
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 54.1584
                       Mean reward: 846.69
               Mean episode length: 226.29
    Episode_Reward/reaching_object: 1.2089
     Episode_Reward/lifting_object: 170.1645
      Episode_Reward/object_height: 0.0695
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 156794880
                    Iteration time: 1.99s
                      Time elapsed: 00:56:27
                               ETA: 00:14:22

################################################################################
                     [1m Learning iteration 1595/2000 [0m                     

                       Computation: 48930 steps/s (collection: 1.901s, learning 0.108s)
             Mean action noise std: 2.97
          Mean value_function loss: 102.3859
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 54.1657
                       Mean reward: 858.56
               Mean episode length: 228.88
    Episode_Reward/reaching_object: 1.2218
     Episode_Reward/lifting_object: 171.9834
      Episode_Reward/object_height: 0.0700
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 156893184
                    Iteration time: 2.01s
                      Time elapsed: 00:56:29
                               ETA: 00:14:20

################################################################################
                     [1m Learning iteration 1596/2000 [0m                     

                       Computation: 48411 steps/s (collection: 1.927s, learning 0.104s)
             Mean action noise std: 2.97
          Mean value_function loss: 86.2260
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.1756
                       Mean reward: 883.66
               Mean episode length: 236.48
    Episode_Reward/reaching_object: 1.2705
     Episode_Reward/lifting_object: 178.6263
      Episode_Reward/object_height: 0.0731
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 156991488
                    Iteration time: 2.03s
                      Time elapsed: 00:56:31
                               ETA: 00:14:17

################################################################################
                     [1m Learning iteration 1597/2000 [0m                     

                       Computation: 49442 steps/s (collection: 1.897s, learning 0.091s)
             Mean action noise std: 2.97
          Mean value_function loss: 132.1979
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 54.1870
                       Mean reward: 884.53
               Mean episode length: 234.68
    Episode_Reward/reaching_object: 1.2237
     Episode_Reward/lifting_object: 172.4193
      Episode_Reward/object_height: 0.0704
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 157089792
                    Iteration time: 1.99s
                      Time elapsed: 00:56:33
                               ETA: 00:14:15

################################################################################
                     [1m Learning iteration 1598/2000 [0m                     

                       Computation: 48101 steps/s (collection: 1.930s, learning 0.114s)
             Mean action noise std: 2.97
          Mean value_function loss: 128.5051
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.1997
                       Mean reward: 897.95
               Mean episode length: 238.86
    Episode_Reward/reaching_object: 1.2267
     Episode_Reward/lifting_object: 172.3606
      Episode_Reward/object_height: 0.0701
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 157188096
                    Iteration time: 2.04s
                      Time elapsed: 00:56:35
                               ETA: 00:14:13

################################################################################
                     [1m Learning iteration 1599/2000 [0m                     

                       Computation: 48729 steps/s (collection: 1.910s, learning 0.107s)
             Mean action noise std: 2.97
          Mean value_function loss: 159.9575
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 54.2091
                       Mean reward: 850.36
               Mean episode length: 227.18
    Episode_Reward/reaching_object: 1.2443
     Episode_Reward/lifting_object: 175.0958
      Episode_Reward/object_height: 0.0717
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 157286400
                    Iteration time: 2.02s
                      Time elapsed: 00:56:37
                               ETA: 00:14:11

################################################################################
                     [1m Learning iteration 1600/2000 [0m                     

                       Computation: 48966 steps/s (collection: 1.910s, learning 0.098s)
             Mean action noise std: 2.98
          Mean value_function loss: 132.8358
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.2160
                       Mean reward: 867.30
               Mean episode length: 231.12
    Episode_Reward/reaching_object: 1.2108
     Episode_Reward/lifting_object: 170.3247
      Episode_Reward/object_height: 0.0696
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 157384704
                    Iteration time: 2.01s
                      Time elapsed: 00:56:39
                               ETA: 00:14:09

################################################################################
                     [1m Learning iteration 1601/2000 [0m                     

                       Computation: 48970 steps/s (collection: 1.903s, learning 0.104s)
             Mean action noise std: 2.98
          Mean value_function loss: 115.2158
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 54.2219
                       Mean reward: 864.95
               Mean episode length: 230.74
    Episode_Reward/reaching_object: 1.2428
     Episode_Reward/lifting_object: 175.3376
      Episode_Reward/object_height: 0.0716
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 157483008
                    Iteration time: 2.01s
                      Time elapsed: 00:56:41
                               ETA: 00:14:07

################################################################################
                     [1m Learning iteration 1602/2000 [0m                     

                       Computation: 46779 steps/s (collection: 1.994s, learning 0.108s)
             Mean action noise std: 2.98
          Mean value_function loss: 119.9412
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.2259
                       Mean reward: 866.27
               Mean episode length: 231.69
    Episode_Reward/reaching_object: 1.2294
     Episode_Reward/lifting_object: 173.1246
      Episode_Reward/object_height: 0.0710
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 157581312
                    Iteration time: 2.10s
                      Time elapsed: 00:56:43
                               ETA: 00:14:05

################################################################################
                     [1m Learning iteration 1603/2000 [0m                     

                       Computation: 48665 steps/s (collection: 1.926s, learning 0.094s)
             Mean action noise std: 2.98
          Mean value_function loss: 146.0993
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.2341
                       Mean reward: 895.89
               Mean episode length: 237.98
    Episode_Reward/reaching_object: 1.2313
     Episode_Reward/lifting_object: 173.9783
      Episode_Reward/object_height: 0.0716
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 157679616
                    Iteration time: 2.02s
                      Time elapsed: 00:56:45
                               ETA: 00:14:02

################################################################################
                     [1m Learning iteration 1604/2000 [0m                     

                       Computation: 49177 steps/s (collection: 1.890s, learning 0.109s)
             Mean action noise std: 2.98
          Mean value_function loss: 130.1012
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 54.2444
                       Mean reward: 845.70
               Mean episode length: 227.15
    Episode_Reward/reaching_object: 1.2467
     Episode_Reward/lifting_object: 175.6245
      Episode_Reward/object_height: 0.0728
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 157777920
                    Iteration time: 2.00s
                      Time elapsed: 00:56:47
                               ETA: 00:14:00

################################################################################
                     [1m Learning iteration 1605/2000 [0m                     

                       Computation: 48318 steps/s (collection: 1.939s, learning 0.096s)
             Mean action noise std: 2.98
          Mean value_function loss: 115.4955
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 54.2539
                       Mean reward: 861.20
               Mean episode length: 229.14
    Episode_Reward/reaching_object: 1.2336
     Episode_Reward/lifting_object: 173.5367
      Episode_Reward/object_height: 0.0718
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 157876224
                    Iteration time: 2.03s
                      Time elapsed: 00:56:49
                               ETA: 00:13:58

################################################################################
                     [1m Learning iteration 1606/2000 [0m                     

                       Computation: 49079 steps/s (collection: 1.904s, learning 0.099s)
             Mean action noise std: 2.98
          Mean value_function loss: 103.0000
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 54.2626
                       Mean reward: 876.76
               Mean episode length: 233.35
    Episode_Reward/reaching_object: 1.2535
     Episode_Reward/lifting_object: 176.6750
      Episode_Reward/object_height: 0.0726
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 157974528
                    Iteration time: 2.00s
                      Time elapsed: 00:56:51
                               ETA: 00:13:56

################################################################################
                     [1m Learning iteration 1607/2000 [0m                     

                       Computation: 48171 steps/s (collection: 1.916s, learning 0.125s)
             Mean action noise std: 2.98
          Mean value_function loss: 149.6298
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 54.2724
                       Mean reward: 900.08
               Mean episode length: 238.72
    Episode_Reward/reaching_object: 1.2194
     Episode_Reward/lifting_object: 171.7183
      Episode_Reward/object_height: 0.0700
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 158072832
                    Iteration time: 2.04s
                      Time elapsed: 00:56:53
                               ETA: 00:13:54

################################################################################
                     [1m Learning iteration 1608/2000 [0m                     

                       Computation: 50125 steps/s (collection: 1.866s, learning 0.096s)
             Mean action noise std: 2.98
          Mean value_function loss: 104.9535
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 54.2817
                       Mean reward: 847.01
               Mean episode length: 225.92
    Episode_Reward/reaching_object: 1.2043
     Episode_Reward/lifting_object: 168.6228
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 158171136
                    Iteration time: 1.96s
                      Time elapsed: 00:56:55
                               ETA: 00:13:52

################################################################################
                     [1m Learning iteration 1609/2000 [0m                     

                       Computation: 48403 steps/s (collection: 1.924s, learning 0.107s)
             Mean action noise std: 2.99
          Mean value_function loss: 109.6700
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 54.2904
                       Mean reward: 897.08
               Mean episode length: 238.18
    Episode_Reward/reaching_object: 1.2358
     Episode_Reward/lifting_object: 173.7639
      Episode_Reward/object_height: 0.0708
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 158269440
                    Iteration time: 2.03s
                      Time elapsed: 00:56:57
                               ETA: 00:13:50

################################################################################
                     [1m Learning iteration 1610/2000 [0m                     

                       Computation: 48455 steps/s (collection: 1.926s, learning 0.103s)
             Mean action noise std: 2.99
          Mean value_function loss: 139.4026
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 54.2996
                       Mean reward: 883.96
               Mean episode length: 235.62
    Episode_Reward/reaching_object: 1.2365
     Episode_Reward/lifting_object: 173.8404
      Episode_Reward/object_height: 0.0711
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 158367744
                    Iteration time: 2.03s
                      Time elapsed: 00:56:59
                               ETA: 00:13:47

################################################################################
                     [1m Learning iteration 1611/2000 [0m                     

                       Computation: 49189 steps/s (collection: 1.903s, learning 0.095s)
             Mean action noise std: 2.99
          Mean value_function loss: 118.6724
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 54.3053
                       Mean reward: 889.52
               Mean episode length: 236.44
    Episode_Reward/reaching_object: 1.2459
     Episode_Reward/lifting_object: 175.4172
      Episode_Reward/object_height: 0.0711
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 158466048
                    Iteration time: 2.00s
                      Time elapsed: 00:57:01
                               ETA: 00:13:45

################################################################################
                     [1m Learning iteration 1612/2000 [0m                     

                       Computation: 49386 steps/s (collection: 1.880s, learning 0.110s)
             Mean action noise std: 2.99
          Mean value_function loss: 112.4975
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 54.3092
                       Mean reward: 846.82
               Mean episode length: 225.39
    Episode_Reward/reaching_object: 1.2408
     Episode_Reward/lifting_object: 174.2603
      Episode_Reward/object_height: 0.0712
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 158564352
                    Iteration time: 1.99s
                      Time elapsed: 00:57:03
                               ETA: 00:13:43

################################################################################
                     [1m Learning iteration 1613/2000 [0m                     

                       Computation: 48516 steps/s (collection: 1.927s, learning 0.099s)
             Mean action noise std: 2.99
          Mean value_function loss: 119.2298
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 54.3162
                       Mean reward: 907.17
               Mean episode length: 241.51
    Episode_Reward/reaching_object: 1.2415
     Episode_Reward/lifting_object: 174.9323
      Episode_Reward/object_height: 0.0708
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 158662656
                    Iteration time: 2.03s
                      Time elapsed: 00:57:05
                               ETA: 00:13:41

################################################################################
                     [1m Learning iteration 1614/2000 [0m                     

                       Computation: 49241 steps/s (collection: 1.901s, learning 0.096s)
             Mean action noise std: 2.99
          Mean value_function loss: 110.7510
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 54.3230
                       Mean reward: 895.99
               Mean episode length: 238.51
    Episode_Reward/reaching_object: 1.2453
     Episode_Reward/lifting_object: 175.5167
      Episode_Reward/object_height: 0.0716
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 158760960
                    Iteration time: 2.00s
                      Time elapsed: 00:57:07
                               ETA: 00:13:39

################################################################################
                     [1m Learning iteration 1615/2000 [0m                     

                       Computation: 48829 steps/s (collection: 1.901s, learning 0.112s)
             Mean action noise std: 2.99
          Mean value_function loss: 135.1616
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 54.3306
                       Mean reward: 876.23
               Mean episode length: 233.67
    Episode_Reward/reaching_object: 1.2399
     Episode_Reward/lifting_object: 174.8099
      Episode_Reward/object_height: 0.0707
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 158859264
                    Iteration time: 2.01s
                      Time elapsed: 00:57:09
                               ETA: 00:13:37

################################################################################
                     [1m Learning iteration 1616/2000 [0m                     

                       Computation: 48418 steps/s (collection: 1.917s, learning 0.114s)
             Mean action noise std: 2.99
          Mean value_function loss: 96.3498
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 54.3367
                       Mean reward: 871.30
               Mean episode length: 232.51
    Episode_Reward/reaching_object: 1.2290
     Episode_Reward/lifting_object: 173.6601
      Episode_Reward/object_height: 0.0702
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 158957568
                    Iteration time: 2.03s
                      Time elapsed: 00:57:11
                               ETA: 00:13:34

################################################################################
                     [1m Learning iteration 1617/2000 [0m                     

                       Computation: 49684 steps/s (collection: 1.868s, learning 0.111s)
             Mean action noise std: 2.99
          Mean value_function loss: 114.3871
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 54.3424
                       Mean reward: 887.83
               Mean episode length: 236.45
    Episode_Reward/reaching_object: 1.2591
     Episode_Reward/lifting_object: 178.1299
      Episode_Reward/object_height: 0.0716
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 159055872
                    Iteration time: 1.98s
                      Time elapsed: 00:57:13
                               ETA: 00:13:32

################################################################################
                     [1m Learning iteration 1618/2000 [0m                     

                       Computation: 47243 steps/s (collection: 1.952s, learning 0.129s)
             Mean action noise std: 2.99
          Mean value_function loss: 116.8010
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 54.3482
                       Mean reward: 848.48
               Mean episode length: 227.18
    Episode_Reward/reaching_object: 1.2319
     Episode_Reward/lifting_object: 173.6581
      Episode_Reward/object_height: 0.0701
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 159154176
                    Iteration time: 2.08s
                      Time elapsed: 00:57:15
                               ETA: 00:13:30

################################################################################
                     [1m Learning iteration 1619/2000 [0m                     

                       Computation: 46760 steps/s (collection: 1.944s, learning 0.158s)
             Mean action noise std: 2.99
          Mean value_function loss: 126.5037
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 54.3558
                       Mean reward: 886.81
               Mean episode length: 236.33
    Episode_Reward/reaching_object: 1.2394
     Episode_Reward/lifting_object: 174.7153
      Episode_Reward/object_height: 0.0702
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 159252480
                    Iteration time: 2.10s
                      Time elapsed: 00:57:17
                               ETA: 00:13:28

################################################################################
                     [1m Learning iteration 1620/2000 [0m                     

                       Computation: 45595 steps/s (collection: 2.012s, learning 0.144s)
             Mean action noise std: 3.00
          Mean value_function loss: 137.0863
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 54.3641
                       Mean reward: 869.34
               Mean episode length: 231.85
    Episode_Reward/reaching_object: 1.2168
     Episode_Reward/lifting_object: 170.8041
      Episode_Reward/object_height: 0.0689
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 159350784
                    Iteration time: 2.16s
                      Time elapsed: 00:57:20
                               ETA: 00:13:26

################################################################################
                     [1m Learning iteration 1621/2000 [0m                     

                       Computation: 47941 steps/s (collection: 1.888s, learning 0.163s)
             Mean action noise std: 3.00
          Mean value_function loss: 123.3726
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 54.3703
                       Mean reward: 916.19
               Mean episode length: 243.39
    Episode_Reward/reaching_object: 1.2431
     Episode_Reward/lifting_object: 174.4039
      Episode_Reward/object_height: 0.0708
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 159449088
                    Iteration time: 2.05s
                      Time elapsed: 00:57:22
                               ETA: 00:13:24

################################################################################
                     [1m Learning iteration 1622/2000 [0m                     

                       Computation: 48749 steps/s (collection: 1.887s, learning 0.130s)
             Mean action noise std: 3.00
          Mean value_function loss: 143.2044
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 54.3769
                       Mean reward: 877.14
               Mean episode length: 233.88
    Episode_Reward/reaching_object: 1.2433
     Episode_Reward/lifting_object: 175.0002
      Episode_Reward/object_height: 0.0708
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 159547392
                    Iteration time: 2.02s
                      Time elapsed: 00:57:24
                               ETA: 00:13:22

################################################################################
                     [1m Learning iteration 1623/2000 [0m                     

                       Computation: 48829 steps/s (collection: 1.896s, learning 0.117s)
             Mean action noise std: 3.00
          Mean value_function loss: 75.7880
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 54.3906
                       Mean reward: 873.26
               Mean episode length: 232.56
    Episode_Reward/reaching_object: 1.2367
     Episode_Reward/lifting_object: 173.7262
      Episode_Reward/object_height: 0.0704
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 159645696
                    Iteration time: 2.01s
                      Time elapsed: 00:57:26
                               ETA: 00:13:20

################################################################################
                     [1m Learning iteration 1624/2000 [0m                     

                       Computation: 48873 steps/s (collection: 1.869s, learning 0.143s)
             Mean action noise std: 3.00
          Mean value_function loss: 100.8453
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 54.4038
                       Mean reward: 883.89
               Mean episode length: 235.74
    Episode_Reward/reaching_object: 1.2467
     Episode_Reward/lifting_object: 175.2116
      Episode_Reward/object_height: 0.0709
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 159744000
                    Iteration time: 2.01s
                      Time elapsed: 00:57:28
                               ETA: 00:13:17

################################################################################
                     [1m Learning iteration 1625/2000 [0m                     

                       Computation: 49425 steps/s (collection: 1.879s, learning 0.110s)
             Mean action noise std: 3.00
          Mean value_function loss: 126.5819
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 54.4096
                       Mean reward: 868.60
               Mean episode length: 232.04
    Episode_Reward/reaching_object: 1.2494
     Episode_Reward/lifting_object: 175.6716
      Episode_Reward/object_height: 0.0713
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 159842304
                    Iteration time: 1.99s
                      Time elapsed: 00:57:30
                               ETA: 00:13:15

################################################################################
                     [1m Learning iteration 1626/2000 [0m                     

                       Computation: 48989 steps/s (collection: 1.908s, learning 0.099s)
             Mean action noise std: 3.00
          Mean value_function loss: 93.4096
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.4156
                       Mean reward: 900.12
               Mean episode length: 239.41
    Episode_Reward/reaching_object: 1.2630
     Episode_Reward/lifting_object: 178.0753
      Episode_Reward/object_height: 0.0719
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 159940608
                    Iteration time: 2.01s
                      Time elapsed: 00:57:32
                               ETA: 00:13:13

################################################################################
                     [1m Learning iteration 1627/2000 [0m                     

                       Computation: 49156 steps/s (collection: 1.898s, learning 0.102s)
             Mean action noise std: 3.00
          Mean value_function loss: 101.1456
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 54.4207
                       Mean reward: 821.13
               Mean episode length: 219.98
    Episode_Reward/reaching_object: 1.2263
     Episode_Reward/lifting_object: 172.3215
      Episode_Reward/object_height: 0.0697
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 160038912
                    Iteration time: 2.00s
                      Time elapsed: 00:57:34
                               ETA: 00:13:11

################################################################################
                     [1m Learning iteration 1628/2000 [0m                     

                       Computation: 48661 steps/s (collection: 1.901s, learning 0.119s)
             Mean action noise std: 3.00
          Mean value_function loss: 132.7471
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 54.4255
                       Mean reward: 865.06
               Mean episode length: 230.60
    Episode_Reward/reaching_object: 1.2314
     Episode_Reward/lifting_object: 173.3517
      Episode_Reward/object_height: 0.0700
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 160137216
                    Iteration time: 2.02s
                      Time elapsed: 00:57:36
                               ETA: 00:13:09

################################################################################
                     [1m Learning iteration 1629/2000 [0m                     

                       Computation: 48557 steps/s (collection: 1.909s, learning 0.115s)
             Mean action noise std: 3.01
          Mean value_function loss: 112.5856
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.4334
                       Mean reward: 876.94
               Mean episode length: 235.35
    Episode_Reward/reaching_object: 1.2745
     Episode_Reward/lifting_object: 179.3133
      Episode_Reward/object_height: 0.0734
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 160235520
                    Iteration time: 2.02s
                      Time elapsed: 00:57:38
                               ETA: 00:13:07

################################################################################
                     [1m Learning iteration 1630/2000 [0m                     

                       Computation: 47326 steps/s (collection: 1.972s, learning 0.105s)
             Mean action noise std: 3.01
          Mean value_function loss: 130.5249
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 54.4418
                       Mean reward: 856.92
               Mean episode length: 229.81
    Episode_Reward/reaching_object: 1.2390
     Episode_Reward/lifting_object: 174.3398
      Episode_Reward/object_height: 0.0710
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 160333824
                    Iteration time: 2.08s
                      Time elapsed: 00:57:40
                               ETA: 00:13:04

################################################################################
                     [1m Learning iteration 1631/2000 [0m                     

                       Computation: 46254 steps/s (collection: 1.985s, learning 0.140s)
             Mean action noise std: 3.01
          Mean value_function loss: 127.8706
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 54.4483
                       Mean reward: 858.48
               Mean episode length: 228.95
    Episode_Reward/reaching_object: 1.2288
     Episode_Reward/lifting_object: 172.7308
      Episode_Reward/object_height: 0.0703
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 160432128
                    Iteration time: 2.13s
                      Time elapsed: 00:57:42
                               ETA: 00:13:02

################################################################################
                     [1m Learning iteration 1632/2000 [0m                     

                       Computation: 46972 steps/s (collection: 1.985s, learning 0.108s)
             Mean action noise std: 3.01
          Mean value_function loss: 119.6334
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 54.4549
                       Mean reward: 846.51
               Mean episode length: 226.09
    Episode_Reward/reaching_object: 1.2479
     Episode_Reward/lifting_object: 175.6277
      Episode_Reward/object_height: 0.0714
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 160530432
                    Iteration time: 2.09s
                      Time elapsed: 00:57:44
                               ETA: 00:13:00

################################################################################
                     [1m Learning iteration 1633/2000 [0m                     

                       Computation: 49711 steps/s (collection: 1.881s, learning 0.097s)
             Mean action noise std: 3.01
          Mean value_function loss: 128.0454
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 54.4596
                       Mean reward: 856.28
               Mean episode length: 229.74
    Episode_Reward/reaching_object: 1.2291
     Episode_Reward/lifting_object: 172.8365
      Episode_Reward/object_height: 0.0698
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 160628736
                    Iteration time: 1.98s
                      Time elapsed: 00:57:46
                               ETA: 00:12:58

################################################################################
                     [1m Learning iteration 1634/2000 [0m                     

                       Computation: 48119 steps/s (collection: 1.918s, learning 0.125s)
             Mean action noise std: 3.01
          Mean value_function loss: 124.7743
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 54.4687
                       Mean reward: 860.00
               Mean episode length: 229.83
    Episode_Reward/reaching_object: 1.2281
     Episode_Reward/lifting_object: 172.7312
      Episode_Reward/object_height: 0.0694
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 160727040
                    Iteration time: 2.04s
                      Time elapsed: 00:57:48
                               ETA: 00:12:56

################################################################################
                     [1m Learning iteration 1635/2000 [0m                     

                       Computation: 45202 steps/s (collection: 2.077s, learning 0.098s)
             Mean action noise std: 3.01
          Mean value_function loss: 150.0485
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 54.4797
                       Mean reward: 847.76
               Mean episode length: 226.67
    Episode_Reward/reaching_object: 1.2316
     Episode_Reward/lifting_object: 173.0449
      Episode_Reward/object_height: 0.0694
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 160825344
                    Iteration time: 2.17s
                      Time elapsed: 00:57:50
                               ETA: 00:12:54

################################################################################
                     [1m Learning iteration 1636/2000 [0m                     

                       Computation: 47878 steps/s (collection: 1.948s, learning 0.105s)
             Mean action noise std: 3.01
          Mean value_function loss: 100.0430
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 54.4886
                       Mean reward: 904.50
               Mean episode length: 239.67
    Episode_Reward/reaching_object: 1.2496
     Episode_Reward/lifting_object: 175.6484
      Episode_Reward/object_height: 0.0707
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 160923648
                    Iteration time: 2.05s
                      Time elapsed: 00:57:52
                               ETA: 00:12:52

################################################################################
                     [1m Learning iteration 1637/2000 [0m                     

                       Computation: 48111 steps/s (collection: 1.945s, learning 0.098s)
             Mean action noise std: 3.01
          Mean value_function loss: 90.7649
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 54.4947
                       Mean reward: 836.62
               Mean episode length: 224.10
    Episode_Reward/reaching_object: 1.2346
     Episode_Reward/lifting_object: 173.2370
      Episode_Reward/object_height: 0.0703
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 161021952
                    Iteration time: 2.04s
                      Time elapsed: 00:57:54
                               ETA: 00:12:50

################################################################################
                     [1m Learning iteration 1638/2000 [0m                     

                       Computation: 47583 steps/s (collection: 1.931s, learning 0.135s)
             Mean action noise std: 3.02
          Mean value_function loss: 117.6310
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.5023
                       Mean reward: 888.17
               Mean episode length: 237.72
    Episode_Reward/reaching_object: 1.2462
     Episode_Reward/lifting_object: 174.4803
      Episode_Reward/object_height: 0.0710
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 161120256
                    Iteration time: 2.07s
                      Time elapsed: 00:57:56
                               ETA: 00:12:47

################################################################################
                     [1m Learning iteration 1639/2000 [0m                     

                       Computation: 48473 steps/s (collection: 1.902s, learning 0.126s)
             Mean action noise std: 3.02
          Mean value_function loss: 123.4954
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 54.5115
                       Mean reward: 871.20
               Mean episode length: 232.60
    Episode_Reward/reaching_object: 1.2476
     Episode_Reward/lifting_object: 175.5187
      Episode_Reward/object_height: 0.0706
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 161218560
                    Iteration time: 2.03s
                      Time elapsed: 00:57:58
                               ETA: 00:12:45

################################################################################
                     [1m Learning iteration 1640/2000 [0m                     

                       Computation: 48443 steps/s (collection: 1.918s, learning 0.112s)
             Mean action noise std: 3.02
          Mean value_function loss: 105.9083
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 54.5216
                       Mean reward: 880.30
               Mean episode length: 235.81
    Episode_Reward/reaching_object: 1.2370
     Episode_Reward/lifting_object: 173.2867
      Episode_Reward/object_height: 0.0701
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 161316864
                    Iteration time: 2.03s
                      Time elapsed: 00:58:00
                               ETA: 00:12:43

################################################################################
                     [1m Learning iteration 1641/2000 [0m                     

                       Computation: 48506 steps/s (collection: 1.909s, learning 0.118s)
             Mean action noise std: 3.02
          Mean value_function loss: 91.7456
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 54.5298
                       Mean reward: 879.82
               Mean episode length: 235.43
    Episode_Reward/reaching_object: 1.2748
     Episode_Reward/lifting_object: 179.2306
      Episode_Reward/object_height: 0.0732
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 161415168
                    Iteration time: 2.03s
                      Time elapsed: 00:58:02
                               ETA: 00:12:41

################################################################################
                     [1m Learning iteration 1642/2000 [0m                     

                       Computation: 48533 steps/s (collection: 1.913s, learning 0.112s)
             Mean action noise std: 3.02
          Mean value_function loss: 116.8587
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 54.5405
                       Mean reward: 855.88
               Mean episode length: 229.15
    Episode_Reward/reaching_object: 1.2572
     Episode_Reward/lifting_object: 176.8497
      Episode_Reward/object_height: 0.0722
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 161513472
                    Iteration time: 2.03s
                      Time elapsed: 00:58:04
                               ETA: 00:12:39

################################################################################
                     [1m Learning iteration 1643/2000 [0m                     

                       Computation: 47719 steps/s (collection: 1.957s, learning 0.103s)
             Mean action noise std: 3.02
          Mean value_function loss: 130.8427
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 54.5506
                       Mean reward: 874.78
               Mean episode length: 233.27
    Episode_Reward/reaching_object: 1.2245
     Episode_Reward/lifting_object: 172.3167
      Episode_Reward/object_height: 0.0702
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 161611776
                    Iteration time: 2.06s
                      Time elapsed: 00:58:07
                               ETA: 00:12:37

################################################################################
                     [1m Learning iteration 1644/2000 [0m                     

                       Computation: 48137 steps/s (collection: 1.916s, learning 0.126s)
             Mean action noise std: 3.02
          Mean value_function loss: 106.1768
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 54.5609
                       Mean reward: 877.32
               Mean episode length: 233.41
    Episode_Reward/reaching_object: 1.2233
     Episode_Reward/lifting_object: 172.0041
      Episode_Reward/object_height: 0.0705
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 161710080
                    Iteration time: 2.04s
                      Time elapsed: 00:58:09
                               ETA: 00:12:35

################################################################################
                     [1m Learning iteration 1645/2000 [0m                     

                       Computation: 48235 steps/s (collection: 1.940s, learning 0.098s)
             Mean action noise std: 3.02
          Mean value_function loss: 136.0180
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 54.5734
                       Mean reward: 862.99
               Mean episode length: 230.56
    Episode_Reward/reaching_object: 1.2427
     Episode_Reward/lifting_object: 174.3895
      Episode_Reward/object_height: 0.0716
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 161808384
                    Iteration time: 2.04s
                      Time elapsed: 00:58:11
                               ETA: 00:12:32

################################################################################
                     [1m Learning iteration 1646/2000 [0m                     

                       Computation: 48034 steps/s (collection: 1.935s, learning 0.112s)
             Mean action noise std: 3.03
          Mean value_function loss: 128.0365
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.5820
                       Mean reward: 839.51
               Mean episode length: 224.89
    Episode_Reward/reaching_object: 1.2464
     Episode_Reward/lifting_object: 175.4824
      Episode_Reward/object_height: 0.0723
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 161906688
                    Iteration time: 2.05s
                      Time elapsed: 00:58:13
                               ETA: 00:12:30

################################################################################
                     [1m Learning iteration 1647/2000 [0m                     

                       Computation: 46803 steps/s (collection: 1.924s, learning 0.177s)
             Mean action noise std: 3.03
          Mean value_function loss: 149.2408
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 54.5928
                       Mean reward: 863.19
               Mean episode length: 230.68
    Episode_Reward/reaching_object: 1.2231
     Episode_Reward/lifting_object: 171.5818
      Episode_Reward/object_height: 0.0708
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 162004992
                    Iteration time: 2.10s
                      Time elapsed: 00:58:15
                               ETA: 00:12:28

################################################################################
                     [1m Learning iteration 1648/2000 [0m                     

                       Computation: 47647 steps/s (collection: 1.929s, learning 0.135s)
             Mean action noise std: 3.03
          Mean value_function loss: 107.1114
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.6015
                       Mean reward: 916.43
               Mean episode length: 243.11
    Episode_Reward/reaching_object: 1.2348
     Episode_Reward/lifting_object: 173.9183
      Episode_Reward/object_height: 0.0719
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 162103296
                    Iteration time: 2.06s
                      Time elapsed: 00:58:17
                               ETA: 00:12:26

################################################################################
                     [1m Learning iteration 1649/2000 [0m                     

                       Computation: 46609 steps/s (collection: 1.997s, learning 0.113s)
             Mean action noise std: 3.03
          Mean value_function loss: 121.9353
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 54.6092
                       Mean reward: 891.53
               Mean episode length: 236.17
    Episode_Reward/reaching_object: 1.2174
     Episode_Reward/lifting_object: 171.4665
      Episode_Reward/object_height: 0.0711
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 162201600
                    Iteration time: 2.11s
                      Time elapsed: 00:58:19
                               ETA: 00:12:24

################################################################################
                     [1m Learning iteration 1650/2000 [0m                     

                       Computation: 49105 steps/s (collection: 1.907s, learning 0.095s)
             Mean action noise std: 3.03
          Mean value_function loss: 89.8285
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 54.6178
                       Mean reward: 886.49
               Mean episode length: 235.63
    Episode_Reward/reaching_object: 1.2528
     Episode_Reward/lifting_object: 176.4417
      Episode_Reward/object_height: 0.0734
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 162299904
                    Iteration time: 2.00s
                      Time elapsed: 00:58:21
                               ETA: 00:12:22

################################################################################
                     [1m Learning iteration 1651/2000 [0m                     

                       Computation: 48211 steps/s (collection: 1.918s, learning 0.121s)
             Mean action noise std: 3.03
          Mean value_function loss: 126.7874
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.6217
                       Mean reward: 850.85
               Mean episode length: 226.77
    Episode_Reward/reaching_object: 1.2595
     Episode_Reward/lifting_object: 177.6237
      Episode_Reward/object_height: 0.0740
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 162398208
                    Iteration time: 2.04s
                      Time elapsed: 00:58:23
                               ETA: 00:12:20

################################################################################
                     [1m Learning iteration 1652/2000 [0m                     

                       Computation: 49119 steps/s (collection: 1.879s, learning 0.122s)
             Mean action noise std: 3.03
          Mean value_function loss: 83.5105
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.6302
                       Mean reward: 911.56
               Mean episode length: 241.79
    Episode_Reward/reaching_object: 1.2408
     Episode_Reward/lifting_object: 174.5566
      Episode_Reward/object_height: 0.0732
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 162496512
                    Iteration time: 2.00s
                      Time elapsed: 00:58:25
                               ETA: 00:12:17

################################################################################
                     [1m Learning iteration 1653/2000 [0m                     

                       Computation: 49560 steps/s (collection: 1.893s, learning 0.090s)
             Mean action noise std: 3.03
          Mean value_function loss: 157.5319
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 54.6392
                       Mean reward: 859.39
               Mean episode length: 229.37
    Episode_Reward/reaching_object: 1.2399
     Episode_Reward/lifting_object: 174.5790
      Episode_Reward/object_height: 0.0728
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 162594816
                    Iteration time: 1.98s
                      Time elapsed: 00:58:27
                               ETA: 00:12:15

################################################################################
                     [1m Learning iteration 1654/2000 [0m                     

                       Computation: 49414 steps/s (collection: 1.895s, learning 0.094s)
             Mean action noise std: 3.03
          Mean value_function loss: 143.4724
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 54.6486
                       Mean reward: 856.65
               Mean episode length: 229.63
    Episode_Reward/reaching_object: 1.2280
     Episode_Reward/lifting_object: 172.6112
      Episode_Reward/object_height: 0.0724
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 162693120
                    Iteration time: 1.99s
                      Time elapsed: 00:58:29
                               ETA: 00:12:13

################################################################################
                     [1m Learning iteration 1655/2000 [0m                     

                       Computation: 49672 steps/s (collection: 1.879s, learning 0.100s)
             Mean action noise std: 3.04
          Mean value_function loss: 128.7349
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 54.6603
                       Mean reward: 854.96
               Mean episode length: 228.10
    Episode_Reward/reaching_object: 1.2099
     Episode_Reward/lifting_object: 170.6277
      Episode_Reward/object_height: 0.0714
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 162791424
                    Iteration time: 1.98s
                      Time elapsed: 00:58:31
                               ETA: 00:12:11

################################################################################
                     [1m Learning iteration 1656/2000 [0m                     

                       Computation: 49089 steps/s (collection: 1.907s, learning 0.096s)
             Mean action noise std: 3.04
          Mean value_function loss: 104.2031
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 54.6683
                       Mean reward: 884.13
               Mean episode length: 236.81
    Episode_Reward/reaching_object: 1.2244
     Episode_Reward/lifting_object: 172.3952
      Episode_Reward/object_height: 0.0725
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 162889728
                    Iteration time: 2.00s
                      Time elapsed: 00:58:33
                               ETA: 00:12:09

################################################################################
                     [1m Learning iteration 1657/2000 [0m                     

                       Computation: 48548 steps/s (collection: 1.921s, learning 0.104s)
             Mean action noise std: 3.04
          Mean value_function loss: 127.7499
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 54.6732
                       Mean reward: 843.91
               Mean episode length: 226.15
    Episode_Reward/reaching_object: 1.2291
     Episode_Reward/lifting_object: 173.1525
      Episode_Reward/object_height: 0.0732
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 162988032
                    Iteration time: 2.02s
                      Time elapsed: 00:58:35
                               ETA: 00:12:07

################################################################################
                     [1m Learning iteration 1658/2000 [0m                     

                       Computation: 49264 steps/s (collection: 1.901s, learning 0.094s)
             Mean action noise std: 3.04
          Mean value_function loss: 117.5703
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 54.6771
                       Mean reward: 855.39
               Mean episode length: 228.55
    Episode_Reward/reaching_object: 1.2244
     Episode_Reward/lifting_object: 172.7546
      Episode_Reward/object_height: 0.0736
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 163086336
                    Iteration time: 2.00s
                      Time elapsed: 00:58:37
                               ETA: 00:12:05

################################################################################
                     [1m Learning iteration 1659/2000 [0m                     

                       Computation: 48488 steps/s (collection: 1.912s, learning 0.115s)
             Mean action noise std: 3.04
          Mean value_function loss: 110.1653
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 54.6850
                       Mean reward: 866.07
               Mean episode length: 232.11
    Episode_Reward/reaching_object: 1.2332
     Episode_Reward/lifting_object: 174.1507
      Episode_Reward/object_height: 0.0742
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 163184640
                    Iteration time: 2.03s
                      Time elapsed: 00:58:39
                               ETA: 00:12:02

################################################################################
                     [1m Learning iteration 1660/2000 [0m                     

                       Computation: 49577 steps/s (collection: 1.862s, learning 0.121s)
             Mean action noise std: 3.04
          Mean value_function loss: 145.7234
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 54.6935
                       Mean reward: 855.40
               Mean episode length: 228.35
    Episode_Reward/reaching_object: 1.2152
     Episode_Reward/lifting_object: 171.9328
      Episode_Reward/object_height: 0.0731
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 163282944
                    Iteration time: 1.98s
                      Time elapsed: 00:58:41
                               ETA: 00:12:00

################################################################################
                     [1m Learning iteration 1661/2000 [0m                     

                       Computation: 48571 steps/s (collection: 1.917s, learning 0.107s)
             Mean action noise std: 3.04
          Mean value_function loss: 155.4650
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 54.7037
                       Mean reward: 874.44
               Mean episode length: 232.46
    Episode_Reward/reaching_object: 1.2161
     Episode_Reward/lifting_object: 173.0118
      Episode_Reward/object_height: 0.0737
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 163381248
                    Iteration time: 2.02s
                      Time elapsed: 00:58:43
                               ETA: 00:11:58

################################################################################
                     [1m Learning iteration 1662/2000 [0m                     

                       Computation: 47939 steps/s (collection: 1.947s, learning 0.104s)
             Mean action noise std: 3.04
          Mean value_function loss: 144.6529
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 54.7123
                       Mean reward: 865.56
               Mean episode length: 229.96
    Episode_Reward/reaching_object: 1.2028
     Episode_Reward/lifting_object: 171.0681
      Episode_Reward/object_height: 0.0725
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 163479552
                    Iteration time: 2.05s
                      Time elapsed: 00:58:45
                               ETA: 00:11:56

################################################################################
                     [1m Learning iteration 1663/2000 [0m                     

                       Computation: 49342 steps/s (collection: 1.874s, learning 0.118s)
             Mean action noise std: 3.04
          Mean value_function loss: 198.0261
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 54.7164
                       Mean reward: 842.16
               Mean episode length: 224.82
    Episode_Reward/reaching_object: 1.2116
     Episode_Reward/lifting_object: 171.8345
      Episode_Reward/object_height: 0.0731
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 163577856
                    Iteration time: 1.99s
                      Time elapsed: 00:58:47
                               ETA: 00:11:54

################################################################################
                     [1m Learning iteration 1664/2000 [0m                     

                       Computation: 48023 steps/s (collection: 1.921s, learning 0.126s)
             Mean action noise std: 3.05
          Mean value_function loss: 127.3853
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 54.7221
                       Mean reward: 893.98
               Mean episode length: 237.57
    Episode_Reward/reaching_object: 1.2199
     Episode_Reward/lifting_object: 173.5635
      Episode_Reward/object_height: 0.0744
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 163676160
                    Iteration time: 2.05s
                      Time elapsed: 00:58:49
                               ETA: 00:11:52

################################################################################
                     [1m Learning iteration 1665/2000 [0m                     

                       Computation: 48766 steps/s (collection: 1.907s, learning 0.109s)
             Mean action noise std: 3.05
          Mean value_function loss: 155.3576
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 54.7319
                       Mean reward: 804.49
               Mean episode length: 216.45
    Episode_Reward/reaching_object: 1.1866
     Episode_Reward/lifting_object: 168.2637
      Episode_Reward/object_height: 0.0718
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 163774464
                    Iteration time: 2.02s
                      Time elapsed: 00:58:51
                               ETA: 00:11:50

################################################################################
                     [1m Learning iteration 1666/2000 [0m                     

                       Computation: 26818 steps/s (collection: 3.548s, learning 0.118s)
             Mean action noise std: 3.05
          Mean value_function loss: 146.2105
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.7369
                       Mean reward: 867.82
               Mean episode length: 230.88
    Episode_Reward/reaching_object: 1.1867
     Episode_Reward/lifting_object: 168.6419
      Episode_Reward/object_height: 0.0719
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163872768
                    Iteration time: 3.67s
                      Time elapsed: 00:58:55
                               ETA: 00:11:48

################################################################################
                     [1m Learning iteration 1667/2000 [0m                     

                       Computation: 14478 steps/s (collection: 6.650s, learning 0.140s)
             Mean action noise std: 3.05
          Mean value_function loss: 126.6884
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 54.7434
                       Mean reward: 876.53
               Mean episode length: 233.27
    Episode_Reward/reaching_object: 1.2378
     Episode_Reward/lifting_object: 176.4576
      Episode_Reward/object_height: 0.0756
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 163971072
                    Iteration time: 6.79s
                      Time elapsed: 00:59:02
                               ETA: 00:11:47

################################################################################
                     [1m Learning iteration 1668/2000 [0m                     

                       Computation: 14096 steps/s (collection: 6.851s, learning 0.123s)
             Mean action noise std: 3.05
          Mean value_function loss: 159.1069
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.7515
                       Mean reward: 858.70
               Mean episode length: 230.07
    Episode_Reward/reaching_object: 1.1905
     Episode_Reward/lifting_object: 169.5399
      Episode_Reward/object_height: 0.0725
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 164069376
                    Iteration time: 6.97s
                      Time elapsed: 00:59:09
                               ETA: 00:11:45

################################################################################
                     [1m Learning iteration 1669/2000 [0m                     

                       Computation: 14646 steps/s (collection: 6.593s, learning 0.119s)
             Mean action noise std: 3.05
          Mean value_function loss: 170.6831
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.7620
                       Mean reward: 845.79
               Mean episode length: 228.20
    Episode_Reward/reaching_object: 1.2139
     Episode_Reward/lifting_object: 172.5306
      Episode_Reward/object_height: 0.0741
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 164167680
                    Iteration time: 6.71s
                      Time elapsed: 00:59:15
                               ETA: 00:11:44

################################################################################
                     [1m Learning iteration 1670/2000 [0m                     

                       Computation: 14498 steps/s (collection: 6.647s, learning 0.134s)
             Mean action noise std: 3.05
          Mean value_function loss: 132.7268
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 54.7737
                       Mean reward: 873.28
               Mean episode length: 232.20
    Episode_Reward/reaching_object: 1.1818
     Episode_Reward/lifting_object: 168.6767
      Episode_Reward/object_height: 0.0726
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 164265984
                    Iteration time: 6.78s
                      Time elapsed: 00:59:22
                               ETA: 00:11:43

################################################################################
                     [1m Learning iteration 1671/2000 [0m                     

                       Computation: 14143 steps/s (collection: 6.813s, learning 0.138s)
             Mean action noise std: 3.05
          Mean value_function loss: 124.3286
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 54.7821
                       Mean reward: 897.24
               Mean episode length: 238.21
    Episode_Reward/reaching_object: 1.2131
     Episode_Reward/lifting_object: 173.4995
      Episode_Reward/object_height: 0.0750
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 164364288
                    Iteration time: 6.95s
                      Time elapsed: 00:59:29
                               ETA: 00:11:42

################################################################################
                     [1m Learning iteration 1672/2000 [0m                     

                       Computation: 14255 steps/s (collection: 6.775s, learning 0.121s)
             Mean action noise std: 3.05
          Mean value_function loss: 131.3114
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 54.7905
                       Mean reward: 872.06
               Mean episode length: 232.88
    Episode_Reward/reaching_object: 1.1997
     Episode_Reward/lifting_object: 172.0490
      Episode_Reward/object_height: 0.0742
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 164462592
                    Iteration time: 6.90s
                      Time elapsed: 00:59:36
                               ETA: 00:11:41

################################################################################
                     [1m Learning iteration 1673/2000 [0m                     

                       Computation: 14493 steps/s (collection: 6.666s, learning 0.116s)
             Mean action noise std: 3.06
          Mean value_function loss: 110.8245
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 54.7967
                       Mean reward: 871.13
               Mean episode length: 232.44
    Episode_Reward/reaching_object: 1.2119
     Episode_Reward/lifting_object: 173.9350
      Episode_Reward/object_height: 0.0754
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 164560896
                    Iteration time: 6.78s
                      Time elapsed: 00:59:43
                               ETA: 00:11:39

################################################################################
                     [1m Learning iteration 1674/2000 [0m                     

                       Computation: 14507 steps/s (collection: 6.649s, learning 0.128s)
             Mean action noise std: 3.06
          Mean value_function loss: 103.4494
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 54.8041
                       Mean reward: 889.13
               Mean episode length: 236.86
    Episode_Reward/reaching_object: 1.2277
     Episode_Reward/lifting_object: 176.6511
      Episode_Reward/object_height: 0.0767
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 164659200
                    Iteration time: 6.78s
                      Time elapsed: 00:59:49
                               ETA: 00:11:38

################################################################################
                     [1m Learning iteration 1675/2000 [0m                     

                       Computation: 22456 steps/s (collection: 4.267s, learning 0.111s)
             Mean action noise std: 3.06
          Mean value_function loss: 98.7688
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 54.8090
                       Mean reward: 885.42
               Mean episode length: 235.93
    Episode_Reward/reaching_object: 1.2011
     Episode_Reward/lifting_object: 172.7982
      Episode_Reward/object_height: 0.0748
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 164757504
                    Iteration time: 4.38s
                      Time elapsed: 00:59:54
                               ETA: 00:11:36

################################################################################
                     [1m Learning iteration 1676/2000 [0m                     

                       Computation: 49983 steps/s (collection: 1.849s, learning 0.118s)
             Mean action noise std: 3.06
          Mean value_function loss: 103.8650
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 54.8173
                       Mean reward: 862.39
               Mean episode length: 230.83
    Episode_Reward/reaching_object: 1.2156
     Episode_Reward/lifting_object: 174.6429
      Episode_Reward/object_height: 0.0759
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 164855808
                    Iteration time: 1.97s
                      Time elapsed: 00:59:56
                               ETA: 00:11:34

################################################################################
                     [1m Learning iteration 1677/2000 [0m                     

                       Computation: 50833 steps/s (collection: 1.842s, learning 0.092s)
             Mean action noise std: 3.06
          Mean value_function loss: 130.6867
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 54.8203
                       Mean reward: 884.30
               Mean episode length: 235.13
    Episode_Reward/reaching_object: 1.2189
     Episode_Reward/lifting_object: 175.6830
      Episode_Reward/object_height: 0.0763
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 164954112
                    Iteration time: 1.93s
                      Time elapsed: 00:59:58
                               ETA: 00:11:32

################################################################################
                     [1m Learning iteration 1678/2000 [0m                     

                       Computation: 49056 steps/s (collection: 1.843s, learning 0.161s)
             Mean action noise std: 3.06
          Mean value_function loss: 257.8371
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 54.8212
                       Mean reward: 883.46
               Mean episode length: 234.29
    Episode_Reward/reaching_object: 1.1860
     Episode_Reward/lifting_object: 170.5132
      Episode_Reward/object_height: 0.0735
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 165052416
                    Iteration time: 2.00s
                      Time elapsed: 01:00:00
                               ETA: 00:11:30

################################################################################
                     [1m Learning iteration 1679/2000 [0m                     

                       Computation: 51624 steps/s (collection: 1.792s, learning 0.112s)
             Mean action noise std: 3.06
          Mean value_function loss: 220.0646
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 54.8255
                       Mean reward: 875.10
               Mean episode length: 233.11
    Episode_Reward/reaching_object: 1.1924
     Episode_Reward/lifting_object: 171.6802
      Episode_Reward/object_height: 0.0737
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 165150720
                    Iteration time: 1.90s
                      Time elapsed: 01:00:02
                               ETA: 00:11:28

################################################################################
                     [1m Learning iteration 1680/2000 [0m                     

                       Computation: 51873 steps/s (collection: 1.804s, learning 0.091s)
             Mean action noise std: 3.06
          Mean value_function loss: 139.1105
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.8374
                       Mean reward: 795.05
               Mean episode length: 214.52
    Episode_Reward/reaching_object: 1.1817
     Episode_Reward/lifting_object: 170.0458
      Episode_Reward/object_height: 0.0731
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 165249024
                    Iteration time: 1.90s
                      Time elapsed: 01:00:03
                               ETA: 00:11:26

################################################################################
                     [1m Learning iteration 1681/2000 [0m                     

                       Computation: 52876 steps/s (collection: 1.754s, learning 0.106s)
             Mean action noise std: 3.06
          Mean value_function loss: 114.3342
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.8505
                       Mean reward: 866.13
               Mean episode length: 231.09
    Episode_Reward/reaching_object: 1.2064
     Episode_Reward/lifting_object: 173.4257
      Episode_Reward/object_height: 0.0745
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 165347328
                    Iteration time: 1.86s
                      Time elapsed: 01:00:05
                               ETA: 00:11:23

################################################################################
                     [1m Learning iteration 1682/2000 [0m                     

                       Computation: 51332 steps/s (collection: 1.780s, learning 0.135s)
             Mean action noise std: 3.06
          Mean value_function loss: 114.3612
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 54.8599
                       Mean reward: 877.44
               Mean episode length: 232.58
    Episode_Reward/reaching_object: 1.2253
     Episode_Reward/lifting_object: 176.5699
      Episode_Reward/object_height: 0.0759
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 165445632
                    Iteration time: 1.92s
                      Time elapsed: 01:00:07
                               ETA: 00:11:21

################################################################################
                     [1m Learning iteration 1683/2000 [0m                     

                       Computation: 51622 steps/s (collection: 1.808s, learning 0.097s)
             Mean action noise std: 3.06
          Mean value_function loss: 116.0638
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 54.8682
                       Mean reward: 863.60
               Mean episode length: 230.45
    Episode_Reward/reaching_object: 1.2074
     Episode_Reward/lifting_object: 173.2566
      Episode_Reward/object_height: 0.0743
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 165543936
                    Iteration time: 1.90s
                      Time elapsed: 01:00:09
                               ETA: 00:11:19

################################################################################
                     [1m Learning iteration 1684/2000 [0m                     

                       Computation: 51647 steps/s (collection: 1.792s, learning 0.111s)
             Mean action noise std: 3.07
          Mean value_function loss: 108.6755
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.8764
                       Mean reward: 849.61
               Mean episode length: 227.04
    Episode_Reward/reaching_object: 1.2014
     Episode_Reward/lifting_object: 172.3051
      Episode_Reward/object_height: 0.0739
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 165642240
                    Iteration time: 1.90s
                      Time elapsed: 01:00:11
                               ETA: 00:11:17

################################################################################
                     [1m Learning iteration 1685/2000 [0m                     

                       Computation: 52401 steps/s (collection: 1.772s, learning 0.104s)
             Mean action noise std: 3.07
          Mean value_function loss: 106.6359
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.8858
                       Mean reward: 850.34
               Mean episode length: 226.88
    Episode_Reward/reaching_object: 1.2048
     Episode_Reward/lifting_object: 172.8629
      Episode_Reward/object_height: 0.0741
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 165740544
                    Iteration time: 1.88s
                      Time elapsed: 01:00:13
                               ETA: 00:11:15

################################################################################
                     [1m Learning iteration 1686/2000 [0m                     

                       Computation: 52419 steps/s (collection: 1.786s, learning 0.089s)
             Mean action noise std: 3.07
          Mean value_function loss: 118.4447
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 54.8930
                       Mean reward: 887.87
               Mean episode length: 235.75
    Episode_Reward/reaching_object: 1.1985
     Episode_Reward/lifting_object: 172.1310
      Episode_Reward/object_height: 0.0728
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 165838848
                    Iteration time: 1.88s
                      Time elapsed: 01:00:15
                               ETA: 00:11:12

################################################################################
                     [1m Learning iteration 1687/2000 [0m                     

                       Computation: 50619 steps/s (collection: 1.783s, learning 0.159s)
             Mean action noise std: 3.07
          Mean value_function loss: 94.3005
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 54.8986
                       Mean reward: 861.80
               Mean episode length: 229.67
    Episode_Reward/reaching_object: 1.1987
     Episode_Reward/lifting_object: 172.0826
      Episode_Reward/object_height: 0.0729
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 165937152
                    Iteration time: 1.94s
                      Time elapsed: 01:00:17
                               ETA: 00:11:10

################################################################################
                     [1m Learning iteration 1688/2000 [0m                     

                       Computation: 50420 steps/s (collection: 1.839s, learning 0.111s)
             Mean action noise std: 3.07
          Mean value_function loss: 96.5843
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 54.9048
                       Mean reward: 877.29
               Mean episode length: 234.63
    Episode_Reward/reaching_object: 1.2223
     Episode_Reward/lifting_object: 175.0182
      Episode_Reward/object_height: 0.0737
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 166035456
                    Iteration time: 1.95s
                      Time elapsed: 01:00:19
                               ETA: 00:11:08

################################################################################
                     [1m Learning iteration 1689/2000 [0m                     

                       Computation: 52818 steps/s (collection: 1.772s, learning 0.089s)
             Mean action noise std: 3.07
          Mean value_function loss: 114.2383
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.9097
                       Mean reward: 851.08
               Mean episode length: 226.26
    Episode_Reward/reaching_object: 1.2022
     Episode_Reward/lifting_object: 172.0516
      Episode_Reward/object_height: 0.0722
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 166133760
                    Iteration time: 1.86s
                      Time elapsed: 01:00:21
                               ETA: 00:11:06

################################################################################
                     [1m Learning iteration 1690/2000 [0m                     

                       Computation: 52169 steps/s (collection: 1.798s, learning 0.087s)
             Mean action noise std: 3.07
          Mean value_function loss: 133.6305
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 54.9184
                       Mean reward: 887.15
               Mean episode length: 236.03
    Episode_Reward/reaching_object: 1.1772
     Episode_Reward/lifting_object: 168.3431
      Episode_Reward/object_height: 0.0699
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 166232064
                    Iteration time: 1.88s
                      Time elapsed: 01:00:22
                               ETA: 00:11:04

################################################################################
                     [1m Learning iteration 1691/2000 [0m                     

                       Computation: 52379 steps/s (collection: 1.781s, learning 0.096s)
             Mean action noise std: 3.07
          Mean value_function loss: 184.7220
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.9266
                       Mean reward: 902.38
               Mean episode length: 239.45
    Episode_Reward/reaching_object: 1.2253
     Episode_Reward/lifting_object: 175.5998
      Episode_Reward/object_height: 0.0734
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 166330368
                    Iteration time: 1.88s
                      Time elapsed: 01:00:24
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 1692/2000 [0m                     

                       Computation: 51757 steps/s (collection: 1.794s, learning 0.105s)
             Mean action noise std: 3.07
          Mean value_function loss: 192.0360
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 54.9365
                       Mean reward: 893.59
               Mean episode length: 237.10
    Episode_Reward/reaching_object: 1.2028
     Episode_Reward/lifting_object: 171.8333
      Episode_Reward/object_height: 0.0717
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 166428672
                    Iteration time: 1.90s
                      Time elapsed: 01:00:26
                               ETA: 00:10:59

################################################################################
                     [1m Learning iteration 1693/2000 [0m                     

                       Computation: 51427 steps/s (collection: 1.819s, learning 0.093s)
             Mean action noise std: 3.08
          Mean value_function loss: 117.8281
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.9453
                       Mean reward: 869.35
               Mean episode length: 231.51
    Episode_Reward/reaching_object: 1.2175
     Episode_Reward/lifting_object: 174.1084
      Episode_Reward/object_height: 0.0728
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 166526976
                    Iteration time: 1.91s
                      Time elapsed: 01:00:28
                               ETA: 00:10:57

################################################################################
                     [1m Learning iteration 1694/2000 [0m                     

                       Computation: 53251 steps/s (collection: 1.753s, learning 0.093s)
             Mean action noise std: 3.08
          Mean value_function loss: 110.4797
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.9512
                       Mean reward: 862.73
               Mean episode length: 231.16
    Episode_Reward/reaching_object: 1.2238
     Episode_Reward/lifting_object: 174.7847
      Episode_Reward/object_height: 0.0732
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 166625280
                    Iteration time: 1.85s
                      Time elapsed: 01:00:30
                               ETA: 00:10:55

################################################################################
                     [1m Learning iteration 1695/2000 [0m                     

                       Computation: 51228 steps/s (collection: 1.791s, learning 0.128s)
             Mean action noise std: 3.08
          Mean value_function loss: 177.2341
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 54.9617
                       Mean reward: 887.08
               Mean episode length: 236.33
    Episode_Reward/reaching_object: 1.2035
     Episode_Reward/lifting_object: 171.5828
      Episode_Reward/object_height: 0.0718
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 166723584
                    Iteration time: 1.92s
                      Time elapsed: 01:00:32
                               ETA: 00:10:53

################################################################################
                     [1m Learning iteration 1696/2000 [0m                     

                       Computation: 50654 steps/s (collection: 1.827s, learning 0.114s)
             Mean action noise std: 3.08
          Mean value_function loss: 171.2059
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 54.9715
                       Mean reward: 848.25
               Mean episode length: 226.61
    Episode_Reward/reaching_object: 1.2039
     Episode_Reward/lifting_object: 171.8926
      Episode_Reward/object_height: 0.0714
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 166821888
                    Iteration time: 1.94s
                      Time elapsed: 01:00:34
                               ETA: 00:10:51

################################################################################
                     [1m Learning iteration 1697/2000 [0m                     

                       Computation: 51064 steps/s (collection: 1.820s, learning 0.106s)
             Mean action noise std: 3.08
          Mean value_function loss: 117.1204
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 54.9812
                       Mean reward: 853.44
               Mean episode length: 227.44
    Episode_Reward/reaching_object: 1.1851
     Episode_Reward/lifting_object: 169.2312
      Episode_Reward/object_height: 0.0707
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 166920192
                    Iteration time: 1.93s
                      Time elapsed: 01:00:36
                               ETA: 00:10:48

################################################################################
                     [1m Learning iteration 1698/2000 [0m                     

                       Computation: 51917 steps/s (collection: 1.784s, learning 0.110s)
             Mean action noise std: 3.08
          Mean value_function loss: 132.0028
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 54.9875
                       Mean reward: 919.17
               Mean episode length: 243.10
    Episode_Reward/reaching_object: 1.1825
     Episode_Reward/lifting_object: 168.3788
      Episode_Reward/object_height: 0.0705
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 167018496
                    Iteration time: 1.89s
                      Time elapsed: 01:00:38
                               ETA: 00:10:46

################################################################################
                     [1m Learning iteration 1699/2000 [0m                     

                       Computation: 52041 steps/s (collection: 1.784s, learning 0.105s)
             Mean action noise std: 3.08
          Mean value_function loss: 136.4102
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 54.9953
                       Mean reward: 902.65
               Mean episode length: 239.64
    Episode_Reward/reaching_object: 1.2185
     Episode_Reward/lifting_object: 173.5029
      Episode_Reward/object_height: 0.0727
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 167116800
                    Iteration time: 1.89s
                      Time elapsed: 01:00:40
                               ETA: 00:10:44

################################################################################
                     [1m Learning iteration 1700/2000 [0m                     

                       Computation: 51838 steps/s (collection: 1.773s, learning 0.123s)
             Mean action noise std: 3.08
          Mean value_function loss: 116.7803
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 55.0020
                       Mean reward: 878.49
               Mean episode length: 233.77
    Episode_Reward/reaching_object: 1.2062
     Episode_Reward/lifting_object: 171.4508
      Episode_Reward/object_height: 0.0723
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 167215104
                    Iteration time: 1.90s
                      Time elapsed: 01:00:41
                               ETA: 00:10:42

################################################################################
                     [1m Learning iteration 1701/2000 [0m                     

                       Computation: 50474 steps/s (collection: 1.822s, learning 0.126s)
             Mean action noise std: 3.08
          Mean value_function loss: 111.9818
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.0089
                       Mean reward: 862.83
               Mean episode length: 231.03
    Episode_Reward/reaching_object: 1.2307
     Episode_Reward/lifting_object: 175.1208
      Episode_Reward/object_height: 0.0738
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 167313408
                    Iteration time: 1.95s
                      Time elapsed: 01:00:43
                               ETA: 00:10:40

################################################################################
                     [1m Learning iteration 1702/2000 [0m                     

                       Computation: 50829 steps/s (collection: 1.804s, learning 0.130s)
             Mean action noise std: 3.09
          Mean value_function loss: 158.6400
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 55.0161
                       Mean reward: 796.74
               Mean episode length: 213.92
    Episode_Reward/reaching_object: 1.1758
     Episode_Reward/lifting_object: 167.1047
      Episode_Reward/object_height: 0.0700
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 167411712
                    Iteration time: 1.93s
                      Time elapsed: 01:00:45
                               ETA: 00:10:37

################################################################################
                     [1m Learning iteration 1703/2000 [0m                     

                       Computation: 52550 steps/s (collection: 1.779s, learning 0.092s)
             Mean action noise std: 3.09
          Mean value_function loss: 135.1034
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 55.0237
                       Mean reward: 849.78
               Mean episode length: 226.93
    Episode_Reward/reaching_object: 1.2081
     Episode_Reward/lifting_object: 171.8354
      Episode_Reward/object_height: 0.0719
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 167510016
                    Iteration time: 1.87s
                      Time elapsed: 01:00:47
                               ETA: 00:10:35

################################################################################
                     [1m Learning iteration 1704/2000 [0m                     

                       Computation: 52266 steps/s (collection: 1.787s, learning 0.094s)
             Mean action noise std: 3.09
          Mean value_function loss: 112.5228
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.0290
                       Mean reward: 876.44
               Mean episode length: 232.85
    Episode_Reward/reaching_object: 1.2385
     Episode_Reward/lifting_object: 175.7924
      Episode_Reward/object_height: 0.0739
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 167608320
                    Iteration time: 1.88s
                      Time elapsed: 01:00:49
                               ETA: 00:10:33

################################################################################
                     [1m Learning iteration 1705/2000 [0m                     

                       Computation: 52624 steps/s (collection: 1.763s, learning 0.105s)
             Mean action noise std: 3.09
          Mean value_function loss: 120.4329
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 55.0410
                       Mean reward: 887.04
               Mean episode length: 235.74
    Episode_Reward/reaching_object: 1.2297
     Episode_Reward/lifting_object: 174.4926
      Episode_Reward/object_height: 0.0734
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 167706624
                    Iteration time: 1.87s
                      Time elapsed: 01:00:51
                               ETA: 00:10:31

################################################################################
                     [1m Learning iteration 1706/2000 [0m                     

                       Computation: 52500 steps/s (collection: 1.773s, learning 0.100s)
             Mean action noise std: 3.09
          Mean value_function loss: 152.5721
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 55.0497
                       Mean reward: 842.37
               Mean episode length: 224.62
    Episode_Reward/reaching_object: 1.2174
     Episode_Reward/lifting_object: 172.7278
      Episode_Reward/object_height: 0.0725
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 167804928
                    Iteration time: 1.87s
                      Time elapsed: 01:00:53
                               ETA: 00:10:29

################################################################################
                     [1m Learning iteration 1707/2000 [0m                     

                       Computation: 51536 steps/s (collection: 1.795s, learning 0.113s)
             Mean action noise std: 3.09
          Mean value_function loss: 111.7978
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 55.0522
                       Mean reward: 902.47
               Mean episode length: 239.77
    Episode_Reward/reaching_object: 1.2066
     Episode_Reward/lifting_object: 171.3717
      Episode_Reward/object_height: 0.0716
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 167903232
                    Iteration time: 1.91s
                      Time elapsed: 01:00:55
                               ETA: 00:10:27

################################################################################
                     [1m Learning iteration 1708/2000 [0m                     

                       Computation: 51632 steps/s (collection: 1.796s, learning 0.108s)
             Mean action noise std: 3.09
          Mean value_function loss: 134.5149
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.0553
                       Mean reward: 876.84
               Mean episode length: 233.21
    Episode_Reward/reaching_object: 1.1973
     Episode_Reward/lifting_object: 169.9699
      Episode_Reward/object_height: 0.0707
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 168001536
                    Iteration time: 1.90s
                      Time elapsed: 01:00:57
                               ETA: 00:10:24

################################################################################
                     [1m Learning iteration 1709/2000 [0m                     

                       Computation: 51412 steps/s (collection: 1.791s, learning 0.121s)
             Mean action noise std: 3.09
          Mean value_function loss: 168.0220
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.0648
                       Mean reward: 838.03
               Mean episode length: 224.89
    Episode_Reward/reaching_object: 1.2102
     Episode_Reward/lifting_object: 171.4625
      Episode_Reward/object_height: 0.0712
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 168099840
                    Iteration time: 1.91s
                      Time elapsed: 01:00:59
                               ETA: 00:10:22

################################################################################
                     [1m Learning iteration 1710/2000 [0m                     

                       Computation: 53160 steps/s (collection: 1.759s, learning 0.091s)
             Mean action noise std: 3.09
          Mean value_function loss: 214.0948
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 55.0747
                       Mean reward: 854.64
               Mean episode length: 227.81
    Episode_Reward/reaching_object: 1.1912
     Episode_Reward/lifting_object: 168.5623
      Episode_Reward/object_height: 0.0700
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168198144
                    Iteration time: 1.85s
                      Time elapsed: 01:01:00
                               ETA: 00:10:20

################################################################################
                     [1m Learning iteration 1711/2000 [0m                     

                       Computation: 52340 steps/s (collection: 1.794s, learning 0.085s)
             Mean action noise std: 3.09
          Mean value_function loss: 168.3993
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 55.0798
                       Mean reward: 888.36
               Mean episode length: 238.68
    Episode_Reward/reaching_object: 1.2020
     Episode_Reward/lifting_object: 170.4323
      Episode_Reward/object_height: 0.0704
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 168296448
                    Iteration time: 1.88s
                      Time elapsed: 01:01:02
                               ETA: 00:10:18

################################################################################
                     [1m Learning iteration 1712/2000 [0m                     

                       Computation: 51970 steps/s (collection: 1.802s, learning 0.089s)
             Mean action noise std: 3.10
          Mean value_function loss: 112.4309
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.0884
                       Mean reward: 862.42
               Mean episode length: 230.59
    Episode_Reward/reaching_object: 1.2206
     Episode_Reward/lifting_object: 173.8678
      Episode_Reward/object_height: 0.0716
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 168394752
                    Iteration time: 1.89s
                      Time elapsed: 01:01:04
                               ETA: 00:10:16

################################################################################
                     [1m Learning iteration 1713/2000 [0m                     

                       Computation: 51614 steps/s (collection: 1.794s, learning 0.111s)
             Mean action noise std: 3.10
          Mean value_function loss: 149.2731
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.1016
                       Mean reward: 860.96
               Mean episode length: 230.04
    Episode_Reward/reaching_object: 1.1924
     Episode_Reward/lifting_object: 169.8287
      Episode_Reward/object_height: 0.0695
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 168493056
                    Iteration time: 1.90s
                      Time elapsed: 01:01:06
                               ETA: 00:10:13

################################################################################
                     [1m Learning iteration 1714/2000 [0m                     

                       Computation: 51428 steps/s (collection: 1.799s, learning 0.113s)
             Mean action noise std: 3.10
          Mean value_function loss: 132.6781
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 55.1113
                       Mean reward: 880.81
               Mean episode length: 235.01
    Episode_Reward/reaching_object: 1.2223
     Episode_Reward/lifting_object: 174.5431
      Episode_Reward/object_height: 0.0713
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 168591360
                    Iteration time: 1.91s
                      Time elapsed: 01:01:08
                               ETA: 00:10:11

################################################################################
                     [1m Learning iteration 1715/2000 [0m                     

                       Computation: 52108 steps/s (collection: 1.796s, learning 0.091s)
             Mean action noise std: 3.10
          Mean value_function loss: 223.2800
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 55.1217
                       Mean reward: 859.56
               Mean episode length: 230.83
    Episode_Reward/reaching_object: 1.2141
     Episode_Reward/lifting_object: 172.6132
      Episode_Reward/object_height: 0.0707
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 168689664
                    Iteration time: 1.89s
                      Time elapsed: 01:01:10
                               ETA: 00:10:09

################################################################################
                     [1m Learning iteration 1716/2000 [0m                     

                       Computation: 52218 steps/s (collection: 1.790s, learning 0.093s)
             Mean action noise std: 3.10
          Mean value_function loss: 112.1174
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 55.1315
                       Mean reward: 889.90
               Mean episode length: 236.56
    Episode_Reward/reaching_object: 1.2269
     Episode_Reward/lifting_object: 175.2797
      Episode_Reward/object_height: 0.0717
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 168787968
                    Iteration time: 1.88s
                      Time elapsed: 01:01:12
                               ETA: 00:10:07

################################################################################
                     [1m Learning iteration 1717/2000 [0m                     

                       Computation: 52628 steps/s (collection: 1.782s, learning 0.086s)
             Mean action noise std: 3.10
          Mean value_function loss: 126.3349
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.1373
                       Mean reward: 816.47
               Mean episode length: 220.21
    Episode_Reward/reaching_object: 1.2252
     Episode_Reward/lifting_object: 174.5027
      Episode_Reward/object_height: 0.0717
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168886272
                    Iteration time: 1.87s
                      Time elapsed: 01:01:14
                               ETA: 00:10:05

################################################################################
                     [1m Learning iteration 1718/2000 [0m                     

                       Computation: 51546 steps/s (collection: 1.789s, learning 0.118s)
             Mean action noise std: 3.10
          Mean value_function loss: 117.2737
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.1467
                       Mean reward: 902.92
               Mean episode length: 238.80
    Episode_Reward/reaching_object: 1.2167
     Episode_Reward/lifting_object: 173.5063
      Episode_Reward/object_height: 0.0707
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168984576
                    Iteration time: 1.91s
                      Time elapsed: 01:01:16
                               ETA: 00:10:03

################################################################################
                     [1m Learning iteration 1719/2000 [0m                     

                       Computation: 51268 steps/s (collection: 1.829s, learning 0.089s)
             Mean action noise std: 3.11
          Mean value_function loss: 136.9521
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 55.1541
                       Mean reward: 864.15
               Mean episode length: 229.74
    Episode_Reward/reaching_object: 1.2053
     Episode_Reward/lifting_object: 171.6349
      Episode_Reward/object_height: 0.0702
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 169082880
                    Iteration time: 1.92s
                      Time elapsed: 01:01:17
                               ETA: 00:10:00

################################################################################
                     [1m Learning iteration 1720/2000 [0m                     

                       Computation: 52095 steps/s (collection: 1.777s, learning 0.110s)
             Mean action noise std: 3.11
          Mean value_function loss: 142.2006
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 55.1619
                       Mean reward: 853.78
               Mean episode length: 227.64
    Episode_Reward/reaching_object: 1.1976
     Episode_Reward/lifting_object: 170.5226
      Episode_Reward/object_height: 0.0702
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 169181184
                    Iteration time: 1.89s
                      Time elapsed: 01:01:19
                               ETA: 00:09:58

################################################################################
                     [1m Learning iteration 1721/2000 [0m                     

                       Computation: 51918 steps/s (collection: 1.796s, learning 0.097s)
             Mean action noise std: 3.11
          Mean value_function loss: 97.8770
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 55.1717
                       Mean reward: 841.35
               Mean episode length: 225.30
    Episode_Reward/reaching_object: 1.2078
     Episode_Reward/lifting_object: 171.6335
      Episode_Reward/object_height: 0.0707
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 169279488
                    Iteration time: 1.89s
                      Time elapsed: 01:01:21
                               ETA: 00:09:56

################################################################################
                     [1m Learning iteration 1722/2000 [0m                     

                       Computation: 52177 steps/s (collection: 1.780s, learning 0.104s)
             Mean action noise std: 3.11
          Mean value_function loss: 131.3930
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 55.1822
                       Mean reward: 864.62
               Mean episode length: 230.46
    Episode_Reward/reaching_object: 1.1952
     Episode_Reward/lifting_object: 169.8054
      Episode_Reward/object_height: 0.0700
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 169377792
                    Iteration time: 1.88s
                      Time elapsed: 01:01:23
                               ETA: 00:09:54

################################################################################
                     [1m Learning iteration 1723/2000 [0m                     

                       Computation: 52445 steps/s (collection: 1.783s, learning 0.092s)
             Mean action noise std: 3.11
          Mean value_function loss: 102.4546
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 55.1903
                       Mean reward: 916.97
               Mean episode length: 242.74
    Episode_Reward/reaching_object: 1.2328
     Episode_Reward/lifting_object: 175.6555
      Episode_Reward/object_height: 0.0730
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 169476096
                    Iteration time: 1.87s
                      Time elapsed: 01:01:25
                               ETA: 00:09:52

################################################################################
                     [1m Learning iteration 1724/2000 [0m                     

                       Computation: 52583 steps/s (collection: 1.778s, learning 0.091s)
             Mean action noise std: 3.11
          Mean value_function loss: 128.2658
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.1998
                       Mean reward: 819.65
               Mean episode length: 220.63
    Episode_Reward/reaching_object: 1.2166
     Episode_Reward/lifting_object: 172.8734
      Episode_Reward/object_height: 0.0712
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 169574400
                    Iteration time: 1.87s
                      Time elapsed: 01:01:27
                               ETA: 00:09:49

################################################################################
                     [1m Learning iteration 1725/2000 [0m                     

                       Computation: 52042 steps/s (collection: 1.781s, learning 0.108s)
             Mean action noise std: 3.11
          Mean value_function loss: 106.7989
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 55.2082
                       Mean reward: 885.44
               Mean episode length: 235.29
    Episode_Reward/reaching_object: 1.2294
     Episode_Reward/lifting_object: 175.0621
      Episode_Reward/object_height: 0.0724
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 169672704
                    Iteration time: 1.89s
                      Time elapsed: 01:01:29
                               ETA: 00:09:47

################################################################################
                     [1m Learning iteration 1726/2000 [0m                     

                       Computation: 51242 steps/s (collection: 1.826s, learning 0.093s)
             Mean action noise std: 3.11
          Mean value_function loss: 85.0261
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 55.2160
                       Mean reward: 891.00
               Mean episode length: 237.86
    Episode_Reward/reaching_object: 1.2322
     Episode_Reward/lifting_object: 174.8856
      Episode_Reward/object_height: 0.0725
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 169771008
                    Iteration time: 1.92s
                      Time elapsed: 01:01:31
                               ETA: 00:09:45

################################################################################
                     [1m Learning iteration 1727/2000 [0m                     

                       Computation: 52040 steps/s (collection: 1.776s, learning 0.113s)
             Mean action noise std: 3.12
          Mean value_function loss: 148.5028
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 55.2245
                       Mean reward: 854.75
               Mean episode length: 229.67
    Episode_Reward/reaching_object: 1.2047
     Episode_Reward/lifting_object: 170.8719
      Episode_Reward/object_height: 0.0704
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 169869312
                    Iteration time: 1.89s
                      Time elapsed: 01:01:33
                               ETA: 00:09:43

################################################################################
                     [1m Learning iteration 1728/2000 [0m                     

                       Computation: 52056 steps/s (collection: 1.800s, learning 0.088s)
             Mean action noise std: 3.12
          Mean value_function loss: 118.5365
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 55.2306
                       Mean reward: 866.36
               Mean episode length: 230.95
    Episode_Reward/reaching_object: 1.2159
     Episode_Reward/lifting_object: 172.3869
      Episode_Reward/object_height: 0.0715
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 169967616
                    Iteration time: 1.89s
                      Time elapsed: 01:01:34
                               ETA: 00:09:41

################################################################################
                     [1m Learning iteration 1729/2000 [0m                     

                       Computation: 51600 steps/s (collection: 1.798s, learning 0.108s)
             Mean action noise std: 3.12
          Mean value_function loss: 82.9703
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 55.2357
                       Mean reward: 890.64
               Mean episode length: 235.91
    Episode_Reward/reaching_object: 1.2303
     Episode_Reward/lifting_object: 175.0942
      Episode_Reward/object_height: 0.0729
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 170065920
                    Iteration time: 1.91s
                      Time elapsed: 01:01:36
                               ETA: 00:09:39

################################################################################
                     [1m Learning iteration 1730/2000 [0m                     

                       Computation: 50711 steps/s (collection: 1.832s, learning 0.107s)
             Mean action noise std: 3.12
          Mean value_function loss: 127.3942
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 55.2411
                       Mean reward: 886.40
               Mean episode length: 235.05
    Episode_Reward/reaching_object: 1.2037
     Episode_Reward/lifting_object: 171.2934
      Episode_Reward/object_height: 0.0705
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 170164224
                    Iteration time: 1.94s
                      Time elapsed: 01:01:38
                               ETA: 00:09:36

################################################################################
                     [1m Learning iteration 1731/2000 [0m                     

                       Computation: 50832 steps/s (collection: 1.821s, learning 0.113s)
             Mean action noise std: 3.12
          Mean value_function loss: 100.1482
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 55.2467
                       Mean reward: 904.03
               Mean episode length: 240.28
    Episode_Reward/reaching_object: 1.2269
     Episode_Reward/lifting_object: 174.4848
      Episode_Reward/object_height: 0.0724
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 170262528
                    Iteration time: 1.93s
                      Time elapsed: 01:01:40
                               ETA: 00:09:34

################################################################################
                     [1m Learning iteration 1732/2000 [0m                     

                       Computation: 52581 steps/s (collection: 1.784s, learning 0.086s)
             Mean action noise std: 3.12
          Mean value_function loss: 111.3396
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.2529
                       Mean reward: 888.04
               Mean episode length: 235.14
    Episode_Reward/reaching_object: 1.2088
     Episode_Reward/lifting_object: 171.3375
      Episode_Reward/object_height: 0.0710
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 170360832
                    Iteration time: 1.87s
                      Time elapsed: 01:01:42
                               ETA: 00:09:32

################################################################################
                     [1m Learning iteration 1733/2000 [0m                     

                       Computation: 51760 steps/s (collection: 1.790s, learning 0.110s)
             Mean action noise std: 3.12
          Mean value_function loss: 93.4646
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 55.2590
                       Mean reward: 869.18
               Mean episode length: 232.54
    Episode_Reward/reaching_object: 1.2173
     Episode_Reward/lifting_object: 172.5125
      Episode_Reward/object_height: 0.0720
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 170459136
                    Iteration time: 1.90s
                      Time elapsed: 01:01:44
                               ETA: 00:09:30

################################################################################
                     [1m Learning iteration 1734/2000 [0m                     

                       Computation: 51929 steps/s (collection: 1.795s, learning 0.098s)
             Mean action noise std: 3.12
          Mean value_function loss: 106.0591
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.2648
                       Mean reward: 905.30
               Mean episode length: 240.66
    Episode_Reward/reaching_object: 1.2252
     Episode_Reward/lifting_object: 173.4437
      Episode_Reward/object_height: 0.0727
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 170557440
                    Iteration time: 1.89s
                      Time elapsed: 01:01:46
                               ETA: 00:09:28

################################################################################
                     [1m Learning iteration 1735/2000 [0m                     

                       Computation: 51131 steps/s (collection: 1.826s, learning 0.097s)
             Mean action noise std: 3.12
          Mean value_function loss: 102.8774
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 55.2727
                       Mean reward: 913.27
               Mean episode length: 242.61
    Episode_Reward/reaching_object: 1.2312
     Episode_Reward/lifting_object: 174.4187
      Episode_Reward/object_height: 0.0733
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 170655744
                    Iteration time: 1.92s
                      Time elapsed: 01:01:48
                               ETA: 00:09:26

################################################################################
                     [1m Learning iteration 1736/2000 [0m                     

                       Computation: 51667 steps/s (collection: 1.790s, learning 0.113s)
             Mean action noise std: 3.12
          Mean value_function loss: 96.1147
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 55.2789
                       Mean reward: 875.70
               Mean episode length: 234.67
    Episode_Reward/reaching_object: 1.2050
     Episode_Reward/lifting_object: 170.4845
      Episode_Reward/object_height: 0.0712
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 170754048
                    Iteration time: 1.90s
                      Time elapsed: 01:01:50
                               ETA: 00:09:23

################################################################################
                     [1m Learning iteration 1737/2000 [0m                     

                       Computation: 51310 steps/s (collection: 1.779s, learning 0.137s)
             Mean action noise std: 3.12
          Mean value_function loss: 105.2806
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.2855
                       Mean reward: 901.66
               Mean episode length: 239.39
    Episode_Reward/reaching_object: 1.2372
     Episode_Reward/lifting_object: 175.0236
      Episode_Reward/object_height: 0.0734
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 170852352
                    Iteration time: 1.92s
                      Time elapsed: 01:01:52
                               ETA: 00:09:21

################################################################################
                     [1m Learning iteration 1738/2000 [0m                     

                       Computation: 51837 steps/s (collection: 1.799s, learning 0.098s)
             Mean action noise std: 3.13
          Mean value_function loss: 93.5005
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.2968
                       Mean reward: 878.98
               Mean episode length: 233.35
    Episode_Reward/reaching_object: 1.2381
     Episode_Reward/lifting_object: 175.3852
      Episode_Reward/object_height: 0.0738
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 170950656
                    Iteration time: 1.90s
                      Time elapsed: 01:01:54
                               ETA: 00:09:19

################################################################################
                     [1m Learning iteration 1739/2000 [0m                     

                       Computation: 51863 steps/s (collection: 1.809s, learning 0.086s)
             Mean action noise std: 3.13
          Mean value_function loss: 113.9618
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.3081
                       Mean reward: 853.85
               Mean episode length: 227.95
    Episode_Reward/reaching_object: 1.2150
     Episode_Reward/lifting_object: 171.2706
      Episode_Reward/object_height: 0.0721
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 171048960
                    Iteration time: 1.90s
                      Time elapsed: 01:01:55
                               ETA: 00:09:17

################################################################################
                     [1m Learning iteration 1740/2000 [0m                     

                       Computation: 51588 steps/s (collection: 1.817s, learning 0.088s)
             Mean action noise std: 3.13
          Mean value_function loss: 119.8492
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.3153
                       Mean reward: 853.86
               Mean episode length: 227.75
    Episode_Reward/reaching_object: 1.2313
     Episode_Reward/lifting_object: 173.8669
      Episode_Reward/object_height: 0.0729
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 171147264
                    Iteration time: 1.91s
                      Time elapsed: 01:01:57
                               ETA: 00:09:15

################################################################################
                     [1m Learning iteration 1741/2000 [0m                     

                       Computation: 50700 steps/s (collection: 1.828s, learning 0.111s)
             Mean action noise std: 3.13
          Mean value_function loss: 118.7915
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 55.3240
                       Mean reward: 842.24
               Mean episode length: 224.40
    Episode_Reward/reaching_object: 1.2459
     Episode_Reward/lifting_object: 176.3451
      Episode_Reward/object_height: 0.0743
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 171245568
                    Iteration time: 1.94s
                      Time elapsed: 01:01:59
                               ETA: 00:09:13

################################################################################
                     [1m Learning iteration 1742/2000 [0m                     

                       Computation: 51442 steps/s (collection: 1.818s, learning 0.093s)
             Mean action noise std: 3.13
          Mean value_function loss: 106.2236
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 55.3327
                       Mean reward: 876.88
               Mean episode length: 233.02
    Episode_Reward/reaching_object: 1.2317
     Episode_Reward/lifting_object: 173.7494
      Episode_Reward/object_height: 0.0729
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 171343872
                    Iteration time: 1.91s
                      Time elapsed: 01:02:01
                               ETA: 00:09:10

################################################################################
                     [1m Learning iteration 1743/2000 [0m                     

                       Computation: 51291 steps/s (collection: 1.821s, learning 0.096s)
             Mean action noise std: 3.13
          Mean value_function loss: 126.2527
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.3415
                       Mean reward: 840.64
               Mean episode length: 224.17
    Episode_Reward/reaching_object: 1.2004
     Episode_Reward/lifting_object: 169.5453
      Episode_Reward/object_height: 0.0709
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 171442176
                    Iteration time: 1.92s
                      Time elapsed: 01:02:03
                               ETA: 00:09:08

################################################################################
                     [1m Learning iteration 1744/2000 [0m                     

                       Computation: 51854 steps/s (collection: 1.809s, learning 0.087s)
             Mean action noise std: 3.13
          Mean value_function loss: 141.8087
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 55.3520
                       Mean reward: 894.82
               Mean episode length: 238.89
    Episode_Reward/reaching_object: 1.2475
     Episode_Reward/lifting_object: 176.4235
      Episode_Reward/object_height: 0.0742
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 171540480
                    Iteration time: 1.90s
                      Time elapsed: 01:02:05
                               ETA: 00:09:06

################################################################################
                     [1m Learning iteration 1745/2000 [0m                     

                       Computation: 51253 steps/s (collection: 1.824s, learning 0.094s)
             Mean action noise std: 3.13
          Mean value_function loss: 140.3380
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 55.3639
                       Mean reward: 861.72
               Mean episode length: 229.24
    Episode_Reward/reaching_object: 1.2144
     Episode_Reward/lifting_object: 171.9576
      Episode_Reward/object_height: 0.0718
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 171638784
                    Iteration time: 1.92s
                      Time elapsed: 01:02:07
                               ETA: 00:09:04

################################################################################
                     [1m Learning iteration 1746/2000 [0m                     

                       Computation: 50436 steps/s (collection: 1.811s, learning 0.139s)
             Mean action noise std: 3.14
          Mean value_function loss: 149.2477
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.3702
                       Mean reward: 853.50
               Mean episode length: 227.79
    Episode_Reward/reaching_object: 1.2122
     Episode_Reward/lifting_object: 171.3264
      Episode_Reward/object_height: 0.0720
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 171737088
                    Iteration time: 1.95s
                      Time elapsed: 01:02:09
                               ETA: 00:09:02

################################################################################
                     [1m Learning iteration 1747/2000 [0m                     

                       Computation: 48808 steps/s (collection: 1.897s, learning 0.117s)
             Mean action noise std: 3.14
          Mean value_function loss: 132.0126
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.3781
                       Mean reward: 849.05
               Mean episode length: 225.97
    Episode_Reward/reaching_object: 1.1994
     Episode_Reward/lifting_object: 169.8178
      Episode_Reward/object_height: 0.0713
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 171835392
                    Iteration time: 2.01s
                      Time elapsed: 01:02:11
                               ETA: 00:09:00

################################################################################
                     [1m Learning iteration 1748/2000 [0m                     

                       Computation: 50147 steps/s (collection: 1.830s, learning 0.131s)
             Mean action noise std: 3.14
          Mean value_function loss: 118.2096
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.3878
                       Mean reward: 825.15
               Mean episode length: 222.46
    Episode_Reward/reaching_object: 1.2268
     Episode_Reward/lifting_object: 173.8235
      Episode_Reward/object_height: 0.0731
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 171933696
                    Iteration time: 1.96s
                      Time elapsed: 01:02:13
                               ETA: 00:08:57

################################################################################
                     [1m Learning iteration 1749/2000 [0m                     

                       Computation: 52092 steps/s (collection: 1.795s, learning 0.092s)
             Mean action noise std: 3.14
          Mean value_function loss: 150.7191
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 55.4005
                       Mean reward: 858.06
               Mean episode length: 229.41
    Episode_Reward/reaching_object: 1.2308
     Episode_Reward/lifting_object: 174.6711
      Episode_Reward/object_height: 0.0730
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 172032000
                    Iteration time: 1.89s
                      Time elapsed: 01:02:15
                               ETA: 00:08:55

################################################################################
                     [1m Learning iteration 1750/2000 [0m                     

                       Computation: 51107 steps/s (collection: 1.834s, learning 0.090s)
             Mean action noise std: 3.14
          Mean value_function loss: 134.3532
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 55.4099
                       Mean reward: 878.58
               Mean episode length: 233.32
    Episode_Reward/reaching_object: 1.2230
     Episode_Reward/lifting_object: 173.4577
      Episode_Reward/object_height: 0.0724
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 172130304
                    Iteration time: 1.92s
                      Time elapsed: 01:02:17
                               ETA: 00:08:53

################################################################################
                     [1m Learning iteration 1751/2000 [0m                     

                       Computation: 50467 steps/s (collection: 1.842s, learning 0.106s)
             Mean action noise std: 3.14
          Mean value_function loss: 123.7235
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.4169
                       Mean reward: 885.12
               Mean episode length: 234.90
    Episode_Reward/reaching_object: 1.2189
     Episode_Reward/lifting_object: 173.3413
      Episode_Reward/object_height: 0.0724
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 172228608
                    Iteration time: 1.95s
                      Time elapsed: 01:02:19
                               ETA: 00:08:51

################################################################################
                     [1m Learning iteration 1752/2000 [0m                     

                       Computation: 51561 steps/s (collection: 1.801s, learning 0.105s)
             Mean action noise std: 3.14
          Mean value_function loss: 147.1235
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 55.4267
                       Mean reward: 891.59
               Mean episode length: 236.01
    Episode_Reward/reaching_object: 1.2374
     Episode_Reward/lifting_object: 176.3533
      Episode_Reward/object_height: 0.0731
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 172326912
                    Iteration time: 1.91s
                      Time elapsed: 01:02:20
                               ETA: 00:08:49

################################################################################
                     [1m Learning iteration 1753/2000 [0m                     

                       Computation: 49865 steps/s (collection: 1.865s, learning 0.106s)
             Mean action noise std: 3.14
          Mean value_function loss: 180.3765
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 55.4365
                       Mean reward: 838.24
               Mean episode length: 224.96
    Episode_Reward/reaching_object: 1.2072
     Episode_Reward/lifting_object: 171.8753
      Episode_Reward/object_height: 0.0716
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 172425216
                    Iteration time: 1.97s
                      Time elapsed: 01:02:22
                               ETA: 00:08:47

################################################################################
                     [1m Learning iteration 1754/2000 [0m                     

                       Computation: 51046 steps/s (collection: 1.821s, learning 0.105s)
             Mean action noise std: 3.15
          Mean value_function loss: 248.2475
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 55.4460
                       Mean reward: 902.91
               Mean episode length: 239.58
    Episode_Reward/reaching_object: 1.2251
     Episode_Reward/lifting_object: 174.5165
      Episode_Reward/object_height: 0.0728
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 172523520
                    Iteration time: 1.93s
                      Time elapsed: 01:02:24
                               ETA: 00:08:44

################################################################################
                     [1m Learning iteration 1755/2000 [0m                     

                       Computation: 50765 steps/s (collection: 1.843s, learning 0.093s)
             Mean action noise std: 3.15
          Mean value_function loss: 342.4588
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 55.4540
                       Mean reward: 852.27
               Mean episode length: 227.91
    Episode_Reward/reaching_object: 1.2276
     Episode_Reward/lifting_object: 174.8235
      Episode_Reward/object_height: 0.0731
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 172621824
                    Iteration time: 1.94s
                      Time elapsed: 01:02:26
                               ETA: 00:08:42

################################################################################
                     [1m Learning iteration 1756/2000 [0m                     

                       Computation: 50144 steps/s (collection: 1.856s, learning 0.104s)
             Mean action noise std: 3.15
          Mean value_function loss: 140.0986
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 55.4611
                       Mean reward: 848.80
               Mean episode length: 228.77
    Episode_Reward/reaching_object: 1.2184
     Episode_Reward/lifting_object: 173.5818
      Episode_Reward/object_height: 0.0726
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 172720128
                    Iteration time: 1.96s
                      Time elapsed: 01:02:28
                               ETA: 00:08:40

################################################################################
                     [1m Learning iteration 1757/2000 [0m                     

                       Computation: 51278 steps/s (collection: 1.825s, learning 0.092s)
             Mean action noise std: 3.15
          Mean value_function loss: 121.8079
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 55.4739
                       Mean reward: 897.35
               Mean episode length: 238.84
    Episode_Reward/reaching_object: 1.2087
     Episode_Reward/lifting_object: 172.0051
      Episode_Reward/object_height: 0.0719
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 172818432
                    Iteration time: 1.92s
                      Time elapsed: 01:02:30
                               ETA: 00:08:38

################################################################################
                     [1m Learning iteration 1758/2000 [0m                     

                       Computation: 51052 steps/s (collection: 1.830s, learning 0.096s)
             Mean action noise std: 3.15
          Mean value_function loss: 100.8211
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 55.4873
                       Mean reward: 867.13
               Mean episode length: 230.37
    Episode_Reward/reaching_object: 1.2175
     Episode_Reward/lifting_object: 173.9586
      Episode_Reward/object_height: 0.0725
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 172916736
                    Iteration time: 1.93s
                      Time elapsed: 01:02:32
                               ETA: 00:08:36

################################################################################
                     [1m Learning iteration 1759/2000 [0m                     

                       Computation: 49975 steps/s (collection: 1.861s, learning 0.106s)
             Mean action noise std: 3.15
          Mean value_function loss: 105.0923
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.4980
                       Mean reward: 860.55
               Mean episode length: 228.20
    Episode_Reward/reaching_object: 1.2270
     Episode_Reward/lifting_object: 174.7117
      Episode_Reward/object_height: 0.0723
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 173015040
                    Iteration time: 1.97s
                      Time elapsed: 01:02:34
                               ETA: 00:08:34

################################################################################
                     [1m Learning iteration 1760/2000 [0m                     

                       Computation: 49714 steps/s (collection: 1.868s, learning 0.109s)
             Mean action noise std: 3.16
          Mean value_function loss: 130.0638
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 55.5084
                       Mean reward: 854.60
               Mean episode length: 226.96
    Episode_Reward/reaching_object: 1.2143
     Episode_Reward/lifting_object: 173.2715
      Episode_Reward/object_height: 0.0707
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 173113344
                    Iteration time: 1.98s
                      Time elapsed: 01:02:36
                               ETA: 00:08:31

################################################################################
                     [1m Learning iteration 1761/2000 [0m                     

                       Computation: 49850 steps/s (collection: 1.863s, learning 0.109s)
             Mean action noise std: 3.16
          Mean value_function loss: 114.7667
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 55.5197
                       Mean reward: 883.93
               Mean episode length: 235.04
    Episode_Reward/reaching_object: 1.2401
     Episode_Reward/lifting_object: 176.9255
      Episode_Reward/object_height: 0.0717
        Episode_Reward/action_rate: -0.0521
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 173211648
                    Iteration time: 1.97s
                      Time elapsed: 01:02:38
                               ETA: 00:08:29

################################################################################
                     [1m Learning iteration 1762/2000 [0m                     

                       Computation: 50128 steps/s (collection: 1.852s, learning 0.109s)
             Mean action noise std: 3.16
          Mean value_function loss: 93.4203
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 55.5288
                       Mean reward: 901.02
               Mean episode length: 238.09
    Episode_Reward/reaching_object: 1.2484
     Episode_Reward/lifting_object: 178.0561
      Episode_Reward/object_height: 0.0719
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 173309952
                    Iteration time: 1.96s
                      Time elapsed: 01:02:40
                               ETA: 00:08:27

################################################################################
                     [1m Learning iteration 1763/2000 [0m                     

                       Computation: 50542 steps/s (collection: 1.806s, learning 0.139s)
             Mean action noise std: 3.16
          Mean value_function loss: 114.1525
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 55.5352
                       Mean reward: 850.32
               Mean episode length: 227.40
    Episode_Reward/reaching_object: 1.2146
     Episode_Reward/lifting_object: 173.0467
      Episode_Reward/object_height: 0.0693
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 173408256
                    Iteration time: 1.94s
                      Time elapsed: 01:02:42
                               ETA: 00:08:25

################################################################################
                     [1m Learning iteration 1764/2000 [0m                     

                       Computation: 52551 steps/s (collection: 1.779s, learning 0.092s)
             Mean action noise std: 3.16
          Mean value_function loss: 113.2617
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.5406
                       Mean reward: 887.04
               Mean episode length: 236.11
    Episode_Reward/reaching_object: 1.2042
     Episode_Reward/lifting_object: 171.0474
      Episode_Reward/object_height: 0.0682
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 173506560
                    Iteration time: 1.87s
                      Time elapsed: 01:02:44
                               ETA: 00:08:23

################################################################################
                     [1m Learning iteration 1765/2000 [0m                     

                       Computation: 51332 steps/s (collection: 1.826s, learning 0.089s)
             Mean action noise std: 3.16
          Mean value_function loss: 128.3803
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 55.5474
                       Mean reward: 820.28
               Mean episode length: 219.35
    Episode_Reward/reaching_object: 1.2091
     Episode_Reward/lifting_object: 172.4697
      Episode_Reward/object_height: 0.0687
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 173604864
                    Iteration time: 1.92s
                      Time elapsed: 01:02:46
                               ETA: 00:08:21

################################################################################
                     [1m Learning iteration 1766/2000 [0m                     

                       Computation: 51486 steps/s (collection: 1.813s, learning 0.097s)
             Mean action noise std: 3.16
          Mean value_function loss: 118.8253
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 55.5512
                       Mean reward: 867.31
               Mean episode length: 231.92
    Episode_Reward/reaching_object: 1.2166
     Episode_Reward/lifting_object: 172.8647
      Episode_Reward/object_height: 0.0691
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173703168
                    Iteration time: 1.91s
                      Time elapsed: 01:02:48
                               ETA: 00:08:19

################################################################################
                     [1m Learning iteration 1767/2000 [0m                     

                       Computation: 49451 steps/s (collection: 1.885s, learning 0.103s)
             Mean action noise std: 3.16
          Mean value_function loss: 128.0034
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.5583
                       Mean reward: 891.51
               Mean episode length: 238.02
    Episode_Reward/reaching_object: 1.2003
     Episode_Reward/lifting_object: 170.6001
      Episode_Reward/object_height: 0.0678
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 173801472
                    Iteration time: 1.99s
                      Time elapsed: 01:02:50
                               ETA: 00:08:16

################################################################################
                     [1m Learning iteration 1768/2000 [0m                     

                       Computation: 51780 steps/s (collection: 1.794s, learning 0.104s)
             Mean action noise std: 3.16
          Mean value_function loss: 142.7701
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 55.5674
                       Mean reward: 871.49
               Mean episode length: 231.83
    Episode_Reward/reaching_object: 1.1914
     Episode_Reward/lifting_object: 169.0652
      Episode_Reward/object_height: 0.0671
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 173899776
                    Iteration time: 1.90s
                      Time elapsed: 01:02:52
                               ETA: 00:08:14

################################################################################
                     [1m Learning iteration 1769/2000 [0m                     

                       Computation: 51889 steps/s (collection: 1.807s, learning 0.088s)
             Mean action noise std: 3.16
          Mean value_function loss: 108.6903
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 55.5758
                       Mean reward: 880.11
               Mean episode length: 233.18
    Episode_Reward/reaching_object: 1.2136
     Episode_Reward/lifting_object: 172.6555
      Episode_Reward/object_height: 0.0686
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 173998080
                    Iteration time: 1.89s
                      Time elapsed: 01:02:53
                               ETA: 00:08:12

################################################################################
                     [1m Learning iteration 1770/2000 [0m                     

                       Computation: 50765 steps/s (collection: 1.801s, learning 0.135s)
             Mean action noise std: 3.17
          Mean value_function loss: 126.0426
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.5835
                       Mean reward: 875.20
               Mean episode length: 233.81
    Episode_Reward/reaching_object: 1.2136
     Episode_Reward/lifting_object: 172.3902
      Episode_Reward/object_height: 0.0686
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 174096384
                    Iteration time: 1.94s
                      Time elapsed: 01:02:55
                               ETA: 00:08:10

################################################################################
                     [1m Learning iteration 1771/2000 [0m                     

                       Computation: 51308 steps/s (collection: 1.818s, learning 0.098s)
             Mean action noise std: 3.17
          Mean value_function loss: 130.0366
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 55.5886
                       Mean reward: 905.19
               Mean episode length: 239.47
    Episode_Reward/reaching_object: 1.2165
     Episode_Reward/lifting_object: 173.2939
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 174194688
                    Iteration time: 1.92s
                      Time elapsed: 01:02:57
                               ETA: 00:08:08

################################################################################
                     [1m Learning iteration 1772/2000 [0m                     

                       Computation: 50262 steps/s (collection: 1.852s, learning 0.104s)
             Mean action noise std: 3.17
          Mean value_function loss: 178.5222
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.5983
                       Mean reward: 876.49
               Mean episode length: 233.76
    Episode_Reward/reaching_object: 1.2153
     Episode_Reward/lifting_object: 172.9722
      Episode_Reward/object_height: 0.0685
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 174292992
                    Iteration time: 1.96s
                      Time elapsed: 01:02:59
                               ETA: 00:08:06

################################################################################
                     [1m Learning iteration 1773/2000 [0m                     

                       Computation: 51823 steps/s (collection: 1.809s, learning 0.088s)
             Mean action noise std: 3.17
          Mean value_function loss: 115.3313
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.6126
                       Mean reward: 866.65
               Mean episode length: 231.57
    Episode_Reward/reaching_object: 1.2448
     Episode_Reward/lifting_object: 177.7792
      Episode_Reward/object_height: 0.0709
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 174391296
                    Iteration time: 1.90s
                      Time elapsed: 01:03:01
                               ETA: 00:08:03

################################################################################
                     [1m Learning iteration 1774/2000 [0m                     

                       Computation: 49781 steps/s (collection: 1.853s, learning 0.122s)
             Mean action noise std: 3.17
          Mean value_function loss: 119.5218
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 55.6221
                       Mean reward: 863.10
               Mean episode length: 230.12
    Episode_Reward/reaching_object: 1.2244
     Episode_Reward/lifting_object: 174.7280
      Episode_Reward/object_height: 0.0699
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 174489600
                    Iteration time: 1.97s
                      Time elapsed: 01:03:03
                               ETA: 00:08:01

################################################################################
                     [1m Learning iteration 1775/2000 [0m                     

                       Computation: 50058 steps/s (collection: 1.834s, learning 0.130s)
             Mean action noise std: 3.17
          Mean value_function loss: 149.0372
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 55.6325
                       Mean reward: 859.02
               Mean episode length: 228.97
    Episode_Reward/reaching_object: 1.2172
     Episode_Reward/lifting_object: 173.7051
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 174587904
                    Iteration time: 1.96s
                      Time elapsed: 01:03:05
                               ETA: 00:07:59

################################################################################
                     [1m Learning iteration 1776/2000 [0m                     

                       Computation: 50919 steps/s (collection: 1.827s, learning 0.104s)
             Mean action noise std: 3.17
          Mean value_function loss: 170.1439
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 55.6457
                       Mean reward: 882.81
               Mean episode length: 233.94
    Episode_Reward/reaching_object: 1.2001
     Episode_Reward/lifting_object: 171.4683
      Episode_Reward/object_height: 0.0678
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 174686208
                    Iteration time: 1.93s
                      Time elapsed: 01:03:07
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1777/2000 [0m                     

                       Computation: 46618 steps/s (collection: 1.905s, learning 0.204s)
             Mean action noise std: 3.18
          Mean value_function loss: 160.4879
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 55.6570
                       Mean reward: 856.71
               Mean episode length: 228.46
    Episode_Reward/reaching_object: 1.2046
     Episode_Reward/lifting_object: 172.4169
      Episode_Reward/object_height: 0.0682
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 174784512
                    Iteration time: 2.11s
                      Time elapsed: 01:03:09
                               ETA: 00:07:55

################################################################################
                     [1m Learning iteration 1778/2000 [0m                     

                       Computation: 50861 steps/s (collection: 1.788s, learning 0.145s)
             Mean action noise std: 3.18
          Mean value_function loss: 213.2444
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.6641
                       Mean reward: 834.47
               Mean episode length: 223.29
    Episode_Reward/reaching_object: 1.1630
     Episode_Reward/lifting_object: 165.8272
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 174882816
                    Iteration time: 1.93s
                      Time elapsed: 01:03:11
                               ETA: 00:07:53

################################################################################
                     [1m Learning iteration 1779/2000 [0m                     

                       Computation: 49771 steps/s (collection: 1.869s, learning 0.106s)
             Mean action noise std: 3.18
          Mean value_function loss: 198.2987
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.6750
                       Mean reward: 799.30
               Mean episode length: 213.76
    Episode_Reward/reaching_object: 1.1523
     Episode_Reward/lifting_object: 164.9174
      Episode_Reward/object_height: 0.0651
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 174981120
                    Iteration time: 1.98s
                      Time elapsed: 01:03:13
                               ETA: 00:07:50

################################################################################
                     [1m Learning iteration 1780/2000 [0m                     

                       Computation: 47020 steps/s (collection: 1.953s, learning 0.138s)
             Mean action noise std: 3.18
          Mean value_function loss: 231.6909
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 55.6889
                       Mean reward: 886.38
               Mean episode length: 234.69
    Episode_Reward/reaching_object: 1.1804
     Episode_Reward/lifting_object: 168.7344
      Episode_Reward/object_height: 0.0670
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 175079424
                    Iteration time: 2.09s
                      Time elapsed: 01:03:15
                               ETA: 00:07:48

################################################################################
                     [1m Learning iteration 1781/2000 [0m                     

                       Computation: 50041 steps/s (collection: 1.877s, learning 0.088s)
             Mean action noise std: 3.18
          Mean value_function loss: 230.3712
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 55.6956
                       Mean reward: 860.01
               Mean episode length: 228.48
    Episode_Reward/reaching_object: 1.1778
     Episode_Reward/lifting_object: 169.1158
      Episode_Reward/object_height: 0.0667
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 175177728
                    Iteration time: 1.96s
                      Time elapsed: 01:03:17
                               ETA: 00:07:46

################################################################################
                     [1m Learning iteration 1782/2000 [0m                     

                       Computation: 50807 steps/s (collection: 1.819s, learning 0.116s)
             Mean action noise std: 3.18
          Mean value_function loss: 226.1228
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 55.7040
                       Mean reward: 822.83
               Mean episode length: 219.27
    Episode_Reward/reaching_object: 1.1514
     Episode_Reward/lifting_object: 165.6911
      Episode_Reward/object_height: 0.0651
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 175276032
                    Iteration time: 1.93s
                      Time elapsed: 01:03:19
                               ETA: 00:07:44

################################################################################
                     [1m Learning iteration 1783/2000 [0m                     

                       Computation: 50233 steps/s (collection: 1.812s, learning 0.145s)
             Mean action noise std: 3.18
          Mean value_function loss: 240.2903
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 55.7127
                       Mean reward: 826.34
               Mean episode length: 220.68
    Episode_Reward/reaching_object: 1.1503
     Episode_Reward/lifting_object: 165.2745
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 175374336
                    Iteration time: 1.96s
                      Time elapsed: 01:03:21
                               ETA: 00:07:42

################################################################################
                     [1m Learning iteration 1784/2000 [0m                     

                       Computation: 49032 steps/s (collection: 1.810s, learning 0.195s)
             Mean action noise std: 3.18
          Mean value_function loss: 236.0221
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 55.7202
                       Mean reward: 865.48
               Mean episode length: 230.16
    Episode_Reward/reaching_object: 1.1636
     Episode_Reward/lifting_object: 167.6594
      Episode_Reward/object_height: 0.0661
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 175472640
                    Iteration time: 2.00s
                      Time elapsed: 01:03:23
                               ETA: 00:07:40

################################################################################
                     [1m Learning iteration 1785/2000 [0m                     

                       Computation: 47970 steps/s (collection: 1.918s, learning 0.132s)
             Mean action noise std: 3.19
          Mean value_function loss: 200.6563
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 55.7262
                       Mean reward: 860.75
               Mean episode length: 228.97
    Episode_Reward/reaching_object: 1.1811
     Episode_Reward/lifting_object: 169.9318
      Episode_Reward/object_height: 0.0671
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 175570944
                    Iteration time: 2.05s
                      Time elapsed: 01:03:25
                               ETA: 00:07:38

################################################################################
                     [1m Learning iteration 1786/2000 [0m                     

                       Computation: 52655 steps/s (collection: 1.777s, learning 0.090s)
             Mean action noise std: 3.19
          Mean value_function loss: 200.8207
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 55.7331
                       Mean reward: 859.95
               Mean episode length: 228.35
    Episode_Reward/reaching_object: 1.1940
     Episode_Reward/lifting_object: 172.4083
      Episode_Reward/object_height: 0.0679
        Episode_Reward/action_rate: -0.0521
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 175669248
                    Iteration time: 1.87s
                      Time elapsed: 01:03:27
                               ETA: 00:07:35

################################################################################
                     [1m Learning iteration 1787/2000 [0m                     

                       Computation: 49202 steps/s (collection: 1.823s, learning 0.175s)
             Mean action noise std: 3.19
          Mean value_function loss: 228.1608
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.7416
                       Mean reward: 822.07
               Mean episode length: 219.97
    Episode_Reward/reaching_object: 1.1619
     Episode_Reward/lifting_object: 166.9457
      Episode_Reward/object_height: 0.0656
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 175767552
                    Iteration time: 2.00s
                      Time elapsed: 01:03:29
                               ETA: 00:07:33

################################################################################
                     [1m Learning iteration 1788/2000 [0m                     

                       Computation: 49808 steps/s (collection: 1.882s, learning 0.092s)
             Mean action noise std: 3.19
          Mean value_function loss: 166.2981
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 55.7527
                       Mean reward: 878.09
               Mean episode length: 233.87
    Episode_Reward/reaching_object: 1.1898
     Episode_Reward/lifting_object: 171.3300
      Episode_Reward/object_height: 0.0675
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 175865856
                    Iteration time: 1.97s
                      Time elapsed: 01:03:31
                               ETA: 00:07:31

################################################################################
                     [1m Learning iteration 1789/2000 [0m                     

                       Computation: 50627 steps/s (collection: 1.855s, learning 0.087s)
             Mean action noise std: 3.19
          Mean value_function loss: 156.5871
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 55.7617
                       Mean reward: 869.27
               Mean episode length: 232.10
    Episode_Reward/reaching_object: 1.1900
     Episode_Reward/lifting_object: 171.2695
      Episode_Reward/object_height: 0.0673
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 175964160
                    Iteration time: 1.94s
                      Time elapsed: 01:03:33
                               ETA: 00:07:29

################################################################################
                     [1m Learning iteration 1790/2000 [0m                     

                       Computation: 51075 steps/s (collection: 1.824s, learning 0.100s)
             Mean action noise std: 3.19
          Mean value_function loss: 148.7965
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 55.7677
                       Mean reward: 866.25
               Mean episode length: 229.95
    Episode_Reward/reaching_object: 1.1907
     Episode_Reward/lifting_object: 171.4812
      Episode_Reward/object_height: 0.0670
        Episode_Reward/action_rate: -0.0523
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 176062464
                    Iteration time: 1.92s
                      Time elapsed: 01:03:35
                               ETA: 00:07:27

################################################################################
                     [1m Learning iteration 1791/2000 [0m                     

                       Computation: 50121 steps/s (collection: 1.853s, learning 0.108s)
             Mean action noise std: 3.19
          Mean value_function loss: 128.7799
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 55.7750
                       Mean reward: 895.97
               Mean episode length: 237.36
    Episode_Reward/reaching_object: 1.1889
     Episode_Reward/lifting_object: 171.0491
      Episode_Reward/object_height: 0.0671
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 176160768
                    Iteration time: 1.96s
                      Time elapsed: 01:03:37
                               ETA: 00:07:25

################################################################################
                     [1m Learning iteration 1792/2000 [0m                     

                       Computation: 50774 steps/s (collection: 1.834s, learning 0.102s)
             Mean action noise std: 3.19
          Mean value_function loss: 179.1026
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 55.7803
                       Mean reward: 809.24
               Mean episode length: 216.80
    Episode_Reward/reaching_object: 1.1849
     Episode_Reward/lifting_object: 170.4754
      Episode_Reward/object_height: 0.0665
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 176259072
                    Iteration time: 1.94s
                      Time elapsed: 01:03:39
                               ETA: 00:07:23

################################################################################
                     [1m Learning iteration 1793/2000 [0m                     

                       Computation: 49220 steps/s (collection: 1.899s, learning 0.099s)
             Mean action noise std: 3.19
          Mean value_function loss: 109.3204
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 55.7860
                       Mean reward: 865.03
               Mean episode length: 229.52
    Episode_Reward/reaching_object: 1.1884
     Episode_Reward/lifting_object: 171.0793
      Episode_Reward/object_height: 0.0672
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 176357376
                    Iteration time: 2.00s
                      Time elapsed: 01:03:41
                               ETA: 00:07:20

################################################################################
                     [1m Learning iteration 1794/2000 [0m                     

                       Computation: 51170 steps/s (collection: 1.821s, learning 0.100s)
             Mean action noise std: 3.20
          Mean value_function loss: 116.2544
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 55.7947
                       Mean reward: 852.38
               Mean episode length: 227.84
    Episode_Reward/reaching_object: 1.1780
     Episode_Reward/lifting_object: 168.9088
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 176455680
                    Iteration time: 1.92s
                      Time elapsed: 01:03:43
                               ETA: 00:07:18

################################################################################
                     [1m Learning iteration 1795/2000 [0m                     

                       Computation: 47015 steps/s (collection: 1.945s, learning 0.146s)
             Mean action noise std: 3.20
          Mean value_function loss: 110.2720
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 55.8078
                       Mean reward: 868.13
               Mean episode length: 230.88
    Episode_Reward/reaching_object: 1.2005
     Episode_Reward/lifting_object: 172.2280
      Episode_Reward/object_height: 0.0675
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 176553984
                    Iteration time: 2.09s
                      Time elapsed: 01:03:45
                               ETA: 00:07:16

################################################################################
                     [1m Learning iteration 1796/2000 [0m                     

                       Computation: 51170 steps/s (collection: 1.815s, learning 0.106s)
             Mean action noise std: 3.20
          Mean value_function loss: 114.5430
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.8159
                       Mean reward: 873.73
               Mean episode length: 232.88
    Episode_Reward/reaching_object: 1.2098
     Episode_Reward/lifting_object: 173.6063
      Episode_Reward/object_height: 0.0682
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 176652288
                    Iteration time: 1.92s
                      Time elapsed: 01:03:47
                               ETA: 00:07:14

################################################################################
                     [1m Learning iteration 1797/2000 [0m                     

                       Computation: 50735 steps/s (collection: 1.824s, learning 0.114s)
             Mean action noise std: 3.20
          Mean value_function loss: 112.3430
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 55.8238
                       Mean reward: 832.27
               Mean episode length: 221.08
    Episode_Reward/reaching_object: 1.1777
     Episode_Reward/lifting_object: 168.7239
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 176750592
                    Iteration time: 1.94s
                      Time elapsed: 01:03:48
                               ETA: 00:07:12

################################################################################
                     [1m Learning iteration 1798/2000 [0m                     

                       Computation: 50040 steps/s (collection: 1.875s, learning 0.090s)
             Mean action noise std: 3.20
          Mean value_function loss: 117.3562
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 55.8322
                       Mean reward: 859.94
               Mean episode length: 229.09
    Episode_Reward/reaching_object: 1.1986
     Episode_Reward/lifting_object: 171.5938
      Episode_Reward/object_height: 0.0677
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 176848896
                    Iteration time: 1.96s
                      Time elapsed: 01:03:50
                               ETA: 00:07:10

################################################################################
                     [1m Learning iteration 1799/2000 [0m                     

                       Computation: 49053 steps/s (collection: 1.904s, learning 0.100s)
             Mean action noise std: 3.20
          Mean value_function loss: 146.5293
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 55.8405
                       Mean reward: 873.81
               Mean episode length: 232.09
    Episode_Reward/reaching_object: 1.1935
     Episode_Reward/lifting_object: 171.1910
      Episode_Reward/object_height: 0.0673
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 176947200
                    Iteration time: 2.00s
                      Time elapsed: 01:03:52
                               ETA: 00:07:08

################################################################################
                     [1m Learning iteration 1800/2000 [0m                     

                       Computation: 50076 steps/s (collection: 1.874s, learning 0.089s)
             Mean action noise std: 3.20
          Mean value_function loss: 124.7228
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 55.8467
                       Mean reward: 853.39
               Mean episode length: 228.22
    Episode_Reward/reaching_object: 1.1962
     Episode_Reward/lifting_object: 171.0925
      Episode_Reward/object_height: 0.0674
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 177045504
                    Iteration time: 1.96s
                      Time elapsed: 01:03:54
                               ETA: 00:07:05

################################################################################
                     [1m Learning iteration 1801/2000 [0m                     

                       Computation: 49982 steps/s (collection: 1.868s, learning 0.099s)
             Mean action noise std: 3.20
          Mean value_function loss: 124.2892
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.8504
                       Mean reward: 805.51
               Mean episode length: 216.69
    Episode_Reward/reaching_object: 1.1845
     Episode_Reward/lifting_object: 170.0535
      Episode_Reward/object_height: 0.0670
        Episode_Reward/action_rate: -0.0523
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 177143808
                    Iteration time: 1.97s
                      Time elapsed: 01:03:56
                               ETA: 00:07:03

################################################################################
                     [1m Learning iteration 1802/2000 [0m                     

                       Computation: 50695 steps/s (collection: 1.826s, learning 0.114s)
             Mean action noise std: 3.20
          Mean value_function loss: 138.2577
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 55.8590
                       Mean reward: 877.29
               Mean episode length: 232.85
    Episode_Reward/reaching_object: 1.2177
     Episode_Reward/lifting_object: 175.2728
      Episode_Reward/object_height: 0.0695
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 177242112
                    Iteration time: 1.94s
                      Time elapsed: 01:03:58
                               ETA: 00:07:01

################################################################################
                     [1m Learning iteration 1803/2000 [0m                     

                       Computation: 49124 steps/s (collection: 1.890s, learning 0.112s)
             Mean action noise std: 3.21
          Mean value_function loss: 222.3088
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.8672
                       Mean reward: 870.11
               Mean episode length: 230.60
    Episode_Reward/reaching_object: 1.1926
     Episode_Reward/lifting_object: 171.4969
      Episode_Reward/object_height: 0.0684
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 177340416
                    Iteration time: 2.00s
                      Time elapsed: 01:04:00
                               ETA: 00:06:59

################################################################################
                     [1m Learning iteration 1804/2000 [0m                     

                       Computation: 50817 steps/s (collection: 1.778s, learning 0.157s)
             Mean action noise std: 3.21
          Mean value_function loss: 136.0050
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.8767
                       Mean reward: 850.42
               Mean episode length: 226.58
    Episode_Reward/reaching_object: 1.1537
     Episode_Reward/lifting_object: 166.0870
      Episode_Reward/object_height: 0.0660
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 177438720
                    Iteration time: 1.93s
                      Time elapsed: 01:04:02
                               ETA: 00:06:57

################################################################################
                     [1m Learning iteration 1805/2000 [0m                     

                       Computation: 50128 steps/s (collection: 1.823s, learning 0.138s)
             Mean action noise std: 3.21
          Mean value_function loss: 126.0471
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 55.8841
                       Mean reward: 895.19
               Mean episode length: 237.39
    Episode_Reward/reaching_object: 1.1763
     Episode_Reward/lifting_object: 169.7387
      Episode_Reward/object_height: 0.0679
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 177537024
                    Iteration time: 1.96s
                      Time elapsed: 01:04:04
                               ETA: 00:06:55

################################################################################
                     [1m Learning iteration 1806/2000 [0m                     

                       Computation: 50923 steps/s (collection: 1.817s, learning 0.113s)
             Mean action noise std: 3.21
          Mean value_function loss: 155.9620
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 55.8912
                       Mean reward: 843.88
               Mean episode length: 225.64
    Episode_Reward/reaching_object: 1.1714
     Episode_Reward/lifting_object: 168.9476
      Episode_Reward/object_height: 0.0678
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 177635328
                    Iteration time: 1.93s
                      Time elapsed: 01:04:06
                               ETA: 00:06:52

################################################################################
                     [1m Learning iteration 1807/2000 [0m                     

                       Computation: 50775 steps/s (collection: 1.817s, learning 0.119s)
             Mean action noise std: 3.21
          Mean value_function loss: 146.6491
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.8990
                       Mean reward: 818.62
               Mean episode length: 220.58
    Episode_Reward/reaching_object: 1.1563
     Episode_Reward/lifting_object: 166.9410
      Episode_Reward/object_height: 0.0670
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 177733632
                    Iteration time: 1.94s
                      Time elapsed: 01:04:08
                               ETA: 00:06:50

################################################################################
                     [1m Learning iteration 1808/2000 [0m                     

                       Computation: 48631 steps/s (collection: 1.909s, learning 0.112s)
             Mean action noise std: 3.21
          Mean value_function loss: 180.4267
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.9074
                       Mean reward: 821.21
               Mean episode length: 219.50
    Episode_Reward/reaching_object: 1.1744
     Episode_Reward/lifting_object: 169.9093
      Episode_Reward/object_height: 0.0687
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 177831936
                    Iteration time: 2.02s
                      Time elapsed: 01:04:10
                               ETA: 00:06:48

################################################################################
                     [1m Learning iteration 1809/2000 [0m                     

                       Computation: 50087 steps/s (collection: 1.863s, learning 0.100s)
             Mean action noise std: 3.21
          Mean value_function loss: 159.5335
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 55.9177
                       Mean reward: 847.55
               Mean episode length: 226.45
    Episode_Reward/reaching_object: 1.1614
     Episode_Reward/lifting_object: 167.3782
      Episode_Reward/object_height: 0.0685
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 177930240
                    Iteration time: 1.96s
                      Time elapsed: 01:04:12
                               ETA: 00:06:46

################################################################################
                     [1m Learning iteration 1810/2000 [0m                     

                       Computation: 49838 steps/s (collection: 1.854s, learning 0.118s)
             Mean action noise std: 3.21
          Mean value_function loss: 182.9457
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 55.9257
                       Mean reward: 849.64
               Mean episode length: 226.45
    Episode_Reward/reaching_object: 1.1572
     Episode_Reward/lifting_object: 167.0980
      Episode_Reward/object_height: 0.0685
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 178028544
                    Iteration time: 1.97s
                      Time elapsed: 01:04:14
                               ETA: 00:06:44

################################################################################
                     [1m Learning iteration 1811/2000 [0m                     

                       Computation: 46698 steps/s (collection: 1.897s, learning 0.208s)
             Mean action noise std: 3.21
          Mean value_function loss: 190.3017
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 55.9305
                       Mean reward: 819.64
               Mean episode length: 220.88
    Episode_Reward/reaching_object: 1.1344
     Episode_Reward/lifting_object: 162.8083
      Episode_Reward/object_height: 0.0673
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 178126848
                    Iteration time: 2.11s
                      Time elapsed: 01:04:16
                               ETA: 00:06:42

################################################################################
                     [1m Learning iteration 1812/2000 [0m                     

                       Computation: 49944 steps/s (collection: 1.864s, learning 0.104s)
             Mean action noise std: 3.22
          Mean value_function loss: 128.4273
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 55.9375
                       Mean reward: 896.76
               Mean episode length: 237.40
    Episode_Reward/reaching_object: 1.1865
     Episode_Reward/lifting_object: 171.4911
      Episode_Reward/object_height: 0.0710
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 178225152
                    Iteration time: 1.97s
                      Time elapsed: 01:04:18
                               ETA: 00:06:40

################################################################################
                     [1m Learning iteration 1813/2000 [0m                     

                       Computation: 51668 steps/s (collection: 1.799s, learning 0.104s)
             Mean action noise std: 3.22
          Mean value_function loss: 180.5501
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.9456
                       Mean reward: 886.82
               Mean episode length: 235.42
    Episode_Reward/reaching_object: 1.1806
     Episode_Reward/lifting_object: 170.4716
      Episode_Reward/object_height: 0.0708
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 178323456
                    Iteration time: 1.90s
                      Time elapsed: 01:04:20
                               ETA: 00:06:37

################################################################################
                     [1m Learning iteration 1814/2000 [0m                     

                       Computation: 50839 steps/s (collection: 1.835s, learning 0.099s)
             Mean action noise std: 3.22
          Mean value_function loss: 137.0219
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 55.9562
                       Mean reward: 878.74
               Mean episode length: 233.51
    Episode_Reward/reaching_object: 1.1874
     Episode_Reward/lifting_object: 171.1794
      Episode_Reward/object_height: 0.0712
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 178421760
                    Iteration time: 1.93s
                      Time elapsed: 01:04:22
                               ETA: 00:06:35

################################################################################
                     [1m Learning iteration 1815/2000 [0m                     

                       Computation: 50858 steps/s (collection: 1.821s, learning 0.112s)
             Mean action noise std: 3.22
          Mean value_function loss: 121.5471
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 55.9676
                       Mean reward: 848.17
               Mean episode length: 226.31
    Episode_Reward/reaching_object: 1.1813
     Episode_Reward/lifting_object: 170.2587
      Episode_Reward/object_height: 0.0710
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 178520064
                    Iteration time: 1.93s
                      Time elapsed: 01:04:24
                               ETA: 00:06:33

################################################################################
                     [1m Learning iteration 1816/2000 [0m                     

                       Computation: 50610 steps/s (collection: 1.839s, learning 0.103s)
             Mean action noise std: 3.22
          Mean value_function loss: 150.6714
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.9804
                       Mean reward: 855.29
               Mean episode length: 227.67
    Episode_Reward/reaching_object: 1.1944
     Episode_Reward/lifting_object: 172.7375
      Episode_Reward/object_height: 0.0718
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 178618368
                    Iteration time: 1.94s
                      Time elapsed: 01:04:26
                               ETA: 00:06:31

################################################################################
                     [1m Learning iteration 1817/2000 [0m                     

                       Computation: 49988 steps/s (collection: 1.850s, learning 0.116s)
             Mean action noise std: 3.22
          Mean value_function loss: 125.0159
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 55.9908
                       Mean reward: 870.67
               Mean episode length: 232.07
    Episode_Reward/reaching_object: 1.2025
     Episode_Reward/lifting_object: 172.9762
      Episode_Reward/object_height: 0.0723
        Episode_Reward/action_rate: -0.0536
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 178716672
                    Iteration time: 1.97s
                      Time elapsed: 01:04:28
                               ETA: 00:06:29

################################################################################
                     [1m Learning iteration 1818/2000 [0m                     

                       Computation: 51494 steps/s (collection: 1.806s, learning 0.103s)
             Mean action noise std: 3.22
          Mean value_function loss: 121.8543
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.9957
                       Mean reward: 849.90
               Mean episode length: 227.47
    Episode_Reward/reaching_object: 1.1734
     Episode_Reward/lifting_object: 168.6268
      Episode_Reward/object_height: 0.0700
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 178814976
                    Iteration time: 1.91s
                      Time elapsed: 01:04:30
                               ETA: 00:06:27

################################################################################
                     [1m Learning iteration 1819/2000 [0m                     

                       Computation: 51220 steps/s (collection: 1.829s, learning 0.091s)
             Mean action noise std: 3.23
          Mean value_function loss: 132.0235
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 56.0027
                       Mean reward: 879.10
               Mean episode length: 233.49
    Episode_Reward/reaching_object: 1.2122
     Episode_Reward/lifting_object: 174.7434
      Episode_Reward/object_height: 0.0726
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 178913280
                    Iteration time: 1.92s
                      Time elapsed: 01:04:32
                               ETA: 00:06:25

################################################################################
                     [1m Learning iteration 1820/2000 [0m                     

                       Computation: 51439 steps/s (collection: 1.825s, learning 0.086s)
             Mean action noise std: 3.23
          Mean value_function loss: 121.6685
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 56.0063
                       Mean reward: 875.11
               Mean episode length: 233.08
    Episode_Reward/reaching_object: 1.1983
     Episode_Reward/lifting_object: 172.6870
      Episode_Reward/object_height: 0.0714
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 179011584
                    Iteration time: 1.91s
                      Time elapsed: 01:04:34
                               ETA: 00:06:22

################################################################################
                     [1m Learning iteration 1821/2000 [0m                     

                       Computation: 50648 steps/s (collection: 1.823s, learning 0.118s)
             Mean action noise std: 3.23
          Mean value_function loss: 148.9752
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.0100
                       Mean reward: 818.61
               Mean episode length: 218.51
    Episode_Reward/reaching_object: 1.1958
     Episode_Reward/lifting_object: 172.3270
      Episode_Reward/object_height: 0.0703
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 179109888
                    Iteration time: 1.94s
                      Time elapsed: 01:04:35
                               ETA: 00:06:20

################################################################################
                     [1m Learning iteration 1822/2000 [0m                     

                       Computation: 51119 steps/s (collection: 1.817s, learning 0.106s)
             Mean action noise std: 3.23
          Mean value_function loss: 139.9223
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 56.0178
                       Mean reward: 850.33
               Mean episode length: 227.63
    Episode_Reward/reaching_object: 1.1950
     Episode_Reward/lifting_object: 172.0096
      Episode_Reward/object_height: 0.0708
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 179208192
                    Iteration time: 1.92s
                      Time elapsed: 01:04:37
                               ETA: 00:06:18

################################################################################
                     [1m Learning iteration 1823/2000 [0m                     

                       Computation: 50734 steps/s (collection: 1.833s, learning 0.105s)
             Mean action noise std: 3.23
          Mean value_function loss: 162.4830
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 56.0313
                       Mean reward: 891.62
               Mean episode length: 236.22
    Episode_Reward/reaching_object: 1.1835
     Episode_Reward/lifting_object: 170.2733
      Episode_Reward/object_height: 0.0698
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 179306496
                    Iteration time: 1.94s
                      Time elapsed: 01:04:39
                               ETA: 00:06:16

################################################################################
                     [1m Learning iteration 1824/2000 [0m                     

                       Computation: 51222 steps/s (collection: 1.829s, learning 0.091s)
             Mean action noise std: 3.23
          Mean value_function loss: 150.6908
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 56.0375
                       Mean reward: 886.50
               Mean episode length: 234.82
    Episode_Reward/reaching_object: 1.1957
     Episode_Reward/lifting_object: 172.3847
      Episode_Reward/object_height: 0.0707
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 179404800
                    Iteration time: 1.92s
                      Time elapsed: 01:04:41
                               ETA: 00:06:14

################################################################################
                     [1m Learning iteration 1825/2000 [0m                     

                       Computation: 51563 steps/s (collection: 1.795s, learning 0.111s)
             Mean action noise std: 3.23
          Mean value_function loss: 128.2174
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.0448
                       Mean reward: 893.13
               Mean episode length: 237.40
    Episode_Reward/reaching_object: 1.1880
     Episode_Reward/lifting_object: 171.0744
      Episode_Reward/object_height: 0.0700
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 179503104
                    Iteration time: 1.91s
                      Time elapsed: 01:04:43
                               ETA: 00:06:12

################################################################################
                     [1m Learning iteration 1826/2000 [0m                     

                       Computation: 51365 steps/s (collection: 1.816s, learning 0.098s)
             Mean action noise std: 3.23
          Mean value_function loss: 125.5706
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 56.0543
                       Mean reward: 851.23
               Mean episode length: 227.17
    Episode_Reward/reaching_object: 1.1807
     Episode_Reward/lifting_object: 170.2585
      Episode_Reward/object_height: 0.0701
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 179601408
                    Iteration time: 1.91s
                      Time elapsed: 01:04:45
                               ETA: 00:06:10

################################################################################
                     [1m Learning iteration 1827/2000 [0m                     

                       Computation: 50721 steps/s (collection: 1.842s, learning 0.097s)
             Mean action noise std: 3.23
          Mean value_function loss: 122.6260
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 56.0618
                       Mean reward: 872.63
               Mean episode length: 231.06
    Episode_Reward/reaching_object: 1.2057
     Episode_Reward/lifting_object: 174.3509
      Episode_Reward/object_height: 0.0714
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 179699712
                    Iteration time: 1.94s
                      Time elapsed: 01:04:47
                               ETA: 00:06:07

################################################################################
                     [1m Learning iteration 1828/2000 [0m                     

                       Computation: 50739 steps/s (collection: 1.832s, learning 0.106s)
             Mean action noise std: 3.24
          Mean value_function loss: 136.6003
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 56.0685
                       Mean reward: 848.38
               Mean episode length: 227.19
    Episode_Reward/reaching_object: 1.1842
     Episode_Reward/lifting_object: 170.6028
      Episode_Reward/object_height: 0.0698
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 179798016
                    Iteration time: 1.94s
                      Time elapsed: 01:04:49
                               ETA: 00:06:05

################################################################################
                     [1m Learning iteration 1829/2000 [0m                     

                       Computation: 50860 steps/s (collection: 1.815s, learning 0.118s)
             Mean action noise std: 3.24
          Mean value_function loss: 142.4334
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 56.0748
                       Mean reward: 839.17
               Mean episode length: 224.86
    Episode_Reward/reaching_object: 1.1852
     Episode_Reward/lifting_object: 170.5804
      Episode_Reward/object_height: 0.0697
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 179896320
                    Iteration time: 1.93s
                      Time elapsed: 01:04:51
                               ETA: 00:06:03

################################################################################
                     [1m Learning iteration 1830/2000 [0m                     

                       Computation: 50757 steps/s (collection: 1.813s, learning 0.124s)
             Mean action noise std: 3.24
          Mean value_function loss: 134.5971
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 56.0825
                       Mean reward: 879.46
               Mean episode length: 233.91
    Episode_Reward/reaching_object: 1.2021
     Episode_Reward/lifting_object: 173.1095
      Episode_Reward/object_height: 0.0700
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 179994624
                    Iteration time: 1.94s
                      Time elapsed: 01:04:53
                               ETA: 00:06:01

################################################################################
                     [1m Learning iteration 1831/2000 [0m                     

                       Computation: 50990 steps/s (collection: 1.819s, learning 0.109s)
             Mean action noise std: 3.24
          Mean value_function loss: 108.9769
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 56.0909
                       Mean reward: 877.86
               Mean episode length: 233.58
    Episode_Reward/reaching_object: 1.1947
     Episode_Reward/lifting_object: 171.8927
      Episode_Reward/object_height: 0.0696
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 180092928
                    Iteration time: 1.93s
                      Time elapsed: 01:04:55
                               ETA: 00:05:59

################################################################################
                     [1m Learning iteration 1832/2000 [0m                     

                       Computation: 51053 steps/s (collection: 1.811s, learning 0.114s)
             Mean action noise std: 3.24
          Mean value_function loss: 117.1614
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 56.0953
                       Mean reward: 870.62
               Mean episode length: 233.01
    Episode_Reward/reaching_object: 1.1891
     Episode_Reward/lifting_object: 170.6695
      Episode_Reward/object_height: 0.0686
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 180191232
                    Iteration time: 1.93s
                      Time elapsed: 01:04:57
                               ETA: 00:05:57

################################################################################
                     [1m Learning iteration 1833/2000 [0m                     

                       Computation: 50347 steps/s (collection: 1.815s, learning 0.138s)
             Mean action noise std: 3.24
          Mean value_function loss: 126.1760
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 56.1005
                       Mean reward: 843.22
               Mean episode length: 225.04
    Episode_Reward/reaching_object: 1.1803
     Episode_Reward/lifting_object: 169.7900
      Episode_Reward/object_height: 0.0675
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 180289536
                    Iteration time: 1.95s
                      Time elapsed: 01:04:59
                               ETA: 00:05:55

################################################################################
                     [1m Learning iteration 1834/2000 [0m                     

                       Computation: 51253 steps/s (collection: 1.830s, learning 0.088s)
             Mean action noise std: 3.24
          Mean value_function loss: 132.3358
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 56.1056
                       Mean reward: 800.15
               Mean episode length: 214.86
    Episode_Reward/reaching_object: 1.1900
     Episode_Reward/lifting_object: 170.5800
      Episode_Reward/object_height: 0.0678
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 180387840
                    Iteration time: 1.92s
                      Time elapsed: 01:05:01
                               ETA: 00:05:52

################################################################################
                     [1m Learning iteration 1835/2000 [0m                     

                       Computation: 51053 steps/s (collection: 1.840s, learning 0.085s)
             Mean action noise std: 3.24
          Mean value_function loss: 134.2890
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 56.1136
                       Mean reward: 890.29
               Mean episode length: 236.69
    Episode_Reward/reaching_object: 1.2125
     Episode_Reward/lifting_object: 173.9948
      Episode_Reward/object_height: 0.0693
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 180486144
                    Iteration time: 1.93s
                      Time elapsed: 01:05:02
                               ETA: 00:05:50

################################################################################
                     [1m Learning iteration 1836/2000 [0m                     

                       Computation: 51547 steps/s (collection: 1.819s, learning 0.088s)
             Mean action noise std: 3.24
          Mean value_function loss: 165.1791
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.1210
                       Mean reward: 821.70
               Mean episode length: 220.25
    Episode_Reward/reaching_object: 1.1839
     Episode_Reward/lifting_object: 169.4245
      Episode_Reward/object_height: 0.0667
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 180584448
                    Iteration time: 1.91s
                      Time elapsed: 01:05:04
                               ETA: 00:05:48

################################################################################
                     [1m Learning iteration 1837/2000 [0m                     

                       Computation: 51024 steps/s (collection: 1.827s, learning 0.100s)
             Mean action noise std: 3.24
          Mean value_function loss: 114.1067
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.1302
                       Mean reward: 889.68
               Mean episode length: 235.67
    Episode_Reward/reaching_object: 1.1975
     Episode_Reward/lifting_object: 172.1054
      Episode_Reward/object_height: 0.0676
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 180682752
                    Iteration time: 1.93s
                      Time elapsed: 01:05:06
                               ETA: 00:05:46

################################################################################
                     [1m Learning iteration 1838/2000 [0m                     

                       Computation: 50654 steps/s (collection: 1.816s, learning 0.125s)
             Mean action noise std: 3.25
          Mean value_function loss: 129.4589
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 56.1394
                       Mean reward: 825.71
               Mean episode length: 220.46
    Episode_Reward/reaching_object: 1.1711
     Episode_Reward/lifting_object: 167.8901
      Episode_Reward/object_height: 0.0653
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 180781056
                    Iteration time: 1.94s
                      Time elapsed: 01:05:08
                               ETA: 00:05:44

################################################################################
                     [1m Learning iteration 1839/2000 [0m                     

                       Computation: 49217 steps/s (collection: 1.886s, learning 0.112s)
             Mean action noise std: 3.25
          Mean value_function loss: 135.5119
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.1476
                       Mean reward: 881.45
               Mean episode length: 233.81
    Episode_Reward/reaching_object: 1.1683
     Episode_Reward/lifting_object: 167.2989
      Episode_Reward/object_height: 0.0659
        Episode_Reward/action_rate: -0.0530
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 180879360
                    Iteration time: 2.00s
                      Time elapsed: 01:05:10
                               ETA: 00:05:42

################################################################################
                     [1m Learning iteration 1840/2000 [0m                     

                       Computation: 51742 steps/s (collection: 1.814s, learning 0.086s)
             Mean action noise std: 3.25
          Mean value_function loss: 133.5595
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 56.1552
                       Mean reward: 869.51
               Mean episode length: 230.59
    Episode_Reward/reaching_object: 1.1963
     Episode_Reward/lifting_object: 171.7747
      Episode_Reward/object_height: 0.0673
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 180977664
                    Iteration time: 1.90s
                      Time elapsed: 01:05:12
                               ETA: 00:05:40

################################################################################
                     [1m Learning iteration 1841/2000 [0m                     

                       Computation: 46415 steps/s (collection: 1.946s, learning 0.172s)
             Mean action noise std: 3.25
          Mean value_function loss: 133.4934
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 56.1687
                       Mean reward: 844.20
               Mean episode length: 225.31
    Episode_Reward/reaching_object: 1.1764
     Episode_Reward/lifting_object: 168.9681
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 181075968
                    Iteration time: 2.12s
                      Time elapsed: 01:05:14
                               ETA: 00:05:37

################################################################################
                     [1m Learning iteration 1842/2000 [0m                     

                       Computation: 46990 steps/s (collection: 1.967s, learning 0.125s)
             Mean action noise std: 3.25
          Mean value_function loss: 103.5524
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 56.1793
                       Mean reward: 866.42
               Mean episode length: 231.36
    Episode_Reward/reaching_object: 1.1803
     Episode_Reward/lifting_object: 169.2950
      Episode_Reward/object_height: 0.0666
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 181174272
                    Iteration time: 2.09s
                      Time elapsed: 01:05:16
                               ETA: 00:05:35

################################################################################
                     [1m Learning iteration 1843/2000 [0m                     

                       Computation: 49731 steps/s (collection: 1.877s, learning 0.100s)
             Mean action noise std: 3.25
          Mean value_function loss: 105.7738
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 56.1861
                       Mean reward: 857.92
               Mean episode length: 228.48
    Episode_Reward/reaching_object: 1.2073
     Episode_Reward/lifting_object: 173.4358
      Episode_Reward/object_height: 0.0682
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 181272576
                    Iteration time: 1.98s
                      Time elapsed: 01:05:18
                               ETA: 00:05:33

################################################################################
                     [1m Learning iteration 1844/2000 [0m                     

                       Computation: 49985 steps/s (collection: 1.855s, learning 0.112s)
             Mean action noise std: 3.25
          Mean value_function loss: 128.2008
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.1899
                       Mean reward: 878.29
               Mean episode length: 232.45
    Episode_Reward/reaching_object: 1.1798
     Episode_Reward/lifting_object: 169.3715
      Episode_Reward/object_height: 0.0665
        Episode_Reward/action_rate: -0.0536
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 181370880
                    Iteration time: 1.97s
                      Time elapsed: 01:05:20
                               ETA: 00:05:31

################################################################################
                     [1m Learning iteration 1845/2000 [0m                     

                       Computation: 49774 steps/s (collection: 1.870s, learning 0.105s)
             Mean action noise std: 3.25
          Mean value_function loss: 97.1573
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.1956
                       Mean reward: 900.96
               Mean episode length: 239.48
    Episode_Reward/reaching_object: 1.2187
     Episode_Reward/lifting_object: 175.1567
      Episode_Reward/object_height: 0.0692
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 181469184
                    Iteration time: 1.97s
                      Time elapsed: 01:05:22
                               ETA: 00:05:29

################################################################################
                     [1m Learning iteration 1846/2000 [0m                     

                       Computation: 49952 steps/s (collection: 1.869s, learning 0.099s)
             Mean action noise std: 3.26
          Mean value_function loss: 167.6916
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 56.2035
                       Mean reward: 805.99
               Mean episode length: 215.30
    Episode_Reward/reaching_object: 1.1440
     Episode_Reward/lifting_object: 164.1162
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 181567488
                    Iteration time: 1.97s
                      Time elapsed: 01:05:24
                               ETA: 00:05:27

################################################################################
                     [1m Learning iteration 1847/2000 [0m                     

                       Computation: 49057 steps/s (collection: 1.882s, learning 0.122s)
             Mean action noise std: 3.26
          Mean value_function loss: 129.6382
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 56.2152
                       Mean reward: 820.48
               Mean episode length: 219.95
    Episode_Reward/reaching_object: 1.1621
     Episode_Reward/lifting_object: 166.1880
      Episode_Reward/object_height: 0.0653
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 181665792
                    Iteration time: 2.00s
                      Time elapsed: 01:05:26
                               ETA: 00:05:25

################################################################################
                     [1m Learning iteration 1848/2000 [0m                     

                       Computation: 49592 steps/s (collection: 1.882s, learning 0.100s)
             Mean action noise std: 3.26
          Mean value_function loss: 116.2349
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 56.2246
                       Mean reward: 890.30
               Mean episode length: 236.37
    Episode_Reward/reaching_object: 1.1844
     Episode_Reward/lifting_object: 170.0837
      Episode_Reward/object_height: 0.0671
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 181764096
                    Iteration time: 1.98s
                      Time elapsed: 01:05:28
                               ETA: 00:05:22

################################################################################
                     [1m Learning iteration 1849/2000 [0m                     

                       Computation: 50286 steps/s (collection: 1.832s, learning 0.123s)
             Mean action noise std: 3.26
          Mean value_function loss: 109.5055
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.2318
                       Mean reward: 889.00
               Mean episode length: 235.93
    Episode_Reward/reaching_object: 1.1911
     Episode_Reward/lifting_object: 171.0031
      Episode_Reward/object_height: 0.0674
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 181862400
                    Iteration time: 1.95s
                      Time elapsed: 01:05:30
                               ETA: 00:05:20

################################################################################
                     [1m Learning iteration 1850/2000 [0m                     

                       Computation: 50332 steps/s (collection: 1.852s, learning 0.101s)
             Mean action noise std: 3.26
          Mean value_function loss: 134.6829
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.2426
                       Mean reward: 803.50
               Mean episode length: 215.65
    Episode_Reward/reaching_object: 1.1825
     Episode_Reward/lifting_object: 169.4196
      Episode_Reward/object_height: 0.0664
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 181960704
                    Iteration time: 1.95s
                      Time elapsed: 01:05:32
                               ETA: 00:05:18

################################################################################
                     [1m Learning iteration 1851/2000 [0m                     

                       Computation: 49723 steps/s (collection: 1.867s, learning 0.110s)
             Mean action noise std: 3.26
          Mean value_function loss: 104.5121
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.2535
                       Mean reward: 831.50
               Mean episode length: 221.56
    Episode_Reward/reaching_object: 1.1793
     Episode_Reward/lifting_object: 169.0585
      Episode_Reward/object_height: 0.0665
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 182059008
                    Iteration time: 1.98s
                      Time elapsed: 01:05:34
                               ETA: 00:05:16

################################################################################
                     [1m Learning iteration 1852/2000 [0m                     

                       Computation: 50973 steps/s (collection: 1.802s, learning 0.126s)
             Mean action noise std: 3.26
          Mean value_function loss: 112.2864
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.2600
                       Mean reward: 866.37
               Mean episode length: 229.90
    Episode_Reward/reaching_object: 1.2040
     Episode_Reward/lifting_object: 172.7542
      Episode_Reward/object_height: 0.0684
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182157312
                    Iteration time: 1.93s
                      Time elapsed: 01:05:36
                               ETA: 00:05:14

################################################################################
                     [1m Learning iteration 1853/2000 [0m                     

                       Computation: 50965 steps/s (collection: 1.799s, learning 0.130s)
             Mean action noise std: 3.27
          Mean value_function loss: 108.9687
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 56.2692
                       Mean reward: 866.71
               Mean episode length: 231.03
    Episode_Reward/reaching_object: 1.2004
     Episode_Reward/lifting_object: 172.1843
      Episode_Reward/object_height: 0.0679
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 182255616
                    Iteration time: 1.93s
                      Time elapsed: 01:05:38
                               ETA: 00:05:12

################################################################################
                     [1m Learning iteration 1854/2000 [0m                     

                       Computation: 51494 steps/s (collection: 1.798s, learning 0.111s)
             Mean action noise std: 3.27
          Mean value_function loss: 136.4187
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 56.2765
                       Mean reward: 837.07
               Mean episode length: 223.85
    Episode_Reward/reaching_object: 1.1797
     Episode_Reward/lifting_object: 169.2108
      Episode_Reward/object_height: 0.0671
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 182353920
                    Iteration time: 1.91s
                      Time elapsed: 01:05:40
                               ETA: 00:05:10

################################################################################
                     [1m Learning iteration 1855/2000 [0m                     

                       Computation: 49564 steps/s (collection: 1.846s, learning 0.138s)
             Mean action noise std: 3.27
          Mean value_function loss: 115.8587
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 56.2815
                       Mean reward: 858.76
               Mean episode length: 228.35
    Episode_Reward/reaching_object: 1.1880
     Episode_Reward/lifting_object: 170.8383
      Episode_Reward/object_height: 0.0678
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 182452224
                    Iteration time: 1.98s
                      Time elapsed: 01:05:42
                               ETA: 00:05:07

################################################################################
                     [1m Learning iteration 1856/2000 [0m                     

                       Computation: 49438 steps/s (collection: 1.862s, learning 0.127s)
             Mean action noise std: 3.27
          Mean value_function loss: 165.4829
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 56.2850
                       Mean reward: 766.77
               Mean episode length: 206.33
    Episode_Reward/reaching_object: 1.1306
     Episode_Reward/lifting_object: 162.4634
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 182550528
                    Iteration time: 1.99s
                      Time elapsed: 01:05:44
                               ETA: 00:05:05

################################################################################
                     [1m Learning iteration 1857/2000 [0m                     

                       Computation: 50829 steps/s (collection: 1.842s, learning 0.092s)
             Mean action noise std: 3.27
          Mean value_function loss: 172.4677
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.2917
                       Mean reward: 805.82
               Mean episode length: 215.82
    Episode_Reward/reaching_object: 1.1688
     Episode_Reward/lifting_object: 168.2628
      Episode_Reward/object_height: 0.0664
        Episode_Reward/action_rate: -0.0536
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 182648832
                    Iteration time: 1.93s
                      Time elapsed: 01:05:46
                               ETA: 00:05:03

################################################################################
                     [1m Learning iteration 1858/2000 [0m                     

                       Computation: 50597 steps/s (collection: 1.842s, learning 0.101s)
             Mean action noise std: 3.27
          Mean value_function loss: 211.0089
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 56.3008
                       Mean reward: 895.20
               Mean episode length: 237.26
    Episode_Reward/reaching_object: 1.1861
     Episode_Reward/lifting_object: 171.2403
      Episode_Reward/object_height: 0.0672
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182747136
                    Iteration time: 1.94s
                      Time elapsed: 01:05:48
                               ETA: 00:05:01

################################################################################
                     [1m Learning iteration 1859/2000 [0m                     

                       Computation: 51538 steps/s (collection: 1.819s, learning 0.088s)
             Mean action noise std: 3.27
          Mean value_function loss: 238.1894
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.3089
                       Mean reward: 804.22
               Mean episode length: 216.45
    Episode_Reward/reaching_object: 1.1339
     Episode_Reward/lifting_object: 163.4867
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 182845440
                    Iteration time: 1.91s
                      Time elapsed: 01:05:50
                               ETA: 00:04:59

################################################################################
                     [1m Learning iteration 1860/2000 [0m                     

                       Computation: 49094 steps/s (collection: 1.868s, learning 0.135s)
             Mean action noise std: 3.27
          Mean value_function loss: 176.8610
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 56.3169
                       Mean reward: 771.92
               Mean episode length: 208.79
    Episode_Reward/reaching_object: 1.1316
     Episode_Reward/lifting_object: 163.9512
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 182943744
                    Iteration time: 2.00s
                      Time elapsed: 01:05:52
                               ETA: 00:04:57

################################################################################
                     [1m Learning iteration 1861/2000 [0m                     

                       Computation: 46322 steps/s (collection: 1.940s, learning 0.183s)
             Mean action noise std: 3.27
          Mean value_function loss: 153.3456
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.3221
                       Mean reward: 854.52
               Mean episode length: 227.26
    Episode_Reward/reaching_object: 1.1718
     Episode_Reward/lifting_object: 169.7636
      Episode_Reward/object_height: 0.0665
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 183042048
                    Iteration time: 2.12s
                      Time elapsed: 01:05:54
                               ETA: 00:04:55

################################################################################
                     [1m Learning iteration 1862/2000 [0m                     

                       Computation: 49710 steps/s (collection: 1.869s, learning 0.109s)
             Mean action noise std: 3.27
          Mean value_function loss: 179.8948
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 56.3289
                       Mean reward: 852.32
               Mean episode length: 227.58
    Episode_Reward/reaching_object: 1.1358
     Episode_Reward/lifting_object: 164.6642
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0530
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 183140352
                    Iteration time: 1.98s
                      Time elapsed: 01:05:56
                               ETA: 00:04:53

################################################################################
                     [1m Learning iteration 1863/2000 [0m                     

                       Computation: 50786 steps/s (collection: 1.829s, learning 0.107s)
             Mean action noise std: 3.28
          Mean value_function loss: 186.6029
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 56.3344
                       Mean reward: 794.10
               Mean episode length: 214.07
    Episode_Reward/reaching_object: 1.1540
     Episode_Reward/lifting_object: 167.3529
      Episode_Reward/object_height: 0.0662
        Episode_Reward/action_rate: -0.0536
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 183238656
                    Iteration time: 1.94s
                      Time elapsed: 01:05:58
                               ETA: 00:04:50

################################################################################
                     [1m Learning iteration 1864/2000 [0m                     

                       Computation: 51064 steps/s (collection: 1.833s, learning 0.092s)
             Mean action noise std: 3.28
          Mean value_function loss: 207.9251
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.3438
                       Mean reward: 819.33
               Mean episode length: 218.33
    Episode_Reward/reaching_object: 1.1542
     Episode_Reward/lifting_object: 167.8648
      Episode_Reward/object_height: 0.0660
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 183336960
                    Iteration time: 1.93s
                      Time elapsed: 01:06:00
                               ETA: 00:04:48

################################################################################
                     [1m Learning iteration 1865/2000 [0m                     

                       Computation: 51090 steps/s (collection: 1.824s, learning 0.101s)
             Mean action noise std: 3.28
          Mean value_function loss: 235.6675
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 56.3545
                       Mean reward: 836.29
               Mean episode length: 222.86
    Episode_Reward/reaching_object: 1.1419
     Episode_Reward/lifting_object: 165.4848
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 183435264
                    Iteration time: 1.92s
                      Time elapsed: 01:06:02
                               ETA: 00:04:46

################################################################################
                     [1m Learning iteration 1866/2000 [0m                     

                       Computation: 50334 steps/s (collection: 1.835s, learning 0.118s)
             Mean action noise std: 3.28
          Mean value_function loss: 195.0652
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 56.3615
                       Mean reward: 859.17
               Mean episode length: 228.31
    Episode_Reward/reaching_object: 1.1892
     Episode_Reward/lifting_object: 172.7577
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 183533568
                    Iteration time: 1.95s
                      Time elapsed: 01:06:03
                               ETA: 00:04:44

################################################################################
                     [1m Learning iteration 1867/2000 [0m                     

                       Computation: 51291 steps/s (collection: 1.806s, learning 0.111s)
             Mean action noise std: 3.28
          Mean value_function loss: 201.8142
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 56.3662
                       Mean reward: 815.44
               Mean episode length: 218.70
    Episode_Reward/reaching_object: 1.1592
     Episode_Reward/lifting_object: 168.2932
      Episode_Reward/object_height: 0.0667
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 183631872
                    Iteration time: 1.92s
                      Time elapsed: 01:06:05
                               ETA: 00:04:42

################################################################################
                     [1m Learning iteration 1868/2000 [0m                     

                       Computation: 46853 steps/s (collection: 1.934s, learning 0.164s)
             Mean action noise std: 3.28
          Mean value_function loss: 227.4316
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 56.3740
                       Mean reward: 805.89
               Mean episode length: 215.38
    Episode_Reward/reaching_object: 1.1687
     Episode_Reward/lifting_object: 169.7406
      Episode_Reward/object_height: 0.0673
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 183730176
                    Iteration time: 2.10s
                      Time elapsed: 01:06:07
                               ETA: 00:04:40

################################################################################
                     [1m Learning iteration 1869/2000 [0m                     

                       Computation: 50451 steps/s (collection: 1.858s, learning 0.090s)
             Mean action noise std: 3.28
          Mean value_function loss: 202.4934
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 56.3881
                       Mean reward: 873.62
               Mean episode length: 232.50
    Episode_Reward/reaching_object: 1.1645
     Episode_Reward/lifting_object: 168.7102
      Episode_Reward/object_height: 0.0673
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 183828480
                    Iteration time: 1.95s
                      Time elapsed: 01:06:09
                               ETA: 00:04:38

################################################################################
                     [1m Learning iteration 1870/2000 [0m                     

                       Computation: 48809 steps/s (collection: 1.893s, learning 0.121s)
             Mean action noise std: 3.29
          Mean value_function loss: 252.1507
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.3960
                       Mean reward: 843.24
               Mean episode length: 224.50
    Episode_Reward/reaching_object: 1.1343
     Episode_Reward/lifting_object: 164.2457
      Episode_Reward/object_height: 0.0647
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 183926784
                    Iteration time: 2.01s
                      Time elapsed: 01:06:11
                               ETA: 00:04:35

################################################################################
                     [1m Learning iteration 1871/2000 [0m                     

                       Computation: 48859 steps/s (collection: 1.885s, learning 0.127s)
             Mean action noise std: 3.29
          Mean value_function loss: 213.2628
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 56.4054
                       Mean reward: 810.83
               Mean episode length: 217.54
    Episode_Reward/reaching_object: 1.1599
     Episode_Reward/lifting_object: 167.9438
      Episode_Reward/object_height: 0.0664
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 184025088
                    Iteration time: 2.01s
                      Time elapsed: 01:06:13
                               ETA: 00:04:33

################################################################################
                     [1m Learning iteration 1872/2000 [0m                     

                       Computation: 46237 steps/s (collection: 1.977s, learning 0.150s)
             Mean action noise std: 3.29
          Mean value_function loss: 227.2978
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 56.4144
                       Mean reward: 842.41
               Mean episode length: 224.80
    Episode_Reward/reaching_object: 1.1471
     Episode_Reward/lifting_object: 165.5531
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0536
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 184123392
                    Iteration time: 2.13s
                      Time elapsed: 01:06:16
                               ETA: 00:04:31

################################################################################
                     [1m Learning iteration 1873/2000 [0m                     

                       Computation: 50615 steps/s (collection: 1.830s, learning 0.112s)
             Mean action noise std: 3.29
          Mean value_function loss: 176.6593
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 56.4238
                       Mean reward: 850.50
               Mean episode length: 226.63
    Episode_Reward/reaching_object: 1.1808
     Episode_Reward/lifting_object: 170.8335
      Episode_Reward/object_height: 0.0674
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 184221696
                    Iteration time: 1.94s
                      Time elapsed: 01:06:18
                               ETA: 00:04:29

################################################################################
                     [1m Learning iteration 1874/2000 [0m                     

                       Computation: 50194 steps/s (collection: 1.856s, learning 0.102s)
             Mean action noise std: 3.29
          Mean value_function loss: 218.7575
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 56.4355
                       Mean reward: 829.21
               Mean episode length: 222.05
    Episode_Reward/reaching_object: 1.1651
     Episode_Reward/lifting_object: 168.2780
      Episode_Reward/object_height: 0.0660
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 184320000
                    Iteration time: 1.96s
                      Time elapsed: 01:06:19
                               ETA: 00:04:27

################################################################################
                     [1m Learning iteration 1875/2000 [0m                     

                       Computation: 48820 steps/s (collection: 1.902s, learning 0.112s)
             Mean action noise std: 3.29
          Mean value_function loss: 152.3923
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 56.4406
                       Mean reward: 841.88
               Mean episode length: 224.72
    Episode_Reward/reaching_object: 1.1803
     Episode_Reward/lifting_object: 170.8921
      Episode_Reward/object_height: 0.0668
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 184418304
                    Iteration time: 2.01s
                      Time elapsed: 01:06:22
                               ETA: 00:04:25

################################################################################
                     [1m Learning iteration 1876/2000 [0m                     

                       Computation: 50716 steps/s (collection: 1.853s, learning 0.085s)
             Mean action noise std: 3.29
          Mean value_function loss: 147.4489
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.4448
                       Mean reward: 815.73
               Mean episode length: 218.27
    Episode_Reward/reaching_object: 1.1702
     Episode_Reward/lifting_object: 168.7691
      Episode_Reward/object_height: 0.0660
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 184516608
                    Iteration time: 1.94s
                      Time elapsed: 01:06:23
                               ETA: 00:04:23

################################################################################
                     [1m Learning iteration 1877/2000 [0m                     

                       Computation: 50246 steps/s (collection: 1.854s, learning 0.102s)
             Mean action noise std: 3.29
          Mean value_function loss: 149.0652
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 56.4538
                       Mean reward: 856.87
               Mean episode length: 228.09
    Episode_Reward/reaching_object: 1.1879
     Episode_Reward/lifting_object: 171.3065
      Episode_Reward/object_height: 0.0666
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 184614912
                    Iteration time: 1.96s
                      Time elapsed: 01:06:25
                               ETA: 00:04:21

################################################################################
                     [1m Learning iteration 1878/2000 [0m                     

                       Computation: 51716 steps/s (collection: 1.814s, learning 0.087s)
             Mean action noise std: 3.30
          Mean value_function loss: 112.9681
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 56.4638
                       Mean reward: 893.88
               Mean episode length: 237.69
    Episode_Reward/reaching_object: 1.2205
     Episode_Reward/lifting_object: 176.1739
      Episode_Reward/object_height: 0.0689
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 184713216
                    Iteration time: 1.90s
                      Time elapsed: 01:06:27
                               ETA: 00:04:18

################################################################################
                     [1m Learning iteration 1879/2000 [0m                     

                       Computation: 49339 steps/s (collection: 1.874s, learning 0.118s)
             Mean action noise std: 3.30
          Mean value_function loss: 171.3341
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 56.4719
                       Mean reward: 846.05
               Mean episode length: 225.43
    Episode_Reward/reaching_object: 1.1973
     Episode_Reward/lifting_object: 172.1375
      Episode_Reward/object_height: 0.0674
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 184811520
                    Iteration time: 1.99s
                      Time elapsed: 01:06:29
                               ETA: 00:04:16

################################################################################
                     [1m Learning iteration 1880/2000 [0m                     

                       Computation: 50363 steps/s (collection: 1.834s, learning 0.118s)
             Mean action noise std: 3.30
          Mean value_function loss: 154.9552
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 56.4796
                       Mean reward: 886.50
               Mean episode length: 235.72
    Episode_Reward/reaching_object: 1.1913
     Episode_Reward/lifting_object: 171.1168
      Episode_Reward/object_height: 0.0670
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 184909824
                    Iteration time: 1.95s
                      Time elapsed: 01:06:31
                               ETA: 00:04:14

################################################################################
                     [1m Learning iteration 1881/2000 [0m                     

                       Computation: 50056 steps/s (collection: 1.863s, learning 0.101s)
             Mean action noise std: 3.30
          Mean value_function loss: 141.9925
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.4904
                       Mean reward: 846.20
               Mean episode length: 225.97
    Episode_Reward/reaching_object: 1.1776
     Episode_Reward/lifting_object: 169.4651
      Episode_Reward/object_height: 0.0664
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 185008128
                    Iteration time: 1.96s
                      Time elapsed: 01:06:33
                               ETA: 00:04:12

################################################################################
                     [1m Learning iteration 1882/2000 [0m                     

                       Computation: 50077 steps/s (collection: 1.825s, learning 0.138s)
             Mean action noise std: 3.30
          Mean value_function loss: 141.9923
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 56.5043
                       Mean reward: 858.84
               Mean episode length: 228.84
    Episode_Reward/reaching_object: 1.1698
     Episode_Reward/lifting_object: 168.0219
      Episode_Reward/object_height: 0.0657
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 185106432
                    Iteration time: 1.96s
                      Time elapsed: 01:06:35
                               ETA: 00:04:10

################################################################################
                     [1m Learning iteration 1883/2000 [0m                     

                       Computation: 50422 steps/s (collection: 1.798s, learning 0.152s)
             Mean action noise std: 3.30
          Mean value_function loss: 183.0706
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 56.5145
                       Mean reward: 867.44
               Mean episode length: 230.93
    Episode_Reward/reaching_object: 1.2139
     Episode_Reward/lifting_object: 174.2557
      Episode_Reward/object_height: 0.0689
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 185204736
                    Iteration time: 1.95s
                      Time elapsed: 01:06:37
                               ETA: 00:04:08

################################################################################
                     [1m Learning iteration 1884/2000 [0m                     

                       Computation: 44933 steps/s (collection: 1.978s, learning 0.210s)
             Mean action noise std: 3.30
          Mean value_function loss: 162.0645
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 56.5254
                       Mean reward: 827.03
               Mean episode length: 221.88
    Episode_Reward/reaching_object: 1.1718
     Episode_Reward/lifting_object: 167.7212
      Episode_Reward/object_height: 0.0665
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 185303040
                    Iteration time: 2.19s
                      Time elapsed: 01:06:39
                               ETA: 00:04:06

################################################################################
                     [1m Learning iteration 1885/2000 [0m                     

                       Computation: 49320 steps/s (collection: 1.893s, learning 0.101s)
             Mean action noise std: 3.31
          Mean value_function loss: 150.2032
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 56.5343
                       Mean reward: 886.74
               Mean episode length: 236.21
    Episode_Reward/reaching_object: 1.2116
     Episode_Reward/lifting_object: 173.6452
      Episode_Reward/object_height: 0.0694
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 185401344
                    Iteration time: 1.99s
                      Time elapsed: 01:06:41
                               ETA: 00:04:04

################################################################################
                     [1m Learning iteration 1886/2000 [0m                     

                       Computation: 47944 steps/s (collection: 1.891s, learning 0.159s)
             Mean action noise std: 3.31
          Mean value_function loss: 136.1941
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 56.5388
                       Mean reward: 841.89
               Mean episode length: 225.36
    Episode_Reward/reaching_object: 1.1948
     Episode_Reward/lifting_object: 170.8634
      Episode_Reward/object_height: 0.0680
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 185499648
                    Iteration time: 2.05s
                      Time elapsed: 01:06:43
                               ETA: 00:04:01

################################################################################
                     [1m Learning iteration 1887/2000 [0m                     

                       Computation: 49239 steps/s (collection: 1.906s, learning 0.090s)
             Mean action noise std: 3.31
          Mean value_function loss: 181.4143
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 56.5435
                       Mean reward: 832.62
               Mean episode length: 222.85
    Episode_Reward/reaching_object: 1.1868
     Episode_Reward/lifting_object: 169.8693
      Episode_Reward/object_height: 0.0672
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 185597952
                    Iteration time: 2.00s
                      Time elapsed: 01:06:45
                               ETA: 00:03:59

################################################################################
                     [1m Learning iteration 1888/2000 [0m                     

                       Computation: 51403 steps/s (collection: 1.812s, learning 0.101s)
             Mean action noise std: 3.31
          Mean value_function loss: 141.0424
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 56.5511
                       Mean reward: 874.09
               Mean episode length: 232.45
    Episode_Reward/reaching_object: 1.2225
     Episode_Reward/lifting_object: 174.5606
      Episode_Reward/object_height: 0.0698
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 185696256
                    Iteration time: 1.91s
                      Time elapsed: 01:06:47
                               ETA: 00:03:57

################################################################################
                     [1m Learning iteration 1889/2000 [0m                     

                       Computation: 48814 steps/s (collection: 1.891s, learning 0.123s)
             Mean action noise std: 3.31
          Mean value_function loss: 130.0476
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 56.5597
                       Mean reward: 835.67
               Mean episode length: 223.80
    Episode_Reward/reaching_object: 1.1935
     Episode_Reward/lifting_object: 170.0357
      Episode_Reward/object_height: 0.0678
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 185794560
                    Iteration time: 2.01s
                      Time elapsed: 01:06:49
                               ETA: 00:03:55

################################################################################
                     [1m Learning iteration 1890/2000 [0m                     

                       Computation: 49398 steps/s (collection: 1.863s, learning 0.127s)
             Mean action noise std: 3.31
          Mean value_function loss: 137.0573
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 56.5673
                       Mean reward: 861.05
               Mean episode length: 229.97
    Episode_Reward/reaching_object: 1.2158
     Episode_Reward/lifting_object: 172.9149
      Episode_Reward/object_height: 0.0689
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 185892864
                    Iteration time: 1.99s
                      Time elapsed: 01:06:51
                               ETA: 00:03:53

################################################################################
                     [1m Learning iteration 1891/2000 [0m                     

                       Computation: 50910 steps/s (collection: 1.841s, learning 0.090s)
             Mean action noise std: 3.31
          Mean value_function loss: 141.6098
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.5776
                       Mean reward: 893.65
               Mean episode length: 236.71
    Episode_Reward/reaching_object: 1.2043
     Episode_Reward/lifting_object: 171.0127
      Episode_Reward/object_height: 0.0677
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 185991168
                    Iteration time: 1.93s
                      Time elapsed: 01:06:53
                               ETA: 00:03:51

################################################################################
                     [1m Learning iteration 1892/2000 [0m                     

                       Computation: 49726 steps/s (collection: 1.826s, learning 0.151s)
             Mean action noise std: 3.31
          Mean value_function loss: 119.7728
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.5873
                       Mean reward: 854.87
               Mean episode length: 228.66
    Episode_Reward/reaching_object: 1.2071
     Episode_Reward/lifting_object: 170.6197
      Episode_Reward/object_height: 0.0673
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 186089472
                    Iteration time: 1.98s
                      Time elapsed: 01:06:55
                               ETA: 00:03:49

################################################################################
                     [1m Learning iteration 1893/2000 [0m                     

                       Computation: 50535 steps/s (collection: 1.823s, learning 0.122s)
             Mean action noise std: 3.31
          Mean value_function loss: 112.7527
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.5948
                       Mean reward: 874.14
               Mean episode length: 232.20
    Episode_Reward/reaching_object: 1.2147
     Episode_Reward/lifting_object: 171.8637
      Episode_Reward/object_height: 0.0675
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 186187776
                    Iteration time: 1.95s
                      Time elapsed: 01:06:57
                               ETA: 00:03:46

################################################################################
                     [1m Learning iteration 1894/2000 [0m                     

                       Computation: 49384 steps/s (collection: 1.840s, learning 0.151s)
             Mean action noise std: 3.32
          Mean value_function loss: 119.2159
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 56.6008
                       Mean reward: 858.29
               Mean episode length: 228.85
    Episode_Reward/reaching_object: 1.2152
     Episode_Reward/lifting_object: 171.8042
      Episode_Reward/object_height: 0.0671
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 186286080
                    Iteration time: 1.99s
                      Time elapsed: 01:06:59
                               ETA: 00:03:44

################################################################################
                     [1m Learning iteration 1895/2000 [0m                     

                       Computation: 50066 steps/s (collection: 1.863s, learning 0.100s)
             Mean action noise std: 3.32
          Mean value_function loss: 114.5645
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.6106
                       Mean reward: 894.89
               Mean episode length: 237.89
    Episode_Reward/reaching_object: 1.2315
     Episode_Reward/lifting_object: 174.0872
      Episode_Reward/object_height: 0.0678
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 186384384
                    Iteration time: 1.96s
                      Time elapsed: 01:07:01
                               ETA: 00:03:42

################################################################################
                     [1m Learning iteration 1896/2000 [0m                     

                       Computation: 48932 steps/s (collection: 1.901s, learning 0.108s)
             Mean action noise std: 3.32
          Mean value_function loss: 120.2026
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 56.6200
                       Mean reward: 877.94
               Mean episode length: 233.50
    Episode_Reward/reaching_object: 1.2255
     Episode_Reward/lifting_object: 172.8382
      Episode_Reward/object_height: 0.0674
        Episode_Reward/action_rate: -0.0565
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 186482688
                    Iteration time: 2.01s
                      Time elapsed: 01:07:03
                               ETA: 00:03:40

################################################################################
                     [1m Learning iteration 1897/2000 [0m                     

                       Computation: 48475 steps/s (collection: 1.922s, learning 0.106s)
             Mean action noise std: 3.32
          Mean value_function loss: 111.2512
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.6275
                       Mean reward: 871.57
               Mean episode length: 232.31
    Episode_Reward/reaching_object: 1.2196
     Episode_Reward/lifting_object: 171.8173
      Episode_Reward/object_height: 0.0669
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 186580992
                    Iteration time: 2.03s
                      Time elapsed: 01:07:05
                               ETA: 00:03:38

################################################################################
                     [1m Learning iteration 1898/2000 [0m                     

                       Computation: 48646 steps/s (collection: 1.891s, learning 0.130s)
             Mean action noise std: 3.32
          Mean value_function loss: 135.2400
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 56.6367
                       Mean reward: 926.86
               Mean episode length: 245.60
    Episode_Reward/reaching_object: 1.2223
     Episode_Reward/lifting_object: 172.7520
      Episode_Reward/object_height: 0.0672
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 186679296
                    Iteration time: 2.02s
                      Time elapsed: 01:07:07
                               ETA: 00:03:36

################################################################################
                     [1m Learning iteration 1899/2000 [0m                     

                       Computation: 48880 steps/s (collection: 1.878s, learning 0.133s)
             Mean action noise std: 3.32
          Mean value_function loss: 139.1828
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.6448
                       Mean reward: 875.82
               Mean episode length: 233.36
    Episode_Reward/reaching_object: 1.2381
     Episode_Reward/lifting_object: 174.9292
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 186777600
                    Iteration time: 2.01s
                      Time elapsed: 01:07:09
                               ETA: 00:03:34

################################################################################
                     [1m Learning iteration 1900/2000 [0m                     

                       Computation: 49137 steps/s (collection: 1.860s, learning 0.141s)
             Mean action noise std: 3.32
          Mean value_function loss: 157.2538
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.6514
                       Mean reward: 867.67
               Mean episode length: 230.43
    Episode_Reward/reaching_object: 1.2242
     Episode_Reward/lifting_object: 172.8632
      Episode_Reward/object_height: 0.0674
        Episode_Reward/action_rate: -0.0565
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 186875904
                    Iteration time: 2.00s
                      Time elapsed: 01:07:11
                               ETA: 00:03:32

################################################################################
                     [1m Learning iteration 1901/2000 [0m                     

                       Computation: 47547 steps/s (collection: 1.901s, learning 0.166s)
             Mean action noise std: 3.32
          Mean value_function loss: 124.9032
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 56.6551
                       Mean reward: 857.22
               Mean episode length: 227.55
    Episode_Reward/reaching_object: 1.2088
     Episode_Reward/lifting_object: 171.1758
      Episode_Reward/object_height: 0.0667
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 186974208
                    Iteration time: 2.07s
                      Time elapsed: 01:07:13
                               ETA: 00:03:29

################################################################################
                     [1m Learning iteration 1902/2000 [0m                     

                       Computation: 47084 steps/s (collection: 1.938s, learning 0.150s)
             Mean action noise std: 3.32
          Mean value_function loss: 107.7435
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.6584
                       Mean reward: 883.42
               Mean episode length: 234.81
    Episode_Reward/reaching_object: 1.2330
     Episode_Reward/lifting_object: 174.1693
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 187072512
                    Iteration time: 2.09s
                      Time elapsed: 01:07:15
                               ETA: 00:03:27

################################################################################
                     [1m Learning iteration 1903/2000 [0m                     

                       Computation: 46999 steps/s (collection: 1.955s, learning 0.136s)
             Mean action noise std: 3.33
          Mean value_function loss: 123.0696
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 56.6649
                       Mean reward: 844.58
               Mean episode length: 226.97
    Episode_Reward/reaching_object: 1.2307
     Episode_Reward/lifting_object: 174.1229
      Episode_Reward/object_height: 0.0686
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 187170816
                    Iteration time: 2.09s
                      Time elapsed: 01:07:17
                               ETA: 00:03:25

################################################################################
                     [1m Learning iteration 1904/2000 [0m                     

                       Computation: 49472 steps/s (collection: 1.886s, learning 0.101s)
             Mean action noise std: 3.33
          Mean value_function loss: 128.4074
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 56.6712
                       Mean reward: 846.86
               Mean episode length: 225.83
    Episode_Reward/reaching_object: 1.1874
     Episode_Reward/lifting_object: 167.6635
      Episode_Reward/object_height: 0.0660
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 187269120
                    Iteration time: 1.99s
                      Time elapsed: 01:07:19
                               ETA: 00:03:23

################################################################################
                     [1m Learning iteration 1905/2000 [0m                     

                       Computation: 49319 steps/s (collection: 1.865s, learning 0.129s)
             Mean action noise std: 3.33
          Mean value_function loss: 86.4695
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 56.6797
                       Mean reward: 894.15
               Mean episode length: 237.73
    Episode_Reward/reaching_object: 1.2416
     Episode_Reward/lifting_object: 176.2876
      Episode_Reward/object_height: 0.0696
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 187367424
                    Iteration time: 1.99s
                      Time elapsed: 01:07:21
                               ETA: 00:03:21

################################################################################
                     [1m Learning iteration 1906/2000 [0m                     

                       Computation: 46203 steps/s (collection: 2.000s, learning 0.128s)
             Mean action noise std: 3.33
          Mean value_function loss: 107.1186
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.6889
                       Mean reward: 883.26
               Mean episode length: 234.55
    Episode_Reward/reaching_object: 1.2441
     Episode_Reward/lifting_object: 176.3893
      Episode_Reward/object_height: 0.0698
        Episode_Reward/action_rate: -0.0577
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 187465728
                    Iteration time: 2.13s
                      Time elapsed: 01:07:23
                               ETA: 00:03:19

################################################################################
                     [1m Learning iteration 1907/2000 [0m                     

                       Computation: 47252 steps/s (collection: 1.946s, learning 0.134s)
             Mean action noise std: 3.33
          Mean value_function loss: 136.5398
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.6976
                       Mean reward: 884.52
               Mean episode length: 235.16
    Episode_Reward/reaching_object: 1.2218
     Episode_Reward/lifting_object: 172.7617
      Episode_Reward/object_height: 0.0681
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 187564032
                    Iteration time: 2.08s
                      Time elapsed: 01:07:26
                               ETA: 00:03:17

################################################################################
                     [1m Learning iteration 1908/2000 [0m                     

                       Computation: 47746 steps/s (collection: 1.938s, learning 0.121s)
             Mean action noise std: 3.33
          Mean value_function loss: 130.7317
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 56.7078
                       Mean reward: 860.98
               Mean episode length: 229.91
    Episode_Reward/reaching_object: 1.2220
     Episode_Reward/lifting_object: 173.1498
      Episode_Reward/object_height: 0.0687
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 187662336
                    Iteration time: 2.06s
                      Time elapsed: 01:07:28
                               ETA: 00:03:15

################################################################################
                     [1m Learning iteration 1909/2000 [0m                     

                       Computation: 43818 steps/s (collection: 2.152s, learning 0.091s)
             Mean action noise std: 3.33
          Mean value_function loss: 120.2786
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 56.7155
                       Mean reward: 867.54
               Mean episode length: 231.39
    Episode_Reward/reaching_object: 1.2074
     Episode_Reward/lifting_object: 170.8549
      Episode_Reward/object_height: 0.0682
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 187760640
                    Iteration time: 2.24s
                      Time elapsed: 01:07:30
                               ETA: 00:03:12

################################################################################
                     [1m Learning iteration 1910/2000 [0m                     

                       Computation: 49926 steps/s (collection: 1.876s, learning 0.093s)
             Mean action noise std: 3.33
          Mean value_function loss: 130.1352
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 56.7230
                       Mean reward: 830.50
               Mean episode length: 222.15
    Episode_Reward/reaching_object: 1.2194
     Episode_Reward/lifting_object: 172.7959
      Episode_Reward/object_height: 0.0687
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 187858944
                    Iteration time: 1.97s
                      Time elapsed: 01:07:32
                               ETA: 00:03:10

################################################################################
                     [1m Learning iteration 1911/2000 [0m                     

                       Computation: 49374 steps/s (collection: 1.876s, learning 0.115s)
             Mean action noise std: 3.34
          Mean value_function loss: 156.0242
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 56.7299
                       Mean reward: 904.46
               Mean episode length: 239.96
    Episode_Reward/reaching_object: 1.2379
     Episode_Reward/lifting_object: 175.6390
      Episode_Reward/object_height: 0.0699
        Episode_Reward/action_rate: -0.0576
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 187957248
                    Iteration time: 1.99s
                      Time elapsed: 01:07:34
                               ETA: 00:03:08

################################################################################
                     [1m Learning iteration 1912/2000 [0m                     

                       Computation: 49004 steps/s (collection: 1.911s, learning 0.095s)
             Mean action noise std: 3.34
          Mean value_function loss: 171.5779
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 56.7351
                       Mean reward: 864.70
               Mean episode length: 229.70
    Episode_Reward/reaching_object: 1.2139
     Episode_Reward/lifting_object: 172.2281
      Episode_Reward/object_height: 0.0685
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 188055552
                    Iteration time: 2.01s
                      Time elapsed: 01:07:36
                               ETA: 00:03:06

################################################################################
                     [1m Learning iteration 1913/2000 [0m                     

                       Computation: 48112 steps/s (collection: 1.892s, learning 0.151s)
             Mean action noise std: 3.34
          Mean value_function loss: 136.6838
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 56.7414
                       Mean reward: 829.88
               Mean episode length: 222.63
    Episode_Reward/reaching_object: 1.1927
     Episode_Reward/lifting_object: 169.0170
      Episode_Reward/object_height: 0.0668
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 188153856
                    Iteration time: 2.04s
                      Time elapsed: 01:07:38
                               ETA: 00:03:04

################################################################################
                     [1m Learning iteration 1914/2000 [0m                     

                       Computation: 46092 steps/s (collection: 2.013s, learning 0.120s)
             Mean action noise std: 3.34
          Mean value_function loss: 134.5248
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.7473
                       Mean reward: 879.20
               Mean episode length: 235.02
    Episode_Reward/reaching_object: 1.2220
     Episode_Reward/lifting_object: 173.5118
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 188252160
                    Iteration time: 2.13s
                      Time elapsed: 01:07:40
                               ETA: 00:03:02

################################################################################
                     [1m Learning iteration 1915/2000 [0m                     

                       Computation: 49599 steps/s (collection: 1.876s, learning 0.106s)
             Mean action noise std: 3.34
          Mean value_function loss: 141.6258
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 56.7552
                       Mean reward: 859.17
               Mean episode length: 228.62
    Episode_Reward/reaching_object: 1.2177
     Episode_Reward/lifting_object: 173.1746
      Episode_Reward/object_height: 0.0687
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 188350464
                    Iteration time: 1.98s
                      Time elapsed: 01:07:42
                               ETA: 00:03:00

################################################################################
                     [1m Learning iteration 1916/2000 [0m                     

                       Computation: 44258 steps/s (collection: 2.067s, learning 0.155s)
             Mean action noise std: 3.34
          Mean value_function loss: 115.5560
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.7626
                       Mean reward: 883.02
               Mean episode length: 234.97
    Episode_Reward/reaching_object: 1.2076
     Episode_Reward/lifting_object: 171.6228
      Episode_Reward/object_height: 0.0685
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 188448768
                    Iteration time: 2.22s
                      Time elapsed: 01:07:44
                               ETA: 00:02:58

################################################################################
                     [1m Learning iteration 1917/2000 [0m                     

                       Computation: 48331 steps/s (collection: 1.913s, learning 0.121s)
             Mean action noise std: 3.34
          Mean value_function loss: 130.5425
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 56.7704
                       Mean reward: 867.07
               Mean episode length: 230.85
    Episode_Reward/reaching_object: 1.2226
     Episode_Reward/lifting_object: 173.6104
      Episode_Reward/object_height: 0.0692
        Episode_Reward/action_rate: -0.0572
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 188547072
                    Iteration time: 2.03s
                      Time elapsed: 01:07:46
                               ETA: 00:02:55

################################################################################
                     [1m Learning iteration 1918/2000 [0m                     

                       Computation: 48625 steps/s (collection: 1.906s, learning 0.116s)
             Mean action noise std: 3.34
          Mean value_function loss: 135.5991
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 56.7770
                       Mean reward: 870.20
               Mean episode length: 231.13
    Episode_Reward/reaching_object: 1.2220
     Episode_Reward/lifting_object: 174.0132
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0572
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 188645376
                    Iteration time: 2.02s
                      Time elapsed: 01:07:48
                               ETA: 00:02:53

################################################################################
                     [1m Learning iteration 1919/2000 [0m                     

                       Computation: 49067 steps/s (collection: 1.867s, learning 0.137s)
             Mean action noise std: 3.34
          Mean value_function loss: 127.4289
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 56.7842
                       Mean reward: 871.91
               Mean episode length: 232.17
    Episode_Reward/reaching_object: 1.2259
     Episode_Reward/lifting_object: 174.6589
      Episode_Reward/object_height: 0.0686
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 188743680
                    Iteration time: 2.00s
                      Time elapsed: 01:07:50
                               ETA: 00:02:51

################################################################################
                     [1m Learning iteration 1920/2000 [0m                     

                       Computation: 49470 steps/s (collection: 1.850s, learning 0.138s)
             Mean action noise std: 3.35
          Mean value_function loss: 173.3821
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.7938
                       Mean reward: 831.58
               Mean episode length: 221.98
    Episode_Reward/reaching_object: 1.1898
     Episode_Reward/lifting_object: 168.8808
      Episode_Reward/object_height: 0.0661
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 188841984
                    Iteration time: 1.99s
                      Time elapsed: 01:07:52
                               ETA: 00:02:49

################################################################################
                     [1m Learning iteration 1921/2000 [0m                     

                       Computation: 44806 steps/s (collection: 2.013s, learning 0.181s)
             Mean action noise std: 3.35
          Mean value_function loss: 123.4255
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.8012
                       Mean reward: 874.31
               Mean episode length: 232.29
    Episode_Reward/reaching_object: 1.2210
     Episode_Reward/lifting_object: 173.4297
      Episode_Reward/object_height: 0.0680
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 188940288
                    Iteration time: 2.19s
                      Time elapsed: 01:07:54
                               ETA: 00:02:47

################################################################################
                     [1m Learning iteration 1922/2000 [0m                     

                       Computation: 49588 steps/s (collection: 1.873s, learning 0.109s)
             Mean action noise std: 3.35
          Mean value_function loss: 121.6191
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.8071
                       Mean reward: 920.15
               Mean episode length: 243.57
    Episode_Reward/reaching_object: 1.2445
     Episode_Reward/lifting_object: 177.2521
      Episode_Reward/object_height: 0.0689
        Episode_Reward/action_rate: -0.0582
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189038592
                    Iteration time: 1.98s
                      Time elapsed: 01:07:56
                               ETA: 00:02:45

################################################################################
                     [1m Learning iteration 1923/2000 [0m                     

                       Computation: 49548 steps/s (collection: 1.854s, learning 0.130s)
             Mean action noise std: 3.35
          Mean value_function loss: 162.9919
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 56.8163
                       Mean reward: 900.98
               Mean episode length: 238.54
    Episode_Reward/reaching_object: 1.2194
     Episode_Reward/lifting_object: 173.8525
      Episode_Reward/object_height: 0.0669
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 189136896
                    Iteration time: 1.98s
                      Time elapsed: 01:07:58
                               ETA: 00:02:43

################################################################################
                     [1m Learning iteration 1924/2000 [0m                     

                       Computation: 49361 steps/s (collection: 1.846s, learning 0.146s)
             Mean action noise std: 3.35
          Mean value_function loss: 159.3800
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.8276
                       Mean reward: 844.56
               Mean episode length: 225.22
    Episode_Reward/reaching_object: 1.1806
     Episode_Reward/lifting_object: 167.6829
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 189235200
                    Iteration time: 1.99s
                      Time elapsed: 01:08:00
                               ETA: 00:02:41

################################################################################
                     [1m Learning iteration 1925/2000 [0m                     

                       Computation: 49261 steps/s (collection: 1.843s, learning 0.152s)
             Mean action noise std: 3.35
          Mean value_function loss: 321.0154
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 56.8424
                       Mean reward: 842.73
               Mean episode length: 224.94
    Episode_Reward/reaching_object: 1.1646
     Episode_Reward/lifting_object: 165.6995
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 189333504
                    Iteration time: 2.00s
                      Time elapsed: 01:08:02
                               ETA: 00:02:38

################################################################################
                     [1m Learning iteration 1926/2000 [0m                     

                       Computation: 49275 steps/s (collection: 1.840s, learning 0.155s)
             Mean action noise std: 3.35
          Mean value_function loss: 236.1020
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 56.8497
                       Mean reward: 837.51
               Mean episode length: 224.01
    Episode_Reward/reaching_object: 1.1747
     Episode_Reward/lifting_object: 166.4571
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 189431808
                    Iteration time: 1.99s
                      Time elapsed: 01:08:04
                               ETA: 00:02:36

################################################################################
                     [1m Learning iteration 1927/2000 [0m                     

                       Computation: 49249 steps/s (collection: 1.875s, learning 0.121s)
             Mean action noise std: 3.35
          Mean value_function loss: 162.7647
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 56.8534
                       Mean reward: 856.07
               Mean episode length: 227.25
    Episode_Reward/reaching_object: 1.1894
     Episode_Reward/lifting_object: 169.0701
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 189530112
                    Iteration time: 2.00s
                      Time elapsed: 01:08:06
                               ETA: 00:02:34

################################################################################
                     [1m Learning iteration 1928/2000 [0m                     

                       Computation: 48446 steps/s (collection: 1.927s, learning 0.103s)
             Mean action noise std: 3.35
          Mean value_function loss: 165.7312
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 56.8560
                       Mean reward: 897.48
               Mean episode length: 236.91
    Episode_Reward/reaching_object: 1.2264
     Episode_Reward/lifting_object: 174.5332
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 189628416
                    Iteration time: 2.03s
                      Time elapsed: 01:08:08
                               ETA: 00:02:32

################################################################################
                     [1m Learning iteration 1929/2000 [0m                     

                       Computation: 49185 steps/s (collection: 1.906s, learning 0.093s)
             Mean action noise std: 3.36
          Mean value_function loss: 133.5277
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.8620
                       Mean reward: 867.78
               Mean episode length: 231.99
    Episode_Reward/reaching_object: 1.2250
     Episode_Reward/lifting_object: 173.9102
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 189726720
                    Iteration time: 2.00s
                      Time elapsed: 01:08:10
                               ETA: 00:02:30

################################################################################
                     [1m Learning iteration 1930/2000 [0m                     

                       Computation: 47807 steps/s (collection: 1.923s, learning 0.134s)
             Mean action noise std: 3.36
          Mean value_function loss: 94.4179
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 56.8713
                       Mean reward: 901.04
               Mean episode length: 239.14
    Episode_Reward/reaching_object: 1.2381
     Episode_Reward/lifting_object: 176.2353
      Episode_Reward/object_height: 0.0648
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 189825024
                    Iteration time: 2.06s
                      Time elapsed: 01:08:12
                               ETA: 00:02:28

################################################################################
                     [1m Learning iteration 1931/2000 [0m                     

                       Computation: 45864 steps/s (collection: 1.971s, learning 0.172s)
             Mean action noise std: 3.36
          Mean value_function loss: 97.6349
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.8778
                       Mean reward: 884.78
               Mean episode length: 236.20
    Episode_Reward/reaching_object: 1.2439
     Episode_Reward/lifting_object: 176.9740
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 189923328
                    Iteration time: 2.14s
                      Time elapsed: 01:08:15
                               ETA: 00:02:26

################################################################################
                     [1m Learning iteration 1932/2000 [0m                     

                       Computation: 47603 steps/s (collection: 1.968s, learning 0.097s)
             Mean action noise std: 3.36
          Mean value_function loss: 151.0617
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 56.8851
                       Mean reward: 863.61
               Mean episode length: 230.71
    Episode_Reward/reaching_object: 1.2093
     Episode_Reward/lifting_object: 171.5293
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.0572
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 190021632
                    Iteration time: 2.07s
                      Time elapsed: 01:08:17
                               ETA: 00:02:24

################################################################################
                     [1m Learning iteration 1933/2000 [0m                     

                       Computation: 46989 steps/s (collection: 1.994s, learning 0.098s)
             Mean action noise std: 3.36
          Mean value_function loss: 114.5367
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 56.8941
                       Mean reward: 854.27
               Mean episode length: 227.63
    Episode_Reward/reaching_object: 1.2026
     Episode_Reward/lifting_object: 170.9163
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 190119936
                    Iteration time: 2.09s
                      Time elapsed: 01:08:19
                               ETA: 00:02:22

################################################################################
                     [1m Learning iteration 1934/2000 [0m                     

                       Computation: 49640 steps/s (collection: 1.876s, learning 0.104s)
             Mean action noise std: 3.36
          Mean value_function loss: 139.1330
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 56.9098
                       Mean reward: 899.42
               Mean episode length: 238.63
    Episode_Reward/reaching_object: 1.2286
     Episode_Reward/lifting_object: 174.5060
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 190218240
                    Iteration time: 1.98s
                      Time elapsed: 01:08:21
                               ETA: 00:02:19

################################################################################
                     [1m Learning iteration 1935/2000 [0m                     

                       Computation: 46488 steps/s (collection: 2.009s, learning 0.106s)
             Mean action noise std: 3.36
          Mean value_function loss: 130.6072
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 56.9196
                       Mean reward: 867.05
               Mean episode length: 231.44
    Episode_Reward/reaching_object: 1.2055
     Episode_Reward/lifting_object: 171.0767
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0572
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 190316544
                    Iteration time: 2.11s
                      Time elapsed: 01:08:23
                               ETA: 00:02:17

################################################################################
                     [1m Learning iteration 1936/2000 [0m                     

                       Computation: 48977 steps/s (collection: 1.915s, learning 0.093s)
             Mean action noise std: 3.36
          Mean value_function loss: 111.4387
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 56.9245
                       Mean reward: 894.53
               Mean episode length: 237.02
    Episode_Reward/reaching_object: 1.2192
     Episode_Reward/lifting_object: 173.2740
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 190414848
                    Iteration time: 2.01s
                      Time elapsed: 01:08:25
                               ETA: 00:02:15

################################################################################
                     [1m Learning iteration 1937/2000 [0m                     

                       Computation: 49546 steps/s (collection: 1.875s, learning 0.109s)
             Mean action noise std: 3.37
          Mean value_function loss: 127.0574
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.9328
                       Mean reward: 890.81
               Mean episode length: 236.84
    Episode_Reward/reaching_object: 1.2597
     Episode_Reward/lifting_object: 179.6039
      Episode_Reward/object_height: 0.0669
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 190513152
                    Iteration time: 1.98s
                      Time elapsed: 01:08:27
                               ETA: 00:02:13

################################################################################
                     [1m Learning iteration 1938/2000 [0m                     

                       Computation: 48631 steps/s (collection: 1.896s, learning 0.125s)
             Mean action noise std: 3.37
          Mean value_function loss: 142.2554
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 56.9409
                       Mean reward: 808.27
               Mean episode length: 216.38
    Episode_Reward/reaching_object: 1.2052
     Episode_Reward/lifting_object: 171.4129
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 190611456
                    Iteration time: 2.02s
                      Time elapsed: 01:08:29
                               ETA: 00:02:11

################################################################################
                     [1m Learning iteration 1939/2000 [0m                     

                       Computation: 46574 steps/s (collection: 2.000s, learning 0.111s)
             Mean action noise std: 3.37
          Mean value_function loss: 131.7947
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 56.9441
                       Mean reward: 861.97
               Mean episode length: 230.32
    Episode_Reward/reaching_object: 1.2212
     Episode_Reward/lifting_object: 173.8688
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.0582
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 190709760
                    Iteration time: 2.11s
                      Time elapsed: 01:08:31
                               ETA: 00:02:09

################################################################################
                     [1m Learning iteration 1940/2000 [0m                     

                       Computation: 48331 steps/s (collection: 1.925s, learning 0.109s)
             Mean action noise std: 3.37
          Mean value_function loss: 129.3546
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 56.9481
                       Mean reward: 844.90
               Mean episode length: 226.16
    Episode_Reward/reaching_object: 1.1926
     Episode_Reward/lifting_object: 169.7246
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0572
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 190808064
                    Iteration time: 2.03s
                      Time elapsed: 01:08:33
                               ETA: 00:02:07

################################################################################
                     [1m Learning iteration 1941/2000 [0m                     

                       Computation: 48418 steps/s (collection: 1.930s, learning 0.100s)
             Mean action noise std: 3.37
          Mean value_function loss: 113.6006
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 56.9533
                       Mean reward: 888.97
               Mean episode length: 236.06
    Episode_Reward/reaching_object: 1.1958
     Episode_Reward/lifting_object: 170.2495
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 190906368
                    Iteration time: 2.03s
                      Time elapsed: 01:08:35
                               ETA: 00:02:05

################################################################################
                     [1m Learning iteration 1942/2000 [0m                     

                       Computation: 50068 steps/s (collection: 1.865s, learning 0.098s)
             Mean action noise std: 3.37
          Mean value_function loss: 120.2990
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.9590
                       Mean reward: 852.70
               Mean episode length: 227.86
    Episode_Reward/reaching_object: 1.2274
     Episode_Reward/lifting_object: 175.2750
      Episode_Reward/object_height: 0.0655
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 191004672
                    Iteration time: 1.96s
                      Time elapsed: 01:08:37
                               ETA: 00:02:02

################################################################################
                     [1m Learning iteration 1943/2000 [0m                     

                       Computation: 48391 steps/s (collection: 1.937s, learning 0.095s)
             Mean action noise std: 3.37
          Mean value_function loss: 143.3416
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 56.9672
                       Mean reward: 805.67
               Mean episode length: 215.87
    Episode_Reward/reaching_object: 1.1918
     Episode_Reward/lifting_object: 169.9760
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 191102976
                    Iteration time: 2.03s
                      Time elapsed: 01:08:39
                               ETA: 00:02:00

################################################################################
                     [1m Learning iteration 1944/2000 [0m                     

                       Computation: 49589 steps/s (collection: 1.889s, learning 0.093s)
             Mean action noise std: 3.37
          Mean value_function loss: 106.4202
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 56.9758
                       Mean reward: 868.58
               Mean episode length: 231.38
    Episode_Reward/reaching_object: 1.2281
     Episode_Reward/lifting_object: 175.0396
      Episode_Reward/object_height: 0.0655
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 191201280
                    Iteration time: 1.98s
                      Time elapsed: 01:08:41
                               ETA: 00:01:58

################################################################################
                     [1m Learning iteration 1945/2000 [0m                     

                       Computation: 49361 steps/s (collection: 1.893s, learning 0.098s)
             Mean action noise std: 3.37
          Mean value_function loss: 125.6502
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 56.9814
                       Mean reward: 909.98
               Mean episode length: 243.10
    Episode_Reward/reaching_object: 1.2171
     Episode_Reward/lifting_object: 172.8118
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 191299584
                    Iteration time: 1.99s
                      Time elapsed: 01:08:43
                               ETA: 00:01:56

################################################################################
                     [1m Learning iteration 1946/2000 [0m                     

                       Computation: 49437 steps/s (collection: 1.852s, learning 0.137s)
             Mean action noise std: 3.37
          Mean value_function loss: 107.9484
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.9859
                       Mean reward: 843.66
               Mean episode length: 226.24
    Episode_Reward/reaching_object: 1.2026
     Episode_Reward/lifting_object: 170.8013
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 191397888
                    Iteration time: 1.99s
                      Time elapsed: 01:08:45
                               ETA: 00:01:54

################################################################################
                     [1m Learning iteration 1947/2000 [0m                     

                       Computation: 49310 steps/s (collection: 1.896s, learning 0.098s)
             Mean action noise std: 3.38
          Mean value_function loss: 134.8173
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 56.9952
                       Mean reward: 887.01
               Mean episode length: 236.50
    Episode_Reward/reaching_object: 1.2476
     Episode_Reward/lifting_object: 177.4183
      Episode_Reward/object_height: 0.0664
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 191496192
                    Iteration time: 1.99s
                      Time elapsed: 01:08:47
                               ETA: 00:01:52

################################################################################
                     [1m Learning iteration 1948/2000 [0m                     

                       Computation: 48678 steps/s (collection: 1.922s, learning 0.097s)
             Mean action noise std: 3.38
          Mean value_function loss: 108.2769
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 57.0059
                       Mean reward: 875.23
               Mean episode length: 231.90
    Episode_Reward/reaching_object: 1.2320
     Episode_Reward/lifting_object: 174.9794
      Episode_Reward/object_height: 0.0659
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 191594496
                    Iteration time: 2.02s
                      Time elapsed: 01:08:49
                               ETA: 00:01:50

################################################################################
                     [1m Learning iteration 1949/2000 [0m                     

                       Computation: 49374 steps/s (collection: 1.892s, learning 0.099s)
             Mean action noise std: 3.38
          Mean value_function loss: 97.7385
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 57.0170
                       Mean reward: 863.89
               Mean episode length: 230.32
    Episode_Reward/reaching_object: 1.2282
     Episode_Reward/lifting_object: 174.0739
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 191692800
                    Iteration time: 1.99s
                      Time elapsed: 01:08:51
                               ETA: 00:01:48

################################################################################
                     [1m Learning iteration 1950/2000 [0m                     

                       Computation: 49235 steps/s (collection: 1.905s, learning 0.092s)
             Mean action noise std: 3.38
          Mean value_function loss: 124.4193
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 57.0251
                       Mean reward: 906.91
               Mean episode length: 240.35
    Episode_Reward/reaching_object: 1.2113
     Episode_Reward/lifting_object: 171.5747
      Episode_Reward/object_height: 0.0647
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 191791104
                    Iteration time: 2.00s
                      Time elapsed: 01:08:53
                               ETA: 00:01:45

################################################################################
                     [1m Learning iteration 1951/2000 [0m                     

                       Computation: 48173 steps/s (collection: 1.937s, learning 0.104s)
             Mean action noise std: 3.38
          Mean value_function loss: 91.9632
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 57.0313
                       Mean reward: 860.17
               Mean episode length: 229.79
    Episode_Reward/reaching_object: 1.2160
     Episode_Reward/lifting_object: 171.9437
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.0582
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 191889408
                    Iteration time: 2.04s
                      Time elapsed: 01:08:55
                               ETA: 00:01:43

################################################################################
                     [1m Learning iteration 1952/2000 [0m                     

                       Computation: 48631 steps/s (collection: 1.922s, learning 0.099s)
             Mean action noise std: 3.38
          Mean value_function loss: 71.6237
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 57.0375
                       Mean reward: 916.61
               Mean episode length: 242.81
    Episode_Reward/reaching_object: 1.2589
     Episode_Reward/lifting_object: 178.3807
      Episode_Reward/object_height: 0.0679
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 191987712
                    Iteration time: 2.02s
                      Time elapsed: 01:08:57
                               ETA: 00:01:41

################################################################################
                     [1m Learning iteration 1953/2000 [0m                     

                       Computation: 46274 steps/s (collection: 2.014s, learning 0.110s)
             Mean action noise std: 3.38
          Mean value_function loss: 104.5838
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 57.0455
                       Mean reward: 860.17
               Mean episode length: 230.77
    Episode_Reward/reaching_object: 1.2097
     Episode_Reward/lifting_object: 171.2236
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 192086016
                    Iteration time: 2.12s
                      Time elapsed: 01:08:59
                               ETA: 00:01:39

################################################################################
                     [1m Learning iteration 1954/2000 [0m                     

                       Computation: 49388 steps/s (collection: 1.891s, learning 0.099s)
             Mean action noise std: 3.39
          Mean value_function loss: 81.6335
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 57.0521
                       Mean reward: 867.07
               Mean episode length: 231.14
    Episode_Reward/reaching_object: 1.2390
     Episode_Reward/lifting_object: 175.2978
      Episode_Reward/object_height: 0.0666
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 192184320
                    Iteration time: 1.99s
                      Time elapsed: 01:09:01
                               ETA: 00:01:37

################################################################################
                     [1m Learning iteration 1955/2000 [0m                     

                       Computation: 49463 steps/s (collection: 1.889s, learning 0.099s)
             Mean action noise std: 3.39
          Mean value_function loss: 94.8003
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 57.0589
                       Mean reward: 892.56
               Mean episode length: 236.30
    Episode_Reward/reaching_object: 1.2342
     Episode_Reward/lifting_object: 175.1253
      Episode_Reward/object_height: 0.0664
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 192282624
                    Iteration time: 1.99s
                      Time elapsed: 01:09:03
                               ETA: 00:01:35

################################################################################
                     [1m Learning iteration 1956/2000 [0m                     

                       Computation: 49547 steps/s (collection: 1.889s, learning 0.095s)
             Mean action noise std: 3.39
          Mean value_function loss: 143.3755
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 57.0639
                       Mean reward: 845.03
               Mean episode length: 226.28
    Episode_Reward/reaching_object: 1.2266
     Episode_Reward/lifting_object: 173.6500
      Episode_Reward/object_height: 0.0657
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 192380928
                    Iteration time: 1.98s
                      Time elapsed: 01:09:05
                               ETA: 00:01:33

################################################################################
                     [1m Learning iteration 1957/2000 [0m                     

                       Computation: 49024 steps/s (collection: 1.877s, learning 0.129s)
             Mean action noise std: 3.39
          Mean value_function loss: 132.6879
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 57.0700
                       Mean reward: 859.13
               Mean episode length: 230.56
    Episode_Reward/reaching_object: 1.2277
     Episode_Reward/lifting_object: 173.5660
      Episode_Reward/object_height: 0.0661
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 192479232
                    Iteration time: 2.01s
                      Time elapsed: 01:09:07
                               ETA: 00:01:31

################################################################################
                     [1m Learning iteration 1958/2000 [0m                     

                       Computation: 45403 steps/s (collection: 2.014s, learning 0.151s)
             Mean action noise std: 3.39
          Mean value_function loss: 129.5463
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 57.0738
                       Mean reward: 854.43
               Mean episode length: 228.74
    Episode_Reward/reaching_object: 1.2356
     Episode_Reward/lifting_object: 174.6275
      Episode_Reward/object_height: 0.0665
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 192577536
                    Iteration time: 2.17s
                      Time elapsed: 01:09:09
                               ETA: 00:01:28

################################################################################
                     [1m Learning iteration 1959/2000 [0m                     

                       Computation: 48829 steps/s (collection: 1.915s, learning 0.098s)
             Mean action noise std: 3.39
          Mean value_function loss: 109.0946
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.0773
                       Mean reward: 892.84
               Mean episode length: 237.34
    Episode_Reward/reaching_object: 1.2281
     Episode_Reward/lifting_object: 173.9701
      Episode_Reward/object_height: 0.0662
        Episode_Reward/action_rate: -0.0591
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 192675840
                    Iteration time: 2.01s
                      Time elapsed: 01:09:11
                               ETA: 00:01:26

################################################################################
                     [1m Learning iteration 1960/2000 [0m                     

                       Computation: 50419 steps/s (collection: 1.848s, learning 0.102s)
             Mean action noise std: 3.39
          Mean value_function loss: 151.9882
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 57.0853
                       Mean reward: 831.26
               Mean episode length: 222.94
    Episode_Reward/reaching_object: 1.1746
     Episode_Reward/lifting_object: 165.4647
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 192774144
                    Iteration time: 1.95s
                      Time elapsed: 01:09:13
                               ETA: 00:01:24

################################################################################
                     [1m Learning iteration 1961/2000 [0m                     

                       Computation: 45629 steps/s (collection: 1.990s, learning 0.164s)
             Mean action noise std: 3.39
          Mean value_function loss: 112.8031
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 57.0910
                       Mean reward: 865.68
               Mean episode length: 230.81
    Episode_Reward/reaching_object: 1.2425
     Episode_Reward/lifting_object: 176.0749
      Episode_Reward/object_height: 0.0681
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 192872448
                    Iteration time: 2.15s
                      Time elapsed: 01:09:15
                               ETA: 00:01:22

################################################################################
                     [1m Learning iteration 1962/2000 [0m                     

                       Computation: 49341 steps/s (collection: 1.896s, learning 0.096s)
             Mean action noise std: 3.39
          Mean value_function loss: 114.3395
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 57.0968
                       Mean reward: 870.08
               Mean episode length: 231.16
    Episode_Reward/reaching_object: 1.2170
     Episode_Reward/lifting_object: 172.7301
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 192970752
                    Iteration time: 1.99s
                      Time elapsed: 01:09:17
                               ETA: 00:01:20

################################################################################
                     [1m Learning iteration 1963/2000 [0m                     

                       Computation: 47561 steps/s (collection: 1.945s, learning 0.122s)
             Mean action noise std: 3.39
          Mean value_function loss: 117.1350
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.1043
                       Mean reward: 850.82
               Mean episode length: 226.69
    Episode_Reward/reaching_object: 1.1966
     Episode_Reward/lifting_object: 170.0284
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0577
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 193069056
                    Iteration time: 2.07s
                      Time elapsed: 01:09:20
                               ETA: 00:01:18

################################################################################
                     [1m Learning iteration 1964/2000 [0m                     

                       Computation: 49875 steps/s (collection: 1.874s, learning 0.097s)
             Mean action noise std: 3.40
          Mean value_function loss: 122.8531
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 57.1135
                       Mean reward: 888.04
               Mean episode length: 236.50
    Episode_Reward/reaching_object: 1.2189
     Episode_Reward/lifting_object: 173.4401
      Episode_Reward/object_height: 0.0668
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 193167360
                    Iteration time: 1.97s
                      Time elapsed: 01:09:21
                               ETA: 00:01:16

################################################################################
                     [1m Learning iteration 1965/2000 [0m                     

                       Computation: 46104 steps/s (collection: 1.974s, learning 0.158s)
             Mean action noise std: 3.40
          Mean value_function loss: 87.1581
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 57.1258
                       Mean reward: 878.58
               Mean episode length: 232.50
    Episode_Reward/reaching_object: 1.2162
     Episode_Reward/lifting_object: 173.5052
      Episode_Reward/object_height: 0.0670
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 193265664
                    Iteration time: 2.13s
                      Time elapsed: 01:09:24
                               ETA: 00:01:14

################################################################################
                     [1m Learning iteration 1966/2000 [0m                     

                       Computation: 47407 steps/s (collection: 1.931s, learning 0.142s)
             Mean action noise std: 3.40
          Mean value_function loss: 119.4625
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.1408
                       Mean reward: 888.70
               Mean episode length: 236.05
    Episode_Reward/reaching_object: 1.2214
     Episode_Reward/lifting_object: 174.3363
      Episode_Reward/object_height: 0.0667
        Episode_Reward/action_rate: -0.0590
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 193363968
                    Iteration time: 2.07s
                      Time elapsed: 01:09:26
                               ETA: 00:01:12

################################################################################
                     [1m Learning iteration 1967/2000 [0m                     

                       Computation: 46836 steps/s (collection: 1.938s, learning 0.161s)
             Mean action noise std: 3.40
          Mean value_function loss: 95.2900
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 57.1500
                       Mean reward: 910.75
               Mean episode length: 241.36
    Episode_Reward/reaching_object: 1.2363
     Episode_Reward/lifting_object: 176.9989
      Episode_Reward/object_height: 0.0681
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 193462272
                    Iteration time: 2.10s
                      Time elapsed: 01:09:28
                               ETA: 00:01:09

################################################################################
                     [1m Learning iteration 1968/2000 [0m                     

                       Computation: 48114 steps/s (collection: 1.911s, learning 0.132s)
             Mean action noise std: 3.40
          Mean value_function loss: 130.7281
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 57.1600
                       Mean reward: 885.36
               Mean episode length: 234.75
    Episode_Reward/reaching_object: 1.2053
     Episode_Reward/lifting_object: 172.4246
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 193560576
                    Iteration time: 2.04s
                      Time elapsed: 01:09:30
                               ETA: 00:01:07

################################################################################
                     [1m Learning iteration 1969/2000 [0m                     

                       Computation: 50090 steps/s (collection: 1.871s, learning 0.091s)
             Mean action noise std: 3.40
          Mean value_function loss: 116.2731
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.1691
                       Mean reward: 893.84
               Mean episode length: 236.96
    Episode_Reward/reaching_object: 1.2099
     Episode_Reward/lifting_object: 172.9424
      Episode_Reward/object_height: 0.0666
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 193658880
                    Iteration time: 1.96s
                      Time elapsed: 01:09:32
                               ETA: 00:01:05

################################################################################
                     [1m Learning iteration 1970/2000 [0m                     

                       Computation: 48724 steps/s (collection: 1.926s, learning 0.092s)
             Mean action noise std: 3.40
          Mean value_function loss: 107.6492
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 57.1752
                       Mean reward: 881.80
               Mean episode length: 234.47
    Episode_Reward/reaching_object: 1.2121
     Episode_Reward/lifting_object: 173.1567
      Episode_Reward/object_height: 0.0669
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 193757184
                    Iteration time: 2.02s
                      Time elapsed: 01:09:34
                               ETA: 00:01:03

################################################################################
                     [1m Learning iteration 1971/2000 [0m                     

                       Computation: 48543 steps/s (collection: 1.915s, learning 0.111s)
             Mean action noise std: 3.41
          Mean value_function loss: 95.1812
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 57.1813
                       Mean reward: 910.90
               Mean episode length: 241.48
    Episode_Reward/reaching_object: 1.2297
     Episode_Reward/lifting_object: 175.6485
      Episode_Reward/object_height: 0.0675
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 193855488
                    Iteration time: 2.03s
                      Time elapsed: 01:09:36
                               ETA: 00:01:01

################################################################################
                     [1m Learning iteration 1972/2000 [0m                     

                       Computation: 48886 steps/s (collection: 1.856s, learning 0.155s)
             Mean action noise std: 3.41
          Mean value_function loss: 127.6242
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 57.1892
                       Mean reward: 854.77
               Mean episode length: 227.72
    Episode_Reward/reaching_object: 1.2106
     Episode_Reward/lifting_object: 172.3258
      Episode_Reward/object_height: 0.0662
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 193953792
                    Iteration time: 2.01s
                      Time elapsed: 01:09:38
                               ETA: 00:00:59

################################################################################
                     [1m Learning iteration 1973/2000 [0m                     

                       Computation: 49632 steps/s (collection: 1.882s, learning 0.099s)
             Mean action noise std: 3.41
          Mean value_function loss: 216.8622
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 57.1957
                       Mean reward: 897.73
               Mean episode length: 239.50
    Episode_Reward/reaching_object: 1.2406
     Episode_Reward/lifting_object: 176.5157
      Episode_Reward/object_height: 0.0678
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 194052096
                    Iteration time: 1.98s
                      Time elapsed: 01:09:40
                               ETA: 00:00:57

################################################################################
                     [1m Learning iteration 1974/2000 [0m                     

                       Computation: 47810 steps/s (collection: 1.921s, learning 0.135s)
             Mean action noise std: 3.41
          Mean value_function loss: 125.7975
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.2012
                       Mean reward: 884.74
               Mean episode length: 234.61
    Episode_Reward/reaching_object: 1.2049
     Episode_Reward/lifting_object: 171.2498
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 194150400
                    Iteration time: 2.06s
                      Time elapsed: 01:09:42
                               ETA: 00:00:55

################################################################################
                     [1m Learning iteration 1975/2000 [0m                     

                       Computation: 49098 steps/s (collection: 1.887s, learning 0.115s)
             Mean action noise std: 3.41
          Mean value_function loss: 133.8307
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.2095
                       Mean reward: 830.12
               Mean episode length: 221.78
    Episode_Reward/reaching_object: 1.1874
     Episode_Reward/lifting_object: 168.1313
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0575
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 194248704
                    Iteration time: 2.00s
                      Time elapsed: 01:09:44
                               ETA: 00:00:52

################################################################################
                     [1m Learning iteration 1976/2000 [0m                     

                       Computation: 48020 steps/s (collection: 1.911s, learning 0.137s)
             Mean action noise std: 3.41
          Mean value_function loss: 96.6556
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 57.2156
                       Mean reward: 878.08
               Mean episode length: 232.63
    Episode_Reward/reaching_object: 1.2235
     Episode_Reward/lifting_object: 174.0280
      Episode_Reward/object_height: 0.0655
        Episode_Reward/action_rate: -0.0591
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 194347008
                    Iteration time: 2.05s
                      Time elapsed: 01:09:46
                               ETA: 00:00:50

################################################################################
                     [1m Learning iteration 1977/2000 [0m                     

                       Computation: 48400 steps/s (collection: 1.927s, learning 0.105s)
             Mean action noise std: 3.41
          Mean value_function loss: 163.7720
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 57.2211
                       Mean reward: 863.69
               Mean episode length: 230.22
    Episode_Reward/reaching_object: 1.2050
     Episode_Reward/lifting_object: 171.0206
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 194445312
                    Iteration time: 2.03s
                      Time elapsed: 01:09:48
                               ETA: 00:00:48

################################################################################
                     [1m Learning iteration 1978/2000 [0m                     

                       Computation: 49112 steps/s (collection: 1.894s, learning 0.108s)
             Mean action noise std: 3.41
          Mean value_function loss: 146.8234
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 57.2326
                       Mean reward: 804.96
               Mean episode length: 216.35
    Episode_Reward/reaching_object: 1.1890
     Episode_Reward/lifting_object: 168.3116
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.0575
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 194543616
                    Iteration time: 2.00s
                      Time elapsed: 01:09:50
                               ETA: 00:00:46

################################################################################
                     [1m Learning iteration 1979/2000 [0m                     

                       Computation: 49561 steps/s (collection: 1.880s, learning 0.103s)
             Mean action noise std: 3.41
          Mean value_function loss: 156.3777
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 57.2389
                       Mean reward: 861.02
               Mean episode length: 229.30
    Episode_Reward/reaching_object: 1.1943
     Episode_Reward/lifting_object: 169.4293
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 194641920
                    Iteration time: 1.98s
                      Time elapsed: 01:09:52
                               ETA: 00:00:44

################################################################################
                     [1m Learning iteration 1980/2000 [0m                     

                       Computation: 49102 steps/s (collection: 1.897s, learning 0.105s)
             Mean action noise std: 3.42
          Mean value_function loss: 157.8206
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 57.2464
                       Mean reward: 856.91
               Mean episode length: 228.34
    Episode_Reward/reaching_object: 1.1866
     Episode_Reward/lifting_object: 168.0679
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.0576
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 194740224
                    Iteration time: 2.00s
                      Time elapsed: 01:09:54
                               ETA: 00:00:42

################################################################################
                     [1m Learning iteration 1981/2000 [0m                     

                       Computation: 48768 steps/s (collection: 1.885s, learning 0.131s)
             Mean action noise std: 3.42
          Mean value_function loss: 165.0147
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 57.2539
                       Mean reward: 819.21
               Mean episode length: 219.08
    Episode_Reward/reaching_object: 1.1833
     Episode_Reward/lifting_object: 167.5241
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 194838528
                    Iteration time: 2.02s
                      Time elapsed: 01:09:56
                               ETA: 00:00:40

################################################################################
                     [1m Learning iteration 1982/2000 [0m                     

                       Computation: 49532 steps/s (collection: 1.860s, learning 0.125s)
             Mean action noise std: 3.42
          Mean value_function loss: 133.4319
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 57.2590
                       Mean reward: 854.21
               Mean episode length: 227.32
    Episode_Reward/reaching_object: 1.2115
     Episode_Reward/lifting_object: 171.7819
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 194936832
                    Iteration time: 1.98s
                      Time elapsed: 01:09:58
                               ETA: 00:00:38

################################################################################
                     [1m Learning iteration 1983/2000 [0m                     

                       Computation: 49581 steps/s (collection: 1.866s, learning 0.117s)
             Mean action noise std: 3.42
          Mean value_function loss: 139.4017
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 57.2628
                       Mean reward: 868.68
               Mean episode length: 230.40
    Episode_Reward/reaching_object: 1.2066
     Episode_Reward/lifting_object: 171.4341
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 195035136
                    Iteration time: 1.98s
                      Time elapsed: 01:10:00
                               ETA: 00:00:35

################################################################################
                     [1m Learning iteration 1984/2000 [0m                     

                       Computation: 48351 steps/s (collection: 1.914s, learning 0.119s)
             Mean action noise std: 3.42
          Mean value_function loss: 141.4636
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 57.2691
                       Mean reward: 874.75
               Mean episode length: 232.79
    Episode_Reward/reaching_object: 1.1901
     Episode_Reward/lifting_object: 168.9131
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 195133440
                    Iteration time: 2.03s
                      Time elapsed: 01:10:02
                               ETA: 00:00:33

################################################################################
                     [1m Learning iteration 1985/2000 [0m                     

                       Computation: 48420 steps/s (collection: 1.930s, learning 0.100s)
             Mean action noise std: 3.42
          Mean value_function loss: 115.7243
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 57.2774
                       Mean reward: 861.53
               Mean episode length: 230.06
    Episode_Reward/reaching_object: 1.2171
     Episode_Reward/lifting_object: 172.8988
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 195231744
                    Iteration time: 2.03s
                      Time elapsed: 01:10:04
                               ETA: 00:00:31

################################################################################
                     [1m Learning iteration 1986/2000 [0m                     

                       Computation: 48704 steps/s (collection: 1.910s, learning 0.109s)
             Mean action noise std: 3.42
          Mean value_function loss: 172.3762
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 57.2819
                       Mean reward: 805.59
               Mean episode length: 215.70
    Episode_Reward/reaching_object: 1.1647
     Episode_Reward/lifting_object: 165.0913
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 195330048
                    Iteration time: 2.02s
                      Time elapsed: 01:10:06
                               ETA: 00:00:29

################################################################################
                     [1m Learning iteration 1987/2000 [0m                     

                       Computation: 48940 steps/s (collection: 1.901s, learning 0.108s)
             Mean action noise std: 3.42
          Mean value_function loss: 127.8250
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.2871
                       Mean reward: 849.04
               Mean episode length: 226.63
    Episode_Reward/reaching_object: 1.2289
     Episode_Reward/lifting_object: 174.2523
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 195428352
                    Iteration time: 2.01s
                      Time elapsed: 01:10:08
                               ETA: 00:00:27

################################################################################
                     [1m Learning iteration 1988/2000 [0m                     

                       Computation: 49311 steps/s (collection: 1.891s, learning 0.103s)
             Mean action noise std: 3.42
          Mean value_function loss: 143.0028
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.2956
                       Mean reward: 811.80
               Mean episode length: 217.91
    Episode_Reward/reaching_object: 1.2201
     Episode_Reward/lifting_object: 173.2625
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 195526656
                    Iteration time: 1.99s
                      Time elapsed: 01:10:10
                               ETA: 00:00:25

################################################################################
                     [1m Learning iteration 1989/2000 [0m                     

                       Computation: 47720 steps/s (collection: 1.944s, learning 0.116s)
             Mean action noise std: 3.42
          Mean value_function loss: 159.0061
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 57.3057
                       Mean reward: 859.14
               Mean episode length: 228.60
    Episode_Reward/reaching_object: 1.1824
     Episode_Reward/lifting_object: 167.5253
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 195624960
                    Iteration time: 2.06s
                      Time elapsed: 01:10:12
                               ETA: 00:00:23

################################################################################
                     [1m Learning iteration 1990/2000 [0m                     

                       Computation: 47997 steps/s (collection: 1.927s, learning 0.122s)
             Mean action noise std: 3.43
          Mean value_function loss: 97.7882
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.3110
                       Mean reward: 897.80
               Mean episode length: 239.23
    Episode_Reward/reaching_object: 1.2010
     Episode_Reward/lifting_object: 169.5285
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 195723264
                    Iteration time: 2.05s
                      Time elapsed: 01:10:14
                               ETA: 00:00:21

################################################################################
                     [1m Learning iteration 1991/2000 [0m                     

                       Computation: 49063 steps/s (collection: 1.913s, learning 0.091s)
             Mean action noise std: 3.43
          Mean value_function loss: 101.6608
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.3171
                       Mean reward: 895.63
               Mean episode length: 237.24
    Episode_Reward/reaching_object: 1.2331
     Episode_Reward/lifting_object: 175.2337
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 195821568
                    Iteration time: 2.00s
                      Time elapsed: 01:10:16
                               ETA: 00:00:19

################################################################################
                     [1m Learning iteration 1992/2000 [0m                     

                       Computation: 48579 steps/s (collection: 1.905s, learning 0.119s)
             Mean action noise std: 3.43
          Mean value_function loss: 97.6793
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.3259
                       Mean reward: 825.35
               Mean episode length: 219.65
    Episode_Reward/reaching_object: 1.2094
     Episode_Reward/lifting_object: 171.6472
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0590
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 195919872
                    Iteration time: 2.02s
                      Time elapsed: 01:10:18
                               ETA: 00:00:16

################################################################################
                     [1m Learning iteration 1993/2000 [0m                     

                       Computation: 49625 steps/s (collection: 1.861s, learning 0.120s)
             Mean action noise std: 3.43
          Mean value_function loss: 130.5726
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 57.3346
                       Mean reward: 847.62
               Mean episode length: 225.28
    Episode_Reward/reaching_object: 1.1938
     Episode_Reward/lifting_object: 169.1945
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 196018176
                    Iteration time: 1.98s
                      Time elapsed: 01:10:20
                               ETA: 00:00:14

################################################################################
                     [1m Learning iteration 1994/2000 [0m                     

                       Computation: 48481 steps/s (collection: 1.904s, learning 0.124s)
             Mean action noise std: 3.43
          Mean value_function loss: 130.1105
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.3411
                       Mean reward: 863.61
               Mean episode length: 229.58
    Episode_Reward/reaching_object: 1.1974
     Episode_Reward/lifting_object: 170.3152
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 196116480
                    Iteration time: 2.03s
                      Time elapsed: 01:10:22
                               ETA: 00:00:12

################################################################################
                     [1m Learning iteration 1995/2000 [0m                     

                       Computation: 49190 steps/s (collection: 1.876s, learning 0.122s)
             Mean action noise std: 3.43
          Mean value_function loss: 98.5939
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 57.3504
                       Mean reward: 878.18
               Mean episode length: 232.72
    Episode_Reward/reaching_object: 1.2179
     Episode_Reward/lifting_object: 173.5590
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 196214784
                    Iteration time: 2.00s
                      Time elapsed: 01:10:24
                               ETA: 00:00:10

################################################################################
                     [1m Learning iteration 1996/2000 [0m                     

                       Computation: 49007 steps/s (collection: 1.885s, learning 0.121s)
             Mean action noise std: 3.43
          Mean value_function loss: 103.6007
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.3570
                       Mean reward: 862.44
               Mean episode length: 229.83
    Episode_Reward/reaching_object: 1.2126
     Episode_Reward/lifting_object: 172.0246
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0596
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 196313088
                    Iteration time: 2.01s
                      Time elapsed: 01:10:26
                               ETA: 00:00:08

################################################################################
                     [1m Learning iteration 1997/2000 [0m                     

                       Computation: 48843 steps/s (collection: 1.898s, learning 0.115s)
             Mean action noise std: 3.43
          Mean value_function loss: 121.7119
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 57.3655
                       Mean reward: 841.87
               Mean episode length: 224.84
    Episode_Reward/reaching_object: 1.1908
     Episode_Reward/lifting_object: 169.1015
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 196411392
                    Iteration time: 2.01s
                      Time elapsed: 01:10:28
                               ETA: 00:00:06

################################################################################
                     [1m Learning iteration 1998/2000 [0m                     

                       Computation: 48592 steps/s (collection: 1.901s, learning 0.122s)
             Mean action noise std: 3.44
          Mean value_function loss: 137.6913
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 57.3755
                       Mean reward: 864.98
               Mean episode length: 230.70
    Episode_Reward/reaching_object: 1.2354
     Episode_Reward/lifting_object: 176.0583
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0608
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 196509696
                    Iteration time: 2.02s
                      Time elapsed: 01:10:30
                               ETA: 00:00:04

################################################################################
                     [1m Learning iteration 1999/2000 [0m                     

                       Computation: 49350 steps/s (collection: 1.902s, learning 0.090s)
             Mean action noise std: 3.44
          Mean value_function loss: 121.1789
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 57.3860
                       Mean reward: 859.46
               Mean episode length: 228.73
    Episode_Reward/reaching_object: 1.2311
     Episode_Reward/lifting_object: 175.2410
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 196608000
                    Iteration time: 1.99s
                      Time elapsed: 01:10:32
                               ETA: 00:00:02

