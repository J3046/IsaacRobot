################################################################################
                      [1m Learning iteration 0/2000 [0m                       

                       Computation: 18494 steps/s (collection: 5.062s, learning 0.253s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 11.3613
                       Mean reward: 0.00
               Mean episode length: 21.40
    Episode_Reward/reaching_object: 0.0002
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0001
        Episode_Reward/action_rate: -0.0001
          Episode_Reward/joint_vel: -0.0001
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 5.32s
                      Time elapsed: 00:00:05
                               ETA: 02:57:10

################################################################################
                      [1m Learning iteration 1/2000 [0m                       

                       Computation: 29642 steps/s (collection: 3.148s, learning 0.168s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0009
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 11.3679
                       Mean reward: 0.01
               Mean episode length: 45.09
    Episode_Reward/reaching_object: 0.0012
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0003
        Episode_Reward/action_rate: -0.0002
          Episode_Reward/joint_vel: -0.0004
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 3.32s
                      Time elapsed: 00:00:08
                               ETA: 02:23:47

################################################################################
                      [1m Learning iteration 2/2000 [0m                       

                       Computation: 29311 steps/s (collection: 3.190s, learning 0.164s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 11.3522
                       Mean reward: 0.01
               Mean episode length: 69.30
    Episode_Reward/reaching_object: 0.0021
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0005
        Episode_Reward/action_rate: -0.0004
          Episode_Reward/joint_vel: -0.0006
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 3.35s
                      Time elapsed: 00:00:11
                               ETA: 02:13:02

################################################################################
                      [1m Learning iteration 3/2000 [0m                       

                       Computation: 29641 steps/s (collection: 3.146s, learning 0.171s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 11.3696
                       Mean reward: 0.01
               Mean episode length: 93.06
    Episode_Reward/reaching_object: 0.0030
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0007
        Episode_Reward/action_rate: -0.0005
          Episode_Reward/joint_vel: -0.0008
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 3.32s
                      Time elapsed: 00:00:15
                               ETA: 02:07:19

################################################################################
                      [1m Learning iteration 4/2000 [0m                       

                       Computation: 30132 steps/s (collection: 3.116s, learning 0.147s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 11.3728
                       Mean reward: 0.01
               Mean episode length: 117.24
    Episode_Reward/reaching_object: 0.0040
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0007
          Episode_Reward/joint_vel: -0.0010
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 3.26s
                      Time elapsed: 00:00:18
                               ETA: 02:03:30

################################################################################
                      [1m Learning iteration 5/2000 [0m                       

                       Computation: 32715 steps/s (collection: 2.864s, learning 0.141s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 11.3825
                       Mean reward: 0.02
               Mean episode length: 141.16
    Episode_Reward/reaching_object: 0.0055
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0009
          Episode_Reward/joint_vel: -0.0013
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 3.00s
                      Time elapsed: 00:00:21
                               ETA: 01:59:31

################################################################################
                      [1m Learning iteration 6/2000 [0m                       

                       Computation: 30625 steps/s (collection: 3.037s, learning 0.173s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 11.3679
                       Mean reward: 0.03
               Mean episode length: 165.69
    Episode_Reward/reaching_object: 0.0069
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0010
          Episode_Reward/joint_vel: -0.0015
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 3.21s
                      Time elapsed: 00:00:24
                               ETA: 01:57:38

################################################################################
                      [1m Learning iteration 7/2000 [0m                       

                       Computation: 30374 steps/s (collection: 3.098s, learning 0.139s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 11.3656
                       Mean reward: 0.04
               Mean episode length: 189.43
    Episode_Reward/reaching_object: 0.0093
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0012
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 3.24s
                      Time elapsed: 00:00:28
                               ETA: 01:56:19

################################################################################
                      [1m Learning iteration 8/2000 [0m                       

                       Computation: 24878 steps/s (collection: 3.841s, learning 0.111s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 11.3355
                       Mean reward: 0.06
               Mean episode length: 213.24
    Episode_Reward/reaching_object: 0.0114
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0013
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 3.95s
                      Time elapsed: 00:00:31
                               ETA: 01:57:55

################################################################################
                      [1m Learning iteration 9/2000 [0m                       

                       Computation: 114730 steps/s (collection: 0.697s, learning 0.160s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 11.3218
                       Mean reward: 0.09
               Mean episode length: 237.77
    Episode_Reward/reaching_object: 0.0174
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0015
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 0.86s
                      Time elapsed: 00:00:32
                               ETA: 01:48:55

################################################################################
                      [1m Learning iteration 10/2000 [0m                      

                       Computation: 114930 steps/s (collection: 0.735s, learning 0.120s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 11.3145
                       Mean reward: 0.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0213
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 0.86s
                      Time elapsed: 00:00:33
                               ETA: 01:41:32

################################################################################
                      [1m Learning iteration 11/2000 [0m                      

                       Computation: 105605 steps/s (collection: 0.840s, learning 0.091s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 11.3206
                       Mean reward: 0.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0287
     Episode_Reward/lifting_object: 0.0056
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 0.93s
                      Time elapsed: 00:00:34
                               ETA: 01:35:36

################################################################################
                      [1m Learning iteration 12/2000 [0m                      

                       Computation: 111679 steps/s (collection: 0.792s, learning 0.089s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0743
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 11.3532
                       Mean reward: 0.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0374
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 0.88s
                      Time elapsed: 00:00:35
                               ETA: 01:30:27

################################################################################
                      [1m Learning iteration 13/2000 [0m                      

                       Computation: 106902 steps/s (collection: 0.800s, learning 0.119s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0397
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 11.4021
                       Mean reward: 0.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0453
     Episode_Reward/lifting_object: 0.0375
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 0.92s
                      Time elapsed: 00:00:36
                               ETA: 01:26:07

################################################################################
                      [1m Learning iteration 14/2000 [0m                      

                       Computation: 113667 steps/s (collection: 0.763s, learning 0.102s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.1797
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 11.4446
                       Mean reward: 0.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0597
     Episode_Reward/lifting_object: 0.0059
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 0.86s
                      Time elapsed: 00:00:37
                               ETA: 01:22:15

################################################################################
                      [1m Learning iteration 15/2000 [0m                      

                       Computation: 105989 steps/s (collection: 0.802s, learning 0.126s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.1287
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 11.4954
                       Mean reward: 0.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0708
     Episode_Reward/lifting_object: 0.0082
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 0.93s
                      Time elapsed: 00:00:38
                               ETA: 01:18:59

################################################################################
                      [1m Learning iteration 16/2000 [0m                      

                       Computation: 107980 steps/s (collection: 0.808s, learning 0.102s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.2419
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 11.5238
                       Mean reward: 0.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0880
     Episode_Reward/lifting_object: 0.0206
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 0.91s
                      Time elapsed: 00:00:39
                               ETA: 01:16:04

################################################################################
                      [1m Learning iteration 17/2000 [0m                      

                       Computation: 106126 steps/s (collection: 0.800s, learning 0.127s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.2378
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 11.5430
                       Mean reward: 0.67
               Mean episode length: 249.61
    Episode_Reward/reaching_object: 0.0991
     Episode_Reward/lifting_object: 0.0359
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 0.93s
                      Time elapsed: 00:00:40
                               ETA: 01:13:30

################################################################################
                      [1m Learning iteration 18/2000 [0m                      

                       Computation: 112196 steps/s (collection: 0.776s, learning 0.100s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.6776
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 11.5575
                       Mean reward: 0.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1133
     Episode_Reward/lifting_object: 0.0609
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 0.88s
                      Time elapsed: 00:00:40
                               ETA: 01:11:08

################################################################################
                      [1m Learning iteration 19/2000 [0m                      

                       Computation: 100815 steps/s (collection: 0.853s, learning 0.123s)
             Mean action noise std: 1.04
          Mean value_function loss: 0.5820
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 11.6030
                       Mean reward: 0.85
               Mean episode length: 248.86
    Episode_Reward/reaching_object: 0.1306
     Episode_Reward/lifting_object: 0.1156
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 0.98s
                      Time elapsed: 00:00:41
                               ETA: 01:09:09

################################################################################
                      [1m Learning iteration 20/2000 [0m                      

                       Computation: 108672 steps/s (collection: 0.773s, learning 0.132s)
             Mean action noise std: 1.05
          Mean value_function loss: 0.5574
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 11.6744
                       Mean reward: 1.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1485
     Episode_Reward/lifting_object: 0.1092
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 0.90s
                      Time elapsed: 00:00:42
                               ETA: 01:07:14

################################################################################
                      [1m Learning iteration 21/2000 [0m                      

                       Computation: 108777 steps/s (collection: 0.810s, learning 0.094s)
             Mean action noise std: 1.05
          Mean value_function loss: 0.4870
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 11.7224
                       Mean reward: 1.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1558
     Episode_Reward/lifting_object: 0.1232
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 0.90s
                      Time elapsed: 00:00:43
                               ETA: 01:05:30

################################################################################
                      [1m Learning iteration 22/2000 [0m                      

                       Computation: 111477 steps/s (collection: 0.790s, learning 0.092s)
             Mean action noise std: 1.06
          Mean value_function loss: 0.9399
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 11.7731
                       Mean reward: 1.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1654
     Episode_Reward/lifting_object: 0.1530
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 0.88s
                      Time elapsed: 00:00:44
                               ETA: 01:03:53

################################################################################
                      [1m Learning iteration 23/2000 [0m                      

                       Computation: 104691 steps/s (collection: 0.849s, learning 0.090s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.5988
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 11.8229
                       Mean reward: 1.55
               Mean episode length: 248.98
    Episode_Reward/reaching_object: 0.1679
     Episode_Reward/lifting_object: 0.1522
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 0.94s
                      Time elapsed: 00:00:45
                               ETA: 01:02:29

################################################################################
                      [1m Learning iteration 24/2000 [0m                      

                       Computation: 98964 steps/s (collection: 0.891s, learning 0.102s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.7941
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 11.8963
                       Mean reward: 2.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1719
     Episode_Reward/lifting_object: 0.2264
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 0.99s
                      Time elapsed: 00:00:46
                               ETA: 01:01:16

################################################################################
                      [1m Learning iteration 25/2000 [0m                      

                       Computation: 97910 steps/s (collection: 0.894s, learning 0.110s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.7263
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 11.9890
                       Mean reward: 2.87
               Mean episode length: 248.93
    Episode_Reward/reaching_object: 0.1769
     Episode_Reward/lifting_object: 0.2720
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 1.00s
                      Time elapsed: 00:00:47
                               ETA: 01:00:09

################################################################################
                      [1m Learning iteration 26/2000 [0m                      

                       Computation: 95319 steps/s (collection: 0.902s, learning 0.129s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.8143
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 12.0779
                       Mean reward: 2.59
               Mean episode length: 247.59
    Episode_Reward/reaching_object: 0.1736
     Episode_Reward/lifting_object: 0.2366
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 1.03s
                      Time elapsed: 00:00:48
                               ETA: 00:59:09

################################################################################
                      [1m Learning iteration 27/2000 [0m                      

                       Computation: 94507 steps/s (collection: 0.891s, learning 0.149s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.4501
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 12.2073
                       Mean reward: 3.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1739
     Episode_Reward/lifting_object: 0.4625
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 1.04s
                      Time elapsed: 00:00:49
                               ETA: 00:58:14

################################################################################
                      [1m Learning iteration 28/2000 [0m                      

                       Computation: 81893 steps/s (collection: 1.035s, learning 0.166s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.9607
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 12.3030
                       Mean reward: 2.30
               Mean episode length: 247.42
    Episode_Reward/reaching_object: 0.1723
     Episode_Reward/lifting_object: 0.3506
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 1.20s
                      Time elapsed: 00:00:50
                               ETA: 00:57:33

################################################################################
                      [1m Learning iteration 29/2000 [0m                      

                       Computation: 87921 steps/s (collection: 0.964s, learning 0.154s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.7691
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 12.3736
                       Mean reward: 2.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1733
     Episode_Reward/lifting_object: 0.3454
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 1.12s
                      Time elapsed: 00:00:51
                               ETA: 00:56:50

################################################################################
                      [1m Learning iteration 30/2000 [0m                      

                       Computation: 90897 steps/s (collection: 0.899s, learning 0.182s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.7214
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 12.4568
                       Mean reward: 3.75
               Mean episode length: 247.62
    Episode_Reward/reaching_object: 0.1634
     Episode_Reward/lifting_object: 0.5962
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 1.08s
                      Time elapsed: 00:00:52
                               ETA: 00:56:07

################################################################################
                      [1m Learning iteration 31/2000 [0m                      

                       Computation: 92712 steps/s (collection: 0.887s, learning 0.174s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.7880
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 12.5438
                       Mean reward: 2.71
               Mean episode length: 249.32
    Episode_Reward/reaching_object: 0.1613
     Episode_Reward/lifting_object: 0.3973
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 1.06s
                      Time elapsed: 00:00:54
                               ETA: 00:55:25

################################################################################
                      [1m Learning iteration 32/2000 [0m                      

                       Computation: 87717 steps/s (collection: 0.987s, learning 0.134s)
             Mean action noise std: 1.18
          Mean value_function loss: 1.0944
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 12.6085
                       Mean reward: 2.73
               Mean episode length: 242.37
    Episode_Reward/reaching_object: 0.1587
     Episode_Reward/lifting_object: 0.4477
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0020
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 1.12s
                      Time elapsed: 00:00:55
                               ETA: 00:54:50

################################################################################
                      [1m Learning iteration 33/2000 [0m                      

                       Computation: 89149 steps/s (collection: 0.934s, learning 0.169s)
             Mean action noise std: 1.18
          Mean value_function loss: 1.3281
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 12.6642
                       Mean reward: 2.09
               Mean episode length: 249.27
    Episode_Reward/reaching_object: 0.1573
     Episode_Reward/lifting_object: 0.3163
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0020
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 1.10s
                      Time elapsed: 00:00:56
                               ETA: 00:54:15

################################################################################
                      [1m Learning iteration 34/2000 [0m                      

                       Computation: 90839 steps/s (collection: 0.950s, learning 0.132s)
             Mean action noise std: 1.19
          Mean value_function loss: 1.2259
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 12.7075
                       Mean reward: 3.71
               Mean episode length: 249.14
    Episode_Reward/reaching_object: 0.1593
     Episode_Reward/lifting_object: 0.4057
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 1.08s
                      Time elapsed: 00:00:57
                               ETA: 00:53:41

################################################################################
                      [1m Learning iteration 35/2000 [0m                      

                       Computation: 87720 steps/s (collection: 0.989s, learning 0.132s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.8168
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 12.7862
                       Mean reward: 2.71
               Mean episode length: 246.75
    Episode_Reward/reaching_object: 0.1611
     Episode_Reward/lifting_object: 0.5877
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 1.12s
                      Time elapsed: 00:00:58
                               ETA: 00:53:11

################################################################################
                      [1m Learning iteration 36/2000 [0m                      

                       Computation: 89447 steps/s (collection: 0.930s, learning 0.169s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.9039
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 12.8798
                       Mean reward: 5.16
               Mean episode length: 248.59
    Episode_Reward/reaching_object: 0.1598
     Episode_Reward/lifting_object: 0.6730
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0022
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 1.10s
                      Time elapsed: 00:00:59
                               ETA: 00:52:42

################################################################################
                      [1m Learning iteration 37/2000 [0m                      

                       Computation: 89219 steps/s (collection: 0.959s, learning 0.143s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.6951
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 12.9465
                       Mean reward: 3.70
               Mean episode length: 248.17
    Episode_Reward/reaching_object: 0.1510
     Episode_Reward/lifting_object: 0.5962
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0022
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 1.10s
                      Time elapsed: 00:01:00
                               ETA: 00:52:14

################################################################################
                      [1m Learning iteration 38/2000 [0m                      

                       Computation: 89118 steps/s (collection: 0.929s, learning 0.174s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.8340
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 13.0417
                       Mean reward: 2.96
               Mean episode length: 246.57
    Episode_Reward/reaching_object: 0.1502
     Episode_Reward/lifting_object: 0.5052
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 1.10s
                      Time elapsed: 00:01:01
                               ETA: 00:51:47

################################################################################
                      [1m Learning iteration 39/2000 [0m                      

                       Computation: 87321 steps/s (collection: 0.940s, learning 0.186s)
             Mean action noise std: 1.25
          Mean value_function loss: 1.2760
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 13.1224
                       Mean reward: 2.96
               Mean episode length: 245.64
    Episode_Reward/reaching_object: 0.1521
     Episode_Reward/lifting_object: 0.4234
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 1.13s
                      Time elapsed: 00:01:02
                               ETA: 00:51:23

################################################################################
                      [1m Learning iteration 40/2000 [0m                      

                       Computation: 85530 steps/s (collection: 0.998s, learning 0.152s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.9736
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 13.1996
                       Mean reward: 3.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1542
     Episode_Reward/lifting_object: 0.5184
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 1.15s
                      Time elapsed: 00:01:04
                               ETA: 00:51:02

################################################################################
                      [1m Learning iteration 41/2000 [0m                      

                       Computation: 96655 steps/s (collection: 0.886s, learning 0.131s)
             Mean action noise std: 1.28
          Mean value_function loss: 1.3132
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 13.2610
                       Mean reward: 3.51
               Mean episode length: 248.98
    Episode_Reward/reaching_object: 0.1537
     Episode_Reward/lifting_object: 0.5724
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 1.02s
                      Time elapsed: 00:01:05
                               ETA: 00:50:35

################################################################################
                      [1m Learning iteration 42/2000 [0m                      

                       Computation: 88777 steps/s (collection: 0.972s, learning 0.136s)
             Mean action noise std: 1.29
          Mean value_function loss: 1.3860
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 13.3522
                       Mean reward: 3.52
               Mean episode length: 245.10
    Episode_Reward/reaching_object: 0.1602
     Episode_Reward/lifting_object: 0.6464
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 1.11s
                      Time elapsed: 00:01:06
                               ETA: 00:50:13

################################################################################
                      [1m Learning iteration 43/2000 [0m                      

                       Computation: 91761 steps/s (collection: 0.960s, learning 0.111s)
             Mean action noise std: 1.30
          Mean value_function loss: 1.4139
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 13.4382
                       Mean reward: 3.61
               Mean episode length: 244.84
    Episode_Reward/reaching_object: 0.1588
     Episode_Reward/lifting_object: 0.5758
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0025
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 1.07s
                      Time elapsed: 00:01:07
                               ETA: 00:49:51

################################################################################
                      [1m Learning iteration 44/2000 [0m                      

                       Computation: 93189 steps/s (collection: 0.915s, learning 0.140s)
             Mean action noise std: 1.31
          Mean value_function loss: 1.5015
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 13.4721
                       Mean reward: 3.16
               Mean episode length: 248.47
    Episode_Reward/reaching_object: 0.1599
     Episode_Reward/lifting_object: 0.6735
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0025
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 1.05s
                      Time elapsed: 00:01:08
                               ETA: 00:49:28

################################################################################
                      [1m Learning iteration 45/2000 [0m                      

                       Computation: 82167 steps/s (collection: 0.960s, learning 0.236s)
             Mean action noise std: 1.32
          Mean value_function loss: 1.6237
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 13.5303
                       Mean reward: 4.01
               Mean episode length: 247.67
    Episode_Reward/reaching_object: 0.1669
     Episode_Reward/lifting_object: 0.6423
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 1.20s
                      Time elapsed: 00:01:09
                               ETA: 00:49:13

################################################################################
                      [1m Learning iteration 46/2000 [0m                      

                       Computation: 89178 steps/s (collection: 0.992s, learning 0.111s)
             Mean action noise std: 1.34
          Mean value_function loss: 4.0276
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 13.6148
                       Mean reward: 4.14
               Mean episode length: 247.02
    Episode_Reward/reaching_object: 0.1650
     Episode_Reward/lifting_object: 0.5829
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 1.10s
                      Time elapsed: 00:01:10
                               ETA: 00:48:55

################################################################################
                      [1m Learning iteration 47/2000 [0m                      

                       Computation: 81804 steps/s (collection: 0.986s, learning 0.215s)
             Mean action noise std: 1.35
          Mean value_function loss: 1.7905
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 13.7115
                       Mean reward: 3.67
               Mean episode length: 248.83
    Episode_Reward/reaching_object: 0.1716
     Episode_Reward/lifting_object: 0.6132
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 1.20s
                      Time elapsed: 00:01:11
                               ETA: 00:48:41

################################################################################
                      [1m Learning iteration 48/2000 [0m                      

                       Computation: 89286 steps/s (collection: 0.884s, learning 0.217s)
             Mean action noise std: 1.36
          Mean value_function loss: 1.4368
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 13.7617
                       Mean reward: 5.54
               Mean episode length: 243.97
    Episode_Reward/reaching_object: 0.1817
     Episode_Reward/lifting_object: 0.9042
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 1.10s
                      Time elapsed: 00:01:12
                               ETA: 00:48:24

################################################################################
                      [1m Learning iteration 49/2000 [0m                      

                       Computation: 88009 steps/s (collection: 0.948s, learning 0.169s)
             Mean action noise std: 1.36
          Mean value_function loss: 1.6807
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 13.7923
                       Mean reward: 4.55
               Mean episode length: 246.06
    Episode_Reward/reaching_object: 0.1821
     Episode_Reward/lifting_object: 0.7601
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 1.12s
                      Time elapsed: 00:01:14
                               ETA: 00:48:08

################################################################################
                      [1m Learning iteration 50/2000 [0m                      

                       Computation: 92010 steps/s (collection: 0.922s, learning 0.146s)
             Mean action noise std: 1.37
          Mean value_function loss: 3.2961
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 13.8421
                       Mean reward: 4.78
               Mean episode length: 241.73
    Episode_Reward/reaching_object: 0.1813
     Episode_Reward/lifting_object: 0.8858
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 1.07s
                      Time elapsed: 00:01:15
                               ETA: 00:47:51

################################################################################
                      [1m Learning iteration 51/2000 [0m                      

                       Computation: 89024 steps/s (collection: 0.929s, learning 0.176s)
             Mean action noise std: 1.38
          Mean value_function loss: 2.8773
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 13.8757
                       Mean reward: 4.84
               Mean episode length: 243.36
    Episode_Reward/reaching_object: 0.1868
     Episode_Reward/lifting_object: 1.1490
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 1.10s
                      Time elapsed: 00:01:16
                               ETA: 00:47:35

################################################################################
                      [1m Learning iteration 52/2000 [0m                      

                       Computation: 88478 steps/s (collection: 0.971s, learning 0.140s)
             Mean action noise std: 1.38
          Mean value_function loss: 3.0179
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 13.9108
                       Mean reward: 5.27
               Mean episode length: 246.66
    Episode_Reward/reaching_object: 0.1916
     Episode_Reward/lifting_object: 0.9221
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 1.11s
                      Time elapsed: 00:01:17
                               ETA: 00:47:21

################################################################################
                      [1m Learning iteration 53/2000 [0m                      

                       Computation: 87049 steps/s (collection: 0.988s, learning 0.142s)
             Mean action noise std: 1.39
          Mean value_function loss: 2.8330
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 13.9315
                       Mean reward: 4.92
               Mean episode length: 234.04
    Episode_Reward/reaching_object: 0.1915
     Episode_Reward/lifting_object: 0.8147
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 1.13s
                      Time elapsed: 00:01:18
                               ETA: 00:47:08

################################################################################
                      [1m Learning iteration 54/2000 [0m                      

                       Computation: 85930 steps/s (collection: 0.997s, learning 0.147s)
             Mean action noise std: 1.39
          Mean value_function loss: 2.2896
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 13.9474
                       Mean reward: 8.37
               Mean episode length: 246.17
    Episode_Reward/reaching_object: 0.1991
     Episode_Reward/lifting_object: 0.9155
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 1.14s
                      Time elapsed: 00:01:19
                               ETA: 00:46:55

################################################################################
                      [1m Learning iteration 55/2000 [0m                      

                       Computation: 84595 steps/s (collection: 0.967s, learning 0.195s)
             Mean action noise std: 1.39
          Mean value_function loss: 3.4737
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 13.9640
                       Mean reward: 8.31
               Mean episode length: 240.74
    Episode_Reward/reaching_object: 0.2002
     Episode_Reward/lifting_object: 1.1283
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 1.16s
                      Time elapsed: 00:01:20
                               ETA: 00:46:44

################################################################################
                      [1m Learning iteration 56/2000 [0m                      

                       Computation: 85877 steps/s (collection: 0.967s, learning 0.178s)
             Mean action noise std: 1.40
          Mean value_function loss: 3.1958
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 13.9972
                       Mean reward: 9.73
               Mean episode length: 244.22
    Episode_Reward/reaching_object: 0.2083
     Episode_Reward/lifting_object: 1.3702
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 1.14s
                      Time elapsed: 00:01:21
                               ETA: 00:46:32

################################################################################
                      [1m Learning iteration 57/2000 [0m                      

                       Computation: 86050 steps/s (collection: 0.993s, learning 0.150s)
             Mean action noise std: 1.40
          Mean value_function loss: 6.2452
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.0125
                       Mean reward: 8.44
               Mean episode length: 243.63
    Episode_Reward/reaching_object: 0.2088
     Episode_Reward/lifting_object: 1.3210
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 1.14s
                      Time elapsed: 00:01:23
                               ETA: 00:46:21

################################################################################
                      [1m Learning iteration 58/2000 [0m                      

                       Computation: 87586 steps/s (collection: 0.947s, learning 0.176s)
             Mean action noise std: 1.41
          Mean value_function loss: 4.0933
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 14.0380
                       Mean reward: 8.32
               Mean episode length: 243.04
    Episode_Reward/reaching_object: 0.2134
     Episode_Reward/lifting_object: 1.4137
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 1.12s
                      Time elapsed: 00:01:24
                               ETA: 00:46:09

################################################################################
                      [1m Learning iteration 59/2000 [0m                      

                       Computation: 95836 steps/s (collection: 0.909s, learning 0.116s)
             Mean action noise std: 1.41
          Mean value_function loss: 5.4943
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 14.0619
                       Mean reward: 9.59
               Mean episode length: 245.17
    Episode_Reward/reaching_object: 0.2176
     Episode_Reward/lifting_object: 1.6804
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 1.03s
                      Time elapsed: 00:01:25
                               ETA: 00:45:55

################################################################################
                      [1m Learning iteration 60/2000 [0m                      

                       Computation: 81071 steps/s (collection: 1.036s, learning 0.176s)
             Mean action noise std: 1.42
          Mean value_function loss: 13.2205
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 14.0998
                       Mean reward: 7.98
               Mean episode length: 239.71
    Episode_Reward/reaching_object: 0.2156
     Episode_Reward/lifting_object: 1.4771
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 1.21s
                      Time elapsed: 00:01:26
                               ETA: 00:45:47

################################################################################
                      [1m Learning iteration 61/2000 [0m                      

                       Computation: 84871 steps/s (collection: 1.062s, learning 0.097s)
             Mean action noise std: 1.42
          Mean value_function loss: 4.9573
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.1373
                       Mean reward: 9.80
               Mean episode length: 236.64
    Episode_Reward/reaching_object: 0.2074
     Episode_Reward/lifting_object: 1.1177
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 1.16s
                      Time elapsed: 00:01:27
                               ETA: 00:45:37

################################################################################
                      [1m Learning iteration 62/2000 [0m                      

                       Computation: 87324 steps/s (collection: 0.938s, learning 0.188s)
             Mean action noise std: 1.43
          Mean value_function loss: 6.0591
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 14.1662
                       Mean reward: 10.16
               Mean episode length: 244.04
    Episode_Reward/reaching_object: 0.2198
     Episode_Reward/lifting_object: 1.9283
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 1.13s
                      Time elapsed: 00:01:28
                               ETA: 00:45:27

################################################################################
                      [1m Learning iteration 63/2000 [0m                      

                       Computation: 86190 steps/s (collection: 0.875s, learning 0.266s)
             Mean action noise std: 1.44
          Mean value_function loss: 3.9322
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 14.2236
                       Mean reward: 10.46
               Mean episode length: 242.69
    Episode_Reward/reaching_object: 0.2169
     Episode_Reward/lifting_object: 1.9500
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 1.14s
                      Time elapsed: 00:01:29
                               ETA: 00:45:18

################################################################################
                      [1m Learning iteration 64/2000 [0m                      

                       Computation: 88963 steps/s (collection: 0.952s, learning 0.153s)
             Mean action noise std: 1.44
          Mean value_function loss: 6.1025
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.2338
                       Mean reward: 12.99
               Mean episode length: 235.35
    Episode_Reward/reaching_object: 0.2160
     Episode_Reward/lifting_object: 1.9604
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 1.10s
                      Time elapsed: 00:01:30
                               ETA: 00:45:07

################################################################################
                      [1m Learning iteration 65/2000 [0m                      

                       Computation: 93434 steps/s (collection: 0.926s, learning 0.126s)
             Mean action noise std: 1.45
          Mean value_function loss: 4.2019
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 14.2576
                       Mean reward: 10.38
               Mean episode length: 246.65
    Episode_Reward/reaching_object: 0.2224
     Episode_Reward/lifting_object: 1.5587
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 1.05s
                      Time elapsed: 00:01:31
                               ETA: 00:44:56

################################################################################
                      [1m Learning iteration 66/2000 [0m                      

                       Computation: 91197 steps/s (collection: 0.958s, learning 0.120s)
             Mean action noise std: 1.45
          Mean value_function loss: 7.8857
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 14.2868
                       Mean reward: 10.52
               Mean episode length: 245.58
    Episode_Reward/reaching_object: 0.2221
     Episode_Reward/lifting_object: 1.7148
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 1.08s
                      Time elapsed: 00:01:33
                               ETA: 00:44:45

################################################################################
                      [1m Learning iteration 67/2000 [0m                      

                       Computation: 90563 steps/s (collection: 0.929s, learning 0.157s)
             Mean action noise std: 1.46
          Mean value_function loss: 9.0561
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.3361
                       Mean reward: 12.35
               Mean episode length: 243.54
    Episode_Reward/reaching_object: 0.2159
     Episode_Reward/lifting_object: 2.1506
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 1.09s
                      Time elapsed: 00:01:34
                               ETA: 00:44:35

################################################################################
                      [1m Learning iteration 68/2000 [0m                      

                       Computation: 94845 steps/s (collection: 0.901s, learning 0.135s)
             Mean action noise std: 1.47
          Mean value_function loss: 6.9436
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 14.3650
                       Mean reward: 11.68
               Mean episode length: 235.95
    Episode_Reward/reaching_object: 0.2203
     Episode_Reward/lifting_object: 2.1588
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 1.04s
                      Time elapsed: 00:01:35
                               ETA: 00:44:24

################################################################################
                      [1m Learning iteration 69/2000 [0m                      

                       Computation: 93798 steps/s (collection: 0.915s, learning 0.133s)
             Mean action noise std: 1.47
          Mean value_function loss: 5.7889
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.3706
                       Mean reward: 10.47
               Mean episode length: 240.72
    Episode_Reward/reaching_object: 0.2218
     Episode_Reward/lifting_object: 1.9616
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 1.05s
                      Time elapsed: 00:01:36
                               ETA: 00:44:14

################################################################################
                      [1m Learning iteration 70/2000 [0m                      

                       Computation: 89866 steps/s (collection: 0.938s, learning 0.156s)
             Mean action noise std: 1.47
          Mean value_function loss: 8.0103
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.3869
                       Mean reward: 9.36
               Mean episode length: 244.30
    Episode_Reward/reaching_object: 0.2200
     Episode_Reward/lifting_object: 1.9943
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 1.09s
                      Time elapsed: 00:01:37
                               ETA: 00:44:05

################################################################################
                      [1m Learning iteration 71/2000 [0m                      

                       Computation: 95055 steps/s (collection: 0.898s, learning 0.136s)
             Mean action noise std: 1.48
          Mean value_function loss: 10.3154
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.4159
                       Mean reward: 13.74
               Mean episode length: 237.85
    Episode_Reward/reaching_object: 0.2228
     Episode_Reward/lifting_object: 2.1146
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 1.03s
                      Time elapsed: 00:01:38
                               ETA: 00:43:54

################################################################################
                      [1m Learning iteration 72/2000 [0m                      

                       Computation: 87824 steps/s (collection: 0.977s, learning 0.142s)
             Mean action noise std: 1.48
          Mean value_function loss: 7.6039
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 14.4415
                       Mean reward: 11.54
               Mean episode length: 241.68
    Episode_Reward/reaching_object: 0.2248
     Episode_Reward/lifting_object: 2.1609
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 1.12s
                      Time elapsed: 00:01:39
                               ETA: 00:43:46

################################################################################
                      [1m Learning iteration 73/2000 [0m                      

                       Computation: 98746 steps/s (collection: 0.862s, learning 0.133s)
             Mean action noise std: 1.48
          Mean value_function loss: 7.2573
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 14.4623
                       Mean reward: 12.49
               Mean episode length: 244.84
    Episode_Reward/reaching_object: 0.2321
     Episode_Reward/lifting_object: 1.8317
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 1.00s
                      Time elapsed: 00:01:40
                               ETA: 00:43:36

################################################################################
                      [1m Learning iteration 74/2000 [0m                      

                       Computation: 92434 steps/s (collection: 0.963s, learning 0.101s)
             Mean action noise std: 1.49
          Mean value_function loss: 8.5265
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 14.4698
                       Mean reward: 9.58
               Mean episode length: 242.06
    Episode_Reward/reaching_object: 0.2270
     Episode_Reward/lifting_object: 2.0172
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 1.06s
                      Time elapsed: 00:01:41
                               ETA: 00:43:27

################################################################################
                      [1m Learning iteration 75/2000 [0m                      

                       Computation: 96232 steps/s (collection: 0.867s, learning 0.155s)
             Mean action noise std: 1.49
          Mean value_function loss: 6.7098
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.4944
                       Mean reward: 16.09
               Mean episode length: 236.05
    Episode_Reward/reaching_object: 0.2302
     Episode_Reward/lifting_object: 2.4831
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 1.02s
                      Time elapsed: 00:01:42
                               ETA: 00:43:17

################################################################################
                      [1m Learning iteration 76/2000 [0m                      

                       Computation: 105064 steps/s (collection: 0.846s, learning 0.090s)
             Mean action noise std: 1.49
          Mean value_function loss: 6.6434
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.4990
                       Mean reward: 13.62
               Mean episode length: 239.55
    Episode_Reward/reaching_object: 0.2312
     Episode_Reward/lifting_object: 2.6836
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 0.94s
                      Time elapsed: 00:01:43
                               ETA: 00:43:05

################################################################################
                      [1m Learning iteration 77/2000 [0m                      

                       Computation: 102852 steps/s (collection: 0.862s, learning 0.094s)
             Mean action noise std: 1.49
          Mean value_function loss: 8.8696
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 14.5016
                       Mean reward: 16.02
               Mean episode length: 238.38
    Episode_Reward/reaching_object: 0.2281
     Episode_Reward/lifting_object: 2.8339
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 0.96s
                      Time elapsed: 00:01:44
                               ETA: 00:42:54

################################################################################
                      [1m Learning iteration 78/2000 [0m                      

                       Computation: 104544 steps/s (collection: 0.851s, learning 0.089s)
             Mean action noise std: 1.50
          Mean value_function loss: 10.6474
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 14.5117
                       Mean reward: 19.03
               Mean episode length: 242.17
    Episode_Reward/reaching_object: 0.2383
     Episode_Reward/lifting_object: 3.2885
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 0.94s
                      Time elapsed: 00:01:45
                               ETA: 00:42:43

################################################################################
                      [1m Learning iteration 79/2000 [0m                      

                       Computation: 109590 steps/s (collection: 0.809s, learning 0.088s)
             Mean action noise std: 1.50
          Mean value_function loss: 10.2994
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 14.5303
                       Mean reward: 18.01
               Mean episode length: 242.28
    Episode_Reward/reaching_object: 0.2285
     Episode_Reward/lifting_object: 2.8900
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 0.90s
                      Time elapsed: 00:01:46
                               ETA: 00:42:31

################################################################################
                      [1m Learning iteration 80/2000 [0m                      

                       Computation: 106161 steps/s (collection: 0.828s, learning 0.098s)
             Mean action noise std: 1.51
          Mean value_function loss: 8.2954
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.5479
                       Mean reward: 20.33
               Mean episode length: 240.88
    Episode_Reward/reaching_object: 0.2240
     Episode_Reward/lifting_object: 3.0262
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 0.93s
                      Time elapsed: 00:01:47
                               ETA: 00:42:21

################################################################################
                      [1m Learning iteration 81/2000 [0m                      

                       Computation: 105370 steps/s (collection: 0.833s, learning 0.100s)
             Mean action noise std: 1.51
          Mean value_function loss: 14.2175
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 14.5693
                       Mean reward: 15.22
               Mean episode length: 242.93
    Episode_Reward/reaching_object: 0.2299
     Episode_Reward/lifting_object: 3.0241
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 0.93s
                      Time elapsed: 00:01:48
                               ETA: 00:42:10

################################################################################
                      [1m Learning iteration 82/2000 [0m                      

                       Computation: 102848 steps/s (collection: 0.854s, learning 0.102s)
             Mean action noise std: 1.51
          Mean value_function loss: 10.4908
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.5815
                       Mean reward: 15.72
               Mean episode length: 231.48
    Episode_Reward/reaching_object: 0.2270
     Episode_Reward/lifting_object: 2.8723
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 0.96s
                      Time elapsed: 00:01:49
                               ETA: 00:42:00

################################################################################
                      [1m Learning iteration 83/2000 [0m                      

                       Computation: 103501 steps/s (collection: 0.841s, learning 0.109s)
             Mean action noise std: 1.51
          Mean value_function loss: 12.3602
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.5917
                       Mean reward: 14.46
               Mean episode length: 240.86
    Episode_Reward/reaching_object: 0.2277
     Episode_Reward/lifting_object: 3.4192
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 0.95s
                      Time elapsed: 00:01:50
                               ETA: 00:41:51

################################################################################
                      [1m Learning iteration 84/2000 [0m                      

                       Computation: 105543 steps/s (collection: 0.839s, learning 0.092s)
             Mean action noise std: 1.52
          Mean value_function loss: 12.7322
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.6095
                       Mean reward: 20.13
               Mean episode length: 246.39
    Episode_Reward/reaching_object: 0.2367
     Episode_Reward/lifting_object: 3.4560
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 0.93s
                      Time elapsed: 00:01:50
                               ETA: 00:41:41

################################################################################
                      [1m Learning iteration 85/2000 [0m                      

                       Computation: 101874 steps/s (collection: 0.845s, learning 0.120s)
             Mean action noise std: 1.52
          Mean value_function loss: 11.9881
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 14.6230
                       Mean reward: 18.90
               Mean episode length: 241.49
    Episode_Reward/reaching_object: 0.2244
     Episode_Reward/lifting_object: 3.1007
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 0.96s
                      Time elapsed: 00:01:51
                               ETA: 00:41:32

################################################################################
                      [1m Learning iteration 86/2000 [0m                      

                       Computation: 99344 steps/s (collection: 0.882s, learning 0.108s)
             Mean action noise std: 1.52
          Mean value_function loss: 14.3681
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 14.6352
                       Mean reward: 19.51
               Mean episode length: 243.36
    Episode_Reward/reaching_object: 0.2301
     Episode_Reward/lifting_object: 3.4600
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 0.99s
                      Time elapsed: 00:01:52
                               ETA: 00:41:24

################################################################################
                      [1m Learning iteration 87/2000 [0m                      

                       Computation: 83308 steps/s (collection: 0.961s, learning 0.219s)
             Mean action noise std: 1.53
          Mean value_function loss: 16.6455
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 14.6550
                       Mean reward: 20.51
               Mean episode length: 242.40
    Episode_Reward/reaching_object: 0.2258
     Episode_Reward/lifting_object: 3.4820
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 1.18s
                      Time elapsed: 00:01:54
                               ETA: 00:41:20

################################################################################
                      [1m Learning iteration 88/2000 [0m                      

                       Computation: 90576 steps/s (collection: 0.987s, learning 0.098s)
             Mean action noise std: 1.53
          Mean value_function loss: 12.1201
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 14.6727
                       Mean reward: 15.44
               Mean episode length: 238.04
    Episode_Reward/reaching_object: 0.2311
     Episode_Reward/lifting_object: 3.4771
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 1.09s
                      Time elapsed: 00:01:55
                               ETA: 00:41:14

################################################################################
                      [1m Learning iteration 89/2000 [0m                      

                       Computation: 101801 steps/s (collection: 0.876s, learning 0.090s)
             Mean action noise std: 1.53
          Mean value_function loss: 15.3428
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.6818
                       Mean reward: 19.18
               Mean episode length: 229.86
    Episode_Reward/reaching_object: 0.2243
     Episode_Reward/lifting_object: 3.7980
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 0.97s
                      Time elapsed: 00:01:56
                               ETA: 00:41:06

################################################################################
                      [1m Learning iteration 90/2000 [0m                      

                       Computation: 108819 steps/s (collection: 0.814s, learning 0.090s)
             Mean action noise std: 1.53
          Mean value_function loss: 15.4106
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 14.6890
                       Mean reward: 19.20
               Mean episode length: 236.78
    Episode_Reward/reaching_object: 0.2265
     Episode_Reward/lifting_object: 4.0360
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 0.90s
                      Time elapsed: 00:01:57
                               ETA: 00:40:56

################################################################################
                      [1m Learning iteration 91/2000 [0m                      

                       Computation: 99587 steps/s (collection: 0.892s, learning 0.096s)
             Mean action noise std: 1.54
          Mean value_function loss: 11.0186
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.7016
                       Mean reward: 23.85
               Mean episode length: 238.78
    Episode_Reward/reaching_object: 0.2330
     Episode_Reward/lifting_object: 4.4589
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 0.99s
                      Time elapsed: 00:01:58
                               ETA: 00:40:49

################################################################################
                      [1m Learning iteration 92/2000 [0m                      

                       Computation: 94341 steps/s (collection: 0.928s, learning 0.114s)
             Mean action noise std: 1.54
          Mean value_function loss: 18.1813
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.7182
                       Mean reward: 18.53
               Mean episode length: 232.04
    Episode_Reward/reaching_object: 0.2231
     Episode_Reward/lifting_object: 3.7717
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 1.04s
                      Time elapsed: 00:01:59
                               ETA: 00:40:43

################################################################################
                      [1m Learning iteration 93/2000 [0m                      

                       Computation: 103019 steps/s (collection: 0.868s, learning 0.086s)
             Mean action noise std: 1.54
          Mean value_function loss: 15.4809
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 14.7317
                       Mean reward: 19.98
               Mean episode length: 239.60
    Episode_Reward/reaching_object: 0.2294
     Episode_Reward/lifting_object: 3.6509
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 0.95s
                      Time elapsed: 00:02:00
                               ETA: 00:40:35

################################################################################
                      [1m Learning iteration 94/2000 [0m                      

                       Computation: 102665 steps/s (collection: 0.861s, learning 0.097s)
             Mean action noise std: 1.54
          Mean value_function loss: 12.2912
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.7406
                       Mean reward: 20.34
               Mean episode length: 234.75
    Episode_Reward/reaching_object: 0.2202
     Episode_Reward/lifting_object: 3.9932
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 0.96s
                      Time elapsed: 00:02:01
                               ETA: 00:40:27

################################################################################
                      [1m Learning iteration 95/2000 [0m                      

                       Computation: 109238 steps/s (collection: 0.801s, learning 0.099s)
             Mean action noise std: 1.55
          Mean value_function loss: 9.9998
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 14.7464
                       Mean reward: 19.95
               Mean episode length: 240.85
    Episode_Reward/reaching_object: 0.2285
     Episode_Reward/lifting_object: 3.9649
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 0.90s
                      Time elapsed: 00:02:01
                               ETA: 00:40:18

################################################################################
                      [1m Learning iteration 96/2000 [0m                      

                       Computation: 103541 steps/s (collection: 0.812s, learning 0.138s)
             Mean action noise std: 1.55
          Mean value_function loss: 8.3937
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 14.7569
                       Mean reward: 20.86
               Mean episode length: 232.50
    Episode_Reward/reaching_object: 0.2261
     Episode_Reward/lifting_object: 4.0414
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 0.95s
                      Time elapsed: 00:02:02
                               ETA: 00:40:11

################################################################################
                      [1m Learning iteration 97/2000 [0m                      

                       Computation: 106839 steps/s (collection: 0.828s, learning 0.092s)
             Mean action noise std: 1.55
          Mean value_function loss: 11.9349
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 14.7543
                       Mean reward: 24.29
               Mean episode length: 241.02
    Episode_Reward/reaching_object: 0.2357
     Episode_Reward/lifting_object: 4.6742
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 0.92s
                      Time elapsed: 00:02:03
                               ETA: 00:40:03

################################################################################
                      [1m Learning iteration 98/2000 [0m                      

                       Computation: 101554 steps/s (collection: 0.844s, learning 0.124s)
             Mean action noise std: 1.55
          Mean value_function loss: 12.7676
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.7593
                       Mean reward: 23.37
               Mean episode length: 236.98
    Episode_Reward/reaching_object: 0.2305
     Episode_Reward/lifting_object: 4.4228
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 0.97s
                      Time elapsed: 00:02:04
                               ETA: 00:39:56

################################################################################
                      [1m Learning iteration 99/2000 [0m                      

                       Computation: 93147 steps/s (collection: 0.851s, learning 0.204s)
             Mean action noise std: 1.55
          Mean value_function loss: 12.1820
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.7690
                       Mean reward: 26.56
               Mean episode length: 235.98
    Episode_Reward/reaching_object: 0.2385
     Episode_Reward/lifting_object: 4.9627
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 1.06s
                      Time elapsed: 00:02:05
                               ETA: 00:39:51

################################################################################
                     [1m Learning iteration 100/2000 [0m                      

                       Computation: 94009 steps/s (collection: 0.876s, learning 0.170s)
             Mean action noise std: 1.55
          Mean value_function loss: 12.0410
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.7780
                       Mean reward: 24.48
               Mean episode length: 235.93
    Episode_Reward/reaching_object: 0.2318
     Episode_Reward/lifting_object: 4.7728
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 1.05s
                      Time elapsed: 00:02:06
                               ETA: 00:39:46

################################################################################
                     [1m Learning iteration 101/2000 [0m                      

                       Computation: 102560 steps/s (collection: 0.851s, learning 0.107s)
             Mean action noise std: 1.56
          Mean value_function loss: 15.5386
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 14.7858
                       Mean reward: 28.19
               Mean episode length: 237.11
    Episode_Reward/reaching_object: 0.2361
     Episode_Reward/lifting_object: 4.6329
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 0.96s
                      Time elapsed: 00:02:07
                               ETA: 00:39:39

################################################################################
                     [1m Learning iteration 102/2000 [0m                      

                       Computation: 95819 steps/s (collection: 0.847s, learning 0.179s)
             Mean action noise std: 1.56
          Mean value_function loss: 11.5724
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.7894
                       Mean reward: 26.82
               Mean episode length: 233.64
    Episode_Reward/reaching_object: 0.2364
     Episode_Reward/lifting_object: 5.2554
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 1.03s
                      Time elapsed: 00:02:08
                               ETA: 00:39:33

################################################################################
                     [1m Learning iteration 103/2000 [0m                      

                       Computation: 99381 steps/s (collection: 0.876s, learning 0.114s)
             Mean action noise std: 1.56
          Mean value_function loss: 13.8202
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.7963
                       Mean reward: 23.88
               Mean episode length: 230.48
    Episode_Reward/reaching_object: 0.2391
     Episode_Reward/lifting_object: 4.9798
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 0.99s
                      Time elapsed: 00:02:09
                               ETA: 00:39:27

################################################################################
                     [1m Learning iteration 104/2000 [0m                      

                       Computation: 98543 steps/s (collection: 0.855s, learning 0.143s)
             Mean action noise std: 1.56
          Mean value_function loss: 13.6831
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 14.8020
                       Mean reward: 25.96
               Mean episode length: 230.18
    Episode_Reward/reaching_object: 0.2377
     Episode_Reward/lifting_object: 5.2918
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 1.00s
                      Time elapsed: 00:02:10
                               ETA: 00:39:22

################################################################################
                     [1m Learning iteration 105/2000 [0m                      

                       Computation: 91335 steps/s (collection: 0.974s, learning 0.103s)
             Mean action noise std: 1.56
          Mean value_function loss: 12.4152
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.8034
                       Mean reward: 31.76
               Mean episode length: 233.07
    Episode_Reward/reaching_object: 0.2427
     Episode_Reward/lifting_object: 5.3415
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 1.08s
                      Time elapsed: 00:02:11
                               ETA: 00:39:17

################################################################################
                     [1m Learning iteration 106/2000 [0m                      

                       Computation: 94706 steps/s (collection: 0.848s, learning 0.190s)
             Mean action noise std: 1.56
          Mean value_function loss: 17.9468
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.8014
                       Mean reward: 28.10
               Mean episode length: 231.05
    Episode_Reward/reaching_object: 0.2530
     Episode_Reward/lifting_object: 5.3898
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 1.04s
                      Time elapsed: 00:02:12
                               ETA: 00:39:12

################################################################################
                     [1m Learning iteration 107/2000 [0m                      

                       Computation: 99568 steps/s (collection: 0.890s, learning 0.098s)
             Mean action noise std: 1.56
          Mean value_function loss: 16.5360
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 14.8133
                       Mean reward: 31.59
               Mean episode length: 229.04
    Episode_Reward/reaching_object: 0.2502
     Episode_Reward/lifting_object: 5.2622
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 0.99s
                      Time elapsed: 00:02:13
                               ETA: 00:39:07

################################################################################
                     [1m Learning iteration 108/2000 [0m                      

                       Computation: 109973 steps/s (collection: 0.808s, learning 0.086s)
             Mean action noise std: 1.56
          Mean value_function loss: 12.7037
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 14.8188
                       Mean reward: 29.21
               Mean episode length: 229.20
    Episode_Reward/reaching_object: 0.2582
     Episode_Reward/lifting_object: 5.8856
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 0.89s
                      Time elapsed: 00:02:14
                               ETA: 00:38:59

################################################################################
                     [1m Learning iteration 109/2000 [0m                      

                       Computation: 102034 steps/s (collection: 0.819s, learning 0.144s)
             Mean action noise std: 1.56
          Mean value_function loss: 13.4749
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.8081
                       Mean reward: 26.55
               Mean episode length: 233.18
    Episode_Reward/reaching_object: 0.2615
     Episode_Reward/lifting_object: 5.7071
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 0.96s
                      Time elapsed: 00:02:15
                               ETA: 00:38:53

################################################################################
                     [1m Learning iteration 110/2000 [0m                      

                       Computation: 106864 steps/s (collection: 0.824s, learning 0.096s)
             Mean action noise std: 1.56
          Mean value_function loss: 21.4489
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 14.8040
                       Mean reward: 29.46
               Mean episode length: 212.54
    Episode_Reward/reaching_object: 0.2626
     Episode_Reward/lifting_object: 5.9411
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 0.92s
                      Time elapsed: 00:02:16
                               ETA: 00:38:47

################################################################################
                     [1m Learning iteration 111/2000 [0m                      

                       Computation: 110020 steps/s (collection: 0.796s, learning 0.098s)
             Mean action noise std: 1.57
          Mean value_function loss: 14.8373
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 14.8182
                       Mean reward: 30.61
               Mean episode length: 226.11
    Episode_Reward/reaching_object: 0.2697
     Episode_Reward/lifting_object: 6.1208
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 0.89s
                      Time elapsed: 00:02:17
                               ETA: 00:38:40

################################################################################
                     [1m Learning iteration 112/2000 [0m                      

                       Computation: 108521 steps/s (collection: 0.809s, learning 0.097s)
             Mean action noise std: 1.57
          Mean value_function loss: 15.7292
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 14.8174
                       Mean reward: 35.05
               Mean episode length: 227.28
    Episode_Reward/reaching_object: 0.2692
     Episode_Reward/lifting_object: 6.3118
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 0.91s
                      Time elapsed: 00:02:18
                               ETA: 00:38:33

################################################################################
                     [1m Learning iteration 113/2000 [0m                      

                       Computation: 109485 steps/s (collection: 0.801s, learning 0.097s)
             Mean action noise std: 1.57
          Mean value_function loss: 15.9229
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 14.8219
                       Mean reward: 33.78
               Mean episode length: 228.29
    Episode_Reward/reaching_object: 0.2712
     Episode_Reward/lifting_object: 6.0653
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 0.90s
                      Time elapsed: 00:02:19
                               ETA: 00:38:27

################################################################################
                     [1m Learning iteration 114/2000 [0m                      

                       Computation: 103546 steps/s (collection: 0.857s, learning 0.093s)
             Mean action noise std: 1.57
          Mean value_function loss: 18.4775
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.8235
                       Mean reward: 32.24
               Mean episode length: 229.84
    Episode_Reward/reaching_object: 0.2699
     Episode_Reward/lifting_object: 6.3327
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 0.95s
                      Time elapsed: 00:02:20
                               ETA: 00:38:21

################################################################################
                     [1m Learning iteration 115/2000 [0m                      

                       Computation: 108114 steps/s (collection: 0.824s, learning 0.086s)
             Mean action noise std: 1.57
          Mean value_function loss: 23.5325
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 14.8236
                       Mean reward: 32.76
               Mean episode length: 222.67
    Episode_Reward/reaching_object: 0.2690
     Episode_Reward/lifting_object: 6.6753
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 0.91s
                      Time elapsed: 00:02:21
                               ETA: 00:38:15

################################################################################
                     [1m Learning iteration 116/2000 [0m                      

                       Computation: 108581 steps/s (collection: 0.809s, learning 0.097s)
             Mean action noise std: 1.57
          Mean value_function loss: 17.6450
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 14.8331
                       Mean reward: 27.94
               Mean episode length: 233.02
    Episode_Reward/reaching_object: 0.2660
     Episode_Reward/lifting_object: 6.6155
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 0.91s
                      Time elapsed: 00:02:22
                               ETA: 00:38:08

################################################################################
                     [1m Learning iteration 117/2000 [0m                      

                       Computation: 109947 steps/s (collection: 0.805s, learning 0.089s)
             Mean action noise std: 1.57
          Mean value_function loss: 23.7912
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.8373
                       Mean reward: 37.54
               Mean episode length: 231.09
    Episode_Reward/reaching_object: 0.2711
     Episode_Reward/lifting_object: 6.9839
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 0.89s
                      Time elapsed: 00:02:23
                               ETA: 00:38:02

################################################################################
                     [1m Learning iteration 118/2000 [0m                      

                       Computation: 102938 steps/s (collection: 0.837s, learning 0.118s)
             Mean action noise std: 1.57
          Mean value_function loss: 18.9844
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 14.8383
                       Mean reward: 34.48
               Mean episode length: 234.43
    Episode_Reward/reaching_object: 0.2716
     Episode_Reward/lifting_object: 7.0290
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 0.95s
                      Time elapsed: 00:02:23
                               ETA: 00:37:57

################################################################################
                     [1m Learning iteration 119/2000 [0m                      

                       Computation: 106946 steps/s (collection: 0.828s, learning 0.092s)
             Mean action noise std: 1.57
          Mean value_function loss: 30.0350
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 14.8438
                       Mean reward: 30.59
               Mean episode length: 225.21
    Episode_Reward/reaching_object: 0.2625
     Episode_Reward/lifting_object: 6.3487
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 0.92s
                      Time elapsed: 00:02:24
                               ETA: 00:37:51

################################################################################
                     [1m Learning iteration 120/2000 [0m                      

                       Computation: 98110 steps/s (collection: 0.857s, learning 0.145s)
             Mean action noise std: 1.57
          Mean value_function loss: 28.4329
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 14.8443
                       Mean reward: 36.30
               Mean episode length: 221.75
    Episode_Reward/reaching_object: 0.2671
     Episode_Reward/lifting_object: 6.9428
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 1.00s
                      Time elapsed: 00:02:25
                               ETA: 00:37:47

################################################################################
                     [1m Learning iteration 121/2000 [0m                      

                       Computation: 105680 steps/s (collection: 0.839s, learning 0.091s)
             Mean action noise std: 1.58
          Mean value_function loss: 19.7267
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 14.8452
                       Mean reward: 35.90
               Mean episode length: 223.41
    Episode_Reward/reaching_object: 0.2629
     Episode_Reward/lifting_object: 7.4177
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 0.93s
                      Time elapsed: 00:02:26
                               ETA: 00:37:41

################################################################################
                     [1m Learning iteration 122/2000 [0m                      

                       Computation: 104657 steps/s (collection: 0.785s, learning 0.154s)
             Mean action noise std: 1.57
          Mean value_function loss: 19.5008
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 14.8475
                       Mean reward: 36.70
               Mean episode length: 222.89
    Episode_Reward/reaching_object: 0.2524
     Episode_Reward/lifting_object: 7.0447
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 0.94s
                      Time elapsed: 00:02:27
                               ETA: 00:37:36

################################################################################
                     [1m Learning iteration 123/2000 [0m                      

                       Computation: 103521 steps/s (collection: 0.792s, learning 0.158s)
             Mean action noise std: 1.58
          Mean value_function loss: 22.3309
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 14.8420
                       Mean reward: 39.28
               Mean episode length: 214.96
    Episode_Reward/reaching_object: 0.2594
     Episode_Reward/lifting_object: 7.5914
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 0.95s
                      Time elapsed: 00:02:28
                               ETA: 00:37:31

################################################################################
                     [1m Learning iteration 124/2000 [0m                      

                       Computation: 98459 steps/s (collection: 0.895s, learning 0.103s)
             Mean action noise std: 1.58
          Mean value_function loss: 23.7625
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 14.8441
                       Mean reward: 37.79
               Mean episode length: 224.37
    Episode_Reward/reaching_object: 0.2623
     Episode_Reward/lifting_object: 7.4782
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 1.00s
                      Time elapsed: 00:02:29
                               ETA: 00:37:27

################################################################################
                     [1m Learning iteration 125/2000 [0m                      

                       Computation: 96672 steps/s (collection: 0.836s, learning 0.181s)
             Mean action noise std: 1.58
          Mean value_function loss: 27.2456
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.8450
                       Mean reward: 43.16
               Mean episode length: 214.41
    Episode_Reward/reaching_object: 0.2564
     Episode_Reward/lifting_object: 7.5242
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 1.02s
                      Time elapsed: 00:02:30
                               ETA: 00:37:23

################################################################################
                     [1m Learning iteration 126/2000 [0m                      

                       Computation: 109359 steps/s (collection: 0.806s, learning 0.093s)
             Mean action noise std: 1.58
          Mean value_function loss: 21.1116
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 14.8440
                       Mean reward: 28.23
               Mean episode length: 220.34
    Episode_Reward/reaching_object: 0.2570
     Episode_Reward/lifting_object: 6.9487
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 0.90s
                      Time elapsed: 00:02:31
                               ETA: 00:37:17

################################################################################
                     [1m Learning iteration 127/2000 [0m                      

                       Computation: 107031 steps/s (collection: 0.812s, learning 0.107s)
             Mean action noise std: 1.58
          Mean value_function loss: 21.0498
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.8411
                       Mean reward: 41.60
               Mean episode length: 213.29
    Episode_Reward/reaching_object: 0.2613
     Episode_Reward/lifting_object: 7.7511
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 0.92s
                      Time elapsed: 00:02:32
                               ETA: 00:37:12

################################################################################
                     [1m Learning iteration 128/2000 [0m                      

                       Computation: 103976 steps/s (collection: 0.840s, learning 0.105s)
             Mean action noise std: 1.58
          Mean value_function loss: 20.0196
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 14.8406
                       Mean reward: 41.75
               Mean episode length: 229.98
    Episode_Reward/reaching_object: 0.2637
     Episode_Reward/lifting_object: 7.8978
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 0.95s
                      Time elapsed: 00:02:33
                               ETA: 00:37:07

################################################################################
                     [1m Learning iteration 129/2000 [0m                      

                       Computation: 109139 steps/s (collection: 0.800s, learning 0.101s)
             Mean action noise std: 1.58
          Mean value_function loss: 25.4356
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.8405
                       Mean reward: 43.22
               Mean episode length: 219.13
    Episode_Reward/reaching_object: 0.2505
     Episode_Reward/lifting_object: 8.0649
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 0.90s
                      Time elapsed: 00:02:34
                               ETA: 00:37:02

################################################################################
                     [1m Learning iteration 130/2000 [0m                      

                       Computation: 101378 steps/s (collection: 0.861s, learning 0.109s)
             Mean action noise std: 1.58
          Mean value_function loss: 24.7036
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.8416
                       Mean reward: 40.07
               Mean episode length: 216.63
    Episode_Reward/reaching_object: 0.2499
     Episode_Reward/lifting_object: 7.8599
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 0.97s
                      Time elapsed: 00:02:35
                               ETA: 00:36:58

################################################################################
                     [1m Learning iteration 131/2000 [0m                      

                       Computation: 103383 steps/s (collection: 0.860s, learning 0.091s)
             Mean action noise std: 1.58
          Mean value_function loss: 34.4499
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 14.8491
                       Mean reward: 42.10
               Mean episode length: 225.15
    Episode_Reward/reaching_object: 0.2583
     Episode_Reward/lifting_object: 9.0450
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 0.95s
                      Time elapsed: 00:02:36
                               ETA: 00:36:53

################################################################################
                     [1m Learning iteration 132/2000 [0m                      

                       Computation: 105874 steps/s (collection: 0.841s, learning 0.088s)
             Mean action noise std: 1.58
          Mean value_function loss: 32.5302
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 14.8530
                       Mean reward: 32.84
               Mean episode length: 221.04
    Episode_Reward/reaching_object: 0.2528
     Episode_Reward/lifting_object: 7.3147
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 0.93s
                      Time elapsed: 00:02:37
                               ETA: 00:36:48

################################################################################
                     [1m Learning iteration 133/2000 [0m                      

                       Computation: 109080 steps/s (collection: 0.815s, learning 0.086s)
             Mean action noise std: 1.58
          Mean value_function loss: 38.8513
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 14.8565
                       Mean reward: 34.28
               Mean episode length: 218.22
    Episode_Reward/reaching_object: 0.2543
     Episode_Reward/lifting_object: 7.6546
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 0.90s
                      Time elapsed: 00:02:38
                               ETA: 00:36:43

################################################################################
                     [1m Learning iteration 134/2000 [0m                      

                       Computation: 111539 steps/s (collection: 0.794s, learning 0.087s)
             Mean action noise std: 1.58
          Mean value_function loss: 44.0772
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 14.8597
                       Mean reward: 39.38
               Mean episode length: 217.28
    Episode_Reward/reaching_object: 0.2432
     Episode_Reward/lifting_object: 8.1764
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 0.88s
                      Time elapsed: 00:02:39
                               ETA: 00:36:38

################################################################################
                     [1m Learning iteration 135/2000 [0m                      

                       Computation: 98711 steps/s (collection: 0.848s, learning 0.148s)
             Mean action noise std: 1.58
          Mean value_function loss: 36.8561
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 14.8633
                       Mean reward: 39.60
               Mean episode length: 224.25
    Episode_Reward/reaching_object: 0.2554
     Episode_Reward/lifting_object: 8.3804
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 1.00s
                      Time elapsed: 00:02:40
                               ETA: 00:36:34

################################################################################
                     [1m Learning iteration 136/2000 [0m                      

                       Computation: 102584 steps/s (collection: 0.846s, learning 0.112s)
             Mean action noise std: 1.58
          Mean value_function loss: 34.2743
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 14.8666
                       Mean reward: 32.42
               Mean episode length: 224.59
    Episode_Reward/reaching_object: 0.2511
     Episode_Reward/lifting_object: 7.5062
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 0.96s
                      Time elapsed: 00:02:41
                               ETA: 00:36:30

################################################################################
                     [1m Learning iteration 137/2000 [0m                      

                       Computation: 100437 steps/s (collection: 0.829s, learning 0.150s)
             Mean action noise std: 1.59
          Mean value_function loss: 31.5735
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.8760
                       Mean reward: 41.58
               Mean episode length: 222.41
    Episode_Reward/reaching_object: 0.2534
     Episode_Reward/lifting_object: 8.0856
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 0.98s
                      Time elapsed: 00:02:41
                               ETA: 00:36:26

################################################################################
                     [1m Learning iteration 138/2000 [0m                      

                       Computation: 106057 steps/s (collection: 0.814s, learning 0.113s)
             Mean action noise std: 1.59
          Mean value_function loss: 39.2115
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.8810
                       Mean reward: 45.25
               Mean episode length: 217.08
    Episode_Reward/reaching_object: 0.2395
     Episode_Reward/lifting_object: 8.0899
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 0.93s
                      Time elapsed: 00:02:42
                               ETA: 00:36:22

################################################################################
                     [1m Learning iteration 139/2000 [0m                      

                       Computation: 100833 steps/s (collection: 0.837s, learning 0.138s)
             Mean action noise std: 1.59
          Mean value_function loss: 53.8342
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 14.8860
                       Mean reward: 49.34
               Mean episode length: 221.32
    Episode_Reward/reaching_object: 0.2450
     Episode_Reward/lifting_object: 8.4167
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 0.97s
                      Time elapsed: 00:02:43
                               ETA: 00:36:18

################################################################################
                     [1m Learning iteration 140/2000 [0m                      

                       Computation: 101382 steps/s (collection: 0.858s, learning 0.112s)
             Mean action noise std: 1.59
          Mean value_function loss: 50.4674
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.8916
                       Mean reward: 47.85
               Mean episode length: 211.05
    Episode_Reward/reaching_object: 0.2484
     Episode_Reward/lifting_object: 8.2605
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 0.97s
                      Time elapsed: 00:02:44
                               ETA: 00:36:14

################################################################################
                     [1m Learning iteration 141/2000 [0m                      

                       Computation: 98073 steps/s (collection: 0.856s, learning 0.147s)
             Mean action noise std: 1.59
          Mean value_function loss: 44.6130
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 14.9003
                       Mean reward: 39.98
               Mean episode length: 211.76
    Episode_Reward/reaching_object: 0.2399
     Episode_Reward/lifting_object: 6.7477
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 1.00s
                      Time elapsed: 00:02:45
                               ETA: 00:36:11

################################################################################
                     [1m Learning iteration 142/2000 [0m                      

                       Computation: 105264 steps/s (collection: 0.821s, learning 0.113s)
             Mean action noise std: 1.59
          Mean value_function loss: 42.1512
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 14.9114
                       Mean reward: 56.34
               Mean episode length: 219.70
    Episode_Reward/reaching_object: 0.2437
     Episode_Reward/lifting_object: 8.4889
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 0.93s
                      Time elapsed: 00:02:46
                               ETA: 00:36:07

################################################################################
                     [1m Learning iteration 143/2000 [0m                      

                       Computation: 102560 steps/s (collection: 0.849s, learning 0.110s)
             Mean action noise std: 1.59
          Mean value_function loss: 49.8399
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.9161
                       Mean reward: 45.53
               Mean episode length: 217.69
    Episode_Reward/reaching_object: 0.2517
     Episode_Reward/lifting_object: 8.7413
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 0.96s
                      Time elapsed: 00:02:47
                               ETA: 00:36:03

################################################################################
                     [1m Learning iteration 144/2000 [0m                      

                       Computation: 98267 steps/s (collection: 0.857s, learning 0.144s)
             Mean action noise std: 1.60
          Mean value_function loss: 51.2498
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 14.9216
                       Mean reward: 39.18
               Mean episode length: 218.50
    Episode_Reward/reaching_object: 0.2452
     Episode_Reward/lifting_object: 8.0062
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 1.00s
                      Time elapsed: 00:02:48
                               ETA: 00:35:59

################################################################################
                     [1m Learning iteration 145/2000 [0m                      

                       Computation: 101201 steps/s (collection: 0.826s, learning 0.145s)
             Mean action noise std: 1.60
          Mean value_function loss: 41.8234
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 14.9287
                       Mean reward: 43.23
               Mean episode length: 209.76
    Episode_Reward/reaching_object: 0.2491
     Episode_Reward/lifting_object: 8.1306
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 0.97s
                      Time elapsed: 00:02:49
                               ETA: 00:35:56

################################################################################
                     [1m Learning iteration 146/2000 [0m                      

                       Computation: 100244 steps/s (collection: 0.859s, learning 0.122s)
             Mean action noise std: 1.60
          Mean value_function loss: 40.7513
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 14.9341
                       Mean reward: 48.93
               Mean episode length: 218.85
    Episode_Reward/reaching_object: 0.2524
     Episode_Reward/lifting_object: 9.5009
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 0.98s
                      Time elapsed: 00:02:50
                               ETA: 00:35:52

################################################################################
                     [1m Learning iteration 147/2000 [0m                      

                       Computation: 99864 steps/s (collection: 0.853s, learning 0.131s)
             Mean action noise std: 1.60
          Mean value_function loss: 32.9176
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 14.9401
                       Mean reward: 44.40
               Mean episode length: 214.75
    Episode_Reward/reaching_object: 0.2474
     Episode_Reward/lifting_object: 8.2865
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 0.98s
                      Time elapsed: 00:02:51
                               ETA: 00:35:49

################################################################################
                     [1m Learning iteration 148/2000 [0m                      

                       Computation: 102366 steps/s (collection: 0.805s, learning 0.155s)
             Mean action noise std: 1.60
          Mean value_function loss: 34.3764
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.9421
                       Mean reward: 42.85
               Mean episode length: 222.24
    Episode_Reward/reaching_object: 0.2483
     Episode_Reward/lifting_object: 8.1806
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 0.96s
                      Time elapsed: 00:02:52
                               ETA: 00:35:45

################################################################################
                     [1m Learning iteration 149/2000 [0m                      

                       Computation: 100851 steps/s (collection: 0.851s, learning 0.124s)
             Mean action noise std: 1.60
          Mean value_function loss: 57.2667
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 14.9447
                       Mean reward: 44.64
               Mean episode length: 211.93
    Episode_Reward/reaching_object: 0.2468
     Episode_Reward/lifting_object: 8.5115
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 0.97s
                      Time elapsed: 00:02:53
                               ETA: 00:35:42

################################################################################
                     [1m Learning iteration 150/2000 [0m                      

                       Computation: 103414 steps/s (collection: 0.816s, learning 0.135s)
             Mean action noise std: 1.60
          Mean value_function loss: 45.6723
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 14.9520
                       Mean reward: 48.26
               Mean episode length: 217.73
    Episode_Reward/reaching_object: 0.2479
     Episode_Reward/lifting_object: 7.8910
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 0.95s
                      Time elapsed: 00:02:54
                               ETA: 00:35:38

################################################################################
                     [1m Learning iteration 151/2000 [0m                      

                       Computation: 105602 steps/s (collection: 0.839s, learning 0.092s)
             Mean action noise std: 1.60
          Mean value_function loss: 43.0891
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 14.9627
                       Mean reward: 48.23
               Mean episode length: 212.75
    Episode_Reward/reaching_object: 0.2482
     Episode_Reward/lifting_object: 8.4338
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 0.93s
                      Time elapsed: 00:02:55
                               ETA: 00:35:34

################################################################################
                     [1m Learning iteration 152/2000 [0m                      

                       Computation: 103160 steps/s (collection: 0.854s, learning 0.099s)
             Mean action noise std: 1.60
          Mean value_function loss: 44.4497
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 14.9644
                       Mean reward: 45.16
               Mean episode length: 211.97
    Episode_Reward/reaching_object: 0.2517
     Episode_Reward/lifting_object: 8.6090
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 0.95s
                      Time elapsed: 00:02:56
                               ETA: 00:35:31

################################################################################
                     [1m Learning iteration 153/2000 [0m                      

                       Computation: 105265 steps/s (collection: 0.829s, learning 0.105s)
             Mean action noise std: 1.60
          Mean value_function loss: 31.3772
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.9649
                       Mean reward: 45.68
               Mean episode length: 221.97
    Episode_Reward/reaching_object: 0.2558
     Episode_Reward/lifting_object: 9.0857
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 0.93s
                      Time elapsed: 00:02:57
                               ETA: 00:35:27

################################################################################
                     [1m Learning iteration 154/2000 [0m                      

                       Computation: 103676 steps/s (collection: 0.856s, learning 0.092s)
             Mean action noise std: 1.60
          Mean value_function loss: 32.7764
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 14.9661
                       Mean reward: 51.46
               Mean episode length: 204.29
    Episode_Reward/reaching_object: 0.2439
     Episode_Reward/lifting_object: 9.2795
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 0.95s
                      Time elapsed: 00:02:58
                               ETA: 00:35:23

################################################################################
                     [1m Learning iteration 155/2000 [0m                      

                       Computation: 98511 steps/s (collection: 0.880s, learning 0.118s)
             Mean action noise std: 1.61
          Mean value_function loss: 32.3815
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 14.9669
                       Mean reward: 48.35
               Mean episode length: 210.80
    Episode_Reward/reaching_object: 0.2457
     Episode_Reward/lifting_object: 8.9819
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 1.00s
                      Time elapsed: 00:02:59
                               ETA: 00:35:20

################################################################################
                     [1m Learning iteration 156/2000 [0m                      

                       Computation: 104480 steps/s (collection: 0.856s, learning 0.085s)
             Mean action noise std: 1.61
          Mean value_function loss: 33.0826
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.9698
                       Mean reward: 48.32
               Mean episode length: 209.84
    Episode_Reward/reaching_object: 0.2461
     Episode_Reward/lifting_object: 9.4671
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 0.94s
                      Time elapsed: 00:03:00
                               ETA: 00:35:17

################################################################################
                     [1m Learning iteration 157/2000 [0m                      

                       Computation: 108214 steps/s (collection: 0.821s, learning 0.087s)
             Mean action noise std: 1.61
          Mean value_function loss: 30.5988
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.9758
                       Mean reward: 54.07
               Mean episode length: 215.42
    Episode_Reward/reaching_object: 0.2445
     Episode_Reward/lifting_object: 9.4237
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 0.91s
                      Time elapsed: 00:03:01
                               ETA: 00:35:13

################################################################################
                     [1m Learning iteration 158/2000 [0m                      

                       Computation: 111457 steps/s (collection: 0.791s, learning 0.091s)
             Mean action noise std: 1.61
          Mean value_function loss: 41.9285
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.9814
                       Mean reward: 41.91
               Mean episode length: 218.29
    Episode_Reward/reaching_object: 0.2530
     Episode_Reward/lifting_object: 9.7313
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 0.88s
                      Time elapsed: 00:03:02
                               ETA: 00:35:09

################################################################################
                     [1m Learning iteration 159/2000 [0m                      

                       Computation: 104895 steps/s (collection: 0.807s, learning 0.130s)
             Mean action noise std: 1.61
          Mean value_function loss: 47.1743
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.9852
                       Mean reward: 42.37
               Mean episode length: 212.04
    Episode_Reward/reaching_object: 0.2451
     Episode_Reward/lifting_object: 9.7742
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 0.94s
                      Time elapsed: 00:03:03
                               ETA: 00:35:05

################################################################################
                     [1m Learning iteration 160/2000 [0m                      

                       Computation: 105955 steps/s (collection: 0.823s, learning 0.105s)
             Mean action noise std: 1.61
          Mean value_function loss: 37.5388
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.9865
                       Mean reward: 47.81
               Mean episode length: 223.09
    Episode_Reward/reaching_object: 0.2540
     Episode_Reward/lifting_object: 10.1030
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 0.93s
                      Time elapsed: 00:03:03
                               ETA: 00:35:02

################################################################################
                     [1m Learning iteration 161/2000 [0m                      

                       Computation: 107688 steps/s (collection: 0.821s, learning 0.092s)
             Mean action noise std: 1.61
          Mean value_function loss: 33.0963
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 14.9876
                       Mean reward: 49.52
               Mean episode length: 219.04
    Episode_Reward/reaching_object: 0.2513
     Episode_Reward/lifting_object: 9.2629
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 0.91s
                      Time elapsed: 00:03:04
                               ETA: 00:34:58

################################################################################
                     [1m Learning iteration 162/2000 [0m                      

                       Computation: 100323 steps/s (collection: 0.809s, learning 0.171s)
             Mean action noise std: 1.61
          Mean value_function loss: 32.7119
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 14.9903
                       Mean reward: 50.86
               Mean episode length: 211.96
    Episode_Reward/reaching_object: 0.2581
     Episode_Reward/lifting_object: 9.8739
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 0.98s
                      Time elapsed: 00:03:05
                               ETA: 00:34:55

################################################################################
                     [1m Learning iteration 163/2000 [0m                      

                       Computation: 108260 steps/s (collection: 0.813s, learning 0.095s)
             Mean action noise std: 1.61
          Mean value_function loss: 37.4372
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 14.9916
                       Mean reward: 47.51
               Mean episode length: 228.67
    Episode_Reward/reaching_object: 0.2614
     Episode_Reward/lifting_object: 10.0774
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 0.91s
                      Time elapsed: 00:03:06
                               ETA: 00:34:51

################################################################################
                     [1m Learning iteration 164/2000 [0m                      

                       Computation: 102287 steps/s (collection: 0.821s, learning 0.140s)
             Mean action noise std: 1.61
          Mean value_function loss: 31.5782
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 14.9919
                       Mean reward: 48.43
               Mean episode length: 210.57
    Episode_Reward/reaching_object: 0.2572
     Episode_Reward/lifting_object: 10.5195
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 0.96s
                      Time elapsed: 00:03:07
                               ETA: 00:34:48

################################################################################
                     [1m Learning iteration 165/2000 [0m                      

                       Computation: 104085 steps/s (collection: 0.827s, learning 0.117s)
             Mean action noise std: 1.61
          Mean value_function loss: 33.5794
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 14.9928
                       Mean reward: 52.65
               Mean episode length: 216.90
    Episode_Reward/reaching_object: 0.2712
     Episode_Reward/lifting_object: 10.1042
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 0.94s
                      Time elapsed: 00:03:08
                               ETA: 00:34:45

################################################################################
                     [1m Learning iteration 166/2000 [0m                      

                       Computation: 104422 steps/s (collection: 0.813s, learning 0.129s)
             Mean action noise std: 1.61
          Mean value_function loss: 32.5959
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 14.9941
                       Mean reward: 55.74
               Mean episode length: 217.85
    Episode_Reward/reaching_object: 0.2690
     Episode_Reward/lifting_object: 9.5473
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 0.94s
                      Time elapsed: 00:03:09
                               ETA: 00:34:41

################################################################################
                     [1m Learning iteration 167/2000 [0m                      

                       Computation: 107766 steps/s (collection: 0.817s, learning 0.095s)
             Mean action noise std: 1.61
          Mean value_function loss: 34.7539
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 14.9945
                       Mean reward: 53.17
               Mean episode length: 229.85
    Episode_Reward/reaching_object: 0.2730
     Episode_Reward/lifting_object: 10.6938
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 0.91s
                      Time elapsed: 00:03:10
                               ETA: 00:34:38

################################################################################
                     [1m Learning iteration 168/2000 [0m                      

                       Computation: 102632 steps/s (collection: 0.852s, learning 0.106s)
             Mean action noise std: 1.61
          Mean value_function loss: 37.1018
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 14.9946
                       Mean reward: 51.26
               Mean episode length: 206.06
    Episode_Reward/reaching_object: 0.2725
     Episode_Reward/lifting_object: 10.3714
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 0.96s
                      Time elapsed: 00:03:11
                               ETA: 00:34:35

################################################################################
                     [1m Learning iteration 169/2000 [0m                      

                       Computation: 93435 steps/s (collection: 0.861s, learning 0.191s)
             Mean action noise std: 1.61
          Mean value_function loss: 36.3446
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 14.9947
                       Mean reward: 53.68
               Mean episode length: 217.67
    Episode_Reward/reaching_object: 0.2761
     Episode_Reward/lifting_object: 9.9785
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 1.05s
                      Time elapsed: 00:03:12
                               ETA: 00:34:33

################################################################################
                     [1m Learning iteration 170/2000 [0m                      

                       Computation: 103369 steps/s (collection: 0.846s, learning 0.105s)
             Mean action noise std: 1.61
          Mean value_function loss: 44.3235
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 14.9949
                       Mean reward: 65.66
               Mean episode length: 216.26
    Episode_Reward/reaching_object: 0.2831
     Episode_Reward/lifting_object: 12.5436
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 0.95s
                      Time elapsed: 00:03:13
                               ETA: 00:34:30

################################################################################
                     [1m Learning iteration 171/2000 [0m                      

                       Computation: 106600 steps/s (collection: 0.831s, learning 0.092s)
             Mean action noise std: 1.61
          Mean value_function loss: 35.7837
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 14.9953
                       Mean reward: 66.73
               Mean episode length: 222.54
    Episode_Reward/reaching_object: 0.2840
     Episode_Reward/lifting_object: 12.2590
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 0.92s
                      Time elapsed: 00:03:14
                               ETA: 00:34:26

################################################################################
                     [1m Learning iteration 172/2000 [0m                      

                       Computation: 104541 steps/s (collection: 0.846s, learning 0.094s)
             Mean action noise std: 1.61
          Mean value_function loss: 35.8948
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 14.9957
                       Mean reward: 49.93
               Mean episode length: 223.51
    Episode_Reward/reaching_object: 0.2885
     Episode_Reward/lifting_object: 10.7162
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 0.94s
                      Time elapsed: 00:03:15
                               ETA: 00:34:23

################################################################################
                     [1m Learning iteration 173/2000 [0m                      

                       Computation: 107114 steps/s (collection: 0.817s, learning 0.101s)
             Mean action noise std: 1.61
          Mean value_function loss: 38.3520
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.9955
                       Mean reward: 51.55
               Mean episode length: 220.41
    Episode_Reward/reaching_object: 0.2778
     Episode_Reward/lifting_object: 11.0381
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 0.92s
                      Time elapsed: 00:03:16
                               ETA: 00:34:20

################################################################################
                     [1m Learning iteration 174/2000 [0m                      

                       Computation: 107949 steps/s (collection: 0.806s, learning 0.105s)
             Mean action noise std: 1.61
          Mean value_function loss: 37.0767
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.9942
                       Mean reward: 63.30
               Mean episode length: 215.49
    Episode_Reward/reaching_object: 0.2831
     Episode_Reward/lifting_object: 11.0307
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 0.91s
                      Time elapsed: 00:03:17
                               ETA: 00:34:17

################################################################################
                     [1m Learning iteration 175/2000 [0m                      

                       Computation: 106598 steps/s (collection: 0.829s, learning 0.094s)
             Mean action noise std: 1.61
          Mean value_function loss: 35.5518
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 14.9945
                       Mean reward: 77.55
               Mean episode length: 221.34
    Episode_Reward/reaching_object: 0.2780
     Episode_Reward/lifting_object: 12.4785
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 0.92s
                      Time elapsed: 00:03:18
                               ETA: 00:34:13

################################################################################
                     [1m Learning iteration 176/2000 [0m                      

                       Computation: 105421 steps/s (collection: 0.840s, learning 0.093s)
             Mean action noise std: 1.61
          Mean value_function loss: 39.7521
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 14.9969
                       Mean reward: 74.12
               Mean episode length: 220.89
    Episode_Reward/reaching_object: 0.2943
     Episode_Reward/lifting_object: 12.8030
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 0.93s
                      Time elapsed: 00:03:18
                               ETA: 00:34:10

################################################################################
                     [1m Learning iteration 177/2000 [0m                      

                       Computation: 109800 steps/s (collection: 0.806s, learning 0.089s)
             Mean action noise std: 1.61
          Mean value_function loss: 47.1632
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 14.9962
                       Mean reward: 63.27
               Mean episode length: 224.65
    Episode_Reward/reaching_object: 0.2820
     Episode_Reward/lifting_object: 12.2030
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 0.90s
                      Time elapsed: 00:03:19
                               ETA: 00:34:07

################################################################################
                     [1m Learning iteration 178/2000 [0m                      

                       Computation: 106569 steps/s (collection: 0.819s, learning 0.103s)
             Mean action noise std: 1.61
          Mean value_function loss: 45.1063
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 14.9960
                       Mean reward: 69.92
               Mean episode length: 215.64
    Episode_Reward/reaching_object: 0.2777
     Episode_Reward/lifting_object: 13.0524
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 0.92s
                      Time elapsed: 00:03:20
                               ETA: 00:34:04

################################################################################
                     [1m Learning iteration 179/2000 [0m                      

                       Computation: 103489 steps/s (collection: 0.844s, learning 0.106s)
             Mean action noise std: 1.61
          Mean value_function loss: 45.4720
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 14.9943
                       Mean reward: 72.52
               Mean episode length: 217.55
    Episode_Reward/reaching_object: 0.2830
     Episode_Reward/lifting_object: 12.8515
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 0.95s
                      Time elapsed: 00:03:21
                               ETA: 00:34:01

################################################################################
                     [1m Learning iteration 180/2000 [0m                      

                       Computation: 98319 steps/s (collection: 0.811s, learning 0.189s)
             Mean action noise std: 1.61
          Mean value_function loss: 43.7032
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 14.9940
                       Mean reward: 71.19
               Mean episode length: 220.53
    Episode_Reward/reaching_object: 0.2811
     Episode_Reward/lifting_object: 13.8093
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 1.00s
                      Time elapsed: 00:03:22
                               ETA: 00:33:58

################################################################################
                     [1m Learning iteration 181/2000 [0m                      

                       Computation: 105466 steps/s (collection: 0.829s, learning 0.103s)
             Mean action noise std: 1.61
          Mean value_function loss: 40.7893
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 14.9939
                       Mean reward: 65.72
               Mean episode length: 212.09
    Episode_Reward/reaching_object: 0.2740
     Episode_Reward/lifting_object: 13.7192
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 0.93s
                      Time elapsed: 00:03:23
                               ETA: 00:33:55

################################################################################
                     [1m Learning iteration 182/2000 [0m                      

                       Computation: 95622 steps/s (collection: 0.803s, learning 0.225s)
             Mean action noise std: 1.61
          Mean value_function loss: 38.1753
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 14.9937
                       Mean reward: 71.97
               Mean episode length: 225.93
    Episode_Reward/reaching_object: 0.2820
     Episode_Reward/lifting_object: 13.2337
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 1.03s
                      Time elapsed: 00:03:24
                               ETA: 00:33:53

################################################################################
                     [1m Learning iteration 183/2000 [0m                      

                       Computation: 104393 steps/s (collection: 0.836s, learning 0.106s)
             Mean action noise std: 1.61
          Mean value_function loss: 43.7770
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 14.9936
                       Mean reward: 64.13
               Mean episode length: 233.26
    Episode_Reward/reaching_object: 0.2881
     Episode_Reward/lifting_object: 13.4106
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 0.94s
                      Time elapsed: 00:03:25
                               ETA: 00:33:50

################################################################################
                     [1m Learning iteration 184/2000 [0m                      

                       Computation: 102838 steps/s (collection: 0.819s, learning 0.137s)
             Mean action noise std: 1.61
          Mean value_function loss: 48.5107
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 14.9944
                       Mean reward: 74.75
               Mean episode length: 226.99
    Episode_Reward/reaching_object: 0.2977
     Episode_Reward/lifting_object: 14.8189
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 0.96s
                      Time elapsed: 00:03:26
                               ETA: 00:33:48

################################################################################
                     [1m Learning iteration 185/2000 [0m                      

                       Computation: 103782 steps/s (collection: 0.832s, learning 0.116s)
             Mean action noise std: 1.61
          Mean value_function loss: 43.9990
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.9945
                       Mean reward: 66.73
               Mean episode length: 220.83
    Episode_Reward/reaching_object: 0.2889
     Episode_Reward/lifting_object: 14.0026
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 0.95s
                      Time elapsed: 00:03:27
                               ETA: 00:33:45

################################################################################
                     [1m Learning iteration 186/2000 [0m                      

                       Computation: 104535 steps/s (collection: 0.813s, learning 0.128s)
             Mean action noise std: 1.61
          Mean value_function loss: 51.2975
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 14.9933
                       Mean reward: 68.72
               Mean episode length: 223.78
    Episode_Reward/reaching_object: 0.2918
     Episode_Reward/lifting_object: 15.3262
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 0.94s
                      Time elapsed: 00:03:28
                               ETA: 00:33:42

################################################################################
                     [1m Learning iteration 187/2000 [0m                      

                       Computation: 108160 steps/s (collection: 0.801s, learning 0.108s)
             Mean action noise std: 1.61
          Mean value_function loss: 70.0090
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.9933
                       Mean reward: 75.91
               Mean episode length: 221.10
    Episode_Reward/reaching_object: 0.2890
     Episode_Reward/lifting_object: 15.0286
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 0.91s
                      Time elapsed: 00:03:29
                               ETA: 00:33:39

################################################################################
                     [1m Learning iteration 188/2000 [0m                      

                       Computation: 104437 steps/s (collection: 0.837s, learning 0.104s)
             Mean action noise std: 1.61
          Mean value_function loss: 64.9082
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 14.9926
                       Mean reward: 89.84
               Mean episode length: 217.79
    Episode_Reward/reaching_object: 0.2869
     Episode_Reward/lifting_object: 16.6723
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 0.94s
                      Time elapsed: 00:03:30
                               ETA: 00:33:36

################################################################################
                     [1m Learning iteration 189/2000 [0m                      

                       Computation: 105519 steps/s (collection: 0.829s, learning 0.103s)
             Mean action noise std: 1.61
          Mean value_function loss: 66.4482
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 14.9926
                       Mean reward: 73.42
               Mean episode length: 225.62
    Episode_Reward/reaching_object: 0.2770
     Episode_Reward/lifting_object: 14.8544
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 0.93s
                      Time elapsed: 00:03:31
                               ETA: 00:33:33

################################################################################
                     [1m Learning iteration 190/2000 [0m                      

                       Computation: 104523 steps/s (collection: 0.832s, learning 0.108s)
             Mean action noise std: 1.62
          Mean value_function loss: 74.9969
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 14.9961
                       Mean reward: 77.88
               Mean episode length: 220.43
    Episode_Reward/reaching_object: 0.2889
     Episode_Reward/lifting_object: 14.7770
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 0.94s
                      Time elapsed: 00:03:32
                               ETA: 00:33:31

################################################################################
                     [1m Learning iteration 191/2000 [0m                      

                       Computation: 104746 steps/s (collection: 0.853s, learning 0.086s)
             Mean action noise std: 1.62
          Mean value_function loss: 73.6858
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 14.9980
                       Mean reward: 70.87
               Mean episode length: 216.54
    Episode_Reward/reaching_object: 0.2940
     Episode_Reward/lifting_object: 15.7671
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 0.94s
                      Time elapsed: 00:03:33
                               ETA: 00:33:28

################################################################################
                     [1m Learning iteration 192/2000 [0m                      

                       Computation: 107738 steps/s (collection: 0.816s, learning 0.096s)
             Mean action noise std: 1.62
          Mean value_function loss: 77.3146
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.9970
                       Mean reward: 85.12
               Mean episode length: 226.34
    Episode_Reward/reaching_object: 0.2954
     Episode_Reward/lifting_object: 17.1308
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 0.91s
                      Time elapsed: 00:03:34
                               ETA: 00:33:25

################################################################################
                     [1m Learning iteration 193/2000 [0m                      

                       Computation: 104566 steps/s (collection: 0.850s, learning 0.090s)
             Mean action noise std: 1.62
          Mean value_function loss: 77.2845
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 14.9929
                       Mean reward: 95.17
               Mean episode length: 221.69
    Episode_Reward/reaching_object: 0.2903
     Episode_Reward/lifting_object: 16.0453
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 0.94s
                      Time elapsed: 00:03:35
                               ETA: 00:33:22

################################################################################
                     [1m Learning iteration 194/2000 [0m                      

                       Computation: 100929 steps/s (collection: 0.868s, learning 0.106s)
             Mean action noise std: 1.62
          Mean value_function loss: 77.4218
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 14.9925
                       Mean reward: 73.91
               Mean episode length: 219.59
    Episode_Reward/reaching_object: 0.2991
     Episode_Reward/lifting_object: 15.6680
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 0.97s
                      Time elapsed: 00:03:35
                               ETA: 00:33:20

################################################################################
                     [1m Learning iteration 195/2000 [0m                      

                       Computation: 104410 steps/s (collection: 0.849s, learning 0.093s)
             Mean action noise std: 1.62
          Mean value_function loss: 76.4978
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 14.9884
                       Mean reward: 99.10
               Mean episode length: 223.83
    Episode_Reward/reaching_object: 0.3099
     Episode_Reward/lifting_object: 18.1892
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 0.94s
                      Time elapsed: 00:03:36
                               ETA: 00:33:17

################################################################################
                     [1m Learning iteration 196/2000 [0m                      

                       Computation: 105764 steps/s (collection: 0.838s, learning 0.092s)
             Mean action noise std: 1.62
          Mean value_function loss: 82.4891
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 14.9840
                       Mean reward: 93.22
               Mean episode length: 226.46
    Episode_Reward/reaching_object: 0.3054
     Episode_Reward/lifting_object: 18.5425
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 0.93s
                      Time elapsed: 00:03:37
                               ETA: 00:33:15

################################################################################
                     [1m Learning iteration 197/2000 [0m                      

                       Computation: 97852 steps/s (collection: 0.876s, learning 0.129s)
             Mean action noise std: 1.62
          Mean value_function loss: 89.6990
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 14.9794
                       Mean reward: 82.70
               Mean episode length: 223.91
    Episode_Reward/reaching_object: 0.3011
     Episode_Reward/lifting_object: 16.8139
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 1.00s
                      Time elapsed: 00:03:38
                               ETA: 00:33:13

################################################################################
                     [1m Learning iteration 198/2000 [0m                      

                       Computation: 91214 steps/s (collection: 0.954s, learning 0.124s)
             Mean action noise std: 1.62
          Mean value_function loss: 92.3826
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.9784
                       Mean reward: 86.52
               Mean episode length: 219.45
    Episode_Reward/reaching_object: 0.3147
     Episode_Reward/lifting_object: 19.7860
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 1.08s
                      Time elapsed: 00:03:39
                               ETA: 00:33:11

################################################################################
                     [1m Learning iteration 199/2000 [0m                      

                       Computation: 90021 steps/s (collection: 0.960s, learning 0.132s)
             Mean action noise std: 1.62
          Mean value_function loss: 101.9768
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 14.9751
                       Mean reward: 76.06
               Mean episode length: 229.33
    Episode_Reward/reaching_object: 0.3070
     Episode_Reward/lifting_object: 18.1157
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 1.09s
                      Time elapsed: 00:03:41
                               ETA: 00:33:10

################################################################################
                     [1m Learning iteration 200/2000 [0m                      

                       Computation: 103830 steps/s (collection: 0.832s, learning 0.115s)
             Mean action noise std: 1.61
          Mean value_function loss: 95.2358
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 14.9693
                       Mean reward: 121.34
               Mean episode length: 229.19
    Episode_Reward/reaching_object: 0.3140
     Episode_Reward/lifting_object: 21.9210
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 0.95s
                      Time elapsed: 00:03:41
                               ETA: 00:33:07

################################################################################
                     [1m Learning iteration 201/2000 [0m                      

                       Computation: 101630 steps/s (collection: 0.879s, learning 0.088s)
             Mean action noise std: 1.61
          Mean value_function loss: 106.2123
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 14.9656
                       Mean reward: 114.57
               Mean episode length: 226.14
    Episode_Reward/reaching_object: 0.3003
     Episode_Reward/lifting_object: 19.1101
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 0.97s
                      Time elapsed: 00:03:42
                               ETA: 00:33:05

################################################################################
                     [1m Learning iteration 202/2000 [0m                      

                       Computation: 105074 steps/s (collection: 0.850s, learning 0.086s)
             Mean action noise std: 1.61
          Mean value_function loss: 123.5982
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 14.9634
                       Mean reward: 114.70
               Mean episode length: 212.90
    Episode_Reward/reaching_object: 0.3030
     Episode_Reward/lifting_object: 22.1288
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 0.94s
                      Time elapsed: 00:03:43
                               ETA: 00:33:03

################################################################################
                     [1m Learning iteration 203/2000 [0m                      

                       Computation: 102493 steps/s (collection: 0.858s, learning 0.101s)
             Mean action noise std: 1.61
          Mean value_function loss: 123.7773
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 14.9590
                       Mean reward: 86.28
               Mean episode length: 223.07
    Episode_Reward/reaching_object: 0.3008
     Episode_Reward/lifting_object: 20.1104
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 0.96s
                      Time elapsed: 00:03:44
                               ETA: 00:33:00

################################################################################
                     [1m Learning iteration 204/2000 [0m                      

                       Computation: 106340 steps/s (collection: 0.838s, learning 0.086s)
             Mean action noise std: 1.61
          Mean value_function loss: 146.1861
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 14.9553
                       Mean reward: 137.95
               Mean episode length: 213.94
    Episode_Reward/reaching_object: 0.3187
     Episode_Reward/lifting_object: 25.7489
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 0.92s
                      Time elapsed: 00:03:45
                               ETA: 00:32:58

################################################################################
                     [1m Learning iteration 205/2000 [0m                      

                       Computation: 102521 steps/s (collection: 0.819s, learning 0.140s)
             Mean action noise std: 1.61
          Mean value_function loss: 141.3340
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 14.9537
                       Mean reward: 148.02
               Mean episode length: 221.57
    Episode_Reward/reaching_object: 0.3183
     Episode_Reward/lifting_object: 28.5230
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 0.96s
                      Time elapsed: 00:03:46
                               ETA: 00:32:55

################################################################################
                     [1m Learning iteration 206/2000 [0m                      

                       Computation: 99476 steps/s (collection: 0.867s, learning 0.121s)
             Mean action noise std: 1.61
          Mean value_function loss: 145.4846
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 14.9490
                       Mean reward: 141.77
               Mean episode length: 222.14
    Episode_Reward/reaching_object: 0.3184
     Episode_Reward/lifting_object: 27.8569
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 0.99s
                      Time elapsed: 00:03:47
                               ETA: 00:32:53

################################################################################
                     [1m Learning iteration 207/2000 [0m                      

                       Computation: 102341 steps/s (collection: 0.812s, learning 0.148s)
             Mean action noise std: 1.61
          Mean value_function loss: 164.7917
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 14.9419
                       Mean reward: 185.35
               Mean episode length: 232.19
    Episode_Reward/reaching_object: 0.3154
     Episode_Reward/lifting_object: 27.3004
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 0.96s
                      Time elapsed: 00:03:48
                               ETA: 00:32:51

################################################################################
                     [1m Learning iteration 208/2000 [0m                      

                       Computation: 105239 steps/s (collection: 0.837s, learning 0.097s)
             Mean action noise std: 1.61
          Mean value_function loss: 176.2172
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.9364
                       Mean reward: 180.99
               Mean episode length: 227.06
    Episode_Reward/reaching_object: 0.3183
     Episode_Reward/lifting_object: 29.2889
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 0.93s
                      Time elapsed: 00:03:49
                               ETA: 00:32:48

################################################################################
                     [1m Learning iteration 209/2000 [0m                      

                       Computation: 93583 steps/s (collection: 0.870s, learning 0.180s)
             Mean action noise std: 1.61
          Mean value_function loss: 171.2686
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 14.9362
                       Mean reward: 119.43
               Mean episode length: 223.57
    Episode_Reward/reaching_object: 0.3133
     Episode_Reward/lifting_object: 28.6841
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 1.05s
                      Time elapsed: 00:03:50
                               ETA: 00:32:47

################################################################################
                     [1m Learning iteration 210/2000 [0m                      

                       Computation: 104926 steps/s (collection: 0.811s, learning 0.126s)
             Mean action noise std: 1.61
          Mean value_function loss: 190.1773
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.9383
                       Mean reward: 150.06
               Mean episode length: 224.34
    Episode_Reward/reaching_object: 0.3180
     Episode_Reward/lifting_object: 31.2711
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 0.94s
                      Time elapsed: 00:03:51
                               ETA: 00:32:44

################################################################################
                     [1m Learning iteration 211/2000 [0m                      

                       Computation: 108085 steps/s (collection: 0.799s, learning 0.111s)
             Mean action noise std: 1.61
          Mean value_function loss: 206.4354
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.9391
                       Mean reward: 111.42
               Mean episode length: 235.29
    Episode_Reward/reaching_object: 0.3223
     Episode_Reward/lifting_object: 32.5394
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 0.91s
                      Time elapsed: 00:03:52
                               ETA: 00:32:42

################################################################################
                     [1m Learning iteration 212/2000 [0m                      

                       Computation: 105339 steps/s (collection: 0.841s, learning 0.092s)
             Mean action noise std: 1.61
          Mean value_function loss: 203.8257
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 14.9350
                       Mean reward: 154.49
               Mean episode length: 223.00
    Episode_Reward/reaching_object: 0.3260
     Episode_Reward/lifting_object: 33.3357
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 0.93s
                      Time elapsed: 00:03:53
                               ETA: 00:32:39

################################################################################
                     [1m Learning iteration 213/2000 [0m                      

                       Computation: 103019 steps/s (collection: 0.859s, learning 0.096s)
             Mean action noise std: 1.61
          Mean value_function loss: 196.5139
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 14.9324
                       Mean reward: 208.82
               Mean episode length: 227.97
    Episode_Reward/reaching_object: 0.3537
     Episode_Reward/lifting_object: 40.7354
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 0.95s
                      Time elapsed: 00:03:54
                               ETA: 00:32:37

################################################################################
                     [1m Learning iteration 214/2000 [0m                      

                       Computation: 107855 steps/s (collection: 0.820s, learning 0.091s)
             Mean action noise std: 1.61
          Mean value_function loss: 201.1113
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 14.9319
                       Mean reward: 224.93
               Mean episode length: 232.64
    Episode_Reward/reaching_object: 0.3472
     Episode_Reward/lifting_object: 38.9371
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 0.91s
                      Time elapsed: 00:03:55
                               ETA: 00:32:34

################################################################################
                     [1m Learning iteration 215/2000 [0m                      

                       Computation: 107910 steps/s (collection: 0.808s, learning 0.103s)
             Mean action noise std: 1.61
          Mean value_function loss: 193.4233
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 14.9299
                       Mean reward: 198.78
               Mean episode length: 231.63
    Episode_Reward/reaching_object: 0.3257
     Episode_Reward/lifting_object: 34.7704
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 0.91s
                      Time elapsed: 00:03:56
                               ETA: 00:32:32

################################################################################
                     [1m Learning iteration 216/2000 [0m                      

                       Computation: 109994 steps/s (collection: 0.802s, learning 0.092s)
             Mean action noise std: 1.61
          Mean value_function loss: 227.9143
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 14.9277
                       Mean reward: 185.22
               Mean episode length: 228.67
    Episode_Reward/reaching_object: 0.3654
     Episode_Reward/lifting_object: 46.0386
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 0.89s
                      Time elapsed: 00:03:57
                               ETA: 00:32:29

################################################################################
                     [1m Learning iteration 217/2000 [0m                      

                       Computation: 108672 steps/s (collection: 0.808s, learning 0.097s)
             Mean action noise std: 1.61
          Mean value_function loss: 226.5807
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 14.9227
                       Mean reward: 217.00
               Mean episode length: 228.84
    Episode_Reward/reaching_object: 0.3510
     Episode_Reward/lifting_object: 42.1216
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 0.90s
                      Time elapsed: 00:03:58
                               ETA: 00:32:26

################################################################################
                     [1m Learning iteration 218/2000 [0m                      

                       Computation: 107697 steps/s (collection: 0.798s, learning 0.115s)
             Mean action noise std: 1.61
          Mean value_function loss: 225.0300
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 14.9177
                       Mean reward: 288.34
               Mean episode length: 226.82
    Episode_Reward/reaching_object: 0.3632
     Episode_Reward/lifting_object: 45.4633
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 0.91s
                      Time elapsed: 00:03:58
                               ETA: 00:32:24

################################################################################
                     [1m Learning iteration 219/2000 [0m                      

                       Computation: 109348 steps/s (collection: 0.811s, learning 0.088s)
             Mean action noise std: 1.61
          Mean value_function loss: 221.3606
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 14.9173
                       Mean reward: 227.12
               Mean episode length: 221.15
    Episode_Reward/reaching_object: 0.3434
     Episode_Reward/lifting_object: 41.6814
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 0.90s
                      Time elapsed: 00:03:59
                               ETA: 00:32:21

################################################################################
                     [1m Learning iteration 220/2000 [0m                      

                       Computation: 105600 steps/s (collection: 0.838s, learning 0.093s)
             Mean action noise std: 1.61
          Mean value_function loss: 221.5665
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 14.9172
                       Mean reward: 236.91
               Mean episode length: 234.20
    Episode_Reward/reaching_object: 0.3665
     Episode_Reward/lifting_object: 47.5507
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 0.93s
                      Time elapsed: 00:04:00
                               ETA: 00:32:19

################################################################################
                     [1m Learning iteration 221/2000 [0m                      

                       Computation: 103694 steps/s (collection: 0.806s, learning 0.142s)
             Mean action noise std: 1.61
          Mean value_function loss: 235.2395
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 14.9159
                       Mean reward: 244.52
               Mean episode length: 228.54
    Episode_Reward/reaching_object: 0.3735
     Episode_Reward/lifting_object: 47.5428
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 0.95s
                      Time elapsed: 00:04:01
                               ETA: 00:32:16

################################################################################
                     [1m Learning iteration 222/2000 [0m                      

                       Computation: 107179 steps/s (collection: 0.815s, learning 0.102s)
             Mean action noise std: 1.61
          Mean value_function loss: 236.5750
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 14.9143
                       Mean reward: 185.70
               Mean episode length: 241.91
    Episode_Reward/reaching_object: 0.3552
     Episode_Reward/lifting_object: 42.7469
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 0.92s
                      Time elapsed: 00:04:02
                               ETA: 00:32:14

################################################################################
                     [1m Learning iteration 223/2000 [0m                      

                       Computation: 107527 steps/s (collection: 0.805s, learning 0.109s)
             Mean action noise std: 1.61
          Mean value_function loss: 236.5092
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 14.9146
                       Mean reward: 210.50
               Mean episode length: 229.60
    Episode_Reward/reaching_object: 0.3349
     Episode_Reward/lifting_object: 41.1237
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 0.91s
                      Time elapsed: 00:04:03
                               ETA: 00:32:12

################################################################################
                     [1m Learning iteration 224/2000 [0m                      

                       Computation: 105639 steps/s (collection: 0.808s, learning 0.123s)
             Mean action noise std: 1.61
          Mean value_function loss: 229.8807
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 14.9156
                       Mean reward: 235.88
               Mean episode length: 231.39
    Episode_Reward/reaching_object: 0.3633
     Episode_Reward/lifting_object: 46.8605
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 0.93s
                      Time elapsed: 00:04:04
                               ETA: 00:32:09

################################################################################
                     [1m Learning iteration 225/2000 [0m                      

                       Computation: 108600 steps/s (collection: 0.810s, learning 0.095s)
             Mean action noise std: 1.61
          Mean value_function loss: 216.7486
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.9163
                       Mean reward: 271.43
               Mean episode length: 231.38
    Episode_Reward/reaching_object: 0.3717
     Episode_Reward/lifting_object: 51.1372
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 0.91s
                      Time elapsed: 00:04:05
                               ETA: 00:32:07

################################################################################
                     [1m Learning iteration 226/2000 [0m                      

                       Computation: 110529 steps/s (collection: 0.778s, learning 0.112s)
             Mean action noise std: 1.61
          Mean value_function loss: 208.6218
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 14.9177
                       Mean reward: 240.65
               Mean episode length: 220.59
    Episode_Reward/reaching_object: 0.3669
     Episode_Reward/lifting_object: 51.8331
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 0.89s
                      Time elapsed: 00:04:06
                               ETA: 00:32:04

################################################################################
                     [1m Learning iteration 227/2000 [0m                      

                       Computation: 105711 steps/s (collection: 0.820s, learning 0.110s)
             Mean action noise std: 1.61
          Mean value_function loss: 233.6738
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.9208
                       Mean reward: 271.73
               Mean episode length: 220.57
    Episode_Reward/reaching_object: 0.3610
     Episode_Reward/lifting_object: 49.7609
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 0.93s
                      Time elapsed: 00:04:07
                               ETA: 00:32:02

################################################################################
                     [1m Learning iteration 228/2000 [0m                      

                       Computation: 108474 steps/s (collection: 0.817s, learning 0.089s)
             Mean action noise std: 1.61
          Mean value_function loss: 251.4494
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.9200
                       Mean reward: 264.98
               Mean episode length: 227.85
    Episode_Reward/reaching_object: 0.3862
     Episode_Reward/lifting_object: 53.9502
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 0.91s
                      Time elapsed: 00:04:08
                               ETA: 00:31:59

################################################################################
                     [1m Learning iteration 229/2000 [0m                      

                       Computation: 106989 steps/s (collection: 0.819s, learning 0.100s)
             Mean action noise std: 1.61
          Mean value_function loss: 271.3206
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.9173
                       Mean reward: 313.98
               Mean episode length: 229.25
    Episode_Reward/reaching_object: 0.3910
     Episode_Reward/lifting_object: 56.4811
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 0.92s
                      Time elapsed: 00:04:09
                               ETA: 00:31:57

################################################################################
                     [1m Learning iteration 230/2000 [0m                      

                       Computation: 106517 steps/s (collection: 0.825s, learning 0.098s)
             Mean action noise std: 1.61
          Mean value_function loss: 278.4959
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.9114
                       Mean reward: 311.29
               Mean episode length: 236.10
    Episode_Reward/reaching_object: 0.3925
     Episode_Reward/lifting_object: 57.4848
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 0.92s
                      Time elapsed: 00:04:09
                               ETA: 00:31:55

################################################################################
                     [1m Learning iteration 231/2000 [0m                      

                       Computation: 106198 steps/s (collection: 0.821s, learning 0.105s)
             Mean action noise std: 1.61
          Mean value_function loss: 252.4286
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 14.9057
                       Mean reward: 301.25
               Mean episode length: 232.28
    Episode_Reward/reaching_object: 0.3899
     Episode_Reward/lifting_object: 54.2703
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 0.93s
                      Time elapsed: 00:04:10
                               ETA: 00:31:52

################################################################################
                     [1m Learning iteration 232/2000 [0m                      

                       Computation: 99478 steps/s (collection: 0.862s, learning 0.127s)
             Mean action noise std: 1.61
          Mean value_function loss: 254.4327
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 14.9017
                       Mean reward: 320.50
               Mean episode length: 241.98
    Episode_Reward/reaching_object: 0.4004
     Episode_Reward/lifting_object: 59.1917
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 0.99s
                      Time elapsed: 00:04:11
                               ETA: 00:31:51

################################################################################
                     [1m Learning iteration 233/2000 [0m                      

                       Computation: 101246 steps/s (collection: 0.883s, learning 0.088s)
             Mean action noise std: 1.61
          Mean value_function loss: 265.3331
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 14.9017
                       Mean reward: 237.29
               Mean episode length: 231.60
    Episode_Reward/reaching_object: 0.3677
     Episode_Reward/lifting_object: 50.1425
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 0.97s
                      Time elapsed: 00:04:12
                               ETA: 00:31:49

################################################################################
                     [1m Learning iteration 234/2000 [0m                      

                       Computation: 99760 steps/s (collection: 0.823s, learning 0.163s)
             Mean action noise std: 1.61
          Mean value_function loss: 271.2729
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 14.9024
                       Mean reward: 219.65
               Mean episode length: 235.97
    Episode_Reward/reaching_object: 0.3493
     Episode_Reward/lifting_object: 44.3515
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 0.99s
                      Time elapsed: 00:04:13
                               ETA: 00:31:47

################################################################################
                     [1m Learning iteration 235/2000 [0m                      

                       Computation: 104423 steps/s (collection: 0.829s, learning 0.113s)
             Mean action noise std: 1.61
          Mean value_function loss: 275.6093
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 14.9033
                       Mean reward: 228.04
               Mean episode length: 237.95
    Episode_Reward/reaching_object: 0.3264
     Episode_Reward/lifting_object: 38.5665
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 0.94s
                      Time elapsed: 00:04:14
                               ETA: 00:31:45

################################################################################
                     [1m Learning iteration 236/2000 [0m                      

                       Computation: 103827 steps/s (collection: 0.828s, learning 0.119s)
             Mean action noise std: 1.61
          Mean value_function loss: 319.3899
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.9012
                       Mean reward: 338.15
               Mean episode length: 235.11
    Episode_Reward/reaching_object: 0.3895
     Episode_Reward/lifting_object: 57.5420
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 0.95s
                      Time elapsed: 00:04:15
                               ETA: 00:31:43

################################################################################
                     [1m Learning iteration 237/2000 [0m                      

                       Computation: 102583 steps/s (collection: 0.807s, learning 0.152s)
             Mean action noise std: 1.61
          Mean value_function loss: 297.3643
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 14.8975
                       Mean reward: 242.28
               Mean episode length: 230.20
    Episode_Reward/reaching_object: 0.3857
     Episode_Reward/lifting_object: 56.8030
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 0.96s
                      Time elapsed: 00:04:16
                               ETA: 00:31:41

################################################################################
                     [1m Learning iteration 238/2000 [0m                      

                       Computation: 101350 steps/s (collection: 0.811s, learning 0.159s)
             Mean action noise std: 1.61
          Mean value_function loss: 320.1667
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 14.8965
                       Mean reward: 278.15
               Mean episode length: 237.49
    Episode_Reward/reaching_object: 0.3755
     Episode_Reward/lifting_object: 53.9703
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 0.97s
                      Time elapsed: 00:04:17
                               ETA: 00:31:39

################################################################################
                     [1m Learning iteration 239/2000 [0m                      

                       Computation: 111325 steps/s (collection: 0.784s, learning 0.099s)
             Mean action noise std: 1.61
          Mean value_function loss: 315.4739
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 14.8965
                       Mean reward: 341.27
               Mean episode length: 236.71
    Episode_Reward/reaching_object: 0.4173
     Episode_Reward/lifting_object: 66.9000
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 0.88s
                      Time elapsed: 00:04:18
                               ETA: 00:31:36

################################################################################
                     [1m Learning iteration 240/2000 [0m                      

                       Computation: 113327 steps/s (collection: 0.769s, learning 0.098s)
             Mean action noise std: 1.61
          Mean value_function loss: 321.7928
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 14.8965
                       Mean reward: 329.98
               Mean episode length: 241.41
    Episode_Reward/reaching_object: 0.4018
     Episode_Reward/lifting_object: 59.0425
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 0.87s
                      Time elapsed: 00:04:19
                               ETA: 00:31:34

################################################################################
                     [1m Learning iteration 241/2000 [0m                      

                       Computation: 107561 steps/s (collection: 0.822s, learning 0.092s)
             Mean action noise std: 1.61
          Mean value_function loss: 297.1877
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.8948
                       Mean reward: 391.78
               Mean episode length: 232.64
    Episode_Reward/reaching_object: 0.4170
     Episode_Reward/lifting_object: 66.5199
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 0.91s
                      Time elapsed: 00:04:20
                               ETA: 00:31:32

################################################################################
                     [1m Learning iteration 242/2000 [0m                      

                       Computation: 108405 steps/s (collection: 0.810s, learning 0.097s)
             Mean action noise std: 1.61
          Mean value_function loss: 338.3006
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 14.8916
                       Mean reward: 400.83
               Mean episode length: 239.79
    Episode_Reward/reaching_object: 0.4400
     Episode_Reward/lifting_object: 73.0043
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 0.91s
                      Time elapsed: 00:04:21
                               ETA: 00:31:29

################################################################################
                     [1m Learning iteration 243/2000 [0m                      

                       Computation: 107686 steps/s (collection: 0.816s, learning 0.097s)
             Mean action noise std: 1.61
          Mean value_function loss: 320.9009
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 14.8915
                       Mean reward: 400.24
               Mean episode length: 235.85
    Episode_Reward/reaching_object: 0.4319
     Episode_Reward/lifting_object: 70.2463
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 0.91s
                      Time elapsed: 00:04:22
                               ETA: 00:31:27

################################################################################
                     [1m Learning iteration 244/2000 [0m                      

                       Computation: 107028 steps/s (collection: 0.825s, learning 0.094s)
             Mean action noise std: 1.61
          Mean value_function loss: 321.0394
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.8907
                       Mean reward: 380.38
               Mean episode length: 239.92
    Episode_Reward/reaching_object: 0.4152
     Episode_Reward/lifting_object: 66.6891
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 0.92s
                      Time elapsed: 00:04:23
                               ETA: 00:31:25

################################################################################
                     [1m Learning iteration 245/2000 [0m                      

                       Computation: 103080 steps/s (collection: 0.858s, learning 0.096s)
             Mean action noise std: 1.61
          Mean value_function loss: 309.9316
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 14.8851
                       Mean reward: 345.37
               Mean episode length: 240.17
    Episode_Reward/reaching_object: 0.4401
     Episode_Reward/lifting_object: 72.9703
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 0.95s
                      Time elapsed: 00:04:23
                               ETA: 00:31:23

################################################################################
                     [1m Learning iteration 246/2000 [0m                      

                       Computation: 101304 steps/s (collection: 0.872s, learning 0.098s)
             Mean action noise std: 1.61
          Mean value_function loss: 288.1310
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 14.8793
                       Mean reward: 369.31
               Mean episode length: 236.53
    Episode_Reward/reaching_object: 0.4455
     Episode_Reward/lifting_object: 72.5612
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 0.97s
                      Time elapsed: 00:04:24
                               ETA: 00:31:21

################################################################################
                     [1m Learning iteration 247/2000 [0m                      

                       Computation: 107235 steps/s (collection: 0.809s, learning 0.108s)
             Mean action noise std: 1.61
          Mean value_function loss: 321.3354
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 14.8720
                       Mean reward: 407.39
               Mean episode length: 240.40
    Episode_Reward/reaching_object: 0.4672
     Episode_Reward/lifting_object: 78.7488
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 0.92s
                      Time elapsed: 00:04:25
                               ETA: 00:31:19

################################################################################
                     [1m Learning iteration 248/2000 [0m                      

                       Computation: 103065 steps/s (collection: 0.828s, learning 0.126s)
             Mean action noise std: 1.61
          Mean value_function loss: 311.8920
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 14.8647
                       Mean reward: 335.85
               Mean episode length: 232.02
    Episode_Reward/reaching_object: 0.4397
     Episode_Reward/lifting_object: 71.1338
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 0.95s
                      Time elapsed: 00:04:26
                               ETA: 00:31:17

################################################################################
                     [1m Learning iteration 249/2000 [0m                      

                       Computation: 104112 steps/s (collection: 0.817s, learning 0.128s)
             Mean action noise std: 1.61
          Mean value_function loss: 330.9586
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 14.8584
                       Mean reward: 332.73
               Mean episode length: 236.89
    Episode_Reward/reaching_object: 0.4641
     Episode_Reward/lifting_object: 77.3054
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 0.94s
                      Time elapsed: 00:04:27
                               ETA: 00:31:15

################################################################################
                     [1m Learning iteration 250/2000 [0m                      

                       Computation: 106335 steps/s (collection: 0.811s, learning 0.113s)
             Mean action noise std: 1.61
          Mean value_function loss: 337.0662
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 14.8545
                       Mean reward: 344.74
               Mean episode length: 229.30
    Episode_Reward/reaching_object: 0.4604
     Episode_Reward/lifting_object: 74.5428
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 0.92s
                      Time elapsed: 00:04:28
                               ETA: 00:31:13

################################################################################
                     [1m Learning iteration 251/2000 [0m                      

                       Computation: 105026 steps/s (collection: 0.812s, learning 0.124s)
             Mean action noise std: 1.61
          Mean value_function loss: 311.5640
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 14.8488
                       Mean reward: 365.11
               Mean episode length: 238.57
    Episode_Reward/reaching_object: 0.4763
     Episode_Reward/lifting_object: 80.1404
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 0.94s
                      Time elapsed: 00:04:29
                               ETA: 00:31:11

################################################################################
                     [1m Learning iteration 252/2000 [0m                      

                       Computation: 103215 steps/s (collection: 0.801s, learning 0.152s)
             Mean action noise std: 1.61
          Mean value_function loss: 312.6888
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 14.8454
                       Mean reward: 402.43
               Mean episode length: 240.44
    Episode_Reward/reaching_object: 0.4726
     Episode_Reward/lifting_object: 78.3712
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 0.95s
                      Time elapsed: 00:04:30
                               ETA: 00:31:09

################################################################################
                     [1m Learning iteration 253/2000 [0m                      

                       Computation: 111225 steps/s (collection: 0.791s, learning 0.093s)
             Mean action noise std: 1.61
          Mean value_function loss: 308.8170
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.8431
                       Mean reward: 374.72
               Mean episode length: 236.27
    Episode_Reward/reaching_object: 0.4633
     Episode_Reward/lifting_object: 73.5947
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 0.88s
                      Time elapsed: 00:04:31
                               ETA: 00:31:07

################################################################################
                     [1m Learning iteration 254/2000 [0m                      

                       Computation: 111167 steps/s (collection: 0.793s, learning 0.091s)
             Mean action noise std: 1.61
          Mean value_function loss: 316.5374
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 14.8418
                       Mean reward: 314.45
               Mean episode length: 230.50
    Episode_Reward/reaching_object: 0.4499
     Episode_Reward/lifting_object: 69.6099
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 0.88s
                      Time elapsed: 00:04:32
                               ETA: 00:31:04

################################################################################
                     [1m Learning iteration 255/2000 [0m                      

                       Computation: 104729 steps/s (collection: 0.831s, learning 0.108s)
             Mean action noise std: 1.61
          Mean value_function loss: 317.3640
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 14.8439
                       Mean reward: 338.52
               Mean episode length: 232.88
    Episode_Reward/reaching_object: 0.4752
     Episode_Reward/lifting_object: 78.2991
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 0.94s
                      Time elapsed: 00:04:33
                               ETA: 00:31:02

################################################################################
                     [1m Learning iteration 256/2000 [0m                      

                       Computation: 100878 steps/s (collection: 0.879s, learning 0.095s)
             Mean action noise std: 1.61
          Mean value_function loss: 308.0256
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 14.8460
                       Mean reward: 384.85
               Mean episode length: 239.35
    Episode_Reward/reaching_object: 0.4693
     Episode_Reward/lifting_object: 75.7392
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 0.97s
                      Time elapsed: 00:04:34
                               ETA: 00:31:01

################################################################################
                     [1m Learning iteration 257/2000 [0m                      

                       Computation: 97778 steps/s (collection: 0.905s, learning 0.101s)
             Mean action noise std: 1.61
          Mean value_function loss: 368.0957
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 14.8480
                       Mean reward: 374.81
               Mean episode length: 242.14
    Episode_Reward/reaching_object: 0.4760
     Episode_Reward/lifting_object: 77.6639
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 1.01s
                      Time elapsed: 00:04:35
                               ETA: 00:30:59

################################################################################
                     [1m Learning iteration 258/2000 [0m                      

                       Computation: 107732 steps/s (collection: 0.817s, learning 0.095s)
             Mean action noise std: 1.61
          Mean value_function loss: 369.0065
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 14.8474
                       Mean reward: 393.87
               Mean episode length: 237.71
    Episode_Reward/reaching_object: 0.4782
     Episode_Reward/lifting_object: 82.5073
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 0.91s
                      Time elapsed: 00:04:36
                               ETA: 00:30:57

################################################################################
                     [1m Learning iteration 259/2000 [0m                      

                       Computation: 102294 steps/s (collection: 0.853s, learning 0.108s)
             Mean action noise std: 1.61
          Mean value_function loss: 365.0370
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 14.8448
                       Mean reward: 444.38
               Mean episode length: 231.04
    Episode_Reward/reaching_object: 0.4910
     Episode_Reward/lifting_object: 87.4139
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 0.96s
                      Time elapsed: 00:04:37
                               ETA: 00:30:55

################################################################################
                     [1m Learning iteration 260/2000 [0m                      

                       Computation: 108233 steps/s (collection: 0.820s, learning 0.088s)
             Mean action noise std: 1.61
          Mean value_function loss: 348.4242
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.8389
                       Mean reward: 364.85
               Mean episode length: 238.98
    Episode_Reward/reaching_object: 0.4903
     Episode_Reward/lifting_object: 85.2981
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 0.91s
                      Time elapsed: 00:04:38
                               ETA: 00:30:53

################################################################################
                     [1m Learning iteration 261/2000 [0m                      

                       Computation: 99471 steps/s (collection: 0.884s, learning 0.104s)
             Mean action noise std: 1.61
          Mean value_function loss: 369.9625
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 14.8334
                       Mean reward: 468.99
               Mean episode length: 237.32
    Episode_Reward/reaching_object: 0.4821
     Episode_Reward/lifting_object: 85.9522
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 0.99s
                      Time elapsed: 00:04:39
                               ETA: 00:30:52

################################################################################
                     [1m Learning iteration 262/2000 [0m                      

                       Computation: 101835 steps/s (collection: 0.876s, learning 0.090s)
             Mean action noise std: 1.61
          Mean value_function loss: 329.2147
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 14.8317
                       Mean reward: 431.47
               Mean episode length: 239.36
    Episode_Reward/reaching_object: 0.4862
     Episode_Reward/lifting_object: 87.1739
      Episode_Reward/object_height: 0.0277
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 0.97s
                      Time elapsed: 00:04:40
                               ETA: 00:30:50

################################################################################
                     [1m Learning iteration 263/2000 [0m                      

                       Computation: 101856 steps/s (collection: 0.841s, learning 0.125s)
             Mean action noise std: 1.61
          Mean value_function loss: 330.1050
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 14.8302
                       Mean reward: 461.93
               Mean episode length: 238.71
    Episode_Reward/reaching_object: 0.4924
     Episode_Reward/lifting_object: 87.1954
      Episode_Reward/object_height: 0.0280
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 0.97s
                      Time elapsed: 00:04:40
                               ETA: 00:30:48

################################################################################
                     [1m Learning iteration 264/2000 [0m                      

                       Computation: 108585 steps/s (collection: 0.819s, learning 0.086s)
             Mean action noise std: 1.61
          Mean value_function loss: 360.0945
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.8283
                       Mean reward: 422.29
               Mean episode length: 241.65
    Episode_Reward/reaching_object: 0.4887
     Episode_Reward/lifting_object: 87.6766
      Episode_Reward/object_height: 0.0285
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 0.91s
                      Time elapsed: 00:04:41
                               ETA: 00:30:46

################################################################################
                     [1m Learning iteration 265/2000 [0m                      

                       Computation: 111170 steps/s (collection: 0.797s, learning 0.087s)
             Mean action noise std: 1.61
          Mean value_function loss: 330.2970
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.8271
                       Mean reward: 396.95
               Mean episode length: 240.02
    Episode_Reward/reaching_object: 0.4999
     Episode_Reward/lifting_object: 89.6713
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 0.88s
                      Time elapsed: 00:04:42
                               ETA: 00:30:44

################################################################################
                     [1m Learning iteration 266/2000 [0m                      

                       Computation: 99716 steps/s (collection: 0.882s, learning 0.104s)
             Mean action noise std: 1.61
          Mean value_function loss: 361.2163
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 14.8309
                       Mean reward: 422.77
               Mean episode length: 242.97
    Episode_Reward/reaching_object: 0.5010
     Episode_Reward/lifting_object: 90.4774
      Episode_Reward/object_height: 0.0298
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 0.99s
                      Time elapsed: 00:04:43
                               ETA: 00:30:42

################################################################################
                     [1m Learning iteration 267/2000 [0m                      

                       Computation: 106027 steps/s (collection: 0.811s, learning 0.116s)
             Mean action noise std: 1.61
          Mean value_function loss: 344.3259
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 14.8347
                       Mean reward: 484.11
               Mean episode length: 244.82
    Episode_Reward/reaching_object: 0.5055
     Episode_Reward/lifting_object: 93.3558
      Episode_Reward/object_height: 0.0310
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 0.93s
                      Time elapsed: 00:04:44
                               ETA: 00:30:40

################################################################################
                     [1m Learning iteration 268/2000 [0m                      

                       Computation: 103327 steps/s (collection: 0.811s, learning 0.140s)
             Mean action noise std: 1.61
          Mean value_function loss: 375.1207
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 14.8367
                       Mean reward: 500.00
               Mean episode length: 243.81
    Episode_Reward/reaching_object: 0.4730
     Episode_Reward/lifting_object: 85.5471
      Episode_Reward/object_height: 0.0289
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 0.95s
                      Time elapsed: 00:04:45
                               ETA: 00:30:39

################################################################################
                     [1m Learning iteration 269/2000 [0m                      

                       Computation: 101409 steps/s (collection: 0.850s, learning 0.120s)
             Mean action noise std: 1.61
          Mean value_function loss: 373.8689
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.8378
                       Mean reward: 512.72
               Mean episode length: 240.71
    Episode_Reward/reaching_object: 0.5004
     Episode_Reward/lifting_object: 93.3011
      Episode_Reward/object_height: 0.0317
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 0.97s
                      Time elapsed: 00:04:46
                               ETA: 00:30:37

################################################################################
                     [1m Learning iteration 270/2000 [0m                      

                       Computation: 103385 steps/s (collection: 0.819s, learning 0.132s)
             Mean action noise std: 1.61
          Mean value_function loss: 391.6521
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 14.8395
                       Mean reward: 537.73
               Mean episode length: 241.09
    Episode_Reward/reaching_object: 0.5326
     Episode_Reward/lifting_object: 101.4418
      Episode_Reward/object_height: 0.0349
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 0.95s
                      Time elapsed: 00:04:47
                               ETA: 00:30:35

################################################################################
                     [1m Learning iteration 271/2000 [0m                      

                       Computation: 105439 steps/s (collection: 0.816s, learning 0.117s)
             Mean action noise std: 1.61
          Mean value_function loss: 388.9705
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.8405
                       Mean reward: 490.96
               Mean episode length: 243.59
    Episode_Reward/reaching_object: 0.5227
     Episode_Reward/lifting_object: 99.6076
      Episode_Reward/object_height: 0.0342
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 0.93s
                      Time elapsed: 00:04:48
                               ETA: 00:30:33

################################################################################
                     [1m Learning iteration 272/2000 [0m                      

                       Computation: 109649 steps/s (collection: 0.800s, learning 0.097s)
             Mean action noise std: 1.61
          Mean value_function loss: 386.9850
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 14.8407
                       Mean reward: 505.65
               Mean episode length: 244.82
    Episode_Reward/reaching_object: 0.5095
     Episode_Reward/lifting_object: 97.2781
      Episode_Reward/object_height: 0.0337
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 0.90s
                      Time elapsed: 00:04:49
                               ETA: 00:30:31

################################################################################
                     [1m Learning iteration 273/2000 [0m                      

                       Computation: 102412 steps/s (collection: 0.863s, learning 0.097s)
             Mean action noise std: 1.61
          Mean value_function loss: 340.8431
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 14.8410
                       Mean reward: 536.43
               Mean episode length: 238.29
    Episode_Reward/reaching_object: 0.5127
     Episode_Reward/lifting_object: 98.0270
      Episode_Reward/object_height: 0.0344
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 0.96s
                      Time elapsed: 00:04:50
                               ETA: 00:30:29

################################################################################
                     [1m Learning iteration 274/2000 [0m                      

                       Computation: 102466 steps/s (collection: 0.870s, learning 0.090s)
             Mean action noise std: 1.61
          Mean value_function loss: 324.7624
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 14.8402
                       Mean reward: 525.04
               Mean episode length: 239.17
    Episode_Reward/reaching_object: 0.5314
     Episode_Reward/lifting_object: 101.6764
      Episode_Reward/object_height: 0.0359
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 0.96s
                      Time elapsed: 00:04:51
                               ETA: 00:30:28

################################################################################
                     [1m Learning iteration 275/2000 [0m                      

                       Computation: 106888 steps/s (collection: 0.820s, learning 0.100s)
             Mean action noise std: 1.61
          Mean value_function loss: 350.8817
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.8383
                       Mean reward: 605.34
               Mean episode length: 245.96
    Episode_Reward/reaching_object: 0.5293
     Episode_Reward/lifting_object: 105.2977
      Episode_Reward/object_height: 0.0376
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 0.92s
                      Time elapsed: 00:04:52
                               ETA: 00:30:26

################################################################################
                     [1m Learning iteration 276/2000 [0m                      

                       Computation: 111027 steps/s (collection: 0.781s, learning 0.104s)
             Mean action noise std: 1.61
          Mean value_function loss: 384.7397
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.8378
                       Mean reward: 528.25
               Mean episode length: 247.93
    Episode_Reward/reaching_object: 0.5354
     Episode_Reward/lifting_object: 105.1737
      Episode_Reward/object_height: 0.0378
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 0.89s
                      Time elapsed: 00:04:53
                               ETA: 00:30:24

################################################################################
                     [1m Learning iteration 277/2000 [0m                      

                       Computation: 107868 steps/s (collection: 0.815s, learning 0.096s)
             Mean action noise std: 1.61
          Mean value_function loss: 372.4948
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 14.8373
                       Mean reward: 470.30
               Mean episode length: 235.77
    Episode_Reward/reaching_object: 0.5155
     Episode_Reward/lifting_object: 98.4995
      Episode_Reward/object_height: 0.0353
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 0.91s
                      Time elapsed: 00:04:54
                               ETA: 00:30:22

################################################################################
                     [1m Learning iteration 278/2000 [0m                      

                       Computation: 109079 steps/s (collection: 0.804s, learning 0.097s)
             Mean action noise std: 1.61
          Mean value_function loss: 366.2415
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.8384
                       Mean reward: 526.12
               Mean episode length: 246.09
    Episode_Reward/reaching_object: 0.5136
     Episode_Reward/lifting_object: 99.5274
      Episode_Reward/object_height: 0.0363
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 0.90s
                      Time elapsed: 00:04:54
                               ETA: 00:30:20

################################################################################
                     [1m Learning iteration 279/2000 [0m                      

                       Computation: 112837 steps/s (collection: 0.784s, learning 0.087s)
             Mean action noise std: 1.61
          Mean value_function loss: 368.5685
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 14.8400
                       Mean reward: 511.19
               Mean episode length: 241.05
    Episode_Reward/reaching_object: 0.5233
     Episode_Reward/lifting_object: 102.0008
      Episode_Reward/object_height: 0.0374
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 0.87s
                      Time elapsed: 00:04:55
                               ETA: 00:30:18

################################################################################
                     [1m Learning iteration 280/2000 [0m                      

                       Computation: 103042 steps/s (collection: 0.831s, learning 0.123s)
             Mean action noise std: 1.61
          Mean value_function loss: 389.3372
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 14.8390
                       Mean reward: 527.74
               Mean episode length: 242.23
    Episode_Reward/reaching_object: 0.5221
     Episode_Reward/lifting_object: 101.1597
      Episode_Reward/object_height: 0.0378
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 0.95s
                      Time elapsed: 00:04:56
                               ETA: 00:30:16

################################################################################
                     [1m Learning iteration 281/2000 [0m                      

                       Computation: 109691 steps/s (collection: 0.807s, learning 0.089s)
             Mean action noise std: 1.61
          Mean value_function loss: 386.1703
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 14.8349
                       Mean reward: 588.37
               Mean episode length: 242.07
    Episode_Reward/reaching_object: 0.5204
     Episode_Reward/lifting_object: 102.0230
      Episode_Reward/object_height: 0.0386
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 0.90s
                      Time elapsed: 00:04:57
                               ETA: 00:30:14

################################################################################
                     [1m Learning iteration 282/2000 [0m                      

                       Computation: 105444 steps/s (collection: 0.795s, learning 0.138s)
             Mean action noise std: 1.61
          Mean value_function loss: 379.4021
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 14.8351
                       Mean reward: 527.56
               Mean episode length: 239.40
    Episode_Reward/reaching_object: 0.5286
     Episode_Reward/lifting_object: 104.0819
      Episode_Reward/object_height: 0.0392
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 0.93s
                      Time elapsed: 00:04:58
                               ETA: 00:30:12

################################################################################
                     [1m Learning iteration 283/2000 [0m                      

                       Computation: 109742 steps/s (collection: 0.801s, learning 0.095s)
             Mean action noise std: 1.61
          Mean value_function loss: 363.2613
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 14.8376
                       Mean reward: 504.25
               Mean episode length: 242.00
    Episode_Reward/reaching_object: 0.5198
     Episode_Reward/lifting_object: 102.2164
      Episode_Reward/object_height: 0.0386
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 0.90s
                      Time elapsed: 00:04:59
                               ETA: 00:30:10

################################################################################
                     [1m Learning iteration 284/2000 [0m                      

                       Computation: 108770 steps/s (collection: 0.795s, learning 0.109s)
             Mean action noise std: 1.61
          Mean value_function loss: 372.4432
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.8385
                       Mean reward: 537.33
               Mean episode length: 237.32
    Episode_Reward/reaching_object: 0.5399
     Episode_Reward/lifting_object: 109.5977
      Episode_Reward/object_height: 0.0415
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 0.90s
                      Time elapsed: 00:05:00
                               ETA: 00:30:08

################################################################################
                     [1m Learning iteration 285/2000 [0m                      

                       Computation: 106385 steps/s (collection: 0.784s, learning 0.140s)
             Mean action noise std: 1.61
          Mean value_function loss: 342.4467
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 14.8431
                       Mean reward: 511.06
               Mean episode length: 240.41
    Episode_Reward/reaching_object: 0.5153
     Episode_Reward/lifting_object: 101.6813
      Episode_Reward/object_height: 0.0386
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 0.92s
                      Time elapsed: 00:05:01
                               ETA: 00:30:06

################################################################################
                     [1m Learning iteration 286/2000 [0m                      

                       Computation: 104831 steps/s (collection: 0.839s, learning 0.099s)
             Mean action noise std: 1.61
          Mean value_function loss: 365.6122
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.8436
                       Mean reward: 530.57
               Mean episode length: 236.83
    Episode_Reward/reaching_object: 0.5125
     Episode_Reward/lifting_object: 101.6445
      Episode_Reward/object_height: 0.0390
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 0.94s
                      Time elapsed: 00:05:02
                               ETA: 00:30:04

################################################################################
                     [1m Learning iteration 287/2000 [0m                      

                       Computation: 107703 steps/s (collection: 0.814s, learning 0.099s)
             Mean action noise std: 1.61
          Mean value_function loss: 319.4693
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 14.8424
                       Mean reward: 525.84
               Mean episode length: 239.04
    Episode_Reward/reaching_object: 0.5336
     Episode_Reward/lifting_object: 105.8054
      Episode_Reward/object_height: 0.0406
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 0.91s
                      Time elapsed: 00:05:03
                               ETA: 00:30:03

################################################################################
                     [1m Learning iteration 288/2000 [0m                      

                       Computation: 101840 steps/s (collection: 0.844s, learning 0.122s)
             Mean action noise std: 1.61
          Mean value_function loss: 332.2536
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.8444
                       Mean reward: 546.89
               Mean episode length: 237.74
    Episode_Reward/reaching_object: 0.5288
     Episode_Reward/lifting_object: 107.4229
      Episode_Reward/object_height: 0.0412
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 0.97s
                      Time elapsed: 00:05:04
                               ETA: 00:30:01

################################################################################
                     [1m Learning iteration 289/2000 [0m                      

                       Computation: 106799 steps/s (collection: 0.828s, learning 0.093s)
             Mean action noise std: 1.61
          Mean value_function loss: 335.9708
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.8503
                       Mean reward: 549.34
               Mean episode length: 233.27
    Episode_Reward/reaching_object: 0.5538
     Episode_Reward/lifting_object: 114.6797
      Episode_Reward/object_height: 0.0439
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 0.92s
                      Time elapsed: 00:05:05
                               ETA: 00:29:59

################################################################################
                     [1m Learning iteration 290/2000 [0m                      

                       Computation: 106507 steps/s (collection: 0.829s, learning 0.094s)
             Mean action noise std: 1.61
          Mean value_function loss: 361.7605
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.8533
                       Mean reward: 564.49
               Mean episode length: 240.48
    Episode_Reward/reaching_object: 0.5288
     Episode_Reward/lifting_object: 107.7466
      Episode_Reward/object_height: 0.0413
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 0.92s
                      Time elapsed: 00:05:05
                               ETA: 00:29:57

################################################################################
                     [1m Learning iteration 291/2000 [0m                      

                       Computation: 110228 steps/s (collection: 0.797s, learning 0.095s)
             Mean action noise std: 1.61
          Mean value_function loss: 341.6063
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 14.8541
                       Mean reward: 551.38
               Mean episode length: 240.88
    Episode_Reward/reaching_object: 0.5305
     Episode_Reward/lifting_object: 108.3511
      Episode_Reward/object_height: 0.0418
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 0.89s
                      Time elapsed: 00:05:06
                               ETA: 00:29:55

################################################################################
                     [1m Learning iteration 292/2000 [0m                      

                       Computation: 109550 steps/s (collection: 0.809s, learning 0.089s)
             Mean action noise std: 1.62
          Mean value_function loss: 314.1648
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 14.8555
                       Mean reward: 561.15
               Mean episode length: 235.64
    Episode_Reward/reaching_object: 0.5529
     Episode_Reward/lifting_object: 113.2315
      Episode_Reward/object_height: 0.0439
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 0.90s
                      Time elapsed: 00:05:07
                               ETA: 00:29:53

################################################################################
                     [1m Learning iteration 293/2000 [0m                      

                       Computation: 113793 steps/s (collection: 0.778s, learning 0.086s)
             Mean action noise std: 1.62
          Mean value_function loss: 324.7775
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 14.8559
                       Mean reward: 534.62
               Mean episode length: 232.43
    Episode_Reward/reaching_object: 0.5293
     Episode_Reward/lifting_object: 107.2773
      Episode_Reward/object_height: 0.0416
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 0.86s
                      Time elapsed: 00:05:08
                               ETA: 00:29:51

################################################################################
                     [1m Learning iteration 294/2000 [0m                      

                       Computation: 106444 steps/s (collection: 0.773s, learning 0.150s)
             Mean action noise std: 1.62
          Mean value_function loss: 350.4624
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 14.8557
                       Mean reward: 528.75
               Mean episode length: 231.87
    Episode_Reward/reaching_object: 0.5246
     Episode_Reward/lifting_object: 106.4081
      Episode_Reward/object_height: 0.0416
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 0.92s
                      Time elapsed: 00:05:09
                               ETA: 00:29:50

################################################################################
                     [1m Learning iteration 295/2000 [0m                      

                       Computation: 104139 steps/s (collection: 0.811s, learning 0.133s)
             Mean action noise std: 1.62
          Mean value_function loss: 348.2504
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 14.8565
                       Mean reward: 466.71
               Mean episode length: 226.63
    Episode_Reward/reaching_object: 0.5404
     Episode_Reward/lifting_object: 110.8080
      Episode_Reward/object_height: 0.0434
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 0.94s
                      Time elapsed: 00:05:10
                               ETA: 00:29:48

################################################################################
                     [1m Learning iteration 296/2000 [0m                      

                       Computation: 95765 steps/s (collection: 0.838s, learning 0.188s)
             Mean action noise std: 1.62
          Mean value_function loss: 342.4218
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 14.8571
                       Mean reward: 553.77
               Mean episode length: 233.82
    Episode_Reward/reaching_object: 0.5274
     Episode_Reward/lifting_object: 106.3398
      Episode_Reward/object_height: 0.0418
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 1.03s
                      Time elapsed: 00:05:11
                               ETA: 00:29:47

################################################################################
                     [1m Learning iteration 297/2000 [0m                      

                       Computation: 103431 steps/s (collection: 0.795s, learning 0.156s)
             Mean action noise std: 1.62
          Mean value_function loss: 300.0913
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.8570
                       Mean reward: 530.76
               Mean episode length: 239.78
    Episode_Reward/reaching_object: 0.5412
     Episode_Reward/lifting_object: 110.5130
      Episode_Reward/object_height: 0.0434
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 0.95s
                      Time elapsed: 00:05:12
                               ETA: 00:29:45

################################################################################
                     [1m Learning iteration 298/2000 [0m                      

                       Computation: 114338 steps/s (collection: 0.773s, learning 0.087s)
             Mean action noise std: 1.62
          Mean value_function loss: 361.2419
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.8549
                       Mean reward: 496.30
               Mean episode length: 244.97
    Episode_Reward/reaching_object: 0.5214
     Episode_Reward/lifting_object: 105.3816
      Episode_Reward/object_height: 0.0414
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 0.86s
                      Time elapsed: 00:05:13
                               ETA: 00:29:43

################################################################################
                     [1m Learning iteration 299/2000 [0m                      

                       Computation: 106935 steps/s (collection: 0.830s, learning 0.089s)
             Mean action noise std: 1.62
          Mean value_function loss: 345.9100
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 14.8556
                       Mean reward: 599.88
               Mean episode length: 238.26
    Episode_Reward/reaching_object: 0.5521
     Episode_Reward/lifting_object: 114.9174
      Episode_Reward/object_height: 0.0450
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 0.92s
                      Time elapsed: 00:05:14
                               ETA: 00:29:41

################################################################################
                     [1m Learning iteration 300/2000 [0m                      

                       Computation: 103974 steps/s (collection: 0.838s, learning 0.108s)
             Mean action noise std: 1.62
          Mean value_function loss: 344.9058
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.8538
                       Mean reward: 588.55
               Mean episode length: 237.53
    Episode_Reward/reaching_object: 0.5691
     Episode_Reward/lifting_object: 119.3506
      Episode_Reward/object_height: 0.0464
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 0.95s
                      Time elapsed: 00:05:15
                               ETA: 00:29:40

################################################################################
                     [1m Learning iteration 301/2000 [0m                      

                       Computation: 101213 steps/s (collection: 0.849s, learning 0.123s)
             Mean action noise std: 1.62
          Mean value_function loss: 360.9179
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 14.8499
                       Mean reward: 576.71
               Mean episode length: 232.32
    Episode_Reward/reaching_object: 0.5635
     Episode_Reward/lifting_object: 116.8288
      Episode_Reward/object_height: 0.0452
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 0.97s
                      Time elapsed: 00:05:16
                               ETA: 00:29:38

################################################################################
                     [1m Learning iteration 302/2000 [0m                      

                       Computation: 107211 steps/s (collection: 0.828s, learning 0.089s)
             Mean action noise std: 1.62
          Mean value_function loss: 325.2633
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 14.8504
                       Mean reward: 591.46
               Mean episode length: 230.17
    Episode_Reward/reaching_object: 0.5603
     Episode_Reward/lifting_object: 117.1896
      Episode_Reward/object_height: 0.0451
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 0.92s
                      Time elapsed: 00:05:17
                               ETA: 00:29:36

################################################################################
                     [1m Learning iteration 303/2000 [0m                      

                       Computation: 103436 steps/s (collection: 0.858s, learning 0.092s)
             Mean action noise std: 1.62
          Mean value_function loss: 326.8820
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 14.8518
                       Mean reward: 574.47
               Mean episode length: 238.92
    Episode_Reward/reaching_object: 0.5671
     Episode_Reward/lifting_object: 119.2706
      Episode_Reward/object_height: 0.0456
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 0.95s
                      Time elapsed: 00:05:18
                               ETA: 00:29:35

################################################################################
                     [1m Learning iteration 304/2000 [0m                      

                       Computation: 108050 steps/s (collection: 0.801s, learning 0.109s)
             Mean action noise std: 1.62
          Mean value_function loss: 346.7473
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 14.8532
                       Mean reward: 580.94
               Mean episode length: 231.41
    Episode_Reward/reaching_object: 0.5606
     Episode_Reward/lifting_object: 116.9930
      Episode_Reward/object_height: 0.0445
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 0.91s
                      Time elapsed: 00:05:18
                               ETA: 00:29:33

################################################################################
                     [1m Learning iteration 305/2000 [0m                      

                       Computation: 110979 steps/s (collection: 0.792s, learning 0.094s)
             Mean action noise std: 1.62
          Mean value_function loss: 335.2030
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 14.8543
                       Mean reward: 546.00
               Mean episode length: 231.07
    Episode_Reward/reaching_object: 0.5707
     Episode_Reward/lifting_object: 118.5616
      Episode_Reward/object_height: 0.0448
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 0.89s
                      Time elapsed: 00:05:19
                               ETA: 00:29:31

################################################################################
                     [1m Learning iteration 306/2000 [0m                      

                       Computation: 106397 steps/s (collection: 0.834s, learning 0.090s)
             Mean action noise std: 1.62
          Mean value_function loss: 319.0104
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 14.8528
                       Mean reward: 602.43
               Mean episode length: 239.61
    Episode_Reward/reaching_object: 0.5816
     Episode_Reward/lifting_object: 123.0901
      Episode_Reward/object_height: 0.0464
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 0.92s
                      Time elapsed: 00:05:20
                               ETA: 00:29:29

################################################################################
                     [1m Learning iteration 307/2000 [0m                      

                       Computation: 103116 steps/s (collection: 0.825s, learning 0.129s)
             Mean action noise std: 1.62
          Mean value_function loss: 314.3973
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 14.8520
                       Mean reward: 673.16
               Mean episode length: 244.98
    Episode_Reward/reaching_object: 0.5877
     Episode_Reward/lifting_object: 123.6547
      Episode_Reward/object_height: 0.0465
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 0.95s
                      Time elapsed: 00:05:21
                               ETA: 00:29:28

################################################################################
                     [1m Learning iteration 308/2000 [0m                      

                       Computation: 99403 steps/s (collection: 0.802s, learning 0.187s)
             Mean action noise std: 1.62
          Mean value_function loss: 308.9066
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 14.8514
                       Mean reward: 633.66
               Mean episode length: 239.67
    Episode_Reward/reaching_object: 0.5896
     Episode_Reward/lifting_object: 123.7902
      Episode_Reward/object_height: 0.0466
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 0.99s
                      Time elapsed: 00:05:22
                               ETA: 00:29:26

################################################################################
                     [1m Learning iteration 309/2000 [0m                      

                       Computation: 104980 steps/s (collection: 0.824s, learning 0.112s)
             Mean action noise std: 1.62
          Mean value_function loss: 323.0104
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 14.8525
                       Mean reward: 618.79
               Mean episode length: 235.45
    Episode_Reward/reaching_object: 0.5923
     Episode_Reward/lifting_object: 125.2965
      Episode_Reward/object_height: 0.0471
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 0.94s
                      Time elapsed: 00:05:23
                               ETA: 00:29:25

################################################################################
                     [1m Learning iteration 310/2000 [0m                      

                       Computation: 96012 steps/s (collection: 0.851s, learning 0.173s)
             Mean action noise std: 1.62
          Mean value_function loss: 340.6151
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 14.8526
                       Mean reward: 635.00
               Mean episode length: 236.27
    Episode_Reward/reaching_object: 0.5974
     Episode_Reward/lifting_object: 127.4144
      Episode_Reward/object_height: 0.0479
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 1.02s
                      Time elapsed: 00:05:24
                               ETA: 00:29:24

################################################################################
                     [1m Learning iteration 311/2000 [0m                      

                       Computation: 95826 steps/s (collection: 0.895s, learning 0.131s)
             Mean action noise std: 1.62
          Mean value_function loss: 310.0738
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 14.8530
                       Mean reward: 567.13
               Mean episode length: 239.15
    Episode_Reward/reaching_object: 0.5718
     Episode_Reward/lifting_object: 118.0117
      Episode_Reward/object_height: 0.0444
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 1.03s
                      Time elapsed: 00:05:25
                               ETA: 00:29:22

################################################################################
                     [1m Learning iteration 312/2000 [0m                      

                       Computation: 106819 steps/s (collection: 0.790s, learning 0.130s)
             Mean action noise std: 1.62
          Mean value_function loss: 360.2380
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 14.8554
                       Mean reward: 579.58
               Mean episode length: 234.08
    Episode_Reward/reaching_object: 0.5637
     Episode_Reward/lifting_object: 116.7238
      Episode_Reward/object_height: 0.0439
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 0.92s
                      Time elapsed: 00:05:26
                               ETA: 00:29:21

################################################################################
                     [1m Learning iteration 313/2000 [0m                      

                       Computation: 106797 steps/s (collection: 0.800s, learning 0.120s)
             Mean action noise std: 1.62
          Mean value_function loss: 339.3070
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 14.8585
                       Mean reward: 630.49
               Mean episode length: 237.24
    Episode_Reward/reaching_object: 0.5759
     Episode_Reward/lifting_object: 121.0003
      Episode_Reward/object_height: 0.0456
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 0.92s
                      Time elapsed: 00:05:27
                               ETA: 00:29:19

################################################################################
                     [1m Learning iteration 314/2000 [0m                      

                       Computation: 108406 steps/s (collection: 0.795s, learning 0.112s)
             Mean action noise std: 1.62
          Mean value_function loss: 347.2305
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 14.8586
                       Mean reward: 603.98
               Mean episode length: 230.40
    Episode_Reward/reaching_object: 0.5712
     Episode_Reward/lifting_object: 120.4285
      Episode_Reward/object_height: 0.0457
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 0.91s
                      Time elapsed: 00:05:28
                               ETA: 00:29:17

################################################################################
                     [1m Learning iteration 315/2000 [0m                      

                       Computation: 107329 steps/s (collection: 0.810s, learning 0.106s)
             Mean action noise std: 1.62
          Mean value_function loss: 327.4923
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 14.8560
                       Mean reward: 632.29
               Mean episode length: 235.65
    Episode_Reward/reaching_object: 0.5970
     Episode_Reward/lifting_object: 127.4619
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 0.92s
                      Time elapsed: 00:05:29
                               ETA: 00:29:16

################################################################################
                     [1m Learning iteration 316/2000 [0m                      

                       Computation: 107585 steps/s (collection: 0.828s, learning 0.086s)
             Mean action noise std: 1.62
          Mean value_function loss: 318.2280
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.8533
                       Mean reward: 623.79
               Mean episode length: 231.57
    Episode_Reward/reaching_object: 0.5638
     Episode_Reward/lifting_object: 120.3887
      Episode_Reward/object_height: 0.0458
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 0.91s
                      Time elapsed: 00:05:30
                               ETA: 00:29:14

################################################################################
                     [1m Learning iteration 317/2000 [0m                      

                       Computation: 102196 steps/s (collection: 0.862s, learning 0.100s)
             Mean action noise std: 1.62
          Mean value_function loss: 313.4398
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 14.8565
                       Mean reward: 652.14
               Mean episode length: 236.68
    Episode_Reward/reaching_object: 0.5646
     Episode_Reward/lifting_object: 119.2911
      Episode_Reward/object_height: 0.0452
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 0.96s
                      Time elapsed: 00:05:31
                               ETA: 00:29:12

################################################################################
                     [1m Learning iteration 318/2000 [0m                      

                       Computation: 103434 steps/s (collection: 0.852s, learning 0.098s)
             Mean action noise std: 1.62
          Mean value_function loss: 301.7329
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.8592
                       Mean reward: 590.38
               Mean episode length: 229.00
    Episode_Reward/reaching_object: 0.5726
     Episode_Reward/lifting_object: 120.2437
      Episode_Reward/object_height: 0.0456
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 0.95s
                      Time elapsed: 00:05:32
                               ETA: 00:29:11

################################################################################
                     [1m Learning iteration 319/2000 [0m                      

                       Computation: 103005 steps/s (collection: 0.854s, learning 0.101s)
             Mean action noise std: 1.62
          Mean value_function loss: 274.2750
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 14.8623
                       Mean reward: 564.92
               Mean episode length: 232.61
    Episode_Reward/reaching_object: 0.5980
     Episode_Reward/lifting_object: 127.8469
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 0.95s
                      Time elapsed: 00:05:33
                               ETA: 00:29:09

################################################################################
                     [1m Learning iteration 320/2000 [0m                      

                       Computation: 96259 steps/s (collection: 0.936s, learning 0.086s)
             Mean action noise std: 1.62
          Mean value_function loss: 270.8874
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 14.8634
                       Mean reward: 647.46
               Mean episode length: 239.98
    Episode_Reward/reaching_object: 0.6014
     Episode_Reward/lifting_object: 129.2141
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 1.02s
                      Time elapsed: 00:05:34
                               ETA: 00:29:08

################################################################################
                     [1m Learning iteration 321/2000 [0m                      

                       Computation: 106068 steps/s (collection: 0.809s, learning 0.118s)
             Mean action noise std: 1.62
          Mean value_function loss: 299.6118
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.8626
                       Mean reward: 469.72
               Mean episode length: 231.05
    Episode_Reward/reaching_object: 0.5289
     Episode_Reward/lifting_object: 109.8734
      Episode_Reward/object_height: 0.0419
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 0.93s
                      Time elapsed: 00:05:35
                               ETA: 00:29:07

################################################################################
                     [1m Learning iteration 322/2000 [0m                      

                       Computation: 106750 steps/s (collection: 0.832s, learning 0.089s)
             Mean action noise std: 1.62
          Mean value_function loss: 283.4367
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 14.8571
                       Mean reward: 552.57
               Mean episode length: 229.84
    Episode_Reward/reaching_object: 0.5141
     Episode_Reward/lifting_object: 106.0635
      Episode_Reward/object_height: 0.0401
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 0.92s
                      Time elapsed: 00:05:35
                               ETA: 00:29:05

################################################################################
                     [1m Learning iteration 323/2000 [0m                      

                       Computation: 112127 steps/s (collection: 0.782s, learning 0.095s)
             Mean action noise std: 1.62
          Mean value_function loss: 263.2349
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 14.8540
                       Mean reward: 599.01
               Mean episode length: 238.79
    Episode_Reward/reaching_object: 0.5452
     Episode_Reward/lifting_object: 113.4771
      Episode_Reward/object_height: 0.0430
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 0.88s
                      Time elapsed: 00:05:36
                               ETA: 00:29:03

################################################################################
                     [1m Learning iteration 324/2000 [0m                      

                       Computation: 110558 steps/s (collection: 0.798s, learning 0.092s)
             Mean action noise std: 1.62
          Mean value_function loss: 259.4253
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 14.8559
                       Mean reward: 547.67
               Mean episode length: 236.51
    Episode_Reward/reaching_object: 0.5566
     Episode_Reward/lifting_object: 116.1909
      Episode_Reward/object_height: 0.0441
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 0.89s
                      Time elapsed: 00:05:37
                               ETA: 00:29:01

################################################################################
                     [1m Learning iteration 325/2000 [0m                      

                       Computation: 109547 steps/s (collection: 0.798s, learning 0.099s)
             Mean action noise std: 1.62
          Mean value_function loss: 294.9310
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 14.8586
                       Mean reward: 600.85
               Mean episode length: 242.21
    Episode_Reward/reaching_object: 0.5580
     Episode_Reward/lifting_object: 116.2946
      Episode_Reward/object_height: 0.0441
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 0.90s
                      Time elapsed: 00:05:38
                               ETA: 00:28:59

################################################################################
                     [1m Learning iteration 326/2000 [0m                      

                       Computation: 105525 steps/s (collection: 0.794s, learning 0.137s)
             Mean action noise std: 1.62
          Mean value_function loss: 276.2627
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 14.8640
                       Mean reward: 672.63
               Mean episode length: 240.25
    Episode_Reward/reaching_object: 0.5520
     Episode_Reward/lifting_object: 115.9391
      Episode_Reward/object_height: 0.0439
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 0.93s
                      Time elapsed: 00:05:39
                               ETA: 00:28:58

################################################################################
                     [1m Learning iteration 327/2000 [0m                      

                       Computation: 98941 steps/s (collection: 0.863s, learning 0.131s)
             Mean action noise std: 1.62
          Mean value_function loss: 311.1759
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 14.8667
                       Mean reward: 601.36
               Mean episode length: 235.85
    Episode_Reward/reaching_object: 0.5875
     Episode_Reward/lifting_object: 125.8613
      Episode_Reward/object_height: 0.0475
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 0.99s
                      Time elapsed: 00:05:40
                               ETA: 00:28:57

################################################################################
                     [1m Learning iteration 328/2000 [0m                      

                       Computation: 104861 steps/s (collection: 0.794s, learning 0.144s)
             Mean action noise std: 1.62
          Mean value_function loss: 332.1894
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 14.8653
                       Mean reward: 670.92
               Mean episode length: 232.44
    Episode_Reward/reaching_object: 0.5929
     Episode_Reward/lifting_object: 127.4769
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 0.94s
                      Time elapsed: 00:05:41
                               ETA: 00:28:55

################################################################################
                     [1m Learning iteration 329/2000 [0m                      

                       Computation: 106390 steps/s (collection: 0.825s, learning 0.099s)
             Mean action noise std: 1.62
          Mean value_function loss: 313.9572
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 14.8652
                       Mean reward: 558.53
               Mean episode length: 236.91
    Episode_Reward/reaching_object: 0.5629
     Episode_Reward/lifting_object: 118.5540
      Episode_Reward/object_height: 0.0446
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 0.92s
                      Time elapsed: 00:05:42
                               ETA: 00:28:53

################################################################################
                     [1m Learning iteration 330/2000 [0m                      

                       Computation: 115362 steps/s (collection: 0.767s, learning 0.086s)
             Mean action noise std: 1.62
          Mean value_function loss: 297.2008
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 14.8644
                       Mean reward: 599.53
               Mean episode length: 229.55
    Episode_Reward/reaching_object: 0.5812
     Episode_Reward/lifting_object: 123.8499
      Episode_Reward/object_height: 0.0466
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 0.85s
                      Time elapsed: 00:05:43
                               ETA: 00:28:51

################################################################################
                     [1m Learning iteration 331/2000 [0m                      

                       Computation: 109635 steps/s (collection: 0.801s, learning 0.096s)
             Mean action noise std: 1.62
          Mean value_function loss: 290.3467
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.8632
                       Mean reward: 706.38
               Mean episode length: 243.03
    Episode_Reward/reaching_object: 0.6008
     Episode_Reward/lifting_object: 129.2987
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 0.90s
                      Time elapsed: 00:05:44
                               ETA: 00:28:50

################################################################################
                     [1m Learning iteration 332/2000 [0m                      

                       Computation: 112184 steps/s (collection: 0.791s, learning 0.085s)
             Mean action noise std: 1.62
          Mean value_function loss: 316.8005
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 14.8628
                       Mean reward: 598.63
               Mean episode length: 235.11
    Episode_Reward/reaching_object: 0.5972
     Episode_Reward/lifting_object: 126.3784
      Episode_Reward/object_height: 0.0469
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 0.88s
                      Time elapsed: 00:05:45
                               ETA: 00:28:48

################################################################################
                     [1m Learning iteration 333/2000 [0m                      

                       Computation: 44307 steps/s (collection: 2.111s, learning 0.108s)
             Mean action noise std: 1.62
          Mean value_function loss: 270.8520
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.8630
                       Mean reward: 677.33
               Mean episode length: 229.33
    Episode_Reward/reaching_object: 0.6042
     Episode_Reward/lifting_object: 130.2118
      Episode_Reward/object_height: 0.0479
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 2.22s
                      Time elapsed: 00:05:47
                               ETA: 00:28:53

################################################################################
                     [1m Learning iteration 334/2000 [0m                      

                       Computation: 31738 steps/s (collection: 2.969s, learning 0.128s)
             Mean action noise std: 1.62
          Mean value_function loss: 247.9678
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 14.8590
                       Mean reward: 696.86
               Mean episode length: 240.58
    Episode_Reward/reaching_object: 0.6508
     Episode_Reward/lifting_object: 139.6262
      Episode_Reward/object_height: 0.0514
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 3.10s
                      Time elapsed: 00:05:50
                               ETA: 00:29:02

################################################################################
                     [1m Learning iteration 335/2000 [0m                      

                       Computation: 29434 steps/s (collection: 3.208s, learning 0.132s)
             Mean action noise std: 1.62
          Mean value_function loss: 268.0815
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 14.8553
                       Mean reward: 649.36
               Mean episode length: 239.91
    Episode_Reward/reaching_object: 0.6099
     Episode_Reward/lifting_object: 130.7848
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 3.34s
                      Time elapsed: 00:05:53
                               ETA: 00:29:12

################################################################################
                     [1m Learning iteration 336/2000 [0m                      

                       Computation: 31015 steps/s (collection: 3.030s, learning 0.140s)
             Mean action noise std: 1.62
          Mean value_function loss: 236.6980
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 14.8515
                       Mean reward: 728.42
               Mean episode length: 239.83
    Episode_Reward/reaching_object: 0.6414
     Episode_Reward/lifting_object: 138.7219
      Episode_Reward/object_height: 0.0510
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 3.17s
                      Time elapsed: 00:05:56
                               ETA: 00:29:22

################################################################################
                     [1m Learning iteration 337/2000 [0m                      

                       Computation: 31472 steps/s (collection: 3.008s, learning 0.115s)
             Mean action noise std: 1.62
          Mean value_function loss: 259.0568
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 14.8521
                       Mean reward: 690.65
               Mean episode length: 242.53
    Episode_Reward/reaching_object: 0.6304
     Episode_Reward/lifting_object: 135.0302
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 3.12s
                      Time elapsed: 00:05:59
                               ETA: 00:29:31

################################################################################
                     [1m Learning iteration 338/2000 [0m                      

                       Computation: 28185 steps/s (collection: 3.317s, learning 0.171s)
             Mean action noise std: 1.62
          Mean value_function loss: 233.5757
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 14.8540
                       Mean reward: 720.81
               Mean episode length: 246.68
    Episode_Reward/reaching_object: 0.6211
     Episode_Reward/lifting_object: 134.0336
      Episode_Reward/object_height: 0.0495
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 3.49s
                      Time elapsed: 00:06:03
                               ETA: 00:29:42

################################################################################
                     [1m Learning iteration 339/2000 [0m                      

                       Computation: 29735 steps/s (collection: 3.176s, learning 0.130s)
             Mean action noise std: 1.63
          Mean value_function loss: 232.6734
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 14.8571
                       Mean reward: 678.48
               Mean episode length: 238.29
    Episode_Reward/reaching_object: 0.6365
     Episode_Reward/lifting_object: 136.8957
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 3.31s
                      Time elapsed: 00:06:06
                               ETA: 00:29:51

################################################################################
                     [1m Learning iteration 340/2000 [0m                      

                       Computation: 29660 steps/s (collection: 3.172s, learning 0.142s)
             Mean action noise std: 1.63
          Mean value_function loss: 248.2978
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 14.8594
                       Mean reward: 705.66
               Mean episode length: 233.89
    Episode_Reward/reaching_object: 0.6365
     Episode_Reward/lifting_object: 137.4906
      Episode_Reward/object_height: 0.0512
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 3.31s
                      Time elapsed: 00:06:10
                               ETA: 00:30:01

################################################################################
                     [1m Learning iteration 341/2000 [0m                      

                       Computation: 20617 steps/s (collection: 4.621s, learning 0.147s)
             Mean action noise std: 1.63
          Mean value_function loss: 238.1594
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.8618
                       Mean reward: 737.92
               Mean episode length: 241.15
    Episode_Reward/reaching_object: 0.6505
     Episode_Reward/lifting_object: 142.9818
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 4.77s
                      Time elapsed: 00:06:14
                               ETA: 00:30:18

################################################################################
                     [1m Learning iteration 342/2000 [0m                      

                       Computation: 100671 steps/s (collection: 0.866s, learning 0.110s)
             Mean action noise std: 1.63
          Mean value_function loss: 227.7401
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 14.8662
                       Mean reward: 716.31
               Mean episode length: 238.77
    Episode_Reward/reaching_object: 0.6214
     Episode_Reward/lifting_object: 134.3708
      Episode_Reward/object_height: 0.0502
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 0.98s
                      Time elapsed: 00:06:15
                               ETA: 00:30:16

################################################################################
                     [1m Learning iteration 343/2000 [0m                      

                       Computation: 107923 steps/s (collection: 0.779s, learning 0.132s)
             Mean action noise std: 1.63
          Mean value_function loss: 247.0242
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 14.8671
                       Mean reward: 681.52
               Mean episode length: 245.10
    Episode_Reward/reaching_object: 0.6403
     Episode_Reward/lifting_object: 138.1969
      Episode_Reward/object_height: 0.0520
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 0.91s
                      Time elapsed: 00:06:16
                               ETA: 00:30:14

################################################################################
                     [1m Learning iteration 344/2000 [0m                      

                       Computation: 107359 steps/s (collection: 0.812s, learning 0.104s)
             Mean action noise std: 1.63
          Mean value_function loss: 229.0518
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.8687
                       Mean reward: 669.57
               Mean episode length: 230.17
    Episode_Reward/reaching_object: 0.6304
     Episode_Reward/lifting_object: 136.9163
      Episode_Reward/object_height: 0.0514
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 0.92s
                      Time elapsed: 00:06:17
                               ETA: 00:30:12

################################################################################
                     [1m Learning iteration 345/2000 [0m                      

                       Computation: 105578 steps/s (collection: 0.835s, learning 0.096s)
             Mean action noise std: 1.63
          Mean value_function loss: 202.3331
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 14.8705
                       Mean reward: 708.59
               Mean episode length: 236.67
    Episode_Reward/reaching_object: 0.6458
     Episode_Reward/lifting_object: 141.1838
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 0.93s
                      Time elapsed: 00:06:18
                               ETA: 00:30:10

################################################################################
                     [1m Learning iteration 346/2000 [0m                      

                       Computation: 103503 steps/s (collection: 0.803s, learning 0.147s)
             Mean action noise std: 1.63
          Mean value_function loss: 207.0892
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 14.8718
                       Mean reward: 748.95
               Mean episode length: 242.42
    Episode_Reward/reaching_object: 0.6503
     Episode_Reward/lifting_object: 142.1461
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 0.95s
                      Time elapsed: 00:06:19
                               ETA: 00:30:09

################################################################################
                     [1m Learning iteration 347/2000 [0m                      

                       Computation: 106469 steps/s (collection: 0.817s, learning 0.107s)
             Mean action noise std: 1.63
          Mean value_function loss: 181.1197
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 14.8715
                       Mean reward: 669.74
               Mean episode length: 237.00
    Episode_Reward/reaching_object: 0.6513
     Episode_Reward/lifting_object: 142.0258
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 0.92s
                      Time elapsed: 00:06:20
                               ETA: 00:30:07

################################################################################
                     [1m Learning iteration 348/2000 [0m                      

                       Computation: 104202 steps/s (collection: 0.843s, learning 0.100s)
             Mean action noise std: 1.63
          Mean value_function loss: 202.4529
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 14.8711
                       Mean reward: 728.27
               Mean episode length: 240.64
    Episode_Reward/reaching_object: 0.6618
     Episode_Reward/lifting_object: 144.2518
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 0.94s
                      Time elapsed: 00:06:21
                               ETA: 00:30:05

################################################################################
                     [1m Learning iteration 349/2000 [0m                      

                       Computation: 106412 steps/s (collection: 0.829s, learning 0.095s)
             Mean action noise std: 1.63
          Mean value_function loss: 209.8962
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 14.8712
                       Mean reward: 648.66
               Mean episode length: 239.31
    Episode_Reward/reaching_object: 0.6220
     Episode_Reward/lifting_object: 132.8199
      Episode_Reward/object_height: 0.0496
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 0.92s
                      Time elapsed: 00:06:22
                               ETA: 00:30:03

################################################################################
                     [1m Learning iteration 350/2000 [0m                      

                       Computation: 103670 steps/s (collection: 0.850s, learning 0.099s)
             Mean action noise std: 1.63
          Mean value_function loss: 205.2799
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 14.8714
                       Mean reward: 705.26
               Mean episode length: 239.52
    Episode_Reward/reaching_object: 0.6251
     Episode_Reward/lifting_object: 134.8726
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 0.95s
                      Time elapsed: 00:06:23
                               ETA: 00:30:01

################################################################################
                     [1m Learning iteration 351/2000 [0m                      

                       Computation: 95834 steps/s (collection: 0.917s, learning 0.109s)
             Mean action noise std: 1.63
          Mean value_function loss: 211.5355
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.8703
                       Mean reward: 679.29
               Mean episode length: 235.28
    Episode_Reward/reaching_object: 0.6239
     Episode_Reward/lifting_object: 132.6830
      Episode_Reward/object_height: 0.0499
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 1.03s
                      Time elapsed: 00:06:24
                               ETA: 00:30:00

################################################################################
                     [1m Learning iteration 352/2000 [0m                      

                       Computation: 99059 steps/s (collection: 0.877s, learning 0.115s)
             Mean action noise std: 1.63
          Mean value_function loss: 213.6254
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 14.8672
                       Mean reward: 672.09
               Mean episode length: 240.73
    Episode_Reward/reaching_object: 0.6570
     Episode_Reward/lifting_object: 143.7829
      Episode_Reward/object_height: 0.0543
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 0.99s
                      Time elapsed: 00:06:25
                               ETA: 00:29:58

################################################################################
                     [1m Learning iteration 353/2000 [0m                      

                       Computation: 101200 steps/s (collection: 0.873s, learning 0.098s)
             Mean action noise std: 1.63
          Mean value_function loss: 203.9840
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.8655
                       Mean reward: 730.40
               Mean episode length: 242.24
    Episode_Reward/reaching_object: 0.6611
     Episode_Reward/lifting_object: 143.0005
      Episode_Reward/object_height: 0.0538
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 0.97s
                      Time elapsed: 00:06:26
                               ETA: 00:29:57

################################################################################
                     [1m Learning iteration 354/2000 [0m                      

                       Computation: 108459 steps/s (collection: 0.795s, learning 0.111s)
             Mean action noise std: 1.63
          Mean value_function loss: 215.2136
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 14.8673
                       Mean reward: 742.52
               Mean episode length: 242.14
    Episode_Reward/reaching_object: 0.6663
     Episode_Reward/lifting_object: 143.7040
      Episode_Reward/object_height: 0.0541
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 0.91s
                      Time elapsed: 00:06:27
                               ETA: 00:29:55

################################################################################
                     [1m Learning iteration 355/2000 [0m                      

                       Computation: 111271 steps/s (collection: 0.785s, learning 0.099s)
             Mean action noise std: 1.63
          Mean value_function loss: 207.8432
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 14.8718
                       Mean reward: 749.25
               Mean episode length: 241.04
    Episode_Reward/reaching_object: 0.6655
     Episode_Reward/lifting_object: 145.1218
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 0.88s
                      Time elapsed: 00:06:28
                               ETA: 00:29:53

################################################################################
                     [1m Learning iteration 356/2000 [0m                      

                       Computation: 107555 steps/s (collection: 0.796s, learning 0.118s)
             Mean action noise std: 1.63
          Mean value_function loss: 209.5260
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 14.8737
                       Mean reward: 740.22
               Mean episode length: 241.34
    Episode_Reward/reaching_object: 0.6608
     Episode_Reward/lifting_object: 144.8409
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 0.91s
                      Time elapsed: 00:06:28
                               ETA: 00:29:51

################################################################################
                     [1m Learning iteration 357/2000 [0m                      

                       Computation: 108704 steps/s (collection: 0.798s, learning 0.107s)
             Mean action noise std: 1.63
          Mean value_function loss: 209.2562
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.8741
                       Mean reward: 700.22
               Mean episode length: 241.76
    Episode_Reward/reaching_object: 0.6629
     Episode_Reward/lifting_object: 142.7640
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 0.90s
                      Time elapsed: 00:06:29
                               ETA: 00:29:49

################################################################################
                     [1m Learning iteration 358/2000 [0m                      

                       Computation: 110359 steps/s (collection: 0.784s, learning 0.107s)
             Mean action noise std: 1.63
          Mean value_function loss: 226.9167
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 14.8747
                       Mean reward: 746.76
               Mean episode length: 239.00
    Episode_Reward/reaching_object: 0.6675
     Episode_Reward/lifting_object: 146.4874
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 0.89s
                      Time elapsed: 00:06:30
                               ETA: 00:29:47

################################################################################
                     [1m Learning iteration 359/2000 [0m                      

                       Computation: 107782 steps/s (collection: 0.822s, learning 0.091s)
             Mean action noise std: 1.63
          Mean value_function loss: 229.7288
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.8753
                       Mean reward: 726.15
               Mean episode length: 240.79
    Episode_Reward/reaching_object: 0.6480
     Episode_Reward/lifting_object: 140.0155
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 0.91s
                      Time elapsed: 00:06:31
                               ETA: 00:29:45

################################################################################
                     [1m Learning iteration 360/2000 [0m                      

                       Computation: 106809 steps/s (collection: 0.790s, learning 0.130s)
             Mean action noise std: 1.63
          Mean value_function loss: 213.3073
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 14.8741
                       Mean reward: 768.54
               Mean episode length: 245.28
    Episode_Reward/reaching_object: 0.6653
     Episode_Reward/lifting_object: 146.1150
      Episode_Reward/object_height: 0.0544
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 0.92s
                      Time elapsed: 00:06:32
                               ETA: 00:29:43

################################################################################
                     [1m Learning iteration 361/2000 [0m                      

                       Computation: 101401 steps/s (collection: 0.852s, learning 0.118s)
             Mean action noise std: 1.63
          Mean value_function loss: 239.8585
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 14.8713
                       Mean reward: 755.83
               Mean episode length: 240.63
    Episode_Reward/reaching_object: 0.6606
     Episode_Reward/lifting_object: 145.5671
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 0.97s
                      Time elapsed: 00:06:33
                               ETA: 00:29:42

################################################################################
                     [1m Learning iteration 362/2000 [0m                      

                       Computation: 107276 steps/s (collection: 0.781s, learning 0.135s)
             Mean action noise std: 1.63
          Mean value_function loss: 206.5951
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 14.8698
                       Mean reward: 726.41
               Mean episode length: 236.46
    Episode_Reward/reaching_object: 0.6457
     Episode_Reward/lifting_object: 142.6122
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 0.92s
                      Time elapsed: 00:06:34
                               ETA: 00:29:40

################################################################################
                     [1m Learning iteration 363/2000 [0m                      

                       Computation: 109187 steps/s (collection: 0.804s, learning 0.097s)
             Mean action noise std: 1.63
          Mean value_function loss: 196.1442
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.8701
                       Mean reward: 759.12
               Mean episode length: 237.33
    Episode_Reward/reaching_object: 0.6527
     Episode_Reward/lifting_object: 142.9443
      Episode_Reward/object_height: 0.0529
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 0.90s
                      Time elapsed: 00:06:35
                               ETA: 00:29:38

################################################################################
                     [1m Learning iteration 364/2000 [0m                      

                       Computation: 110807 steps/s (collection: 0.787s, learning 0.100s)
             Mean action noise std: 1.63
          Mean value_function loss: 196.0262
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 14.8716
                       Mean reward: 723.50
               Mean episode length: 235.56
    Episode_Reward/reaching_object: 0.6621
     Episode_Reward/lifting_object: 144.8838
      Episode_Reward/object_height: 0.0536
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 0.89s
                      Time elapsed: 00:06:36
                               ETA: 00:29:36

################################################################################
                     [1m Learning iteration 365/2000 [0m                      

                       Computation: 107286 steps/s (collection: 0.822s, learning 0.094s)
             Mean action noise std: 1.63
          Mean value_function loss: 187.6768
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 14.8720
                       Mean reward: 697.76
               Mean episode length: 239.18
    Episode_Reward/reaching_object: 0.6475
     Episode_Reward/lifting_object: 140.8867
      Episode_Reward/object_height: 0.0518
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 0.92s
                      Time elapsed: 00:06:37
                               ETA: 00:29:34

################################################################################
                     [1m Learning iteration 366/2000 [0m                      

                       Computation: 113655 steps/s (collection: 0.761s, learning 0.104s)
             Mean action noise std: 1.63
          Mean value_function loss: 192.6258
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 14.8744
                       Mean reward: 735.42
               Mean episode length: 238.29
    Episode_Reward/reaching_object: 0.6674
     Episode_Reward/lifting_object: 146.4035
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 0.86s
                      Time elapsed: 00:06:38
                               ETA: 00:29:32

################################################################################
                     [1m Learning iteration 367/2000 [0m                      

                       Computation: 110779 steps/s (collection: 0.799s, learning 0.088s)
             Mean action noise std: 1.63
          Mean value_function loss: 186.8728
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.8745
                       Mean reward: 770.95
               Mean episode length: 240.94
    Episode_Reward/reaching_object: 0.6723
     Episode_Reward/lifting_object: 147.4781
      Episode_Reward/object_height: 0.0544
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 0.89s
                      Time elapsed: 00:06:38
                               ETA: 00:29:30

################################################################################
                     [1m Learning iteration 368/2000 [0m                      

                       Computation: 108035 steps/s (collection: 0.819s, learning 0.091s)
             Mean action noise std: 1.63
          Mean value_function loss: 184.3007
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 14.8761
                       Mean reward: 775.40
               Mean episode length: 243.51
    Episode_Reward/reaching_object: 0.6775
     Episode_Reward/lifting_object: 148.9319
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 0.91s
                      Time elapsed: 00:06:39
                               ETA: 00:29:28

################################################################################
                     [1m Learning iteration 369/2000 [0m                      

                       Computation: 110873 steps/s (collection: 0.779s, learning 0.108s)
             Mean action noise std: 1.63
          Mean value_function loss: 183.2286
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.8783
                       Mean reward: 755.70
               Mean episode length: 240.86
    Episode_Reward/reaching_object: 0.6746
     Episode_Reward/lifting_object: 150.4477
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 0.89s
                      Time elapsed: 00:06:40
                               ETA: 00:29:26

################################################################################
                     [1m Learning iteration 370/2000 [0m                      

                       Computation: 111856 steps/s (collection: 0.770s, learning 0.109s)
             Mean action noise std: 1.63
          Mean value_function loss: 186.2956
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 14.8803
                       Mean reward: 810.29
               Mean episode length: 243.72
    Episode_Reward/reaching_object: 0.6875
     Episode_Reward/lifting_object: 152.3318
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 0.88s
                      Time elapsed: 00:06:41
                               ETA: 00:29:24

################################################################################
                     [1m Learning iteration 371/2000 [0m                      

                       Computation: 113470 steps/s (collection: 0.759s, learning 0.108s)
             Mean action noise std: 1.63
          Mean value_function loss: 211.1385
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.8812
                       Mean reward: 751.10
               Mean episode length: 234.71
    Episode_Reward/reaching_object: 0.6759
     Episode_Reward/lifting_object: 150.6671
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 0.87s
                      Time elapsed: 00:06:42
                               ETA: 00:29:22

################################################################################
                     [1m Learning iteration 372/2000 [0m                      

                       Computation: 117346 steps/s (collection: 0.752s, learning 0.086s)
             Mean action noise std: 1.63
          Mean value_function loss: 172.8487
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 14.8820
                       Mean reward: 744.68
               Mean episode length: 242.33
    Episode_Reward/reaching_object: 0.6666
     Episode_Reward/lifting_object: 148.0053
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 0.84s
                      Time elapsed: 00:06:43
                               ETA: 00:29:20

################################################################################
                     [1m Learning iteration 373/2000 [0m                      

                       Computation: 113467 steps/s (collection: 0.762s, learning 0.104s)
             Mean action noise std: 1.63
          Mean value_function loss: 164.8233
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 14.8847
                       Mean reward: 803.10
               Mean episode length: 245.86
    Episode_Reward/reaching_object: 0.6952
     Episode_Reward/lifting_object: 156.1454
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 0.87s
                      Time elapsed: 00:06:44
                               ETA: 00:29:18

################################################################################
                     [1m Learning iteration 374/2000 [0m                      

                       Computation: 108214 steps/s (collection: 0.791s, learning 0.117s)
             Mean action noise std: 1.63
          Mean value_function loss: 165.8421
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 14.8858
                       Mean reward: 805.05
               Mean episode length: 245.76
    Episode_Reward/reaching_object: 0.6884
     Episode_Reward/lifting_object: 154.0738
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 0.91s
                      Time elapsed: 00:06:45
                               ETA: 00:29:16

################################################################################
                     [1m Learning iteration 375/2000 [0m                      

                       Computation: 112000 steps/s (collection: 0.779s, learning 0.099s)
             Mean action noise std: 1.64
          Mean value_function loss: 191.6885
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 14.8864
                       Mean reward: 768.54
               Mean episode length: 242.76
    Episode_Reward/reaching_object: 0.7027
     Episode_Reward/lifting_object: 155.9159
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 0.88s
                      Time elapsed: 00:06:45
                               ETA: 00:29:14

################################################################################
                     [1m Learning iteration 376/2000 [0m                      

                       Computation: 106544 steps/s (collection: 0.832s, learning 0.091s)
             Mean action noise std: 1.64
          Mean value_function loss: 168.4965
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 14.8880
                       Mean reward: 768.47
               Mean episode length: 241.12
    Episode_Reward/reaching_object: 0.6826
     Episode_Reward/lifting_object: 152.2408
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 0.92s
                      Time elapsed: 00:06:46
                               ETA: 00:29:12

################################################################################
                     [1m Learning iteration 377/2000 [0m                      

                       Computation: 109294 steps/s (collection: 0.813s, learning 0.086s)
             Mean action noise std: 1.64
          Mean value_function loss: 186.4968
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.8875
                       Mean reward: 760.44
               Mean episode length: 238.42
    Episode_Reward/reaching_object: 0.6905
     Episode_Reward/lifting_object: 154.1490
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 0.90s
                      Time elapsed: 00:06:47
                               ETA: 00:29:11

################################################################################
                     [1m Learning iteration 378/2000 [0m                      

                       Computation: 114990 steps/s (collection: 0.767s, learning 0.088s)
             Mean action noise std: 1.64
          Mean value_function loss: 171.0788
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.8879
                       Mean reward: 767.53
               Mean episode length: 239.39
    Episode_Reward/reaching_object: 0.6912
     Episode_Reward/lifting_object: 154.1914
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 0.85s
                      Time elapsed: 00:06:48
                               ETA: 00:29:08

################################################################################
                     [1m Learning iteration 379/2000 [0m                      

                       Computation: 103560 steps/s (collection: 0.817s, learning 0.133s)
             Mean action noise std: 1.64
          Mean value_function loss: 197.2248
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 14.8873
                       Mean reward: 727.45
               Mean episode length: 232.88
    Episode_Reward/reaching_object: 0.6799
     Episode_Reward/lifting_object: 151.7962
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 0.95s
                      Time elapsed: 00:06:49
                               ETA: 00:29:07

################################################################################
                     [1m Learning iteration 380/2000 [0m                      

                       Computation: 108025 steps/s (collection: 0.773s, learning 0.137s)
             Mean action noise std: 1.64
          Mean value_function loss: 175.3066
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 14.8874
                       Mean reward: 739.76
               Mean episode length: 233.51
    Episode_Reward/reaching_object: 0.6933
     Episode_Reward/lifting_object: 153.3735
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 0.91s
                      Time elapsed: 00:06:50
                               ETA: 00:29:05

################################################################################
                     [1m Learning iteration 381/2000 [0m                      

                       Computation: 102409 steps/s (collection: 0.814s, learning 0.146s)
             Mean action noise std: 1.64
          Mean value_function loss: 157.7746
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 14.8894
                       Mean reward: 799.34
               Mean episode length: 242.31
    Episode_Reward/reaching_object: 0.6952
     Episode_Reward/lifting_object: 154.8459
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 0.96s
                      Time elapsed: 00:06:51
                               ETA: 00:29:03

################################################################################
                     [1m Learning iteration 382/2000 [0m                      

                       Computation: 111921 steps/s (collection: 0.781s, learning 0.098s)
             Mean action noise std: 1.64
          Mean value_function loss: 152.7262
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 14.8907
                       Mean reward: 771.33
               Mean episode length: 241.39
    Episode_Reward/reaching_object: 0.6989
     Episode_Reward/lifting_object: 156.4256
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 0.88s
                      Time elapsed: 00:06:52
                               ETA: 00:29:02

################################################################################
                     [1m Learning iteration 383/2000 [0m                      

                       Computation: 109292 steps/s (collection: 0.811s, learning 0.089s)
             Mean action noise std: 1.64
          Mean value_function loss: 169.9001
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 14.8913
                       Mean reward: 762.19
               Mean episode length: 237.95
    Episode_Reward/reaching_object: 0.6909
     Episode_Reward/lifting_object: 154.9027
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 0.90s
                      Time elapsed: 00:06:53
                               ETA: 00:29:00

################################################################################
                     [1m Learning iteration 384/2000 [0m                      

                       Computation: 109126 steps/s (collection: 0.805s, learning 0.096s)
             Mean action noise std: 1.64
          Mean value_function loss: 199.7388
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 14.8920
                       Mean reward: 798.03
               Mean episode length: 239.49
    Episode_Reward/reaching_object: 0.6989
     Episode_Reward/lifting_object: 155.9354
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 0.90s
                      Time elapsed: 00:06:54
                               ETA: 00:28:58

################################################################################
                     [1m Learning iteration 385/2000 [0m                      

                       Computation: 109827 steps/s (collection: 0.801s, learning 0.094s)
             Mean action noise std: 1.64
          Mean value_function loss: 162.3834
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 14.8927
                       Mean reward: 787.91
               Mean episode length: 242.40
    Episode_Reward/reaching_object: 0.6599
     Episode_Reward/lifting_object: 146.0997
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 0.90s
                      Time elapsed: 00:06:55
                               ETA: 00:28:56

################################################################################
                     [1m Learning iteration 386/2000 [0m                      

                       Computation: 114246 steps/s (collection: 0.772s, learning 0.089s)
             Mean action noise std: 1.64
          Mean value_function loss: 175.7355
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.8926
                       Mean reward: 759.02
               Mean episode length: 236.05
    Episode_Reward/reaching_object: 0.6895
     Episode_Reward/lifting_object: 153.9019
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 0.86s
                      Time elapsed: 00:06:55
                               ETA: 00:28:54

################################################################################
                     [1m Learning iteration 387/2000 [0m                      

                       Computation: 106472 steps/s (collection: 0.814s, learning 0.109s)
             Mean action noise std: 1.64
          Mean value_function loss: 159.5184
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.8910
                       Mean reward: 767.05
               Mean episode length: 236.30
    Episode_Reward/reaching_object: 0.6982
     Episode_Reward/lifting_object: 155.0247
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 0.92s
                      Time elapsed: 00:06:56
                               ETA: 00:28:52

################################################################################
                     [1m Learning iteration 388/2000 [0m                      

                       Computation: 113082 steps/s (collection: 0.778s, learning 0.091s)
             Mean action noise std: 1.64
          Mean value_function loss: 149.7608
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 14.8923
                       Mean reward: 797.94
               Mean episode length: 243.03
    Episode_Reward/reaching_object: 0.6783
     Episode_Reward/lifting_object: 151.7495
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 0.87s
                      Time elapsed: 00:06:57
                               ETA: 00:28:50

################################################################################
                     [1m Learning iteration 389/2000 [0m                      

                       Computation: 108054 steps/s (collection: 0.781s, learning 0.129s)
             Mean action noise std: 1.64
          Mean value_function loss: 155.2117
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.8935
                       Mean reward: 815.32
               Mean episode length: 243.77
    Episode_Reward/reaching_object: 0.6941
     Episode_Reward/lifting_object: 154.8008
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 0.91s
                      Time elapsed: 00:06:58
                               ETA: 00:28:49

################################################################################
                     [1m Learning iteration 390/2000 [0m                      

                       Computation: 111557 steps/s (collection: 0.769s, learning 0.112s)
             Mean action noise std: 1.64
          Mean value_function loss: 140.6021
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.8964
                       Mean reward: 798.34
               Mean episode length: 245.45
    Episode_Reward/reaching_object: 0.6938
     Episode_Reward/lifting_object: 154.9279
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 0.88s
                      Time elapsed: 00:06:59
                               ETA: 00:28:47

################################################################################
                     [1m Learning iteration 391/2000 [0m                      

                       Computation: 112937 steps/s (collection: 0.784s, learning 0.087s)
             Mean action noise std: 1.64
          Mean value_function loss: 140.2785
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.8982
                       Mean reward: 797.18
               Mean episode length: 239.61
    Episode_Reward/reaching_object: 0.7146
     Episode_Reward/lifting_object: 159.3083
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 0.87s
                      Time elapsed: 00:07:00
                               ETA: 00:28:45

################################################################################
                     [1m Learning iteration 392/2000 [0m                      

                       Computation: 108597 steps/s (collection: 0.814s, learning 0.091s)
             Mean action noise std: 1.64
          Mean value_function loss: 147.9582
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 14.9001
                       Mean reward: 736.40
               Mean episode length: 238.11
    Episode_Reward/reaching_object: 0.6875
     Episode_Reward/lifting_object: 153.1640
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 0.91s
                      Time elapsed: 00:07:01
                               ETA: 00:28:43

################################################################################
                     [1m Learning iteration 393/2000 [0m                      

                       Computation: 109822 steps/s (collection: 0.789s, learning 0.107s)
             Mean action noise std: 1.64
          Mean value_function loss: 159.4419
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 14.9030
                       Mean reward: 760.39
               Mean episode length: 242.52
    Episode_Reward/reaching_object: 0.6841
     Episode_Reward/lifting_object: 151.5763
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 0.90s
                      Time elapsed: 00:07:02
                               ETA: 00:28:41

################################################################################
                     [1m Learning iteration 394/2000 [0m                      

                       Computation: 107050 steps/s (collection: 0.820s, learning 0.099s)
             Mean action noise std: 1.64
          Mean value_function loss: 153.6396
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 14.9107
                       Mean reward: 736.28
               Mean episode length: 241.76
    Episode_Reward/reaching_object: 0.6757
     Episode_Reward/lifting_object: 148.2444
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 0.92s
                      Time elapsed: 00:07:03
                               ETA: 00:28:40

################################################################################
                     [1m Learning iteration 395/2000 [0m                      

                       Computation: 113666 steps/s (collection: 0.773s, learning 0.092s)
             Mean action noise std: 1.64
          Mean value_function loss: 140.1902
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.9150
                       Mean reward: 715.96
               Mean episode length: 243.03
    Episode_Reward/reaching_object: 0.6615
     Episode_Reward/lifting_object: 145.5136
      Episode_Reward/object_height: 0.0544
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 0.86s
                      Time elapsed: 00:07:03
                               ETA: 00:28:38

################################################################################
                     [1m Learning iteration 396/2000 [0m                      

                       Computation: 108783 steps/s (collection: 0.806s, learning 0.098s)
             Mean action noise std: 1.64
          Mean value_function loss: 145.2798
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 14.9158
                       Mean reward: 766.85
               Mean episode length: 238.71
    Episode_Reward/reaching_object: 0.6899
     Episode_Reward/lifting_object: 153.0633
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 0.90s
                      Time elapsed: 00:07:04
                               ETA: 00:28:36

################################################################################
                     [1m Learning iteration 397/2000 [0m                      

                       Computation: 107949 steps/s (collection: 0.815s, learning 0.096s)
             Mean action noise std: 1.64
          Mean value_function loss: 114.6086
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 14.9164
                       Mean reward: 760.02
               Mean episode length: 239.39
    Episode_Reward/reaching_object: 0.6958
     Episode_Reward/lifting_object: 155.3741
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 0.91s
                      Time elapsed: 00:07:05
                               ETA: 00:28:34

################################################################################
                     [1m Learning iteration 398/2000 [0m                      

                       Computation: 104732 steps/s (collection: 0.808s, learning 0.131s)
             Mean action noise std: 1.64
          Mean value_function loss: 121.0692
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 14.9174
                       Mean reward: 800.76
               Mean episode length: 243.84
    Episode_Reward/reaching_object: 0.6981
     Episode_Reward/lifting_object: 155.0594
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 0.94s
                      Time elapsed: 00:07:06
                               ETA: 00:28:33

################################################################################
                     [1m Learning iteration 399/2000 [0m                      

                       Computation: 106400 steps/s (collection: 0.808s, learning 0.116s)
             Mean action noise std: 1.64
          Mean value_function loss: 149.2349
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 14.9206
                       Mean reward: 774.36
               Mean episode length: 237.23
    Episode_Reward/reaching_object: 0.7108
     Episode_Reward/lifting_object: 159.0089
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 0.92s
                      Time elapsed: 00:07:07
                               ETA: 00:28:31

################################################################################
                     [1m Learning iteration 400/2000 [0m                      

                       Computation: 113353 steps/s (collection: 0.774s, learning 0.093s)
             Mean action noise std: 1.65
          Mean value_function loss: 117.3846
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 14.9239
                       Mean reward: 782.62
               Mean episode length: 242.29
    Episode_Reward/reaching_object: 0.7032
     Episode_Reward/lifting_object: 157.2139
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 0.87s
                      Time elapsed: 00:07:08
                               ETA: 00:28:29

################################################################################
                     [1m Learning iteration 401/2000 [0m                      

                       Computation: 107349 steps/s (collection: 0.789s, learning 0.127s)
             Mean action noise std: 1.65
          Mean value_function loss: 110.5193
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 14.9225
                       Mean reward: 798.88
               Mean episode length: 245.68
    Episode_Reward/reaching_object: 0.7097
     Episode_Reward/lifting_object: 158.1766
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 0.92s
                      Time elapsed: 00:07:09
                               ETA: 00:28:28

################################################################################
                     [1m Learning iteration 402/2000 [0m                      

                       Computation: 95863 steps/s (collection: 0.890s, learning 0.136s)
             Mean action noise std: 1.65
          Mean value_function loss: 113.3221
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.9245
                       Mean reward: 784.06
               Mean episode length: 244.81
    Episode_Reward/reaching_object: 0.7130
     Episode_Reward/lifting_object: 159.2655
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 1.03s
                      Time elapsed: 00:07:10
                               ETA: 00:28:26

################################################################################
                     [1m Learning iteration 403/2000 [0m                      

                       Computation: 109337 steps/s (collection: 0.810s, learning 0.089s)
             Mean action noise std: 1.65
          Mean value_function loss: 117.7137
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.9296
                       Mean reward: 796.91
               Mean episode length: 240.15
    Episode_Reward/reaching_object: 0.7292
     Episode_Reward/lifting_object: 162.7929
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 0.90s
                      Time elapsed: 00:07:11
                               ETA: 00:28:25

################################################################################
                     [1m Learning iteration 404/2000 [0m                      

                       Computation: 108523 steps/s (collection: 0.805s, learning 0.101s)
             Mean action noise std: 1.65
          Mean value_function loss: 110.3964
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.9330
                       Mean reward: 805.17
               Mean episode length: 244.64
    Episode_Reward/reaching_object: 0.7271
     Episode_Reward/lifting_object: 161.9040
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 0.91s
                      Time elapsed: 00:07:12
                               ETA: 00:28:23

################################################################################
                     [1m Learning iteration 405/2000 [0m                      

                       Computation: 110659 steps/s (collection: 0.780s, learning 0.108s)
             Mean action noise std: 1.65
          Mean value_function loss: 134.1667
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.9361
                       Mean reward: 829.28
               Mean episode length: 242.76
    Episode_Reward/reaching_object: 0.7119
     Episode_Reward/lifting_object: 160.8000
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 0.89s
                      Time elapsed: 00:07:13
                               ETA: 00:28:21

################################################################################
                     [1m Learning iteration 406/2000 [0m                      

                       Computation: 104372 steps/s (collection: 0.833s, learning 0.109s)
             Mean action noise std: 1.65
          Mean value_function loss: 103.0285
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 14.9396
                       Mean reward: 797.20
               Mean episode length: 242.46
    Episode_Reward/reaching_object: 0.7011
     Episode_Reward/lifting_object: 157.9262
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 0.94s
                      Time elapsed: 00:07:14
                               ETA: 00:28:20

################################################################################
                     [1m Learning iteration 407/2000 [0m                      

                       Computation: 110944 steps/s (collection: 0.795s, learning 0.092s)
             Mean action noise std: 1.65
          Mean value_function loss: 135.3565
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 14.9412
                       Mean reward: 827.34
               Mean episode length: 246.45
    Episode_Reward/reaching_object: 0.7236
     Episode_Reward/lifting_object: 163.3036
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 0.89s
                      Time elapsed: 00:07:14
                               ETA: 00:28:18

################################################################################
                     [1m Learning iteration 408/2000 [0m                      

                       Computation: 101636 steps/s (collection: 0.865s, learning 0.102s)
             Mean action noise std: 1.65
          Mean value_function loss: 123.9944
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 14.9427
                       Mean reward: 784.11
               Mean episode length: 238.75
    Episode_Reward/reaching_object: 0.6961
     Episode_Reward/lifting_object: 155.4572
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 0.97s
                      Time elapsed: 00:07:15
                               ETA: 00:28:16

################################################################################
                     [1m Learning iteration 409/2000 [0m                      

                       Computation: 108759 steps/s (collection: 0.815s, learning 0.089s)
             Mean action noise std: 1.65
          Mean value_function loss: 118.8105
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 14.9444
                       Mean reward: 802.45
               Mean episode length: 241.86
    Episode_Reward/reaching_object: 0.7034
     Episode_Reward/lifting_object: 157.2900
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 0.90s
                      Time elapsed: 00:07:16
                               ETA: 00:28:15

################################################################################
                     [1m Learning iteration 410/2000 [0m                      

                       Computation: 110585 steps/s (collection: 0.774s, learning 0.115s)
             Mean action noise std: 1.65
          Mean value_function loss: 129.1496
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 14.9494
                       Mean reward: 811.13
               Mean episode length: 243.77
    Episode_Reward/reaching_object: 0.7219
     Episode_Reward/lifting_object: 162.5202
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 0.89s
                      Time elapsed: 00:07:17
                               ETA: 00:28:13

################################################################################
                     [1m Learning iteration 411/2000 [0m                      

                       Computation: 107308 steps/s (collection: 0.783s, learning 0.133s)
             Mean action noise std: 1.65
          Mean value_function loss: 102.1544
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 14.9531
                       Mean reward: 814.61
               Mean episode length: 241.89
    Episode_Reward/reaching_object: 0.7155
     Episode_Reward/lifting_object: 162.4684
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 0.92s
                      Time elapsed: 00:07:18
                               ETA: 00:28:11

################################################################################
                     [1m Learning iteration 412/2000 [0m                      

                       Computation: 110522 steps/s (collection: 0.780s, learning 0.110s)
             Mean action noise std: 1.65
          Mean value_function loss: 110.3999
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 14.9551
                       Mean reward: 837.18
               Mean episode length: 247.45
    Episode_Reward/reaching_object: 0.7307
     Episode_Reward/lifting_object: 163.4978
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 0.89s
                      Time elapsed: 00:07:19
                               ETA: 00:28:10

################################################################################
                     [1m Learning iteration 413/2000 [0m                      

                       Computation: 110476 steps/s (collection: 0.769s, learning 0.121s)
             Mean action noise std: 1.65
          Mean value_function loss: 109.7649
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.9547
                       Mean reward: 771.50
               Mean episode length: 238.39
    Episode_Reward/reaching_object: 0.7080
     Episode_Reward/lifting_object: 158.1912
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 0.89s
                      Time elapsed: 00:07:20
                               ETA: 00:28:08

################################################################################
                     [1m Learning iteration 414/2000 [0m                      

                       Computation: 111463 steps/s (collection: 0.790s, learning 0.092s)
             Mean action noise std: 1.65
          Mean value_function loss: 107.9385
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.9564
                       Mean reward: 794.15
               Mean episode length: 240.33
    Episode_Reward/reaching_object: 0.7112
     Episode_Reward/lifting_object: 159.5328
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 0.88s
                      Time elapsed: 00:07:21
                               ETA: 00:28:06

################################################################################
                     [1m Learning iteration 415/2000 [0m                      

                       Computation: 112683 steps/s (collection: 0.780s, learning 0.093s)
             Mean action noise std: 1.65
          Mean value_function loss: 112.8740
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 14.9568
                       Mean reward: 797.58
               Mean episode length: 238.08
    Episode_Reward/reaching_object: 0.7231
     Episode_Reward/lifting_object: 162.1345
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 0.87s
                      Time elapsed: 00:07:22
                               ETA: 00:28:04

################################################################################
                     [1m Learning iteration 416/2000 [0m                      

                       Computation: 109129 steps/s (collection: 0.815s, learning 0.086s)
             Mean action noise std: 1.65
          Mean value_function loss: 114.7737
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.9610
                       Mean reward: 814.14
               Mean episode length: 243.20
    Episode_Reward/reaching_object: 0.7238
     Episode_Reward/lifting_object: 162.8288
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 0.90s
                      Time elapsed: 00:07:23
                               ETA: 00:28:03

################################################################################
                     [1m Learning iteration 417/2000 [0m                      

                       Computation: 110369 steps/s (collection: 0.796s, learning 0.095s)
             Mean action noise std: 1.65
          Mean value_function loss: 120.8923
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 14.9653
                       Mean reward: 814.07
               Mean episode length: 238.44
    Episode_Reward/reaching_object: 0.7280
     Episode_Reward/lifting_object: 163.8030
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 0.89s
                      Time elapsed: 00:07:23
                               ETA: 00:28:01

################################################################################
                     [1m Learning iteration 418/2000 [0m                      

                       Computation: 105766 steps/s (collection: 0.816s, learning 0.114s)
             Mean action noise std: 1.65
          Mean value_function loss: 163.7056
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 14.9681
                       Mean reward: 798.68
               Mean episode length: 241.39
    Episode_Reward/reaching_object: 0.7159
     Episode_Reward/lifting_object: 161.5052
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 0.93s
                      Time elapsed: 00:07:24
                               ETA: 00:27:59

################################################################################
                     [1m Learning iteration 419/2000 [0m                      

                       Computation: 102926 steps/s (collection: 0.816s, learning 0.139s)
             Mean action noise std: 1.66
          Mean value_function loss: 116.0959
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.9695
                       Mean reward: 817.95
               Mean episode length: 243.72
    Episode_Reward/reaching_object: 0.7065
     Episode_Reward/lifting_object: 160.3988
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 0.96s
                      Time elapsed: 00:07:25
                               ETA: 00:27:58

################################################################################
                     [1m Learning iteration 420/2000 [0m                      

                       Computation: 111940 steps/s (collection: 0.779s, learning 0.099s)
             Mean action noise std: 1.66
          Mean value_function loss: 110.2094
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 14.9711
                       Mean reward: 803.79
               Mean episode length: 245.54
    Episode_Reward/reaching_object: 0.7134
     Episode_Reward/lifting_object: 161.4725
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 0.88s
                      Time elapsed: 00:07:26
                               ETA: 00:27:56

################################################################################
                     [1m Learning iteration 421/2000 [0m                      

                       Computation: 110030 steps/s (collection: 0.782s, learning 0.111s)
             Mean action noise std: 1.66
          Mean value_function loss: 122.9039
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 14.9744
                       Mean reward: 815.88
               Mean episode length: 241.15
    Episode_Reward/reaching_object: 0.7007
     Episode_Reward/lifting_object: 158.6551
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 0.89s
                      Time elapsed: 00:07:27
                               ETA: 00:27:54

################################################################################
                     [1m Learning iteration 422/2000 [0m                      

                       Computation: 109586 steps/s (collection: 0.798s, learning 0.100s)
             Mean action noise std: 1.66
          Mean value_function loss: 107.2215
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 14.9798
                       Mean reward: 826.86
               Mean episode length: 243.73
    Episode_Reward/reaching_object: 0.7223
     Episode_Reward/lifting_object: 161.5375
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 0.90s
                      Time elapsed: 00:07:28
                               ETA: 00:27:53

################################################################################
                     [1m Learning iteration 423/2000 [0m                      

                       Computation: 105549 steps/s (collection: 0.797s, learning 0.134s)
             Mean action noise std: 1.66
          Mean value_function loss: 104.6017
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.9815
                       Mean reward: 840.73
               Mean episode length: 246.27
    Episode_Reward/reaching_object: 0.7214
     Episode_Reward/lifting_object: 162.8097
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 0.93s
                      Time elapsed: 00:07:29
                               ETA: 00:27:51

################################################################################
                     [1m Learning iteration 424/2000 [0m                      

                       Computation: 113341 steps/s (collection: 0.781s, learning 0.086s)
             Mean action noise std: 1.66
          Mean value_function loss: 109.1194
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 14.9838
                       Mean reward: 832.28
               Mean episode length: 243.70
    Episode_Reward/reaching_object: 0.7223
     Episode_Reward/lifting_object: 163.9485
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 0.87s
                      Time elapsed: 00:07:30
                               ETA: 00:27:49

################################################################################
                     [1m Learning iteration 425/2000 [0m                      

                       Computation: 110127 steps/s (collection: 0.805s, learning 0.088s)
             Mean action noise std: 1.66
          Mean value_function loss: 95.1627
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.9893
                       Mean reward: 810.65
               Mean episode length: 239.08
    Episode_Reward/reaching_object: 0.7285
     Episode_Reward/lifting_object: 163.5958
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 0.89s
                      Time elapsed: 00:07:31
                               ETA: 00:27:48

################################################################################
                     [1m Learning iteration 426/2000 [0m                      

                       Computation: 107471 steps/s (collection: 0.825s, learning 0.090s)
             Mean action noise std: 1.66
          Mean value_function loss: 102.6241
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 14.9955
                       Mean reward: 841.66
               Mean episode length: 246.77
    Episode_Reward/reaching_object: 0.7202
     Episode_Reward/lifting_object: 161.9832
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 0.91s
                      Time elapsed: 00:07:32
                               ETA: 00:27:46

################################################################################
                     [1m Learning iteration 427/2000 [0m                      

                       Computation: 109767 steps/s (collection: 0.780s, learning 0.115s)
             Mean action noise std: 1.66
          Mean value_function loss: 91.9019
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.9963
                       Mean reward: 846.65
               Mean episode length: 246.55
    Episode_Reward/reaching_object: 0.7265
     Episode_Reward/lifting_object: 163.9753
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 0.90s
                      Time elapsed: 00:07:33
                               ETA: 00:27:44

################################################################################
                     [1m Learning iteration 428/2000 [0m                      

                       Computation: 104575 steps/s (collection: 0.844s, learning 0.096s)
             Mean action noise std: 1.66
          Mean value_function loss: 96.8140
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 14.9969
                       Mean reward: 847.56
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7210
     Episode_Reward/lifting_object: 163.3651
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 0.94s
                      Time elapsed: 00:07:33
                               ETA: 00:27:43

################################################################################
                     [1m Learning iteration 429/2000 [0m                      

                       Computation: 107648 steps/s (collection: 0.804s, learning 0.109s)
             Mean action noise std: 1.66
          Mean value_function loss: 81.2712
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 15.0030
                       Mean reward: 820.31
               Mean episode length: 242.31
    Episode_Reward/reaching_object: 0.7246
     Episode_Reward/lifting_object: 165.0446
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 0.91s
                      Time elapsed: 00:07:34
                               ETA: 00:27:41

################################################################################
                     [1m Learning iteration 430/2000 [0m                      

                       Computation: 95126 steps/s (collection: 0.896s, learning 0.138s)
             Mean action noise std: 1.66
          Mean value_function loss: 73.0411
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.0077
                       Mean reward: 819.87
               Mean episode length: 244.01
    Episode_Reward/reaching_object: 0.7216
     Episode_Reward/lifting_object: 162.9368
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 1.03s
                      Time elapsed: 00:07:35
                               ETA: 00:27:40

################################################################################
                     [1m Learning iteration 431/2000 [0m                      

                       Computation: 104261 steps/s (collection: 0.831s, learning 0.112s)
             Mean action noise std: 1.66
          Mean value_function loss: 97.2872
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.0134
                       Mean reward: 839.48
               Mean episode length: 242.90
    Episode_Reward/reaching_object: 0.7387
     Episode_Reward/lifting_object: 166.7019
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 0.94s
                      Time elapsed: 00:07:36
                               ETA: 00:27:39

################################################################################
                     [1m Learning iteration 432/2000 [0m                      

                       Computation: 105777 steps/s (collection: 0.829s, learning 0.101s)
             Mean action noise std: 1.67
          Mean value_function loss: 78.0014
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.0211
                       Mean reward: 794.52
               Mean episode length: 241.51
    Episode_Reward/reaching_object: 0.7252
     Episode_Reward/lifting_object: 163.3010
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 0.93s
                      Time elapsed: 00:07:37
                               ETA: 00:27:37

################################################################################
                     [1m Learning iteration 433/2000 [0m                      

                       Computation: 104686 steps/s (collection: 0.850s, learning 0.090s)
             Mean action noise std: 1.67
          Mean value_function loss: 95.8992
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.0287
                       Mean reward: 837.43
               Mean episode length: 243.06
    Episode_Reward/reaching_object: 0.7254
     Episode_Reward/lifting_object: 164.1186
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 0.94s
                      Time elapsed: 00:07:38
                               ETA: 00:27:36

################################################################################
                     [1m Learning iteration 434/2000 [0m                      

                       Computation: 104742 steps/s (collection: 0.819s, learning 0.119s)
             Mean action noise std: 1.67
          Mean value_function loss: 82.5727
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.0327
                       Mean reward: 854.67
               Mean episode length: 247.64
    Episode_Reward/reaching_object: 0.7352
     Episode_Reward/lifting_object: 165.8649
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 0.94s
                      Time elapsed: 00:07:39
                               ETA: 00:27:34

################################################################################
                     [1m Learning iteration 435/2000 [0m                      

                       Computation: 109336 steps/s (collection: 0.808s, learning 0.091s)
             Mean action noise std: 1.67
          Mean value_function loss: 88.6730
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.0346
                       Mean reward: 857.50
               Mean episode length: 247.65
    Episode_Reward/reaching_object: 0.7419
     Episode_Reward/lifting_object: 167.8706
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 0.90s
                      Time elapsed: 00:07:40
                               ETA: 00:27:33

################################################################################
                     [1m Learning iteration 436/2000 [0m                      

                       Computation: 99971 steps/s (collection: 0.805s, learning 0.178s)
             Mean action noise std: 1.67
          Mean value_function loss: 88.5593
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.0377
                       Mean reward: 816.30
               Mean episode length: 242.67
    Episode_Reward/reaching_object: 0.7215
     Episode_Reward/lifting_object: 162.6247
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 0.98s
                      Time elapsed: 00:07:41
                               ETA: 00:27:31

################################################################################
                     [1m Learning iteration 437/2000 [0m                      

                       Computation: 108745 steps/s (collection: 0.776s, learning 0.128s)
             Mean action noise std: 1.67
          Mean value_function loss: 79.5628
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.0388
                       Mean reward: 855.39
               Mean episode length: 247.76
    Episode_Reward/reaching_object: 0.7520
     Episode_Reward/lifting_object: 168.6752
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 0.90s
                      Time elapsed: 00:07:42
                               ETA: 00:27:30

################################################################################
                     [1m Learning iteration 438/2000 [0m                      

                       Computation: 110009 steps/s (collection: 0.793s, learning 0.101s)
             Mean action noise std: 1.67
          Mean value_function loss: 83.0951
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.0420
                       Mean reward: 800.79
               Mean episode length: 237.66
    Episode_Reward/reaching_object: 0.7352
     Episode_Reward/lifting_object: 165.2585
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 0.89s
                      Time elapsed: 00:07:43
                               ETA: 00:27:28

################################################################################
                     [1m Learning iteration 439/2000 [0m                      

                       Computation: 111602 steps/s (collection: 0.793s, learning 0.088s)
             Mean action noise std: 1.67
          Mean value_function loss: 84.3811
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.0503
                       Mean reward: 819.81
               Mean episode length: 242.11
    Episode_Reward/reaching_object: 0.7324
     Episode_Reward/lifting_object: 164.4562
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 0.88s
                      Time elapsed: 00:07:44
                               ETA: 00:27:26

################################################################################
                     [1m Learning iteration 440/2000 [0m                      

                       Computation: 111491 steps/s (collection: 0.789s, learning 0.093s)
             Mean action noise std: 1.67
          Mean value_function loss: 83.0322
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.0552
                       Mean reward: 861.30
               Mean episode length: 247.59
    Episode_Reward/reaching_object: 0.7366
     Episode_Reward/lifting_object: 166.5906
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 0.88s
                      Time elapsed: 00:07:45
                               ETA: 00:27:25

################################################################################
                     [1m Learning iteration 441/2000 [0m                      

                       Computation: 111351 steps/s (collection: 0.793s, learning 0.090s)
             Mean action noise std: 1.67
          Mean value_function loss: 91.8072
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.0547
                       Mean reward: 852.22
               Mean episode length: 247.89
    Episode_Reward/reaching_object: 0.7428
     Episode_Reward/lifting_object: 167.2642
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 0.88s
                      Time elapsed: 00:07:45
                               ETA: 00:27:23

################################################################################
                     [1m Learning iteration 442/2000 [0m                      

                       Computation: 105960 steps/s (collection: 0.815s, learning 0.113s)
             Mean action noise std: 1.67
          Mean value_function loss: 91.6279
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.0546
                       Mean reward: 793.93
               Mean episode length: 240.30
    Episode_Reward/reaching_object: 0.7261
     Episode_Reward/lifting_object: 163.1942
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 0.93s
                      Time elapsed: 00:07:46
                               ETA: 00:27:22

################################################################################
                     [1m Learning iteration 443/2000 [0m                      

                       Computation: 112168 steps/s (collection: 0.791s, learning 0.086s)
             Mean action noise std: 1.67
          Mean value_function loss: 72.5297
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.0562
                       Mean reward: 839.15
               Mean episode length: 245.58
    Episode_Reward/reaching_object: 0.7202
     Episode_Reward/lifting_object: 161.1922
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 0.88s
                      Time elapsed: 00:07:47
                               ETA: 00:27:20

################################################################################
                     [1m Learning iteration 444/2000 [0m                      

                       Computation: 104944 steps/s (collection: 0.785s, learning 0.152s)
             Mean action noise std: 1.67
          Mean value_function loss: 79.6545
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.0585
                       Mean reward: 851.26
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.7409
     Episode_Reward/lifting_object: 166.9711
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 0.94s
                      Time elapsed: 00:07:48
                               ETA: 00:27:18

################################################################################
                     [1m Learning iteration 445/2000 [0m                      

                       Computation: 101248 steps/s (collection: 0.830s, learning 0.141s)
             Mean action noise std: 1.68
          Mean value_function loss: 77.2208
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 15.0641
                       Mean reward: 850.04
               Mean episode length: 246.48
    Episode_Reward/reaching_object: 0.7445
     Episode_Reward/lifting_object: 167.8241
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 0.97s
                      Time elapsed: 00:07:49
                               ETA: 00:27:17

################################################################################
                     [1m Learning iteration 446/2000 [0m                      

                       Computation: 104178 steps/s (collection: 0.779s, learning 0.165s)
             Mean action noise std: 1.68
          Mean value_function loss: 87.4049
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.0700
                       Mean reward: 845.93
               Mean episode length: 247.75
    Episode_Reward/reaching_object: 0.7471
     Episode_Reward/lifting_object: 168.3477
      Episode_Reward/object_height: 0.0647
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 0.94s
                      Time elapsed: 00:07:50
                               ETA: 00:27:16

################################################################################
                     [1m Learning iteration 447/2000 [0m                      

                       Computation: 108706 steps/s (collection: 0.820s, learning 0.085s)
             Mean action noise std: 1.68
          Mean value_function loss: 82.2577
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.0748
                       Mean reward: 834.77
               Mean episode length: 244.20
    Episode_Reward/reaching_object: 0.7185
     Episode_Reward/lifting_object: 161.8690
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 0.90s
                      Time elapsed: 00:07:51
                               ETA: 00:27:14

################################################################################
                     [1m Learning iteration 448/2000 [0m                      

                       Computation: 104136 steps/s (collection: 0.773s, learning 0.171s)
             Mean action noise std: 1.68
          Mean value_function loss: 69.9741
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.0791
                       Mean reward: 846.11
               Mean episode length: 245.61
    Episode_Reward/reaching_object: 0.7386
     Episode_Reward/lifting_object: 166.2549
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 0.94s
                      Time elapsed: 00:07:52
                               ETA: 00:27:13

################################################################################
                     [1m Learning iteration 449/2000 [0m                      

                       Computation: 104208 steps/s (collection: 0.790s, learning 0.153s)
             Mean action noise std: 1.68
          Mean value_function loss: 70.6936
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.0816
                       Mean reward: 820.13
               Mean episode length: 245.46
    Episode_Reward/reaching_object: 0.7257
     Episode_Reward/lifting_object: 162.8572
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 0.94s
                      Time elapsed: 00:07:53
                               ETA: 00:27:11

################################################################################
                     [1m Learning iteration 450/2000 [0m                      

                       Computation: 105952 steps/s (collection: 0.791s, learning 0.137s)
             Mean action noise std: 1.68
          Mean value_function loss: 90.7896
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 15.0844
                       Mean reward: 858.08
               Mean episode length: 247.97
    Episode_Reward/reaching_object: 0.7523
     Episode_Reward/lifting_object: 169.4409
      Episode_Reward/object_height: 0.0647
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 0.93s
                      Time elapsed: 00:07:54
                               ETA: 00:27:10

################################################################################
                     [1m Learning iteration 451/2000 [0m                      

                       Computation: 108403 steps/s (collection: 0.808s, learning 0.099s)
             Mean action noise std: 1.68
          Mean value_function loss: 69.8109
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.0854
                       Mean reward: 822.91
               Mean episode length: 242.83
    Episode_Reward/reaching_object: 0.7249
     Episode_Reward/lifting_object: 163.0106
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 0.91s
                      Time elapsed: 00:07:55
                               ETA: 00:27:08

################################################################################
                     [1m Learning iteration 452/2000 [0m                      

                       Computation: 107167 steps/s (collection: 0.821s, learning 0.096s)
             Mean action noise std: 1.68
          Mean value_function loss: 66.5579
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.0867
                       Mean reward: 823.25
               Mean episode length: 244.51
    Episode_Reward/reaching_object: 0.7418
     Episode_Reward/lifting_object: 166.0600
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 0.92s
                      Time elapsed: 00:07:56
                               ETA: 00:27:07

################################################################################
                     [1m Learning iteration 453/2000 [0m                      

                       Computation: 105857 steps/s (collection: 0.839s, learning 0.090s)
             Mean action noise std: 1.68
          Mean value_function loss: 72.5065
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.0909
                       Mean reward: 853.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7337
     Episode_Reward/lifting_object: 165.6629
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 0.93s
                      Time elapsed: 00:07:57
                               ETA: 00:27:05

################################################################################
                     [1m Learning iteration 454/2000 [0m                      

                       Computation: 104211 steps/s (collection: 0.838s, learning 0.105s)
             Mean action noise std: 1.68
          Mean value_function loss: 69.4237
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.0956
                       Mean reward: 818.59
               Mean episode length: 243.71
    Episode_Reward/reaching_object: 0.7325
     Episode_Reward/lifting_object: 164.9721
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 0.94s
                      Time elapsed: 00:07:58
                               ETA: 00:27:04

################################################################################
                     [1m Learning iteration 455/2000 [0m                      

                       Computation: 107436 steps/s (collection: 0.814s, learning 0.101s)
             Mean action noise std: 1.68
          Mean value_function loss: 59.6561
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.0968
                       Mean reward: 850.53
               Mean episode length: 246.33
    Episode_Reward/reaching_object: 0.7383
     Episode_Reward/lifting_object: 165.5647
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 0.92s
                      Time elapsed: 00:07:58
                               ETA: 00:27:02

################################################################################
                     [1m Learning iteration 456/2000 [0m                      

                       Computation: 107035 steps/s (collection: 0.832s, learning 0.087s)
             Mean action noise std: 1.68
          Mean value_function loss: 76.8849
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.1012
                       Mean reward: 854.88
               Mean episode length: 246.84
    Episode_Reward/reaching_object: 0.7514
     Episode_Reward/lifting_object: 168.2125
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 0.92s
                      Time elapsed: 00:07:59
                               ETA: 00:27:01

################################################################################
                     [1m Learning iteration 457/2000 [0m                      

                       Computation: 102716 steps/s (collection: 0.872s, learning 0.085s)
             Mean action noise std: 1.68
          Mean value_function loss: 80.4653
               Mean surrogate loss: 0.0114
                 Mean entropy loss: 15.1077
                       Mean reward: 827.40
               Mean episode length: 241.62
    Episode_Reward/reaching_object: 0.7396
     Episode_Reward/lifting_object: 165.7545
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 0.96s
                      Time elapsed: 00:08:00
                               ETA: 00:26:59

################################################################################
                     [1m Learning iteration 458/2000 [0m                      

                       Computation: 101459 steps/s (collection: 0.865s, learning 0.104s)
             Mean action noise std: 1.68
          Mean value_function loss: 69.5329
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.1084
                       Mean reward: 834.18
               Mean episode length: 245.51
    Episode_Reward/reaching_object: 0.7410
     Episode_Reward/lifting_object: 166.5795
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 0.97s
                      Time elapsed: 00:08:01
                               ETA: 00:26:58

################################################################################
                     [1m Learning iteration 459/2000 [0m                      

                       Computation: 105241 steps/s (collection: 0.838s, learning 0.096s)
             Mean action noise std: 1.68
          Mean value_function loss: 68.7428
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.1107
                       Mean reward: 811.63
               Mean episode length: 241.98
    Episode_Reward/reaching_object: 0.7486
     Episode_Reward/lifting_object: 168.0363
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 0.93s
                      Time elapsed: 00:08:02
                               ETA: 00:26:57

################################################################################
                     [1m Learning iteration 460/2000 [0m                      

                       Computation: 111641 steps/s (collection: 0.779s, learning 0.102s)
             Mean action noise std: 1.69
          Mean value_function loss: 83.4474
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.1176
                       Mean reward: 853.54
               Mean episode length: 247.44
    Episode_Reward/reaching_object: 0.7463
     Episode_Reward/lifting_object: 169.4546
      Episode_Reward/object_height: 0.0647
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 0.88s
                      Time elapsed: 00:08:03
                               ETA: 00:26:55

################################################################################
                     [1m Learning iteration 461/2000 [0m                      

                       Computation: 104830 steps/s (collection: 0.801s, learning 0.137s)
             Mean action noise std: 1.69
          Mean value_function loss: 57.3714
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.1254
                       Mean reward: 844.94
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.7377
     Episode_Reward/lifting_object: 167.1029
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 0.94s
                      Time elapsed: 00:08:04
                               ETA: 00:26:54

################################################################################
                     [1m Learning iteration 462/2000 [0m                      

                       Computation: 102830 steps/s (collection: 0.780s, learning 0.176s)
             Mean action noise std: 1.69
          Mean value_function loss: 65.2949
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.1284
                       Mean reward: 840.73
               Mean episode length: 246.27
    Episode_Reward/reaching_object: 0.7576
     Episode_Reward/lifting_object: 169.5327
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 0.96s
                      Time elapsed: 00:08:05
                               ETA: 00:26:52

################################################################################
                     [1m Learning iteration 463/2000 [0m                      

                       Computation: 111918 steps/s (collection: 0.789s, learning 0.089s)
             Mean action noise std: 1.69
          Mean value_function loss: 70.1928
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.1280
                       Mean reward: 834.51
               Mean episode length: 244.70
    Episode_Reward/reaching_object: 0.7494
     Episode_Reward/lifting_object: 168.2060
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 0.88s
                      Time elapsed: 00:08:06
                               ETA: 00:26:51

################################################################################
                     [1m Learning iteration 464/2000 [0m                      

                       Computation: 113223 steps/s (collection: 0.772s, learning 0.096s)
             Mean action noise std: 1.69
          Mean value_function loss: 84.5978
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.1248
                       Mean reward: 836.25
               Mean episode length: 242.64
    Episode_Reward/reaching_object: 0.7471
     Episode_Reward/lifting_object: 167.8113
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 0.87s
                      Time elapsed: 00:08:07
                               ETA: 00:26:49

################################################################################
                     [1m Learning iteration 465/2000 [0m                      

                       Computation: 104144 steps/s (collection: 0.852s, learning 0.092s)
             Mean action noise std: 1.69
          Mean value_function loss: 74.6685
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.1256
                       Mean reward: 847.29
               Mean episode length: 246.57
    Episode_Reward/reaching_object: 0.7276
     Episode_Reward/lifting_object: 163.8288
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 0.94s
                      Time elapsed: 00:08:08
                               ETA: 00:26:48

################################################################################
                     [1m Learning iteration 466/2000 [0m                      

                       Computation: 97245 steps/s (collection: 0.839s, learning 0.172s)
             Mean action noise std: 1.69
          Mean value_function loss: 59.5606
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.1333
                       Mean reward: 830.67
               Mean episode length: 246.62
    Episode_Reward/reaching_object: 0.7377
     Episode_Reward/lifting_object: 165.0661
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 1.01s
                      Time elapsed: 00:08:09
                               ETA: 00:26:46

################################################################################
                     [1m Learning iteration 467/2000 [0m                      

                       Computation: 107609 steps/s (collection: 0.824s, learning 0.089s)
             Mean action noise std: 1.69
          Mean value_function loss: 66.4223
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.1379
                       Mean reward: 860.19
               Mean episode length: 247.33
    Episode_Reward/reaching_object: 0.7489
     Episode_Reward/lifting_object: 170.0722
      Episode_Reward/object_height: 0.0648
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 0.91s
                      Time elapsed: 00:08:10
                               ETA: 00:26:45

################################################################################
                     [1m Learning iteration 468/2000 [0m                      

                       Computation: 96680 steps/s (collection: 0.930s, learning 0.087s)
             Mean action noise std: 1.69
          Mean value_function loss: 69.3733
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.1476
                       Mean reward: 833.54
               Mean episode length: 240.46
    Episode_Reward/reaching_object: 0.7471
     Episode_Reward/lifting_object: 168.8164
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 1.02s
                      Time elapsed: 00:08:11
                               ETA: 00:26:44

################################################################################
                     [1m Learning iteration 469/2000 [0m                      

                       Computation: 106594 steps/s (collection: 0.832s, learning 0.090s)
             Mean action noise std: 1.69
          Mean value_function loss: 66.1227
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.1560
                       Mean reward: 859.88
               Mean episode length: 247.31
    Episode_Reward/reaching_object: 0.7503
     Episode_Reward/lifting_object: 168.9957
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 0.92s
                      Time elapsed: 00:08:12
                               ETA: 00:26:42

################################################################################
                     [1m Learning iteration 470/2000 [0m                      

                       Computation: 102005 steps/s (collection: 0.870s, learning 0.094s)
             Mean action noise std: 1.70
          Mean value_function loss: 86.3611
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.1680
                       Mean reward: 830.36
               Mean episode length: 244.86
    Episode_Reward/reaching_object: 0.7400
     Episode_Reward/lifting_object: 166.5032
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 0.96s
                      Time elapsed: 00:08:13
                               ETA: 00:26:41

################################################################################
                     [1m Learning iteration 471/2000 [0m                      

                       Computation: 104570 steps/s (collection: 0.829s, learning 0.111s)
             Mean action noise std: 1.70
          Mean value_function loss: 83.2243
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.1788
                       Mean reward: 826.01
               Mean episode length: 244.00
    Episode_Reward/reaching_object: 0.7337
     Episode_Reward/lifting_object: 165.0825
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 0.94s
                      Time elapsed: 00:08:13
                               ETA: 00:26:40

################################################################################
                     [1m Learning iteration 472/2000 [0m                      

                       Computation: 106497 steps/s (collection: 0.803s, learning 0.120s)
             Mean action noise std: 1.70
          Mean value_function loss: 73.1186
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.1796
                       Mean reward: 812.81
               Mean episode length: 243.02
    Episode_Reward/reaching_object: 0.7354
     Episode_Reward/lifting_object: 164.6164
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 0.92s
                      Time elapsed: 00:08:14
                               ETA: 00:26:38

################################################################################
                     [1m Learning iteration 473/2000 [0m                      

                       Computation: 108586 steps/s (collection: 0.784s, learning 0.121s)
             Mean action noise std: 1.70
          Mean value_function loss: 66.7698
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.1810
                       Mean reward: 829.30
               Mean episode length: 246.89
    Episode_Reward/reaching_object: 0.7403
     Episode_Reward/lifting_object: 166.6590
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 0.91s
                      Time elapsed: 00:08:15
                               ETA: 00:26:37

################################################################################
                     [1m Learning iteration 474/2000 [0m                      

                       Computation: 108475 steps/s (collection: 0.803s, learning 0.104s)
             Mean action noise std: 1.70
          Mean value_function loss: 59.6888
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.1865
                       Mean reward: 830.27
               Mean episode length: 242.76
    Episode_Reward/reaching_object: 0.7357
     Episode_Reward/lifting_object: 166.4404
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 0.91s
                      Time elapsed: 00:08:16
                               ETA: 00:26:35

################################################################################
                     [1m Learning iteration 475/2000 [0m                      

                       Computation: 107186 steps/s (collection: 0.797s, learning 0.120s)
             Mean action noise std: 1.70
          Mean value_function loss: 83.7901
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.1870
                       Mean reward: 831.98
               Mean episode length: 243.81
    Episode_Reward/reaching_object: 0.7474
     Episode_Reward/lifting_object: 168.8509
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 0.92s
                      Time elapsed: 00:08:17
                               ETA: 00:26:34

################################################################################
                     [1m Learning iteration 476/2000 [0m                      

                       Computation: 101717 steps/s (collection: 0.843s, learning 0.123s)
             Mean action noise std: 1.70
          Mean value_function loss: 81.6685
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.1901
                       Mean reward: 829.19
               Mean episode length: 240.40
    Episode_Reward/reaching_object: 0.7348
     Episode_Reward/lifting_object: 165.6974
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 0.97s
                      Time elapsed: 00:08:18
                               ETA: 00:26:33

################################################################################
                     [1m Learning iteration 477/2000 [0m                      

                       Computation: 106029 steps/s (collection: 0.835s, learning 0.092s)
             Mean action noise std: 1.70
          Mean value_function loss: 73.6300
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.2031
                       Mean reward: 861.15
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7417
     Episode_Reward/lifting_object: 167.5809
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 0.93s
                      Time elapsed: 00:08:19
                               ETA: 00:26:31

################################################################################
                     [1m Learning iteration 478/2000 [0m                      

                       Computation: 97366 steps/s (collection: 0.849s, learning 0.161s)
             Mean action noise std: 1.71
          Mean value_function loss: 94.5935
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.2094
                       Mean reward: 829.42
               Mean episode length: 243.39
    Episode_Reward/reaching_object: 0.7499
     Episode_Reward/lifting_object: 168.5747
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 1.01s
                      Time elapsed: 00:08:20
                               ETA: 00:26:30

################################################################################
                     [1m Learning iteration 479/2000 [0m                      

                       Computation: 100844 steps/s (collection: 0.836s, learning 0.139s)
             Mean action noise std: 1.71
          Mean value_function loss: 85.8719
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.2099
                       Mean reward: 840.63
               Mean episode length: 242.35
    Episode_Reward/reaching_object: 0.7323
     Episode_Reward/lifting_object: 165.8633
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 0.97s
                      Time elapsed: 00:08:21
                               ETA: 00:26:29

################################################################################
                     [1m Learning iteration 480/2000 [0m                      

                       Computation: 96098 steps/s (collection: 0.836s, learning 0.187s)
             Mean action noise std: 1.71
          Mean value_function loss: 59.7942
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.2109
                       Mean reward: 820.51
               Mean episode length: 242.10
    Episode_Reward/reaching_object: 0.7226
     Episode_Reward/lifting_object: 163.9702
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 1.02s
                      Time elapsed: 00:08:22
                               ETA: 00:26:28

################################################################################
                     [1m Learning iteration 481/2000 [0m                      

                       Computation: 87712 steps/s (collection: 0.982s, learning 0.139s)
             Mean action noise std: 1.71
          Mean value_function loss: 57.9071
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 15.2133
                       Mean reward: 856.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7334
     Episode_Reward/lifting_object: 166.8103
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 1.12s
                      Time elapsed: 00:08:23
                               ETA: 00:26:27

################################################################################
                     [1m Learning iteration 482/2000 [0m                      

                       Computation: 101555 steps/s (collection: 0.778s, learning 0.190s)
             Mean action noise std: 1.71
          Mean value_function loss: 67.7572
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.2184
                       Mean reward: 827.58
               Mean episode length: 243.87
    Episode_Reward/reaching_object: 0.7455
     Episode_Reward/lifting_object: 167.8105
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 0.97s
                      Time elapsed: 00:08:24
                               ETA: 00:26:25

################################################################################
                     [1m Learning iteration 483/2000 [0m                      

                       Computation: 92894 steps/s (collection: 0.916s, learning 0.142s)
             Mean action noise std: 1.71
          Mean value_function loss: 70.6220
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.2279
                       Mean reward: 832.68
               Mean episode length: 244.80
    Episode_Reward/reaching_object: 0.7286
     Episode_Reward/lifting_object: 165.4701
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 1.06s
                      Time elapsed: 00:08:25
                               ETA: 00:26:24

################################################################################
                     [1m Learning iteration 484/2000 [0m                      

                       Computation: 105543 steps/s (collection: 0.822s, learning 0.109s)
             Mean action noise std: 1.71
          Mean value_function loss: 71.0448
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.2392
                       Mean reward: 834.45
               Mean episode length: 245.48
    Episode_Reward/reaching_object: 0.7323
     Episode_Reward/lifting_object: 165.9737
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 0.93s
                      Time elapsed: 00:08:26
                               ETA: 00:26:23

################################################################################
                     [1m Learning iteration 485/2000 [0m                      

                       Computation: 106927 steps/s (collection: 0.790s, learning 0.129s)
             Mean action noise std: 1.71
          Mean value_function loss: 64.8883
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 15.2489
                       Mean reward: 833.07
               Mean episode length: 245.94
    Episode_Reward/reaching_object: 0.7241
     Episode_Reward/lifting_object: 164.1418
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 0.92s
                      Time elapsed: 00:08:27
                               ETA: 00:26:22

################################################################################
                     [1m Learning iteration 486/2000 [0m                      

                       Computation: 104570 steps/s (collection: 0.823s, learning 0.117s)
             Mean action noise std: 1.71
          Mean value_function loss: 47.7349
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.2511
                       Mean reward: 822.25
               Mean episode length: 242.73
    Episode_Reward/reaching_object: 0.7350
     Episode_Reward/lifting_object: 167.0298
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 0.94s
                      Time elapsed: 00:08:28
                               ETA: 00:26:20

################################################################################
                     [1m Learning iteration 487/2000 [0m                      

                       Computation: 112101 steps/s (collection: 0.760s, learning 0.117s)
             Mean action noise std: 1.72
          Mean value_function loss: 47.4942
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 15.2584
                       Mean reward: 878.59
               Mean episode length: 249.88
    Episode_Reward/reaching_object: 0.7515
     Episode_Reward/lifting_object: 170.8650
      Episode_Reward/object_height: 0.0658
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 0.88s
                      Time elapsed: 00:08:29
                               ETA: 00:26:19

################################################################################
                     [1m Learning iteration 488/2000 [0m                      

                       Computation: 108513 steps/s (collection: 0.777s, learning 0.129s)
             Mean action noise std: 1.72
          Mean value_function loss: 45.9167
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.2691
                       Mean reward: 838.87
               Mean episode length: 245.73
    Episode_Reward/reaching_object: 0.7440
     Episode_Reward/lifting_object: 167.8672
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 0.91s
                      Time elapsed: 00:08:30
                               ETA: 00:26:17

################################################################################
                     [1m Learning iteration 489/2000 [0m                      

                       Computation: 110549 steps/s (collection: 0.795s, learning 0.094s)
             Mean action noise std: 1.72
          Mean value_function loss: 48.7187
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.2760
                       Mean reward: 869.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7504
     Episode_Reward/lifting_object: 168.9902
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 0.89s
                      Time elapsed: 00:08:31
                               ETA: 00:26:16

################################################################################
                     [1m Learning iteration 490/2000 [0m                      

                       Computation: 108172 steps/s (collection: 0.781s, learning 0.128s)
             Mean action noise std: 1.72
          Mean value_function loss: 51.5169
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.2791
                       Mean reward: 861.24
               Mean episode length: 245.83
    Episode_Reward/reaching_object: 0.7540
     Episode_Reward/lifting_object: 170.5438
      Episode_Reward/object_height: 0.0657
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 0.91s
                      Time elapsed: 00:08:32
                               ETA: 00:26:14

################################################################################
                     [1m Learning iteration 491/2000 [0m                      

                       Computation: 109233 steps/s (collection: 0.787s, learning 0.113s)
             Mean action noise std: 1.72
          Mean value_function loss: 60.0382
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.2832
                       Mean reward: 833.08
               Mean episode length: 245.16
    Episode_Reward/reaching_object: 0.7439
     Episode_Reward/lifting_object: 169.0610
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 0.90s
                      Time elapsed: 00:08:32
                               ETA: 00:26:13

################################################################################
                     [1m Learning iteration 492/2000 [0m                      

                       Computation: 113142 steps/s (collection: 0.773s, learning 0.096s)
             Mean action noise std: 1.72
          Mean value_function loss: 53.9306
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.2851
                       Mean reward: 864.78
               Mean episode length: 246.30
    Episode_Reward/reaching_object: 0.7414
     Episode_Reward/lifting_object: 167.3192
      Episode_Reward/object_height: 0.0647
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 0.87s
                      Time elapsed: 00:08:33
                               ETA: 00:26:11

################################################################################
                     [1m Learning iteration 493/2000 [0m                      

                       Computation: 106835 steps/s (collection: 0.833s, learning 0.088s)
             Mean action noise std: 1.72
          Mean value_function loss: 50.0973
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.2879
                       Mean reward: 841.48
               Mean episode length: 244.47
    Episode_Reward/reaching_object: 0.7452
     Episode_Reward/lifting_object: 168.6196
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 0.92s
                      Time elapsed: 00:08:34
                               ETA: 00:26:10

################################################################################
                     [1m Learning iteration 494/2000 [0m                      

                       Computation: 109792 steps/s (collection: 0.789s, learning 0.107s)
             Mean action noise std: 1.72
          Mean value_function loss: 53.0320
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.2868
                       Mean reward: 855.81
               Mean episode length: 246.44
    Episode_Reward/reaching_object: 0.7584
     Episode_Reward/lifting_object: 171.4131
      Episode_Reward/object_height: 0.0661
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 0.90s
                      Time elapsed: 00:08:35
                               ETA: 00:26:08

################################################################################
                     [1m Learning iteration 495/2000 [0m                      

                       Computation: 103307 steps/s (collection: 0.836s, learning 0.116s)
             Mean action noise std: 1.72
          Mean value_function loss: 67.3296
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 15.2845
                       Mean reward: 833.61
               Mean episode length: 243.64
    Episode_Reward/reaching_object: 0.7528
     Episode_Reward/lifting_object: 169.9283
      Episode_Reward/object_height: 0.0656
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 0.95s
                      Time elapsed: 00:08:36
                               ETA: 00:26:07

################################################################################
                     [1m Learning iteration 496/2000 [0m                      

                       Computation: 101234 steps/s (collection: 0.873s, learning 0.098s)
             Mean action noise std: 1.72
          Mean value_function loss: 42.4527
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.2855
                       Mean reward: 858.55
               Mean episode length: 246.87
    Episode_Reward/reaching_object: 0.7500
     Episode_Reward/lifting_object: 170.2960
      Episode_Reward/object_height: 0.0658
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 0.97s
                      Time elapsed: 00:08:37
                               ETA: 00:26:06

################################################################################
                     [1m Learning iteration 497/2000 [0m                      

                       Computation: 103469 steps/s (collection: 0.809s, learning 0.142s)
             Mean action noise std: 1.72
          Mean value_function loss: 50.5927
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.2913
                       Mean reward: 873.78
               Mean episode length: 249.48
    Episode_Reward/reaching_object: 0.7612
     Episode_Reward/lifting_object: 172.7626
      Episode_Reward/object_height: 0.0666
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 0.95s
                      Time elapsed: 00:08:38
                               ETA: 00:26:04

################################################################################
                     [1m Learning iteration 498/2000 [0m                      

                       Computation: 98866 steps/s (collection: 0.855s, learning 0.139s)
             Mean action noise std: 1.72
          Mean value_function loss: 50.2106
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.2955
                       Mean reward: 857.16
               Mean episode length: 246.32
    Episode_Reward/reaching_object: 0.7534
     Episode_Reward/lifting_object: 170.6542
      Episode_Reward/object_height: 0.0658
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 0.99s
                      Time elapsed: 00:08:39
                               ETA: 00:26:03

################################################################################
                     [1m Learning iteration 499/2000 [0m                      

                       Computation: 99410 steps/s (collection: 0.858s, learning 0.131s)
             Mean action noise std: 1.72
          Mean value_function loss: 59.0224
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.3045
                       Mean reward: 851.22
               Mean episode length: 246.74
    Episode_Reward/reaching_object: 0.7620
     Episode_Reward/lifting_object: 171.4902
      Episode_Reward/object_height: 0.0661
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 0.99s
                      Time elapsed: 00:08:40
                               ETA: 00:26:02

################################################################################
                     [1m Learning iteration 500/2000 [0m                      

                       Computation: 110268 steps/s (collection: 0.798s, learning 0.093s)
             Mean action noise std: 1.73
          Mean value_function loss: 62.4964
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.3133
                       Mean reward: 849.53
               Mean episode length: 247.32
    Episode_Reward/reaching_object: 0.7469
     Episode_Reward/lifting_object: 169.6893
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 0.89s
                      Time elapsed: 00:08:41
                               ETA: 00:26:01

################################################################################
                     [1m Learning iteration 501/2000 [0m                      

                       Computation: 104221 steps/s (collection: 0.800s, learning 0.144s)
             Mean action noise std: 1.73
          Mean value_function loss: 61.6649
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.3161
                       Mean reward: 822.68
               Mean episode length: 241.49
    Episode_Reward/reaching_object: 0.7450
     Episode_Reward/lifting_object: 167.8388
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 0.94s
                      Time elapsed: 00:08:42
                               ETA: 00:25:59

################################################################################
                     [1m Learning iteration 502/2000 [0m                      

                       Computation: 95777 steps/s (collection: 0.862s, learning 0.165s)
             Mean action noise std: 1.73
          Mean value_function loss: 49.6865
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.3216
                       Mean reward: 869.28
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7495
     Episode_Reward/lifting_object: 169.6066
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 1.03s
                      Time elapsed: 00:08:43
                               ETA: 00:25:58

################################################################################
                     [1m Learning iteration 503/2000 [0m                      

                       Computation: 109011 steps/s (collection: 0.796s, learning 0.106s)
             Mean action noise std: 1.73
          Mean value_function loss: 56.3210
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.3248
                       Mean reward: 868.87
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.7562
     Episode_Reward/lifting_object: 171.2804
      Episode_Reward/object_height: 0.0657
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 0.90s
                      Time elapsed: 00:08:44
                               ETA: 00:25:57

################################################################################
                     [1m Learning iteration 504/2000 [0m                      

                       Computation: 109670 steps/s (collection: 0.776s, learning 0.120s)
             Mean action noise std: 1.73
          Mean value_function loss: 57.1235
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.3315
                       Mean reward: 838.85
               Mean episode length: 246.32
    Episode_Reward/reaching_object: 0.7479
     Episode_Reward/lifting_object: 169.8066
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 0.90s
                      Time elapsed: 00:08:45
                               ETA: 00:25:55

################################################################################
                     [1m Learning iteration 505/2000 [0m                      

                       Computation: 108441 steps/s (collection: 0.797s, learning 0.110s)
             Mean action noise std: 1.73
          Mean value_function loss: 57.0914
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.3398
                       Mean reward: 834.09
               Mean episode length: 247.88
    Episode_Reward/reaching_object: 0.7406
     Episode_Reward/lifting_object: 167.9135
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 0.91s
                      Time elapsed: 00:08:46
                               ETA: 00:25:54

################################################################################
                     [1m Learning iteration 506/2000 [0m                      

                       Computation: 105386 steps/s (collection: 0.816s, learning 0.117s)
             Mean action noise std: 1.73
          Mean value_function loss: 47.8750
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.3386
                       Mean reward: 854.33
               Mean episode length: 245.25
    Episode_Reward/reaching_object: 0.7468
     Episode_Reward/lifting_object: 170.3752
      Episode_Reward/object_height: 0.0648
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 0.93s
                      Time elapsed: 00:08:46
                               ETA: 00:25:52

################################################################################
                     [1m Learning iteration 507/2000 [0m                      

                       Computation: 108045 steps/s (collection: 0.821s, learning 0.089s)
             Mean action noise std: 1.73
          Mean value_function loss: 59.9697
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 15.3383
                       Mean reward: 841.53
               Mean episode length: 245.50
    Episode_Reward/reaching_object: 0.7443
     Episode_Reward/lifting_object: 168.7613
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 0.91s
                      Time elapsed: 00:08:47
                               ETA: 00:25:51

################################################################################
                     [1m Learning iteration 508/2000 [0m                      

                       Computation: 103180 steps/s (collection: 0.843s, learning 0.110s)
             Mean action noise std: 1.73
          Mean value_function loss: 55.7992
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.3390
                       Mean reward: 845.02
               Mean episode length: 246.31
    Episode_Reward/reaching_object: 0.7326
     Episode_Reward/lifting_object: 166.5975
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 0.95s
                      Time elapsed: 00:08:48
                               ETA: 00:25:50

################################################################################
                     [1m Learning iteration 509/2000 [0m                      

                       Computation: 97621 steps/s (collection: 0.888s, learning 0.119s)
             Mean action noise std: 1.73
          Mean value_function loss: 47.6710
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.3413
                       Mean reward: 837.52
               Mean episode length: 246.08
    Episode_Reward/reaching_object: 0.7447
     Episode_Reward/lifting_object: 168.1743
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 1.01s
                      Time elapsed: 00:08:49
                               ETA: 00:25:49

################################################################################
                     [1m Learning iteration 510/2000 [0m                      

                       Computation: 109286 steps/s (collection: 0.797s, learning 0.103s)
             Mean action noise std: 1.74
          Mean value_function loss: 52.5576
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.3505
                       Mean reward: 854.10
               Mean episode length: 245.90
    Episode_Reward/reaching_object: 0.7467
     Episode_Reward/lifting_object: 168.6155
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 0.90s
                      Time elapsed: 00:08:50
                               ETA: 00:25:47

################################################################################
                     [1m Learning iteration 511/2000 [0m                      

                       Computation: 97905 steps/s (collection: 0.870s, learning 0.135s)
             Mean action noise std: 1.74
          Mean value_function loss: 42.7859
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.3611
                       Mean reward: 859.27
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.7530
     Episode_Reward/lifting_object: 170.2401
      Episode_Reward/object_height: 0.0648
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 1.00s
                      Time elapsed: 00:08:51
                               ETA: 00:25:46

################################################################################
                     [1m Learning iteration 512/2000 [0m                      

                       Computation: 109388 steps/s (collection: 0.803s, learning 0.096s)
             Mean action noise std: 1.74
          Mean value_function loss: 45.0254
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.3716
                       Mean reward: 854.68
               Mean episode length: 247.08
    Episode_Reward/reaching_object: 0.7515
     Episode_Reward/lifting_object: 170.3075
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 0.90s
                      Time elapsed: 00:08:52
                               ETA: 00:25:45

################################################################################
                     [1m Learning iteration 513/2000 [0m                      

                       Computation: 99396 steps/s (collection: 0.819s, learning 0.170s)
             Mean action noise std: 1.74
          Mean value_function loss: 38.0443
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.3837
                       Mean reward: 871.67
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 171.7132
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 0.99s
                      Time elapsed: 00:08:53
                               ETA: 00:25:43

################################################################################
                     [1m Learning iteration 514/2000 [0m                      

                       Computation: 85129 steps/s (collection: 1.057s, learning 0.098s)
             Mean action noise std: 1.74
          Mean value_function loss: 39.3948
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.3909
                       Mean reward: 848.40
               Mean episode length: 249.13
    Episode_Reward/reaching_object: 0.7624
     Episode_Reward/lifting_object: 171.5293
      Episode_Reward/object_height: 0.0655
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 1.15s
                      Time elapsed: 00:08:54
                               ETA: 00:25:43

################################################################################
                     [1m Learning iteration 515/2000 [0m                      

                       Computation: 87868 steps/s (collection: 0.995s, learning 0.124s)
             Mean action noise std: 1.74
          Mean value_function loss: 44.4158
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.3956
                       Mean reward: 862.69
               Mean episode length: 249.66
    Episode_Reward/reaching_object: 0.7508
     Episode_Reward/lifting_object: 170.3021
      Episode_Reward/object_height: 0.0651
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 1.12s
                      Time elapsed: 00:08:55
                               ETA: 00:25:42

################################################################################
                     [1m Learning iteration 516/2000 [0m                      

                       Computation: 101260 steps/s (collection: 0.880s, learning 0.091s)
             Mean action noise std: 1.75
          Mean value_function loss: 51.0251
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 15.3987
                       Mean reward: 871.85
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.7539
     Episode_Reward/lifting_object: 170.8916
      Episode_Reward/object_height: 0.0655
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 0.97s
                      Time elapsed: 00:08:56
                               ETA: 00:25:41

################################################################################
                     [1m Learning iteration 517/2000 [0m                      

                       Computation: 101664 steps/s (collection: 0.845s, learning 0.122s)
             Mean action noise std: 1.75
          Mean value_function loss: 34.0643
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.4001
                       Mean reward: 863.66
               Mean episode length: 249.81
    Episode_Reward/reaching_object: 0.7563
     Episode_Reward/lifting_object: 170.5336
      Episode_Reward/object_height: 0.0655
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 0.97s
                      Time elapsed: 00:08:57
                               ETA: 00:25:39

################################################################################
                     [1m Learning iteration 518/2000 [0m                      

                       Computation: 107037 steps/s (collection: 0.813s, learning 0.105s)
             Mean action noise std: 1.75
          Mean value_function loss: 46.5758
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.4060
                       Mean reward: 857.36
               Mean episode length: 246.80
    Episode_Reward/reaching_object: 0.7489
     Episode_Reward/lifting_object: 168.8851
      Episode_Reward/object_height: 0.0648
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 0.92s
                      Time elapsed: 00:08:58
                               ETA: 00:25:38

################################################################################
                     [1m Learning iteration 519/2000 [0m                      

                       Computation: 110889 steps/s (collection: 0.790s, learning 0.097s)
             Mean action noise std: 1.75
          Mean value_function loss: 38.6555
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.4197
                       Mean reward: 833.68
               Mean episode length: 243.85
    Episode_Reward/reaching_object: 0.7501
     Episode_Reward/lifting_object: 169.8250
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 0.89s
                      Time elapsed: 00:08:59
                               ETA: 00:25:37

################################################################################
                     [1m Learning iteration 520/2000 [0m                      

                       Computation: 106299 steps/s (collection: 0.820s, learning 0.105s)
             Mean action noise std: 1.75
          Mean value_function loss: 53.8424
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 15.4284
                       Mean reward: 869.09
               Mean episode length: 247.76
    Episode_Reward/reaching_object: 0.7524
     Episode_Reward/lifting_object: 169.9109
      Episode_Reward/object_height: 0.0653
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 0.92s
                      Time elapsed: 00:09:00
                               ETA: 00:25:35

################################################################################
                     [1m Learning iteration 521/2000 [0m                      

                       Computation: 103873 steps/s (collection: 0.830s, learning 0.117s)
             Mean action noise std: 1.75
          Mean value_function loss: 51.0099
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.4311
                       Mean reward: 844.86
               Mean episode length: 241.90
    Episode_Reward/reaching_object: 0.7600
     Episode_Reward/lifting_object: 171.1143
      Episode_Reward/object_height: 0.0657
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 0.95s
                      Time elapsed: 00:09:01
                               ETA: 00:25:34

################################################################################
                     [1m Learning iteration 522/2000 [0m                      

                       Computation: 102220 steps/s (collection: 0.835s, learning 0.127s)
             Mean action noise std: 1.75
          Mean value_function loss: 50.5644
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.4343
                       Mean reward: 864.91
               Mean episode length: 247.10
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 172.5952
      Episode_Reward/object_height: 0.0664
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 0.96s
                      Time elapsed: 00:09:02
                               ETA: 00:25:33

################################################################################
                     [1m Learning iteration 523/2000 [0m                      

                       Computation: 108335 steps/s (collection: 0.818s, learning 0.089s)
             Mean action noise std: 1.75
          Mean value_function loss: 42.3995
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.4376
                       Mean reward: 852.12
               Mean episode length: 244.77
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 172.7997
      Episode_Reward/object_height: 0.0662
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 0.91s
                      Time elapsed: 00:09:03
                               ETA: 00:25:31

################################################################################
                     [1m Learning iteration 524/2000 [0m                      

                       Computation: 105735 steps/s (collection: 0.800s, learning 0.129s)
             Mean action noise std: 1.76
          Mean value_function loss: 40.3531
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.4496
                       Mean reward: 856.94
               Mean episode length: 247.20
    Episode_Reward/reaching_object: 0.7513
     Episode_Reward/lifting_object: 170.2852
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 0.93s
                      Time elapsed: 00:09:04
                               ETA: 00:25:30

################################################################################
                     [1m Learning iteration 525/2000 [0m                      

                       Computation: 102519 steps/s (collection: 0.805s, learning 0.154s)
             Mean action noise std: 1.76
          Mean value_function loss: 53.6063
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.4604
                       Mean reward: 862.61
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.7578
     Episode_Reward/lifting_object: 171.6635
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 0.96s
                      Time elapsed: 00:09:05
                               ETA: 00:25:29

################################################################################
                     [1m Learning iteration 526/2000 [0m                      

                       Computation: 101777 steps/s (collection: 0.820s, learning 0.146s)
             Mean action noise std: 1.76
          Mean value_function loss: 52.4420
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 15.4666
                       Mean reward: 840.68
               Mean episode length: 245.31
    Episode_Reward/reaching_object: 0.7499
     Episode_Reward/lifting_object: 169.5924
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 0.97s
                      Time elapsed: 00:09:06
                               ETA: 00:25:27

################################################################################
                     [1m Learning iteration 527/2000 [0m                      

                       Computation: 98764 steps/s (collection: 0.848s, learning 0.148s)
             Mean action noise std: 1.76
          Mean value_function loss: 51.3170
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.4681
                       Mean reward: 852.37
               Mean episode length: 245.42
    Episode_Reward/reaching_object: 0.7526
     Episode_Reward/lifting_object: 169.4847
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 1.00s
                      Time elapsed: 00:09:07
                               ETA: 00:25:26

################################################################################
                     [1m Learning iteration 528/2000 [0m                      

                       Computation: 101283 steps/s (collection: 0.824s, learning 0.147s)
             Mean action noise std: 1.76
          Mean value_function loss: 49.8030
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 15.4713
                       Mean reward: 872.64
               Mean episode length: 249.47
    Episode_Reward/reaching_object: 0.7542
     Episode_Reward/lifting_object: 170.9570
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 0.97s
                      Time elapsed: 00:09:08
                               ETA: 00:25:25

################################################################################
                     [1m Learning iteration 529/2000 [0m                      

                       Computation: 93100 steps/s (collection: 0.856s, learning 0.200s)
             Mean action noise std: 1.76
          Mean value_function loss: 46.7846
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.4756
                       Mean reward: 858.86
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7549
     Episode_Reward/lifting_object: 170.3834
      Episode_Reward/object_height: 0.0647
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 1.06s
                      Time elapsed: 00:09:09
                               ETA: 00:25:24

################################################################################
                     [1m Learning iteration 530/2000 [0m                      

                       Computation: 105929 steps/s (collection: 0.806s, learning 0.122s)
             Mean action noise std: 1.76
          Mean value_function loss: 33.5537
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.4792
                       Mean reward: 870.89
               Mean episode length: 248.67
    Episode_Reward/reaching_object: 0.7566
     Episode_Reward/lifting_object: 170.5860
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 0.93s
                      Time elapsed: 00:09:10
                               ETA: 00:25:23

################################################################################
                     [1m Learning iteration 531/2000 [0m                      

                       Computation: 110116 steps/s (collection: 0.799s, learning 0.094s)
             Mean action noise std: 1.77
          Mean value_function loss: 41.5348
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.4881
                       Mean reward: 858.98
               Mean episode length: 249.47
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 171.3225
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 0.89s
                      Time elapsed: 00:09:11
                               ETA: 00:25:21

################################################################################
                     [1m Learning iteration 532/2000 [0m                      

                       Computation: 102386 steps/s (collection: 0.866s, learning 0.094s)
             Mean action noise std: 1.77
          Mean value_function loss: 48.2707
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.4968
                       Mean reward: 863.70
               Mean episode length: 249.61
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 171.4890
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 0.96s
                      Time elapsed: 00:09:12
                               ETA: 00:25:20

################################################################################
                     [1m Learning iteration 533/2000 [0m                      

                       Computation: 104254 steps/s (collection: 0.824s, learning 0.119s)
             Mean action noise std: 1.77
          Mean value_function loss: 41.5135
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.5047
                       Mean reward: 866.95
               Mean episode length: 247.25
    Episode_Reward/reaching_object: 0.7452
     Episode_Reward/lifting_object: 167.8625
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 0.94s
                      Time elapsed: 00:09:13
                               ETA: 00:25:19

################################################################################
                     [1m Learning iteration 534/2000 [0m                      

                       Computation: 105616 steps/s (collection: 0.839s, learning 0.092s)
             Mean action noise std: 1.77
          Mean value_function loss: 41.3655
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.5127
                       Mean reward: 862.92
               Mean episode length: 247.08
    Episode_Reward/reaching_object: 0.7586
     Episode_Reward/lifting_object: 171.2683
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 0.93s
                      Time elapsed: 00:09:13
                               ETA: 00:25:17

################################################################################
                     [1m Learning iteration 535/2000 [0m                      

                       Computation: 105650 steps/s (collection: 0.824s, learning 0.107s)
             Mean action noise std: 1.78
          Mean value_function loss: 30.5779
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.5245
                       Mean reward: 875.97
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 173.5013
      Episode_Reward/object_height: 0.0657
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 0.93s
                      Time elapsed: 00:09:14
                               ETA: 00:25:16

################################################################################
                     [1m Learning iteration 536/2000 [0m                      

                       Computation: 106868 steps/s (collection: 0.817s, learning 0.103s)
             Mean action noise std: 1.78
          Mean value_function loss: 41.1436
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 15.5395
                       Mean reward: 883.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7663
     Episode_Reward/lifting_object: 173.0230
      Episode_Reward/object_height: 0.0657
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 0.92s
                      Time elapsed: 00:09:15
                               ETA: 00:25:15

################################################################################
                     [1m Learning iteration 537/2000 [0m                      

                       Computation: 99935 steps/s (collection: 0.869s, learning 0.115s)
             Mean action noise std: 1.78
          Mean value_function loss: 41.2289
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 15.5473
                       Mean reward: 848.73
               Mean episode length: 246.92
    Episode_Reward/reaching_object: 0.7529
     Episode_Reward/lifting_object: 170.1866
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 0.98s
                      Time elapsed: 00:09:16
                               ETA: 00:25:14

################################################################################
                     [1m Learning iteration 538/2000 [0m                      

                       Computation: 105604 steps/s (collection: 0.829s, learning 0.102s)
             Mean action noise std: 1.78
          Mean value_function loss: 54.8846
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.5555
                       Mean reward: 856.13
               Mean episode length: 245.68
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 171.8006
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 0.93s
                      Time elapsed: 00:09:17
                               ETA: 00:25:12

################################################################################
                     [1m Learning iteration 539/2000 [0m                      

                       Computation: 107839 steps/s (collection: 0.813s, learning 0.099s)
             Mean action noise std: 1.78
          Mean value_function loss: 38.5332
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.5665
                       Mean reward: 844.93
               Mean episode length: 245.40
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 170.3855
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 0.91s
                      Time elapsed: 00:09:18
                               ETA: 00:25:11

################################################################################
                     [1m Learning iteration 540/2000 [0m                      

                       Computation: 105987 steps/s (collection: 0.814s, learning 0.113s)
             Mean action noise std: 1.79
          Mean value_function loss: 35.5484
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.5729
                       Mean reward: 844.06
               Mean episode length: 245.18
    Episode_Reward/reaching_object: 0.7533
     Episode_Reward/lifting_object: 170.3587
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 0.93s
                      Time elapsed: 00:09:19
                               ETA: 00:25:10

################################################################################
                     [1m Learning iteration 541/2000 [0m                      

                       Computation: 105619 steps/s (collection: 0.819s, learning 0.112s)
             Mean action noise std: 1.79
          Mean value_function loss: 41.2303
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.5827
                       Mean reward: 866.26
               Mean episode length: 248.89
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 173.4619
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 0.93s
                      Time elapsed: 00:09:20
                               ETA: 00:25:08

################################################################################
                     [1m Learning iteration 542/2000 [0m                      

                       Computation: 104215 steps/s (collection: 0.817s, learning 0.127s)
             Mean action noise std: 1.79
          Mean value_function loss: 40.9140
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.5885
                       Mean reward: 870.05
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7566
     Episode_Reward/lifting_object: 170.2995
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 0.94s
                      Time elapsed: 00:09:21
                               ETA: 00:25:07

################################################################################
                     [1m Learning iteration 543/2000 [0m                      

                       Computation: 95347 steps/s (collection: 0.852s, learning 0.179s)
             Mean action noise std: 1.79
          Mean value_function loss: 51.7638
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.5930
                       Mean reward: 845.26
               Mean episode length: 246.13
    Episode_Reward/reaching_object: 0.7493
     Episode_Reward/lifting_object: 169.2067
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 1.03s
                      Time elapsed: 00:09:22
                               ETA: 00:25:06

################################################################################
                     [1m Learning iteration 544/2000 [0m                      

                       Computation: 94923 steps/s (collection: 0.904s, learning 0.132s)
             Mean action noise std: 1.79
          Mean value_function loss: 41.1788
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 15.6071
                       Mean reward: 854.94
               Mean episode length: 246.01
    Episode_Reward/reaching_object: 0.7545
     Episode_Reward/lifting_object: 170.0204
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 1.04s
                      Time elapsed: 00:09:23
                               ETA: 00:25:05

################################################################################
                     [1m Learning iteration 545/2000 [0m                      

                       Computation: 100742 steps/s (collection: 0.822s, learning 0.154s)
             Mean action noise std: 1.79
          Mean value_function loss: 47.2257
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.6187
                       Mean reward: 867.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 172.3711
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 0.98s
                      Time elapsed: 00:09:24
                               ETA: 00:25:04

################################################################################
                     [1m Learning iteration 546/2000 [0m                      

                       Computation: 106137 steps/s (collection: 0.808s, learning 0.118s)
             Mean action noise std: 1.80
          Mean value_function loss: 52.4119
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.6255
                       Mean reward: 858.05
               Mean episode length: 244.34
    Episode_Reward/reaching_object: 0.7527
     Episode_Reward/lifting_object: 171.0381
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 0.93s
                      Time elapsed: 00:09:25
                               ETA: 00:25:02

################################################################################
                     [1m Learning iteration 547/2000 [0m                      

                       Computation: 109662 steps/s (collection: 0.786s, learning 0.110s)
             Mean action noise std: 1.80
          Mean value_function loss: 41.1500
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 15.6321
                       Mean reward: 860.27
               Mean episode length: 247.65
    Episode_Reward/reaching_object: 0.7498
     Episode_Reward/lifting_object: 170.0121
      Episode_Reward/object_height: 0.0648
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 0.90s
                      Time elapsed: 00:09:26
                               ETA: 00:25:01

################################################################################
                     [1m Learning iteration 548/2000 [0m                      

                       Computation: 110099 steps/s (collection: 0.797s, learning 0.096s)
             Mean action noise std: 1.80
          Mean value_function loss: 52.0048
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.6399
                       Mean reward: 869.13
               Mean episode length: 247.00
    Episode_Reward/reaching_object: 0.7590
     Episode_Reward/lifting_object: 171.9071
      Episode_Reward/object_height: 0.0658
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 0.89s
                      Time elapsed: 00:09:27
                               ETA: 00:25:00

################################################################################
                     [1m Learning iteration 549/2000 [0m                      

                       Computation: 104479 steps/s (collection: 0.838s, learning 0.103s)
             Mean action noise std: 1.80
          Mean value_function loss: 47.4748
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.6525
                       Mean reward: 830.41
               Mean episode length: 243.00
    Episode_Reward/reaching_object: 0.7403
     Episode_Reward/lifting_object: 168.3275
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 0.94s
                      Time elapsed: 00:09:28
                               ETA: 00:24:58

################################################################################
                     [1m Learning iteration 550/2000 [0m                      

                       Computation: 105703 steps/s (collection: 0.832s, learning 0.098s)
             Mean action noise std: 1.80
          Mean value_function loss: 44.8150
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.6611
                       Mean reward: 879.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7530
     Episode_Reward/lifting_object: 172.5079
      Episode_Reward/object_height: 0.0665
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 0.93s
                      Time elapsed: 00:09:29
                               ETA: 00:24:57

################################################################################
                     [1m Learning iteration 551/2000 [0m                      

                       Computation: 106629 steps/s (collection: 0.806s, learning 0.116s)
             Mean action noise std: 1.81
          Mean value_function loss: 23.1701
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.6740
                       Mean reward: 854.39
               Mean episode length: 244.20
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 171.8127
      Episode_Reward/object_height: 0.0664
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 0.92s
                      Time elapsed: 00:09:29
                               ETA: 00:24:56

################################################################################
                     [1m Learning iteration 552/2000 [0m                      

                       Computation: 97616 steps/s (collection: 0.864s, learning 0.143s)
             Mean action noise std: 1.81
          Mean value_function loss: 37.6670
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.6920
                       Mean reward: 871.87
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 173.3083
      Episode_Reward/object_height: 0.0667
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 1.01s
                      Time elapsed: 00:09:30
                               ETA: 00:24:55

################################################################################
                     [1m Learning iteration 553/2000 [0m                      

                       Computation: 97524 steps/s (collection: 0.859s, learning 0.149s)
             Mean action noise std: 1.81
          Mean value_function loss: 31.7828
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.7063
                       Mean reward: 867.70
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7610
     Episode_Reward/lifting_object: 171.3398
      Episode_Reward/object_height: 0.0661
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 1.01s
                      Time elapsed: 00:09:31
                               ETA: 00:24:53

################################################################################
                     [1m Learning iteration 554/2000 [0m                      

                       Computation: 106517 steps/s (collection: 0.813s, learning 0.110s)
             Mean action noise std: 1.81
          Mean value_function loss: 38.8612
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.7147
                       Mean reward: 865.54
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7548
     Episode_Reward/lifting_object: 171.5564
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 0.92s
                      Time elapsed: 00:09:32
                               ETA: 00:24:52

################################################################################
                     [1m Learning iteration 555/2000 [0m                      

                       Computation: 103595 steps/s (collection: 0.835s, learning 0.114s)
             Mean action noise std: 1.82
          Mean value_function loss: 46.1763
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.7205
                       Mean reward: 836.43
               Mean episode length: 246.77
    Episode_Reward/reaching_object: 0.7497
     Episode_Reward/lifting_object: 170.8153
      Episode_Reward/object_height: 0.0661
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 0.95s
                      Time elapsed: 00:09:33
                               ETA: 00:24:51

################################################################################
                     [1m Learning iteration 556/2000 [0m                      

                       Computation: 104378 steps/s (collection: 0.847s, learning 0.095s)
             Mean action noise std: 1.82
          Mean value_function loss: 38.3124
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.7368
                       Mean reward: 869.93
               Mean episode length: 247.89
    Episode_Reward/reaching_object: 0.7500
     Episode_Reward/lifting_object: 170.8241
      Episode_Reward/object_height: 0.0659
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 0.94s
                      Time elapsed: 00:09:34
                               ETA: 00:24:50

################################################################################
                     [1m Learning iteration 557/2000 [0m                      

                       Computation: 107155 steps/s (collection: 0.799s, learning 0.119s)
             Mean action noise std: 1.83
          Mean value_function loss: 37.7388
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.7573
                       Mean reward: 861.65
               Mean episode length: 248.72
    Episode_Reward/reaching_object: 0.7481
     Episode_Reward/lifting_object: 169.1970
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 0.92s
                      Time elapsed: 00:09:35
                               ETA: 00:24:48

################################################################################
                     [1m Learning iteration 558/2000 [0m                      

                       Computation: 109030 steps/s (collection: 0.812s, learning 0.089s)
             Mean action noise std: 1.83
          Mean value_function loss: 37.7585
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.7742
                       Mean reward: 859.35
               Mean episode length: 248.72
    Episode_Reward/reaching_object: 0.7588
     Episode_Reward/lifting_object: 171.1218
      Episode_Reward/object_height: 0.0659
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 0.90s
                      Time elapsed: 00:09:36
                               ETA: 00:24:47

################################################################################
                     [1m Learning iteration 559/2000 [0m                      

                       Computation: 105597 steps/s (collection: 0.819s, learning 0.112s)
             Mean action noise std: 1.83
          Mean value_function loss: 42.0498
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.7809
                       Mean reward: 865.57
               Mean episode length: 247.43
    Episode_Reward/reaching_object: 0.7571
     Episode_Reward/lifting_object: 172.1799
      Episode_Reward/object_height: 0.0668
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 0.93s
                      Time elapsed: 00:09:37
                               ETA: 00:24:46

################################################################################
                     [1m Learning iteration 560/2000 [0m                      

                       Computation: 110261 steps/s (collection: 0.795s, learning 0.096s)
             Mean action noise std: 1.83
          Mean value_function loss: 41.0568
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.7862
                       Mean reward: 854.24
               Mean episode length: 246.32
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 173.4690
      Episode_Reward/object_height: 0.0672
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 0.89s
                      Time elapsed: 00:09:38
                               ETA: 00:24:44

################################################################################
                     [1m Learning iteration 561/2000 [0m                      

                       Computation: 95854 steps/s (collection: 0.849s, learning 0.177s)
             Mean action noise std: 1.83
          Mean value_function loss: 30.7475
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 15.7895
                       Mean reward: 875.54
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 173.5016
      Episode_Reward/object_height: 0.0672
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 1.03s
                      Time elapsed: 00:09:39
                               ETA: 00:24:43

################################################################################
                     [1m Learning iteration 562/2000 [0m                      

                       Computation: 104077 steps/s (collection: 0.854s, learning 0.091s)
             Mean action noise std: 1.83
          Mean value_function loss: 33.7434
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.7946
                       Mean reward: 857.88
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 0.7590
     Episode_Reward/lifting_object: 172.0543
      Episode_Reward/object_height: 0.0666
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 0.94s
                      Time elapsed: 00:09:40
                               ETA: 00:24:42

################################################################################
                     [1m Learning iteration 563/2000 [0m                      

                       Computation: 97385 steps/s (collection: 0.825s, learning 0.185s)
             Mean action noise std: 1.84
          Mean value_function loss: 30.9904
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.8072
                       Mean reward: 853.11
               Mean episode length: 246.74
    Episode_Reward/reaching_object: 0.7546
     Episode_Reward/lifting_object: 171.2482
      Episode_Reward/object_height: 0.0662
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 1.01s
                      Time elapsed: 00:09:41
                               ETA: 00:24:41

################################################################################
                     [1m Learning iteration 564/2000 [0m                      

                       Computation: 91084 steps/s (collection: 0.948s, learning 0.131s)
             Mean action noise std: 1.84
          Mean value_function loss: 40.6483
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.8253
                       Mean reward: 870.34
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 0.7634
     Episode_Reward/lifting_object: 173.1223
      Episode_Reward/object_height: 0.0671
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 1.08s
                      Time elapsed: 00:09:42
                               ETA: 00:24:40

################################################################################
                     [1m Learning iteration 565/2000 [0m                      

                       Computation: 101035 steps/s (collection: 0.846s, learning 0.127s)
             Mean action noise std: 1.84
          Mean value_function loss: 27.3907
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.8339
                       Mean reward: 860.33
               Mean episode length: 247.88
    Episode_Reward/reaching_object: 0.7583
     Episode_Reward/lifting_object: 170.9697
      Episode_Reward/object_height: 0.0661
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 0.97s
                      Time elapsed: 00:09:43
                               ETA: 00:24:39

################################################################################
                     [1m Learning iteration 566/2000 [0m                      

                       Computation: 101406 steps/s (collection: 0.861s, learning 0.109s)
             Mean action noise std: 1.84
          Mean value_function loss: 30.8416
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.8410
                       Mean reward: 871.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7667
     Episode_Reward/lifting_object: 173.5155
      Episode_Reward/object_height: 0.0668
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 0.97s
                      Time elapsed: 00:09:44
                               ETA: 00:24:38

################################################################################
                     [1m Learning iteration 567/2000 [0m                      

                       Computation: 101106 steps/s (collection: 0.796s, learning 0.176s)
             Mean action noise std: 1.84
          Mean value_function loss: 39.1559
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.8437
                       Mean reward: 864.54
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 172.6030
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 0.97s
                      Time elapsed: 00:09:45
                               ETA: 00:24:36

################################################################################
                     [1m Learning iteration 568/2000 [0m                      

                       Computation: 98768 steps/s (collection: 0.825s, learning 0.171s)
             Mean action noise std: 1.85
          Mean value_function loss: 22.6884
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 15.8501
                       Mean reward: 860.11
               Mean episode length: 244.45
    Episode_Reward/reaching_object: 0.7575
     Episode_Reward/lifting_object: 171.4866
      Episode_Reward/object_height: 0.0658
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 1.00s
                      Time elapsed: 00:09:46
                               ETA: 00:24:35

################################################################################
                     [1m Learning iteration 569/2000 [0m                      

                       Computation: 108410 steps/s (collection: 0.798s, learning 0.109s)
             Mean action noise std: 1.85
          Mean value_function loss: 37.3209
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.8668
                       Mean reward: 876.81
               Mean episode length: 249.41
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 172.0392
      Episode_Reward/object_height: 0.0660
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 0.91s
                      Time elapsed: 00:09:47
                               ETA: 00:24:34

################################################################################
                     [1m Learning iteration 570/2000 [0m                      

                       Computation: 103223 steps/s (collection: 0.828s, learning 0.125s)
             Mean action noise std: 1.85
          Mean value_function loss: 31.2394
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.8795
                       Mean reward: 866.86
               Mean episode length: 249.23
    Episode_Reward/reaching_object: 0.7594
     Episode_Reward/lifting_object: 171.6343
      Episode_Reward/object_height: 0.0658
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 0.95s
                      Time elapsed: 00:09:48
                               ETA: 00:24:33

################################################################################
                     [1m Learning iteration 571/2000 [0m                      

                       Computation: 103657 steps/s (collection: 0.832s, learning 0.116s)
             Mean action noise std: 1.86
          Mean value_function loss: 39.2257
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.8923
                       Mean reward: 871.50
               Mean episode length: 248.17
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 172.1046
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 0.95s
                      Time elapsed: 00:09:49
                               ETA: 00:24:31

################################################################################
                     [1m Learning iteration 572/2000 [0m                      

                       Computation: 101946 steps/s (collection: 0.849s, learning 0.116s)
             Mean action noise std: 1.86
          Mean value_function loss: 29.4662
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.9093
                       Mean reward: 877.52
               Mean episode length: 249.21
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 172.0608
      Episode_Reward/object_height: 0.0661
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 0.96s
                      Time elapsed: 00:09:50
                               ETA: 00:24:30

################################################################################
                     [1m Learning iteration 573/2000 [0m                      

                       Computation: 97384 steps/s (collection: 0.874s, learning 0.135s)
             Mean action noise std: 1.86
          Mean value_function loss: 39.5183
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.9190
                       Mean reward: 858.03
               Mean episode length: 247.72
    Episode_Reward/reaching_object: 0.7518
     Episode_Reward/lifting_object: 170.7506
      Episode_Reward/object_height: 0.0653
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 1.01s
                      Time elapsed: 00:09:51
                               ETA: 00:24:29

################################################################################
                     [1m Learning iteration 574/2000 [0m                      

                       Computation: 104479 steps/s (collection: 0.839s, learning 0.102s)
             Mean action noise std: 1.86
          Mean value_function loss: 36.3246
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.9292
                       Mean reward: 863.81
               Mean episode length: 246.48
    Episode_Reward/reaching_object: 0.7684
     Episode_Reward/lifting_object: 173.2523
      Episode_Reward/object_height: 0.0665
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 0.94s
                      Time elapsed: 00:09:52
                               ETA: 00:24:28

################################################################################
                     [1m Learning iteration 575/2000 [0m                      

                       Computation: 103470 steps/s (collection: 0.853s, learning 0.098s)
             Mean action noise std: 1.87
          Mean value_function loss: 40.4043
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.9433
                       Mean reward: 861.90
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 171.8575
      Episode_Reward/object_height: 0.0660
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 0.95s
                      Time elapsed: 00:09:53
                               ETA: 00:24:27

################################################################################
                     [1m Learning iteration 576/2000 [0m                      

                       Computation: 106582 steps/s (collection: 0.824s, learning 0.098s)
             Mean action noise std: 1.87
          Mean value_function loss: 39.3147
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.9554
                       Mean reward: 866.26
               Mean episode length: 246.81
    Episode_Reward/reaching_object: 0.7583
     Episode_Reward/lifting_object: 170.8711
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 0.92s
                      Time elapsed: 00:09:53
                               ETA: 00:24:25

################################################################################
                     [1m Learning iteration 577/2000 [0m                      

                       Computation: 95798 steps/s (collection: 0.856s, learning 0.170s)
             Mean action noise std: 1.87
          Mean value_function loss: 34.0113
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.9700
                       Mean reward: 862.98
               Mean episode length: 247.70
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 171.1184
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 1.03s
                      Time elapsed: 00:09:55
                               ETA: 00:24:24

################################################################################
                     [1m Learning iteration 578/2000 [0m                      

                       Computation: 82038 steps/s (collection: 1.030s, learning 0.168s)
             Mean action noise std: 1.88
          Mean value_function loss: 45.7358
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.9958
                       Mean reward: 870.82
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 173.8077
      Episode_Reward/object_height: 0.0666
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 1.20s
                      Time elapsed: 00:09:56
                               ETA: 00:24:24

################################################################################
                     [1m Learning iteration 579/2000 [0m                      

                       Computation: 79149 steps/s (collection: 1.087s, learning 0.155s)
             Mean action noise std: 1.88
          Mean value_function loss: 34.7029
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 16.0195
                       Mean reward: 860.17
               Mean episode length: 248.89
    Episode_Reward/reaching_object: 0.7560
     Episode_Reward/lifting_object: 172.1356
      Episode_Reward/object_height: 0.0661
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 1.24s
                      Time elapsed: 00:09:57
                               ETA: 00:24:23

################################################################################
                     [1m Learning iteration 580/2000 [0m                      

                       Computation: 96897 steps/s (collection: 0.873s, learning 0.141s)
             Mean action noise std: 1.89
          Mean value_function loss: 42.8238
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.0300
                       Mean reward: 872.62
               Mean episode length: 247.30
    Episode_Reward/reaching_object: 0.7609
     Episode_Reward/lifting_object: 171.6691
      Episode_Reward/object_height: 0.0657
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 1.01s
                      Time elapsed: 00:09:58
                               ETA: 00:24:22

################################################################################
                     [1m Learning iteration 581/2000 [0m                      

                       Computation: 106382 steps/s (collection: 0.823s, learning 0.101s)
             Mean action noise std: 1.89
          Mean value_function loss: 31.1456
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.0429
                       Mean reward: 842.92
               Mean episode length: 244.48
    Episode_Reward/reaching_object: 0.7518
     Episode_Reward/lifting_object: 171.1291
      Episode_Reward/object_height: 0.0656
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 0.92s
                      Time elapsed: 00:09:59
                               ETA: 00:24:21

################################################################################
                     [1m Learning iteration 582/2000 [0m                      

                       Computation: 108538 steps/s (collection: 0.807s, learning 0.099s)
             Mean action noise std: 1.89
          Mean value_function loss: 27.6826
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.0586
                       Mean reward: 844.05
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7447
     Episode_Reward/lifting_object: 170.7771
      Episode_Reward/object_height: 0.0655
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 0.91s
                      Time elapsed: 00:10:00
                               ETA: 00:24:20

################################################################################
                     [1m Learning iteration 583/2000 [0m                      

                       Computation: 100572 steps/s (collection: 0.832s, learning 0.145s)
             Mean action noise std: 1.90
          Mean value_function loss: 33.5347
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.0713
                       Mean reward: 859.07
               Mean episode length: 248.66
    Episode_Reward/reaching_object: 0.7547
     Episode_Reward/lifting_object: 172.4375
      Episode_Reward/object_height: 0.0662
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 0.98s
                      Time elapsed: 00:10:01
                               ETA: 00:24:18

################################################################################
                     [1m Learning iteration 584/2000 [0m                      

                       Computation: 101835 steps/s (collection: 0.841s, learning 0.125s)
             Mean action noise std: 1.90
          Mean value_function loss: 37.1369
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.0797
                       Mean reward: 867.64
               Mean episode length: 248.79
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 173.3358
      Episode_Reward/object_height: 0.0664
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 0.97s
                      Time elapsed: 00:10:02
                               ETA: 00:24:17

################################################################################
                     [1m Learning iteration 585/2000 [0m                      

                       Computation: 92775 steps/s (collection: 0.812s, learning 0.247s)
             Mean action noise std: 1.90
          Mean value_function loss: 43.1381
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.0972
                       Mean reward: 867.28
               Mean episode length: 247.11
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 171.9695
      Episode_Reward/object_height: 0.0660
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 1.06s
                      Time elapsed: 00:10:03
                               ETA: 00:24:16

################################################################################
                     [1m Learning iteration 586/2000 [0m                      

                       Computation: 91598 steps/s (collection: 0.960s, learning 0.114s)
             Mean action noise std: 1.91
          Mean value_function loss: 42.2903
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.1158
                       Mean reward: 880.44
               Mean episode length: 249.72
    Episode_Reward/reaching_object: 0.7606
     Episode_Reward/lifting_object: 173.2537
      Episode_Reward/object_height: 0.0665
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 1.07s
                      Time elapsed: 00:10:04
                               ETA: 00:24:15

################################################################################
                     [1m Learning iteration 587/2000 [0m                      

                       Computation: 97384 steps/s (collection: 0.875s, learning 0.134s)
             Mean action noise std: 1.91
          Mean value_function loss: 37.3311
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.1246
                       Mean reward: 864.69
               Mean episode length: 248.00
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 172.4650
      Episode_Reward/object_height: 0.0664
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 1.01s
                      Time elapsed: 00:10:05
                               ETA: 00:24:14

################################################################################
                     [1m Learning iteration 588/2000 [0m                      

                       Computation: 101448 steps/s (collection: 0.850s, learning 0.119s)
             Mean action noise std: 1.91
          Mean value_function loss: 50.1577
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.1304
                       Mean reward: 858.20
               Mean episode length: 247.75
    Episode_Reward/reaching_object: 0.7484
     Episode_Reward/lifting_object: 170.2449
      Episode_Reward/object_height: 0.0656
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 0.97s
                      Time elapsed: 00:10:06
                               ETA: 00:24:13

################################################################################
                     [1m Learning iteration 589/2000 [0m                      

                       Computation: 100752 steps/s (collection: 0.815s, learning 0.161s)
             Mean action noise std: 1.91
          Mean value_function loss: 38.0345
               Mean surrogate loss: 0.0581
                 Mean entropy loss: 16.1419
                       Mean reward: 862.60
               Mean episode length: 247.07
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 170.8409
      Episode_Reward/object_height: 0.0661
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 0.98s
                      Time elapsed: 00:10:07
                               ETA: 00:24:12

################################################################################
                     [1m Learning iteration 590/2000 [0m                      

                       Computation: 95676 steps/s (collection: 0.902s, learning 0.126s)
             Mean action noise std: 1.91
          Mean value_function loss: 130.6604
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.1460
                       Mean reward: 870.06
               Mean episode length: 248.47
    Episode_Reward/reaching_object: 0.7561
     Episode_Reward/lifting_object: 171.7890
      Episode_Reward/object_height: 0.0666
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 1.03s
                      Time elapsed: 00:10:08
                               ETA: 00:24:11

################################################################################
                     [1m Learning iteration 591/2000 [0m                      

                       Computation: 99904 steps/s (collection: 0.841s, learning 0.143s)
             Mean action noise std: 1.92
          Mean value_function loss: 166.8191
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 16.1659
                       Mean reward: 869.91
               Mean episode length: 249.64
    Episode_Reward/reaching_object: 0.7627
     Episode_Reward/lifting_object: 172.2744
      Episode_Reward/object_height: 0.0668
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 0.98s
                      Time elapsed: 00:10:09
                               ETA: 00:24:10

################################################################################
                     [1m Learning iteration 592/2000 [0m                      

                       Computation: 95800 steps/s (collection: 0.851s, learning 0.175s)
             Mean action noise std: 1.92
          Mean value_function loss: 209.1926
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 16.1907
                       Mean reward: 873.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 171.6988
      Episode_Reward/object_height: 0.0668
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 1.03s
                      Time elapsed: 00:10:10
                               ETA: 00:24:09

################################################################################
                     [1m Learning iteration 593/2000 [0m                      

                       Computation: 99591 steps/s (collection: 0.895s, learning 0.093s)
             Mean action noise std: 1.93
          Mean value_function loss: 227.0009
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 16.2127
                       Mean reward: 874.65
               Mean episode length: 249.75
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 171.5587
      Episode_Reward/object_height: 0.0666
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 0.99s
                      Time elapsed: 00:10:11
                               ETA: 00:24:08

################################################################################
                     [1m Learning iteration 594/2000 [0m                      

                       Computation: 95291 steps/s (collection: 0.859s, learning 0.173s)
             Mean action noise std: 1.93
          Mean value_function loss: 246.2337
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 16.2311
                       Mean reward: 854.90
               Mean episode length: 245.65
    Episode_Reward/reaching_object: 0.7669
     Episode_Reward/lifting_object: 172.3853
      Episode_Reward/object_height: 0.0669
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 1.03s
                      Time elapsed: 00:10:12
                               ETA: 00:24:07

################################################################################
                     [1m Learning iteration 595/2000 [0m                      

                       Computation: 107881 steps/s (collection: 0.809s, learning 0.102s)
             Mean action noise std: 1.94
          Mean value_function loss: 244.3521
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 16.2554
                       Mean reward: 817.06
               Mean episode length: 241.15
    Episode_Reward/reaching_object: 0.7464
     Episode_Reward/lifting_object: 168.9847
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 0.91s
                      Time elapsed: 00:10:13
                               ETA: 00:24:05

################################################################################
                     [1m Learning iteration 596/2000 [0m                      

                       Computation: 95327 steps/s (collection: 0.874s, learning 0.158s)
             Mean action noise std: 1.94
          Mean value_function loss: 244.6190
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 16.2806
                       Mean reward: 850.64
               Mean episode length: 247.09
    Episode_Reward/reaching_object: 0.7490
     Episode_Reward/lifting_object: 167.5202
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 1.03s
                      Time elapsed: 00:10:14
                               ETA: 00:24:04

################################################################################
                     [1m Learning iteration 597/2000 [0m                      

                       Computation: 100351 steps/s (collection: 0.883s, learning 0.097s)
             Mean action noise std: 1.95
          Mean value_function loss: 273.3419
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 16.3039
                       Mean reward: 857.73
               Mean episode length: 246.74
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 170.9547
      Episode_Reward/object_height: 0.0659
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 0.98s
                      Time elapsed: 00:10:15
                               ETA: 00:24:03

################################################################################
                     [1m Learning iteration 598/2000 [0m                      

                       Computation: 98913 steps/s (collection: 0.868s, learning 0.126s)
             Mean action noise std: 1.95
          Mean value_function loss: 268.7755
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 16.3133
                       Mean reward: 831.36
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.7284
     Episode_Reward/lifting_object: 163.6457
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 0.99s
                      Time elapsed: 00:10:16
                               ETA: 00:24:02

################################################################################
                     [1m Learning iteration 599/2000 [0m                      

                       Computation: 100166 steps/s (collection: 0.865s, learning 0.117s)
             Mean action noise std: 1.95
          Mean value_function loss: 295.9026
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 16.3302
                       Mean reward: 543.75
               Mean episode length: 241.79
    Episode_Reward/reaching_object: 0.6350
     Episode_Reward/lifting_object: 132.3685
      Episode_Reward/object_height: 0.0501
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 0.98s
                      Time elapsed: 00:10:17
                               ETA: 00:24:01

################################################################################
                     [1m Learning iteration 600/2000 [0m                      

                       Computation: 100996 steps/s (collection: 0.830s, learning 0.143s)
             Mean action noise std: 1.96
          Mean value_function loss: 273.0899
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 16.3469
                       Mean reward: 582.57
               Mean episode length: 247.42
    Episode_Reward/reaching_object: 0.5557
     Episode_Reward/lifting_object: 106.0390
      Episode_Reward/object_height: 0.0397
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 0.97s
                      Time elapsed: 00:10:18
                               ETA: 00:24:00

################################################################################
                     [1m Learning iteration 601/2000 [0m                      

                       Computation: 102091 steps/s (collection: 0.828s, learning 0.135s)
             Mean action noise std: 1.96
          Mean value_function loss: 258.0128
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.3611
                       Mean reward: 604.77
               Mean episode length: 243.07
    Episode_Reward/reaching_object: 0.5784
     Episode_Reward/lifting_object: 114.7182
      Episode_Reward/object_height: 0.0430
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 0.96s
                      Time elapsed: 00:10:19
                               ETA: 00:23:59

################################################################################
                     [1m Learning iteration 602/2000 [0m                      

                       Computation: 91814 steps/s (collection: 0.892s, learning 0.179s)
             Mean action noise std: 1.96
          Mean value_function loss: 238.2089
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 16.3730
                       Mean reward: 590.27
               Mean episode length: 246.15
    Episode_Reward/reaching_object: 0.5835
     Episode_Reward/lifting_object: 115.2529
      Episode_Reward/object_height: 0.0435
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 1.07s
                      Time elapsed: 00:10:20
                               ETA: 00:23:58

################################################################################
                     [1m Learning iteration 603/2000 [0m                      

                       Computation: 94943 steps/s (collection: 0.884s, learning 0.151s)
             Mean action noise std: 1.96
          Mean value_function loss: 238.1379
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 16.3814
                       Mean reward: 563.20
               Mean episode length: 240.87
    Episode_Reward/reaching_object: 0.5741
     Episode_Reward/lifting_object: 114.0222
      Episode_Reward/object_height: 0.0429
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 1.04s
                      Time elapsed: 00:10:21
                               ETA: 00:23:57

################################################################################
                     [1m Learning iteration 604/2000 [0m                      

                       Computation: 90325 steps/s (collection: 0.955s, learning 0.134s)
             Mean action noise std: 1.97
          Mean value_function loss: 193.1173
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 16.3920
                       Mean reward: 592.52
               Mean episode length: 242.77
    Episode_Reward/reaching_object: 0.5805
     Episode_Reward/lifting_object: 115.7040
      Episode_Reward/object_height: 0.0436
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 1.09s
                      Time elapsed: 00:10:22
                               ETA: 00:23:56

################################################################################
                     [1m Learning iteration 605/2000 [0m                      

                       Computation: 95314 steps/s (collection: 0.917s, learning 0.114s)
             Mean action noise std: 1.97
          Mean value_function loss: 187.8550
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 16.4029
                       Mean reward: 578.94
               Mean episode length: 242.81
    Episode_Reward/reaching_object: 0.5928
     Episode_Reward/lifting_object: 119.6262
      Episode_Reward/object_height: 0.0453
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 1.03s
                      Time elapsed: 00:10:23
                               ETA: 00:23:55

################################################################################
                     [1m Learning iteration 606/2000 [0m                      

                       Computation: 103167 steps/s (collection: 0.832s, learning 0.121s)
             Mean action noise std: 1.97
          Mean value_function loss: 193.1571
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 16.4114
                       Mean reward: 692.46
               Mean episode length: 249.54
    Episode_Reward/reaching_object: 0.6218
     Episode_Reward/lifting_object: 126.7795
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 0.95s
                      Time elapsed: 00:10:24
                               ETA: 00:23:53

################################################################################
                     [1m Learning iteration 607/2000 [0m                      

                       Computation: 97603 steps/s (collection: 0.875s, learning 0.133s)
             Mean action noise std: 1.97
          Mean value_function loss: 194.2807
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 16.4206
                       Mean reward: 642.70
               Mean episode length: 245.08
    Episode_Reward/reaching_object: 0.5995
     Episode_Reward/lifting_object: 123.4652
      Episode_Reward/object_height: 0.0468
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 1.01s
                      Time elapsed: 00:10:25
                               ETA: 00:23:52

################################################################################
                     [1m Learning iteration 608/2000 [0m                      

                       Computation: 101611 steps/s (collection: 0.858s, learning 0.109s)
             Mean action noise std: 1.98
          Mean value_function loss: 195.4416
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 16.4313
                       Mean reward: 637.21
               Mean episode length: 243.92
    Episode_Reward/reaching_object: 0.6073
     Episode_Reward/lifting_object: 123.2644
      Episode_Reward/object_height: 0.0467
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 0.97s
                      Time elapsed: 00:10:26
                               ETA: 00:23:51

################################################################################
                     [1m Learning iteration 609/2000 [0m                      

                       Computation: 97155 steps/s (collection: 0.900s, learning 0.111s)
             Mean action noise std: 1.98
          Mean value_function loss: 205.0822
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.4464
                       Mean reward: 667.19
               Mean episode length: 246.40
    Episode_Reward/reaching_object: 0.6106
     Episode_Reward/lifting_object: 126.8083
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 1.01s
                      Time elapsed: 00:10:27
                               ETA: 00:23:50

################################################################################
                     [1m Learning iteration 610/2000 [0m                      

                       Computation: 104929 steps/s (collection: 0.843s, learning 0.094s)
             Mean action noise std: 1.98
          Mean value_function loss: 194.8455
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 16.4626
                       Mean reward: 634.97
               Mean episode length: 244.24
    Episode_Reward/reaching_object: 0.6159
     Episode_Reward/lifting_object: 127.8809
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 0.94s
                      Time elapsed: 00:10:28
                               ETA: 00:23:49

################################################################################
                     [1m Learning iteration 611/2000 [0m                      

                       Computation: 95614 steps/s (collection: 0.859s, learning 0.169s)
             Mean action noise std: 1.98
          Mean value_function loss: 196.6001
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.4691
                       Mean reward: 673.42
               Mean episode length: 246.38
    Episode_Reward/reaching_object: 0.6292
     Episode_Reward/lifting_object: 131.7525
      Episode_Reward/object_height: 0.0505
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 1.03s
                      Time elapsed: 00:10:29
                               ETA: 00:23:48

################################################################################
                     [1m Learning iteration 612/2000 [0m                      

                       Computation: 94239 steps/s (collection: 0.939s, learning 0.105s)
             Mean action noise std: 1.99
          Mean value_function loss: 188.4981
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.4750
                       Mean reward: 672.39
               Mean episode length: 242.89
    Episode_Reward/reaching_object: 0.6262
     Episode_Reward/lifting_object: 132.0660
      Episode_Reward/object_height: 0.0509
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 1.04s
                      Time elapsed: 00:10:30
                               ETA: 00:23:47

################################################################################
                     [1m Learning iteration 613/2000 [0m                      

                       Computation: 88852 steps/s (collection: 0.897s, learning 0.209s)
             Mean action noise std: 1.99
          Mean value_function loss: 183.2873
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 16.4852
                       Mean reward: 637.78
               Mean episode length: 242.28
    Episode_Reward/reaching_object: 0.6288
     Episode_Reward/lifting_object: 131.0240
      Episode_Reward/object_height: 0.0502
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 1.11s
                      Time elapsed: 00:10:31
                               ETA: 00:23:46

################################################################################
                     [1m Learning iteration 614/2000 [0m                      

                       Computation: 97188 steps/s (collection: 0.882s, learning 0.129s)
             Mean action noise std: 1.99
          Mean value_function loss: 198.4163
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 16.4917
                       Mean reward: 683.93
               Mean episode length: 247.12
    Episode_Reward/reaching_object: 0.6293
     Episode_Reward/lifting_object: 132.5417
      Episode_Reward/object_height: 0.0509
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 1.01s
                      Time elapsed: 00:10:32
                               ETA: 00:23:45

################################################################################
                     [1m Learning iteration 615/2000 [0m                      

                       Computation: 102054 steps/s (collection: 0.862s, learning 0.101s)
             Mean action noise std: 1.99
          Mean value_function loss: 187.3417
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 16.4980
                       Mean reward: 701.75
               Mean episode length: 245.73
    Episode_Reward/reaching_object: 0.6271
     Episode_Reward/lifting_object: 132.4252
      Episode_Reward/object_height: 0.0510
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 0.96s
                      Time elapsed: 00:10:33
                               ETA: 00:23:44

################################################################################
                     [1m Learning iteration 616/2000 [0m                      

                       Computation: 97973 steps/s (collection: 0.907s, learning 0.096s)
             Mean action noise std: 1.99
          Mean value_function loss: 179.7383
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 16.5053
                       Mean reward: 712.71
               Mean episode length: 240.84
    Episode_Reward/reaching_object: 0.6450
     Episode_Reward/lifting_object: 137.2540
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 1.00s
                      Time elapsed: 00:10:34
                               ETA: 00:23:43

################################################################################
                     [1m Learning iteration 617/2000 [0m                      

                       Computation: 101947 steps/s (collection: 0.866s, learning 0.099s)
             Mean action noise std: 1.99
          Mean value_function loss: 195.0517
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.5073
                       Mean reward: 630.67
               Mean episode length: 241.44
    Episode_Reward/reaching_object: 0.6310
     Episode_Reward/lifting_object: 133.1109
      Episode_Reward/object_height: 0.0516
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 0.96s
                      Time elapsed: 00:10:35
                               ETA: 00:23:42

################################################################################
                     [1m Learning iteration 618/2000 [0m                      

                       Computation: 102627 steps/s (collection: 0.834s, learning 0.124s)
             Mean action noise std: 1.99
          Mean value_function loss: 185.4856
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 16.5040
                       Mean reward: 685.84
               Mean episode length: 245.17
    Episode_Reward/reaching_object: 0.6413
     Episode_Reward/lifting_object: 137.4900
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 0.96s
                      Time elapsed: 00:10:36
                               ETA: 00:23:40

################################################################################
                     [1m Learning iteration 619/2000 [0m                      

                       Computation: 103944 steps/s (collection: 0.835s, learning 0.111s)
             Mean action noise std: 2.00
          Mean value_function loss: 175.2257
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 16.5103
                       Mean reward: 621.42
               Mean episode length: 242.06
    Episode_Reward/reaching_object: 0.6321
     Episode_Reward/lifting_object: 132.5558
      Episode_Reward/object_height: 0.0516
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 0.95s
                      Time elapsed: 00:10:37
                               ETA: 00:23:39

################################################################################
                     [1m Learning iteration 620/2000 [0m                      

                       Computation: 102315 steps/s (collection: 0.844s, learning 0.117s)
             Mean action noise std: 2.00
          Mean value_function loss: 219.2599
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 16.5217
                       Mean reward: 716.76
               Mean episode length: 248.80
    Episode_Reward/reaching_object: 0.6568
     Episode_Reward/lifting_object: 141.2562
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 0.96s
                      Time elapsed: 00:10:38
                               ETA: 00:23:38

################################################################################
                     [1m Learning iteration 621/2000 [0m                      

                       Computation: 104346 steps/s (collection: 0.836s, learning 0.106s)
             Mean action noise std: 2.00
          Mean value_function loss: 251.6893
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.5337
                       Mean reward: 697.31
               Mean episode length: 247.58
    Episode_Reward/reaching_object: 0.6423
     Episode_Reward/lifting_object: 136.6209
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 0.94s
                      Time elapsed: 00:10:39
                               ETA: 00:23:37

################################################################################
                     [1m Learning iteration 622/2000 [0m                      

                       Computation: 104403 steps/s (collection: 0.819s, learning 0.123s)
             Mean action noise std: 2.01
          Mean value_function loss: 278.0104
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 16.5548
                       Mean reward: 679.03
               Mean episode length: 242.39
    Episode_Reward/reaching_object: 0.6352
     Episode_Reward/lifting_object: 136.0897
      Episode_Reward/object_height: 0.0527
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 0.94s
                      Time elapsed: 00:10:40
                               ETA: 00:23:36

################################################################################
                     [1m Learning iteration 623/2000 [0m                      

                       Computation: 102789 steps/s (collection: 0.848s, learning 0.109s)
             Mean action noise std: 2.01
          Mean value_function loss: 240.4058
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 16.5706
                       Mean reward: 697.68
               Mean episode length: 241.72
    Episode_Reward/reaching_object: 0.6514
     Episode_Reward/lifting_object: 139.4442
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 0.96s
                      Time elapsed: 00:10:41
                               ETA: 00:23:34

################################################################################
                     [1m Learning iteration 624/2000 [0m                      

                       Computation: 90514 steps/s (collection: 0.880s, learning 0.206s)
             Mean action noise std: 2.01
          Mean value_function loss: 243.2533
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 16.5786
                       Mean reward: 739.56
               Mean episode length: 247.00
    Episode_Reward/reaching_object: 0.6501
     Episode_Reward/lifting_object: 139.8030
      Episode_Reward/object_height: 0.0541
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 1.09s
                      Time elapsed: 00:10:42
                               ETA: 00:23:33

################################################################################
                     [1m Learning iteration 625/2000 [0m                      

                       Computation: 94473 steps/s (collection: 0.911s, learning 0.129s)
             Mean action noise std: 2.01
          Mean value_function loss: 237.9666
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 16.5830
                       Mean reward: 710.19
               Mean episode length: 241.97
    Episode_Reward/reaching_object: 0.6576
     Episode_Reward/lifting_object: 142.0227
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 1.04s
                      Time elapsed: 00:10:43
                               ETA: 00:23:32

################################################################################
                     [1m Learning iteration 626/2000 [0m                      

                       Computation: 89595 steps/s (collection: 0.896s, learning 0.201s)
             Mean action noise std: 2.01
          Mean value_function loss: 240.5414
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 16.5902
                       Mean reward: 690.64
               Mean episode length: 239.59
    Episode_Reward/reaching_object: 0.6431
     Episode_Reward/lifting_object: 138.9075
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 1.10s
                      Time elapsed: 00:10:44
                               ETA: 00:23:32

################################################################################
                     [1m Learning iteration 627/2000 [0m                      

                       Computation: 98726 steps/s (collection: 0.887s, learning 0.108s)
             Mean action noise std: 2.02
          Mean value_function loss: 230.6641
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.5989
                       Mean reward: 709.17
               Mean episode length: 241.63
    Episode_Reward/reaching_object: 0.6484
     Episode_Reward/lifting_object: 138.9379
      Episode_Reward/object_height: 0.0538
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 1.00s
                      Time elapsed: 00:10:45
                               ETA: 00:23:30

################################################################################
                     [1m Learning iteration 628/2000 [0m                      

                       Computation: 99296 steps/s (collection: 0.873s, learning 0.117s)
             Mean action noise std: 2.02
          Mean value_function loss: 228.8229
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 16.6071
                       Mean reward: 662.40
               Mean episode length: 238.10
    Episode_Reward/reaching_object: 0.6316
     Episode_Reward/lifting_object: 135.3923
      Episode_Reward/object_height: 0.0527
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 0.99s
                      Time elapsed: 00:10:46
                               ETA: 00:23:29

################################################################################
                     [1m Learning iteration 629/2000 [0m                      

                       Computation: 99930 steps/s (collection: 0.861s, learning 0.123s)
             Mean action noise std: 2.02
          Mean value_function loss: 241.2131
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 16.6182
                       Mean reward: 651.20
               Mean episode length: 235.58
    Episode_Reward/reaching_object: 0.6089
     Episode_Reward/lifting_object: 128.2526
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 0.98s
                      Time elapsed: 00:10:47
                               ETA: 00:23:28

################################################################################
                     [1m Learning iteration 630/2000 [0m                      

                       Computation: 103873 steps/s (collection: 0.843s, learning 0.103s)
             Mean action noise std: 2.03
          Mean value_function loss: 215.6212
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.6286
                       Mean reward: 594.27
               Mean episode length: 239.10
    Episode_Reward/reaching_object: 0.5983
     Episode_Reward/lifting_object: 123.2720
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 0.95s
                      Time elapsed: 00:10:48
                               ETA: 00:23:27

################################################################################
                     [1m Learning iteration 631/2000 [0m                      

                       Computation: 95868 steps/s (collection: 0.884s, learning 0.142s)
             Mean action noise std: 2.03
          Mean value_function loss: 177.5025
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 16.6447
                       Mean reward: 682.03
               Mean episode length: 237.29
    Episode_Reward/reaching_object: 0.6311
     Episode_Reward/lifting_object: 135.3073
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 1.03s
                      Time elapsed: 00:10:49
                               ETA: 00:23:26

################################################################################
                     [1m Learning iteration 632/2000 [0m                      

                       Computation: 98514 steps/s (collection: 0.890s, learning 0.108s)
             Mean action noise std: 2.03
          Mean value_function loss: 189.9696
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 16.6492
                       Mean reward: 675.51
               Mean episode length: 242.74
    Episode_Reward/reaching_object: 0.6366
     Episode_Reward/lifting_object: 137.1597
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 1.00s
                      Time elapsed: 00:10:50
                               ETA: 00:23:25

################################################################################
                     [1m Learning iteration 633/2000 [0m                      

                       Computation: 96386 steps/s (collection: 0.851s, learning 0.169s)
             Mean action noise std: 2.03
          Mean value_function loss: 188.1746
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 16.6508
                       Mean reward: 690.53
               Mean episode length: 243.06
    Episode_Reward/reaching_object: 0.6400
     Episode_Reward/lifting_object: 138.5068
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 1.02s
                      Time elapsed: 00:10:51
                               ETA: 00:23:24

################################################################################
                     [1m Learning iteration 634/2000 [0m                      

                       Computation: 101172 steps/s (collection: 0.876s, learning 0.096s)
             Mean action noise std: 2.03
          Mean value_function loss: 184.2098
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 16.6518
                       Mean reward: 721.71
               Mean episode length: 246.84
    Episode_Reward/reaching_object: 0.6419
     Episode_Reward/lifting_object: 137.1606
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 0.97s
                      Time elapsed: 00:10:52
                               ETA: 00:23:23

################################################################################
                     [1m Learning iteration 635/2000 [0m                      

                       Computation: 96911 steps/s (collection: 0.874s, learning 0.140s)
             Mean action noise std: 2.03
          Mean value_function loss: 159.1853
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.6537
                       Mean reward: 689.03
               Mean episode length: 242.70
    Episode_Reward/reaching_object: 0.6391
     Episode_Reward/lifting_object: 139.4369
      Episode_Reward/object_height: 0.0530
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 1.01s
                      Time elapsed: 00:10:53
                               ETA: 00:23:22

################################################################################
                     [1m Learning iteration 636/2000 [0m                      

                       Computation: 98162 steps/s (collection: 0.893s, learning 0.109s)
             Mean action noise std: 2.04
          Mean value_function loss: 163.6867
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.6641
                       Mean reward: 687.96
               Mean episode length: 243.20
    Episode_Reward/reaching_object: 0.6511
     Episode_Reward/lifting_object: 140.8190
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 1.00s
                      Time elapsed: 00:10:54
                               ETA: 00:23:21

################################################################################
                     [1m Learning iteration 637/2000 [0m                      

                       Computation: 97013 steps/s (collection: 0.855s, learning 0.159s)
             Mean action noise std: 2.04
          Mean value_function loss: 172.4045
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 16.6797
                       Mean reward: 761.63
               Mean episode length: 245.00
    Episode_Reward/reaching_object: 0.6818
     Episode_Reward/lifting_object: 150.7519
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 1.01s
                      Time elapsed: 00:10:55
                               ETA: 00:23:20

################################################################################
                     [1m Learning iteration 638/2000 [0m                      

                       Computation: 86348 steps/s (collection: 0.915s, learning 0.224s)
             Mean action noise std: 2.05
          Mean value_function loss: 167.3695
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.6949
                       Mean reward: 739.43
               Mean episode length: 239.23
    Episode_Reward/reaching_object: 0.6683
     Episode_Reward/lifting_object: 146.9989
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 1.14s
                      Time elapsed: 00:10:56
                               ETA: 00:23:19

################################################################################
                     [1m Learning iteration 639/2000 [0m                      

                       Computation: 100175 steps/s (collection: 0.883s, learning 0.099s)
             Mean action noise std: 2.05
          Mean value_function loss: 166.4071
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 16.7100
                       Mean reward: 776.62
               Mean episode length: 246.80
    Episode_Reward/reaching_object: 0.7020
     Episode_Reward/lifting_object: 153.4535
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 0.98s
                      Time elapsed: 00:10:57
                               ETA: 00:23:18

################################################################################
                     [1m Learning iteration 640/2000 [0m                      

                       Computation: 89726 steps/s (collection: 0.932s, learning 0.164s)
             Mean action noise std: 2.05
          Mean value_function loss: 168.3960
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 16.7210
                       Mean reward: 769.34
               Mean episode length: 246.39
    Episode_Reward/reaching_object: 0.6905
     Episode_Reward/lifting_object: 151.1161
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 1.10s
                      Time elapsed: 00:10:58
                               ETA: 00:23:17

################################################################################
                     [1m Learning iteration 641/2000 [0m                      

                       Computation: 95172 steps/s (collection: 0.911s, learning 0.122s)
             Mean action noise std: 2.05
          Mean value_function loss: 149.8447
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 16.7270
                       Mean reward: 765.19
               Mean episode length: 240.73
    Episode_Reward/reaching_object: 0.6853
     Episode_Reward/lifting_object: 150.7610
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 1.03s
                      Time elapsed: 00:10:59
                               ETA: 00:23:16

################################################################################
                     [1m Learning iteration 642/2000 [0m                      

                       Computation: 102545 steps/s (collection: 0.860s, learning 0.099s)
             Mean action noise std: 2.06
          Mean value_function loss: 170.7287
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 16.7309
                       Mean reward: 700.94
               Mean episode length: 241.14
    Episode_Reward/reaching_object: 0.6852
     Episode_Reward/lifting_object: 150.3091
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 0.96s
                      Time elapsed: 00:11:00
                               ETA: 00:23:15

################################################################################
                     [1m Learning iteration 643/2000 [0m                      

                       Computation: 104509 steps/s (collection: 0.844s, learning 0.097s)
             Mean action noise std: 2.06
          Mean value_function loss: 154.2494
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 16.7362
                       Mean reward: 765.46
               Mean episode length: 245.45
    Episode_Reward/reaching_object: 0.6851
     Episode_Reward/lifting_object: 152.4229
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 0.94s
                      Time elapsed: 00:11:01
                               ETA: 00:23:13

################################################################################
                     [1m Learning iteration 644/2000 [0m                      

                       Computation: 95834 steps/s (collection: 0.897s, learning 0.129s)
             Mean action noise std: 2.06
          Mean value_function loss: 173.8489
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 16.7420
                       Mean reward: 765.07
               Mean episode length: 249.71
    Episode_Reward/reaching_object: 0.6988
     Episode_Reward/lifting_object: 152.7500
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 1.03s
                      Time elapsed: 00:11:02
                               ETA: 00:23:12

################################################################################
                     [1m Learning iteration 645/2000 [0m                      

                       Computation: 104167 steps/s (collection: 0.853s, learning 0.091s)
             Mean action noise std: 2.07
          Mean value_function loss: 155.4858
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 16.7559
                       Mean reward: 798.07
               Mean episode length: 243.43
    Episode_Reward/reaching_object: 0.6849
     Episode_Reward/lifting_object: 150.0749
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 0.94s
                      Time elapsed: 00:11:03
                               ETA: 00:23:11

################################################################################
                     [1m Learning iteration 646/2000 [0m                      

                       Computation: 105875 steps/s (collection: 0.832s, learning 0.097s)
             Mean action noise std: 2.07
          Mean value_function loss: 168.4216
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 16.7659
                       Mean reward: 777.63
               Mean episode length: 246.17
    Episode_Reward/reaching_object: 0.6858
     Episode_Reward/lifting_object: 150.0238
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 0.93s
                      Time elapsed: 00:11:04
                               ETA: 00:23:10

################################################################################
                     [1m Learning iteration 647/2000 [0m                      

                       Computation: 97937 steps/s (collection: 0.848s, learning 0.156s)
             Mean action noise std: 2.07
          Mean value_function loss: 143.8007
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 16.7758
                       Mean reward: 787.16
               Mean episode length: 248.31
    Episode_Reward/reaching_object: 0.6966
     Episode_Reward/lifting_object: 154.4055
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 1.00s
                      Time elapsed: 00:11:05
                               ETA: 00:23:09

################################################################################
                     [1m Learning iteration 648/2000 [0m                      

                       Computation: 106174 steps/s (collection: 0.815s, learning 0.111s)
             Mean action noise std: 2.08
          Mean value_function loss: 143.1816
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 16.7931
                       Mean reward: 798.63
               Mean episode length: 248.53
    Episode_Reward/reaching_object: 0.7025
     Episode_Reward/lifting_object: 156.0286
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 0.93s
                      Time elapsed: 00:11:06
                               ETA: 00:23:08

################################################################################
                     [1m Learning iteration 649/2000 [0m                      

                       Computation: 103473 steps/s (collection: 0.825s, learning 0.125s)
             Mean action noise std: 2.08
          Mean value_function loss: 132.8528
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.8059
                       Mean reward: 752.93
               Mean episode length: 243.77
    Episode_Reward/reaching_object: 0.7002
     Episode_Reward/lifting_object: 154.9125
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 0.95s
                      Time elapsed: 00:11:07
                               ETA: 00:23:06

################################################################################
                     [1m Learning iteration 650/2000 [0m                      

                       Computation: 97379 steps/s (collection: 0.888s, learning 0.122s)
             Mean action noise std: 2.09
          Mean value_function loss: 159.0659
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 16.8206
                       Mean reward: 810.18
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.7115
     Episode_Reward/lifting_object: 159.0696
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 1.01s
                      Time elapsed: 00:11:08
                               ETA: 00:23:05

################################################################################
                     [1m Learning iteration 651/2000 [0m                      

                       Computation: 99746 steps/s (collection: 0.864s, learning 0.121s)
             Mean action noise std: 2.09
          Mean value_function loss: 122.2150
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 16.8317
                       Mean reward: 777.15
               Mean episode length: 241.80
    Episode_Reward/reaching_object: 0.6833
     Episode_Reward/lifting_object: 151.3974
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 0.99s
                      Time elapsed: 00:11:09
                               ETA: 00:23:04

################################################################################
                     [1m Learning iteration 652/2000 [0m                      

                       Computation: 99047 steps/s (collection: 0.845s, learning 0.147s)
             Mean action noise std: 2.09
          Mean value_function loss: 134.2021
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 16.8380
                       Mean reward: 787.65
               Mean episode length: 248.90
    Episode_Reward/reaching_object: 0.7052
     Episode_Reward/lifting_object: 155.5559
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 0.99s
                      Time elapsed: 00:11:10
                               ETA: 00:23:03

################################################################################
                     [1m Learning iteration 653/2000 [0m                      

                       Computation: 97227 steps/s (collection: 0.875s, learning 0.137s)
             Mean action noise std: 2.09
          Mean value_function loss: 155.3615
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.8399
                       Mean reward: 754.38
               Mean episode length: 245.56
    Episode_Reward/reaching_object: 0.6964
     Episode_Reward/lifting_object: 153.9797
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 1.01s
                      Time elapsed: 00:11:11
                               ETA: 00:23:02

################################################################################
                     [1m Learning iteration 654/2000 [0m                      

                       Computation: 104706 steps/s (collection: 0.833s, learning 0.106s)
             Mean action noise std: 2.09
          Mean value_function loss: 145.6222
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 16.8432
                       Mean reward: 770.85
               Mean episode length: 243.40
    Episode_Reward/reaching_object: 0.6874
     Episode_Reward/lifting_object: 153.7123
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 0.94s
                      Time elapsed: 00:11:12
                               ETA: 00:23:01

################################################################################
                     [1m Learning iteration 655/2000 [0m                      

                       Computation: 104369 steps/s (collection: 0.825s, learning 0.117s)
             Mean action noise std: 2.10
          Mean value_function loss: 147.1991
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 16.8583
                       Mean reward: 822.37
               Mean episode length: 246.71
    Episode_Reward/reaching_object: 0.7125
     Episode_Reward/lifting_object: 159.4143
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 0.94s
                      Time elapsed: 00:11:13
                               ETA: 00:23:00

################################################################################
                     [1m Learning iteration 656/2000 [0m                      

                       Computation: 96374 steps/s (collection: 0.878s, learning 0.142s)
             Mean action noise std: 2.10
          Mean value_function loss: 164.1681
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 16.8773
                       Mean reward: 757.23
               Mean episode length: 241.95
    Episode_Reward/reaching_object: 0.6983
     Episode_Reward/lifting_object: 153.6167
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 1.02s
                      Time elapsed: 00:11:14
                               ETA: 00:22:59

################################################################################
                     [1m Learning iteration 657/2000 [0m                      

                       Computation: 91541 steps/s (collection: 0.927s, learning 0.147s)
             Mean action noise std: 2.11
          Mean value_function loss: 131.8508
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.8906
                       Mean reward: 793.36
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.7063
     Episode_Reward/lifting_object: 155.0179
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 1.07s
                      Time elapsed: 00:11:15
                               ETA: 00:22:58

################################################################################
                     [1m Learning iteration 658/2000 [0m                      

                       Computation: 92348 steps/s (collection: 0.942s, learning 0.122s)
             Mean action noise std: 2.11
          Mean value_function loss: 142.4851
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.9066
                       Mean reward: 795.56
               Mean episode length: 245.47
    Episode_Reward/reaching_object: 0.7083
     Episode_Reward/lifting_object: 155.9621
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 1.06s
                      Time elapsed: 00:11:16
                               ETA: 00:22:57

################################################################################
                     [1m Learning iteration 659/2000 [0m                      

                       Computation: 96242 steps/s (collection: 0.924s, learning 0.098s)
             Mean action noise std: 2.12
          Mean value_function loss: 142.9430
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 16.9279
                       Mean reward: 802.34
               Mean episode length: 247.15
    Episode_Reward/reaching_object: 0.7174
     Episode_Reward/lifting_object: 158.3034
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 1.02s
                      Time elapsed: 00:11:17
                               ETA: 00:22:56

################################################################################
                     [1m Learning iteration 660/2000 [0m                      

                       Computation: 90842 steps/s (collection: 0.900s, learning 0.182s)
             Mean action noise std: 2.12
          Mean value_function loss: 144.9294
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 16.9383
                       Mean reward: 749.16
               Mean episode length: 243.54
    Episode_Reward/reaching_object: 0.7018
     Episode_Reward/lifting_object: 155.1297
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 1.08s
                      Time elapsed: 00:11:18
                               ETA: 00:22:55

################################################################################
                     [1m Learning iteration 661/2000 [0m                      

                       Computation: 94943 steps/s (collection: 0.903s, learning 0.132s)
             Mean action noise std: 2.12
          Mean value_function loss: 114.1256
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 16.9464
                       Mean reward: 772.55
               Mean episode length: 246.92
    Episode_Reward/reaching_object: 0.6866
     Episode_Reward/lifting_object: 151.8832
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 1.04s
                      Time elapsed: 00:11:19
                               ETA: 00:22:54

################################################################################
                     [1m Learning iteration 662/2000 [0m                      

                       Computation: 96228 steps/s (collection: 0.896s, learning 0.126s)
             Mean action noise std: 2.13
          Mean value_function loss: 107.3978
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 16.9558
                       Mean reward: 780.74
               Mean episode length: 245.86
    Episode_Reward/reaching_object: 0.7011
     Episode_Reward/lifting_object: 153.7744
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 1.02s
                      Time elapsed: 00:11:20
                               ETA: 00:22:53

################################################################################
                     [1m Learning iteration 663/2000 [0m                      

                       Computation: 104428 steps/s (collection: 0.843s, learning 0.098s)
             Mean action noise std: 2.13
          Mean value_function loss: 94.0453
               Mean surrogate loss: 0.0141
                 Mean entropy loss: 16.9614
                       Mean reward: 766.25
               Mean episode length: 247.13
    Episode_Reward/reaching_object: 0.7180
     Episode_Reward/lifting_object: 159.1759
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 0.94s
                      Time elapsed: 00:11:21
                               ETA: 00:22:52

################################################################################
                     [1m Learning iteration 664/2000 [0m                      

                       Computation: 101883 steps/s (collection: 0.845s, learning 0.119s)
             Mean action noise std: 2.13
          Mean value_function loss: 113.2024
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.9684
                       Mean reward: 798.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7129
     Episode_Reward/lifting_object: 158.1033
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 0.96s
                      Time elapsed: 00:11:22
                               ETA: 00:22:50

################################################################################
                     [1m Learning iteration 665/2000 [0m                      

                       Computation: 88923 steps/s (collection: 0.931s, learning 0.174s)
             Mean action noise std: 2.14
          Mean value_function loss: 123.5783
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 16.9896
                       Mean reward: 793.12
               Mean episode length: 247.83
    Episode_Reward/reaching_object: 0.7078
     Episode_Reward/lifting_object: 157.0955
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 1.11s
                      Time elapsed: 00:11:23
                               ETA: 00:22:50

################################################################################
                     [1m Learning iteration 666/2000 [0m                      

                       Computation: 48973 steps/s (collection: 1.907s, learning 0.101s)
             Mean action noise std: 2.14
          Mean value_function loss: 117.4983
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 17.0033
                       Mean reward: 785.47
               Mean episode length: 246.52
    Episode_Reward/reaching_object: 0.7142
     Episode_Reward/lifting_object: 158.3995
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 2.01s
                      Time elapsed: 00:11:25
                               ETA: 00:22:50

################################################################################
                     [1m Learning iteration 667/2000 [0m                      

                       Computation: 28552 steps/s (collection: 3.330s, learning 0.113s)
             Mean action noise std: 2.15
          Mean value_function loss: 118.7884
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 17.0164
                       Mean reward: 809.11
               Mean episode length: 244.89
    Episode_Reward/reaching_object: 0.7265
     Episode_Reward/lifting_object: 161.1384
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 3.44s
                      Time elapsed: 00:11:28
                               ETA: 00:22:54

################################################################################
                     [1m Learning iteration 668/2000 [0m                      

                       Computation: 32693 steps/s (collection: 2.893s, learning 0.114s)
             Mean action noise std: 2.16
          Mean value_function loss: 109.5842
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 17.0410
                       Mean reward: 774.66
               Mean episode length: 241.90
    Episode_Reward/reaching_object: 0.7091
     Episode_Reward/lifting_object: 157.7928
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 3.01s
                      Time elapsed: 00:11:31
                               ETA: 00:22:57

################################################################################
                     [1m Learning iteration 669/2000 [0m                      

                       Computation: 32563 steps/s (collection: 2.908s, learning 0.111s)
             Mean action noise std: 2.16
          Mean value_function loss: 121.0029
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 17.0522
                       Mean reward: 791.43
               Mean episode length: 244.31
    Episode_Reward/reaching_object: 0.7319
     Episode_Reward/lifting_object: 162.9255
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 3.02s
                      Time elapsed: 00:11:34
                               ETA: 00:23:00

################################################################################
                     [1m Learning iteration 670/2000 [0m                      

                       Computation: 30585 steps/s (collection: 3.077s, learning 0.137s)
             Mean action noise std: 2.16
          Mean value_function loss: 86.8943
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 17.0676
                       Mean reward: 811.85
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7326
     Episode_Reward/lifting_object: 162.9439
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 3.21s
                      Time elapsed: 00:11:38
                               ETA: 00:23:03

################################################################################
                     [1m Learning iteration 671/2000 [0m                      

                       Computation: 29967 steps/s (collection: 3.134s, learning 0.147s)
             Mean action noise std: 2.17
          Mean value_function loss: 107.3913
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 17.0807
                       Mean reward: 817.07
               Mean episode length: 246.87
    Episode_Reward/reaching_object: 0.7288
     Episode_Reward/lifting_object: 161.7027
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 3.28s
                      Time elapsed: 00:11:41
                               ETA: 00:23:07

################################################################################
                     [1m Learning iteration 672/2000 [0m                      

                       Computation: 29627 steps/s (collection: 3.195s, learning 0.123s)
             Mean action noise std: 2.17
          Mean value_function loss: 109.7331
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 17.0938
                       Mean reward: 825.61
               Mean episode length: 249.38
    Episode_Reward/reaching_object: 0.7299
     Episode_Reward/lifting_object: 162.2178
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 3.32s
                      Time elapsed: 00:11:44
                               ETA: 00:23:10

################################################################################
                     [1m Learning iteration 673/2000 [0m                      

                       Computation: 30273 steps/s (collection: 3.115s, learning 0.133s)
             Mean action noise std: 2.17
          Mean value_function loss: 91.0012
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 17.1025
                       Mean reward: 827.96
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7211
     Episode_Reward/lifting_object: 161.2654
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 3.25s
                      Time elapsed: 00:11:48
                               ETA: 00:23:13

################################################################################
                     [1m Learning iteration 674/2000 [0m                      

                       Computation: 30220 steps/s (collection: 3.121s, learning 0.132s)
             Mean action noise std: 2.18
          Mean value_function loss: 104.1607
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 17.1141
                       Mean reward: 781.81
               Mean episode length: 244.50
    Episode_Reward/reaching_object: 0.7111
     Episode_Reward/lifting_object: 159.4192
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 3.25s
                      Time elapsed: 00:11:51
                               ETA: 00:23:17

################################################################################
                     [1m Learning iteration 675/2000 [0m                      

                       Computation: 29483 steps/s (collection: 3.229s, learning 0.105s)
             Mean action noise std: 2.18
          Mean value_function loss: 91.9631
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 17.1294
                       Mean reward: 794.10
               Mean episode length: 245.23
    Episode_Reward/reaching_object: 0.7164
     Episode_Reward/lifting_object: 161.7163
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 3.33s
                      Time elapsed: 00:11:54
                               ETA: 00:23:20

################################################################################
                     [1m Learning iteration 676/2000 [0m                      

                       Computation: 99549 steps/s (collection: 0.892s, learning 0.096s)
             Mean action noise std: 2.19
          Mean value_function loss: 81.5715
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 17.1462
                       Mean reward: 805.22
               Mean episode length: 242.58
    Episode_Reward/reaching_object: 0.7246
     Episode_Reward/lifting_object: 162.5219
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 0.99s
                      Time elapsed: 00:11:55
                               ETA: 00:23:19

################################################################################
                     [1m Learning iteration 677/2000 [0m                      

                       Computation: 106749 steps/s (collection: 0.823s, learning 0.098s)
             Mean action noise std: 2.19
          Mean value_function loss: 87.2260
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.1719
                       Mean reward: 811.08
               Mean episode length: 247.69
    Episode_Reward/reaching_object: 0.7202
     Episode_Reward/lifting_object: 159.8136
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 0.92s
                      Time elapsed: 00:11:56
                               ETA: 00:23:18

################################################################################
                     [1m Learning iteration 678/2000 [0m                      

                       Computation: 101670 steps/s (collection: 0.856s, learning 0.111s)
             Mean action noise std: 2.20
          Mean value_function loss: 88.4447
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 17.1837
                       Mean reward: 811.65
               Mean episode length: 244.04
    Episode_Reward/reaching_object: 0.7318
     Episode_Reward/lifting_object: 163.3290
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 0.97s
                      Time elapsed: 00:11:57
                               ETA: 00:23:16

################################################################################
                     [1m Learning iteration 679/2000 [0m                      

                       Computation: 107331 steps/s (collection: 0.816s, learning 0.100s)
             Mean action noise std: 2.21
          Mean value_function loss: 80.1112
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 17.2092
                       Mean reward: 834.59
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 0.7323
     Episode_Reward/lifting_object: 163.7052
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 0.92s
                      Time elapsed: 00:11:58
                               ETA: 00:23:15

################################################################################
                     [1m Learning iteration 680/2000 [0m                      

                       Computation: 100730 steps/s (collection: 0.824s, learning 0.152s)
             Mean action noise std: 2.21
          Mean value_function loss: 69.4024
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 17.2209
                       Mean reward: 817.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7358
     Episode_Reward/lifting_object: 164.1422
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 0.98s
                      Time elapsed: 00:11:59
                               ETA: 00:23:14

################################################################################
                     [1m Learning iteration 681/2000 [0m                      

                       Computation: 103957 steps/s (collection: 0.836s, learning 0.110s)
             Mean action noise std: 2.21
          Mean value_function loss: 78.2625
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.2345
                       Mean reward: 843.46
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.7437
     Episode_Reward/lifting_object: 166.4210
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 0.95s
                      Time elapsed: 00:12:00
                               ETA: 00:23:13

################################################################################
                     [1m Learning iteration 682/2000 [0m                      

                       Computation: 96620 steps/s (collection: 0.818s, learning 0.200s)
             Mean action noise std: 2.22
          Mean value_function loss: 84.7882
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 17.2488
                       Mean reward: 828.95
               Mean episode length: 247.24
    Episode_Reward/reaching_object: 0.7435
     Episode_Reward/lifting_object: 165.7617
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 1.02s
                      Time elapsed: 00:12:01
                               ETA: 00:23:11

################################################################################
                     [1m Learning iteration 683/2000 [0m                      

                       Computation: 107927 steps/s (collection: 0.820s, learning 0.091s)
             Mean action noise std: 2.22
          Mean value_function loss: 79.9789
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 17.2555
                       Mean reward: 849.33
               Mean episode length: 249.42
    Episode_Reward/reaching_object: 0.7431
     Episode_Reward/lifting_object: 166.3046
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 0.91s
                      Time elapsed: 00:12:02
                               ETA: 00:23:10

################################################################################
                     [1m Learning iteration 684/2000 [0m                      

                       Computation: 97294 steps/s (collection: 0.871s, learning 0.140s)
             Mean action noise std: 2.22
          Mean value_function loss: 83.9337
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 17.2627
                       Mean reward: 831.33
               Mean episode length: 247.66
    Episode_Reward/reaching_object: 0.7361
     Episode_Reward/lifting_object: 163.3381
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 1.01s
                      Time elapsed: 00:12:03
                               ETA: 00:23:09

################################################################################
                     [1m Learning iteration 685/2000 [0m                      

                       Computation: 103710 steps/s (collection: 0.822s, learning 0.126s)
             Mean action noise std: 2.23
          Mean value_function loss: 65.6443
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 17.2799
                       Mean reward: 853.56
               Mean episode length: 246.83
    Episode_Reward/reaching_object: 0.7338
     Episode_Reward/lifting_object: 163.5529
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 0.95s
                      Time elapsed: 00:12:04
                               ETA: 00:23:08

################################################################################
                     [1m Learning iteration 686/2000 [0m                      

                       Computation: 104941 steps/s (collection: 0.836s, learning 0.101s)
             Mean action noise std: 2.24
          Mean value_function loss: 76.5078
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 17.3096
                       Mean reward: 862.27
               Mean episode length: 249.30
    Episode_Reward/reaching_object: 0.7433
     Episode_Reward/lifting_object: 166.2636
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 0.94s
                      Time elapsed: 00:12:05
                               ETA: 00:23:06

################################################################################
                     [1m Learning iteration 687/2000 [0m                      

                       Computation: 103167 steps/s (collection: 0.854s, learning 0.099s)
             Mean action noise std: 2.24
          Mean value_function loss: 69.3501
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.3134
                       Mean reward: 831.24
               Mean episode length: 244.53
    Episode_Reward/reaching_object: 0.7395
     Episode_Reward/lifting_object: 164.2595
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 0.95s
                      Time elapsed: 00:12:06
                               ETA: 00:23:05

################################################################################
                     [1m Learning iteration 688/2000 [0m                      

                       Computation: 98753 steps/s (collection: 0.836s, learning 0.160s)
             Mean action noise std: 2.25
          Mean value_function loss: 66.5705
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.3229
                       Mean reward: 867.79
               Mean episode length: 249.11
    Episode_Reward/reaching_object: 0.7484
     Episode_Reward/lifting_object: 168.1960
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 18.2500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 1.00s
                      Time elapsed: 00:12:07
                               ETA: 00:23:04

################################################################################
                     [1m Learning iteration 689/2000 [0m                      

                       Computation: 93947 steps/s (collection: 0.906s, learning 0.140s)
             Mean action noise std: 2.25
          Mean value_function loss: 71.1011
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.3331
                       Mean reward: 865.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7509
     Episode_Reward/lifting_object: 168.2285
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 1.05s
                      Time elapsed: 00:12:08
                               ETA: 00:23:03

################################################################################
                     [1m Learning iteration 690/2000 [0m                      

                       Computation: 105926 steps/s (collection: 0.833s, learning 0.095s)
             Mean action noise std: 2.25
          Mean value_function loss: 72.7954
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.3414
                       Mean reward: 826.89
               Mean episode length: 245.92
    Episode_Reward/reaching_object: 0.7515
     Episode_Reward/lifting_object: 169.2406
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 0.93s
                      Time elapsed: 00:12:09
                               ETA: 00:23:02

################################################################################
                     [1m Learning iteration 691/2000 [0m                      

                       Computation: 106056 steps/s (collection: 0.799s, learning 0.128s)
             Mean action noise std: 2.25
          Mean value_function loss: 77.2949
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 17.3510
                       Mean reward: 848.30
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.7494
     Episode_Reward/lifting_object: 166.9253
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 0.93s
                      Time elapsed: 00:12:09
                               ETA: 00:23:00

################################################################################
                     [1m Learning iteration 692/2000 [0m                      

                       Computation: 104371 steps/s (collection: 0.850s, learning 0.092s)
             Mean action noise std: 2.26
          Mean value_function loss: 62.9017
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.3580
                       Mean reward: 866.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7457
     Episode_Reward/lifting_object: 167.0571
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 0.94s
                      Time elapsed: 00:12:10
                               ETA: 00:22:59

################################################################################
                     [1m Learning iteration 693/2000 [0m                      

                       Computation: 109323 steps/s (collection: 0.800s, learning 0.099s)
             Mean action noise std: 2.27
          Mean value_function loss: 60.4156
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.3889
                       Mean reward: 834.07
               Mean episode length: 249.06
    Episode_Reward/reaching_object: 0.7473
     Episode_Reward/lifting_object: 167.4529
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 0.90s
                      Time elapsed: 00:12:11
                               ETA: 00:22:58

################################################################################
                     [1m Learning iteration 694/2000 [0m                      

                       Computation: 103116 steps/s (collection: 0.851s, learning 0.103s)
             Mean action noise std: 2.27
          Mean value_function loss: 61.7886
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.4062
                       Mean reward: 858.27
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.7474
     Episode_Reward/lifting_object: 167.5647
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 0.95s
                      Time elapsed: 00:12:12
                               ETA: 00:22:56

################################################################################
                     [1m Learning iteration 695/2000 [0m                      

                       Computation: 107812 steps/s (collection: 0.813s, learning 0.099s)
             Mean action noise std: 2.28
          Mean value_function loss: 63.0724
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 17.4233
                       Mean reward: 830.40
               Mean episode length: 246.01
    Episode_Reward/reaching_object: 0.7518
     Episode_Reward/lifting_object: 167.7625
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 0.91s
                      Time elapsed: 00:12:13
                               ETA: 00:22:55

################################################################################
                     [1m Learning iteration 696/2000 [0m                      

                       Computation: 98305 steps/s (collection: 0.886s, learning 0.114s)
             Mean action noise std: 2.28
          Mean value_function loss: 64.7123
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.4347
                       Mean reward: 866.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7537
     Episode_Reward/lifting_object: 169.1602
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 1.00s
                      Time elapsed: 00:12:14
                               ETA: 00:22:54

################################################################################
                     [1m Learning iteration 697/2000 [0m                      

                       Computation: 105056 steps/s (collection: 0.833s, learning 0.102s)
             Mean action noise std: 2.28
          Mean value_function loss: 56.5059
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 17.4494
                       Mean reward: 846.95
               Mean episode length: 246.92
    Episode_Reward/reaching_object: 0.7479
     Episode_Reward/lifting_object: 168.8090
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 0.94s
                      Time elapsed: 00:12:15
                               ETA: 00:22:53

################################################################################
                     [1m Learning iteration 698/2000 [0m                      

                       Computation: 106474 steps/s (collection: 0.826s, learning 0.098s)
             Mean action noise std: 2.29
          Mean value_function loss: 62.2053
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.4632
                       Mean reward: 851.25
               Mean episode length: 246.33
    Episode_Reward/reaching_object: 0.7544
     Episode_Reward/lifting_object: 169.3015
      Episode_Reward/object_height: 0.0615
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 0.92s
                      Time elapsed: 00:12:16
                               ETA: 00:22:51

################################################################################
                     [1m Learning iteration 699/2000 [0m                      

                       Computation: 102327 steps/s (collection: 0.849s, learning 0.112s)
             Mean action noise std: 2.29
          Mean value_function loss: 58.5936
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.4759
                       Mean reward: 853.16
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7469
     Episode_Reward/lifting_object: 167.3601
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 0.96s
                      Time elapsed: 00:12:17
                               ETA: 00:22:50

################################################################################
                     [1m Learning iteration 700/2000 [0m                      

                       Computation: 104986 steps/s (collection: 0.832s, learning 0.104s)
             Mean action noise std: 2.30
          Mean value_function loss: 53.4497
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.4897
                       Mean reward: 836.71
               Mean episode length: 245.80
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 170.2892
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 0.94s
                      Time elapsed: 00:12:18
                               ETA: 00:22:49

################################################################################
                     [1m Learning iteration 701/2000 [0m                      

                       Computation: 107219 steps/s (collection: 0.798s, learning 0.119s)
             Mean action noise std: 2.30
          Mean value_function loss: 51.9320
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 17.5093
                       Mean reward: 844.07
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 169.5537
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 0.92s
                      Time elapsed: 00:12:19
                               ETA: 00:22:48

################################################################################
                     [1m Learning iteration 702/2000 [0m                      

                       Computation: 104417 steps/s (collection: 0.794s, learning 0.147s)
             Mean action noise std: 2.31
          Mean value_function loss: 70.6711
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 17.5292
                       Mean reward: 854.74
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 0.7539
     Episode_Reward/lifting_object: 168.3333
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 0.94s
                      Time elapsed: 00:12:20
                               ETA: 00:22:46

################################################################################
                     [1m Learning iteration 703/2000 [0m                      

                       Computation: 91555 steps/s (collection: 0.881s, learning 0.193s)
             Mean action noise std: 2.31
          Mean value_function loss: 57.7636
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 17.5404
                       Mean reward: 859.50
               Mean episode length: 247.62
    Episode_Reward/reaching_object: 0.7564
     Episode_Reward/lifting_object: 168.8995
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 1.07s
                      Time elapsed: 00:12:21
                               ETA: 00:22:45

################################################################################
                     [1m Learning iteration 704/2000 [0m                      

                       Computation: 100900 steps/s (collection: 0.848s, learning 0.126s)
             Mean action noise std: 2.32
          Mean value_function loss: 60.5725
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 17.5674
                       Mean reward: 866.03
               Mean episode length: 248.88
    Episode_Reward/reaching_object: 0.7600
     Episode_Reward/lifting_object: 171.1461
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 0.97s
                      Time elapsed: 00:12:22
                               ETA: 00:22:44

################################################################################
                     [1m Learning iteration 705/2000 [0m                      

                       Computation: 102027 steps/s (collection: 0.820s, learning 0.144s)
             Mean action noise std: 2.33
          Mean value_function loss: 49.1264
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.5888
                       Mean reward: 856.41
               Mean episode length: 247.58
    Episode_Reward/reaching_object: 0.7498
     Episode_Reward/lifting_object: 167.6837
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 0.96s
                      Time elapsed: 00:12:23
                               ETA: 00:22:43

################################################################################
                     [1m Learning iteration 706/2000 [0m                      

                       Computation: 106099 steps/s (collection: 0.829s, learning 0.097s)
             Mean action noise std: 2.33
          Mean value_function loss: 56.6663
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 17.6085
                       Mean reward: 850.85
               Mean episode length: 247.16
    Episode_Reward/reaching_object: 0.7568
     Episode_Reward/lifting_object: 169.8387
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 0.93s
                      Time elapsed: 00:12:24
                               ETA: 00:22:42

################################################################################
                     [1m Learning iteration 707/2000 [0m                      

                       Computation: 96384 steps/s (collection: 0.803s, learning 0.217s)
             Mean action noise std: 2.33
          Mean value_function loss: 54.3887
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.6229
                       Mean reward: 860.29
               Mean episode length: 248.89
    Episode_Reward/reaching_object: 0.7609
     Episode_Reward/lifting_object: 170.5482
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 1.02s
                      Time elapsed: 00:12:25
                               ETA: 00:22:41

################################################################################
                     [1m Learning iteration 708/2000 [0m                      

                       Computation: 105838 steps/s (collection: 0.828s, learning 0.100s)
             Mean action noise std: 2.34
          Mean value_function loss: 42.7686
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 17.6305
                       Mean reward: 844.82
               Mean episode length: 244.45
    Episode_Reward/reaching_object: 0.7630
     Episode_Reward/lifting_object: 169.9537
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 0.93s
                      Time elapsed: 00:12:26
                               ETA: 00:22:39

################################################################################
                     [1m Learning iteration 709/2000 [0m                      

                       Computation: 101369 steps/s (collection: 0.819s, learning 0.151s)
             Mean action noise std: 2.34
          Mean value_function loss: 51.9012
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.6443
                       Mean reward: 857.50
               Mean episode length: 249.48
    Episode_Reward/reaching_object: 0.7672
     Episode_Reward/lifting_object: 171.5827
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 0.97s
                      Time elapsed: 00:12:27
                               ETA: 00:22:38

################################################################################
                     [1m Learning iteration 710/2000 [0m                      

                       Computation: 102642 steps/s (collection: 0.824s, learning 0.133s)
             Mean action noise std: 2.35
          Mean value_function loss: 50.3646
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 17.6571
                       Mean reward: 859.07
               Mean episode length: 249.33
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 171.1289
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 0.96s
                      Time elapsed: 00:12:28
                               ETA: 00:22:37

################################################################################
                     [1m Learning iteration 711/2000 [0m                      

                       Computation: 100937 steps/s (collection: 0.832s, learning 0.142s)
             Mean action noise std: 2.36
          Mean value_function loss: 53.7293
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.6795
                       Mean reward: 851.34
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 169.1400
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 0.97s
                      Time elapsed: 00:12:29
                               ETA: 00:22:36

################################################################################
                     [1m Learning iteration 712/2000 [0m                      

                       Computation: 104257 steps/s (collection: 0.855s, learning 0.088s)
             Mean action noise std: 2.36
          Mean value_function loss: 53.5036
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.6958
                       Mean reward: 854.17
               Mean episode length: 247.01
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 169.2639
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 0.94s
                      Time elapsed: 00:12:30
                               ETA: 00:22:34

################################################################################
                     [1m Learning iteration 713/2000 [0m                      

                       Computation: 105725 steps/s (collection: 0.813s, learning 0.117s)
             Mean action noise std: 2.36
          Mean value_function loss: 54.7744
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.6986
                       Mean reward: 876.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 170.9928
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 0.93s
                      Time elapsed: 00:12:30
                               ETA: 00:22:33

################################################################################
                     [1m Learning iteration 714/2000 [0m                      

                       Computation: 105221 steps/s (collection: 0.843s, learning 0.091s)
             Mean action noise std: 2.37
          Mean value_function loss: 53.6176
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 17.7017
                       Mean reward: 872.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 170.2021
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 0.93s
                      Time elapsed: 00:12:31
                               ETA: 00:22:32

################################################################################
                     [1m Learning iteration 715/2000 [0m                      

                       Computation: 105693 steps/s (collection: 0.827s, learning 0.103s)
             Mean action noise std: 2.38
          Mean value_function loss: 40.9444
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.7189
                       Mean reward: 852.84
               Mean episode length: 246.43
    Episode_Reward/reaching_object: 0.7542
     Episode_Reward/lifting_object: 168.0914
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 0.93s
                      Time elapsed: 00:12:32
                               ETA: 00:22:31

################################################################################
                     [1m Learning iteration 716/2000 [0m                      

                       Computation: 96960 steps/s (collection: 0.876s, learning 0.138s)
             Mean action noise std: 2.38
          Mean value_function loss: 44.6721
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 17.7439
                       Mean reward: 865.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 171.1184
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 1.01s
                      Time elapsed: 00:12:33
                               ETA: 00:22:29

################################################################################
                     [1m Learning iteration 717/2000 [0m                      

                       Computation: 79508 steps/s (collection: 1.112s, learning 0.124s)
             Mean action noise std: 2.39
          Mean value_function loss: 57.6441
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.7643
                       Mean reward: 846.24
               Mean episode length: 246.00
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 171.5311
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 1.24s
                      Time elapsed: 00:12:35
                               ETA: 00:22:29

################################################################################
                     [1m Learning iteration 718/2000 [0m                      

                       Computation: 76361 steps/s (collection: 1.109s, learning 0.178s)
             Mean action noise std: 2.40
          Mean value_function loss: 54.1628
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.7836
                       Mean reward: 828.24
               Mean episode length: 244.69
    Episode_Reward/reaching_object: 0.7508
     Episode_Reward/lifting_object: 167.2074
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 1.29s
                      Time elapsed: 00:12:36
                               ETA: 00:22:28

################################################################################
                     [1m Learning iteration 719/2000 [0m                      

                       Computation: 73597 steps/s (collection: 1.113s, learning 0.223s)
             Mean action noise std: 2.40
          Mean value_function loss: 42.3110
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.7965
                       Mean reward: 849.71
               Mean episode length: 247.63
    Episode_Reward/reaching_object: 0.7529
     Episode_Reward/lifting_object: 169.1445
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 1.34s
                      Time elapsed: 00:12:37
                               ETA: 00:22:28

################################################################################
                     [1m Learning iteration 720/2000 [0m                      

                       Computation: 80409 steps/s (collection: 1.098s, learning 0.124s)
             Mean action noise std: 2.41
          Mean value_function loss: 46.2232
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 17.8175
                       Mean reward: 864.47
               Mean episode length: 249.37
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 172.1714
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 1.22s
                      Time elapsed: 00:12:38
                               ETA: 00:22:27

################################################################################
                     [1m Learning iteration 721/2000 [0m                      

                       Computation: 82777 steps/s (collection: 0.986s, learning 0.201s)
             Mean action noise std: 2.41
          Mean value_function loss: 41.8470
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.8263
                       Mean reward: 840.87
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 0.7549
     Episode_Reward/lifting_object: 167.6629
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 1.19s
                      Time elapsed: 00:12:40
                               ETA: 00:22:26

################################################################################
                     [1m Learning iteration 722/2000 [0m                      

                       Computation: 74005 steps/s (collection: 1.133s, learning 0.195s)
             Mean action noise std: 2.41
          Mean value_function loss: 42.6276
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.8314
                       Mean reward: 834.88
               Mean episode length: 247.37
    Episode_Reward/reaching_object: 0.7620
     Episode_Reward/lifting_object: 170.4917
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 1.33s
                      Time elapsed: 00:12:41
                               ETA: 00:22:25

################################################################################
                     [1m Learning iteration 723/2000 [0m                      

                       Computation: 92506 steps/s (collection: 0.940s, learning 0.123s)
             Mean action noise std: 2.42
          Mean value_function loss: 53.8429
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.8365
                       Mean reward: 852.86
               Mean episode length: 246.20
    Episode_Reward/reaching_object: 0.7611
     Episode_Reward/lifting_object: 169.7976
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 1.06s
                      Time elapsed: 00:12:42
                               ETA: 00:22:24

################################################################################
                     [1m Learning iteration 724/2000 [0m                      

                       Computation: 103197 steps/s (collection: 0.841s, learning 0.112s)
             Mean action noise std: 2.42
          Mean value_function loss: 42.8000
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.8468
                       Mean reward: 868.09
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 0.7682
     Episode_Reward/lifting_object: 171.9765
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 0.95s
                      Time elapsed: 00:12:43
                               ETA: 00:22:23

################################################################################
                     [1m Learning iteration 725/2000 [0m                      

                       Computation: 105714 steps/s (collection: 0.811s, learning 0.119s)
             Mean action noise std: 2.43
          Mean value_function loss: 36.3906
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.8679
                       Mean reward: 854.06
               Mean episode length: 249.16
    Episode_Reward/reaching_object: 0.7601
     Episode_Reward/lifting_object: 169.9347
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 0.93s
                      Time elapsed: 00:12:44
                               ETA: 00:22:22

################################################################################
                     [1m Learning iteration 726/2000 [0m                      

                       Computation: 108655 steps/s (collection: 0.786s, learning 0.119s)
             Mean action noise std: 2.43
          Mean value_function loss: 35.6281
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.8914
                       Mean reward: 867.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7691
     Episode_Reward/lifting_object: 171.7146
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 0.90s
                      Time elapsed: 00:12:45
                               ETA: 00:22:21

################################################################################
                     [1m Learning iteration 727/2000 [0m                      

                       Computation: 106224 steps/s (collection: 0.778s, learning 0.147s)
             Mean action noise std: 2.44
          Mean value_function loss: 46.8275
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 17.9236
                       Mean reward: 862.40
               Mean episode length: 247.63
    Episode_Reward/reaching_object: 0.7668
     Episode_Reward/lifting_object: 170.9587
      Episode_Reward/object_height: 0.0615
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 0.93s
                      Time elapsed: 00:12:46
                               ETA: 00:22:19

################################################################################
                     [1m Learning iteration 728/2000 [0m                      

                       Computation: 98346 steps/s (collection: 0.832s, learning 0.168s)
             Mean action noise std: 2.45
          Mean value_function loss: 42.9427
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.9479
                       Mean reward: 848.30
               Mean episode length: 248.51
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 170.0883
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 1.00s
                      Time elapsed: 00:12:47
                               ETA: 00:22:18

################################################################################
                     [1m Learning iteration 729/2000 [0m                      

                       Computation: 102707 steps/s (collection: 0.807s, learning 0.151s)
             Mean action noise std: 2.45
          Mean value_function loss: 45.4744
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.9611
                       Mean reward: 850.43
               Mean episode length: 249.92
    Episode_Reward/reaching_object: 0.7567
     Episode_Reward/lifting_object: 170.9787
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 0.96s
                      Time elapsed: 00:12:48
                               ETA: 00:22:17

################################################################################
                     [1m Learning iteration 730/2000 [0m                      

                       Computation: 100417 steps/s (collection: 0.799s, learning 0.180s)
             Mean action noise std: 2.46
          Mean value_function loss: 56.5135
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.9636
                       Mean reward: 854.76
               Mean episode length: 247.06
    Episode_Reward/reaching_object: 0.7630
     Episode_Reward/lifting_object: 172.0480
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 0.98s
                      Time elapsed: 00:12:49
                               ETA: 00:22:16

################################################################################
                     [1m Learning iteration 731/2000 [0m                      

                       Computation: 102155 steps/s (collection: 0.828s, learning 0.135s)
             Mean action noise std: 2.46
          Mean value_function loss: 42.9039
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 17.9877
                       Mean reward: 868.62
               Mean episode length: 248.95
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 170.4665
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 0.96s
                      Time elapsed: 00:12:50
                               ETA: 00:22:15

################################################################################
                     [1m Learning iteration 732/2000 [0m                      

                       Computation: 104927 steps/s (collection: 0.797s, learning 0.140s)
             Mean action noise std: 2.47
          Mean value_function loss: 45.4487
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.0134
                       Mean reward: 850.22
               Mean episode length: 248.75
    Episode_Reward/reaching_object: 0.7564
     Episode_Reward/lifting_object: 168.8292
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 0.94s
                      Time elapsed: 00:12:51
                               ETA: 00:22:13

################################################################################
                     [1m Learning iteration 733/2000 [0m                      

                       Computation: 88669 steps/s (collection: 0.899s, learning 0.209s)
             Mean action noise std: 2.48
          Mean value_function loss: 43.9707
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.0365
                       Mean reward: 863.47
               Mean episode length: 249.22
    Episode_Reward/reaching_object: 0.7640
     Episode_Reward/lifting_object: 171.4953
      Episode_Reward/object_height: 0.0615
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 1.11s
                      Time elapsed: 00:12:52
                               ETA: 00:22:12

################################################################################
                     [1m Learning iteration 734/2000 [0m                      

                       Computation: 102776 steps/s (collection: 0.802s, learning 0.154s)
             Mean action noise std: 2.48
          Mean value_function loss: 43.3270
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 18.0483
                       Mean reward: 848.18
               Mean episode length: 246.11
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 170.6006
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 0.96s
                      Time elapsed: 00:12:53
                               ETA: 00:22:11

################################################################################
                     [1m Learning iteration 735/2000 [0m                      

                       Computation: 88107 steps/s (collection: 0.958s, learning 0.158s)
             Mean action noise std: 2.49
          Mean value_function loss: 53.8798
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 18.0582
                       Mean reward: 850.01
               Mean episode length: 249.38
    Episode_Reward/reaching_object: 0.7596
     Episode_Reward/lifting_object: 169.8115
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 1.12s
                      Time elapsed: 00:12:54
                               ETA: 00:22:10

################################################################################
                     [1m Learning iteration 736/2000 [0m                      

                       Computation: 101649 steps/s (collection: 0.845s, learning 0.122s)
             Mean action noise std: 2.49
          Mean value_function loss: 52.4795
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.0704
                       Mean reward: 854.18
               Mean episode length: 246.21
    Episode_Reward/reaching_object: 0.7529
     Episode_Reward/lifting_object: 168.1293
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 0.97s
                      Time elapsed: 00:12:55
                               ETA: 00:22:09

################################################################################
                     [1m Learning iteration 737/2000 [0m                      

                       Computation: 109598 steps/s (collection: 0.762s, learning 0.135s)
             Mean action noise std: 2.49
          Mean value_function loss: 46.0244
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.0738
                       Mean reward: 858.31
               Mean episode length: 249.27
    Episode_Reward/reaching_object: 0.7447
     Episode_Reward/lifting_object: 164.4023
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 0.90s
                      Time elapsed: 00:12:56
                               ETA: 00:22:08

################################################################################
                     [1m Learning iteration 738/2000 [0m                      

                       Computation: 115447 steps/s (collection: 0.759s, learning 0.093s)
             Mean action noise std: 2.50
          Mean value_function loss: 49.3303
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.0939
                       Mean reward: 866.32
               Mean episode length: 249.40
    Episode_Reward/reaching_object: 0.7704
     Episode_Reward/lifting_object: 170.9656
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 0.85s
                      Time elapsed: 00:12:56
                               ETA: 00:22:06

################################################################################
                     [1m Learning iteration 739/2000 [0m                      

                       Computation: 111671 steps/s (collection: 0.759s, learning 0.122s)
             Mean action noise std: 2.51
          Mean value_function loss: 42.6404
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.1271
                       Mean reward: 847.36
               Mean episode length: 247.28
    Episode_Reward/reaching_object: 0.7564
     Episode_Reward/lifting_object: 169.1573
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 0.88s
                      Time elapsed: 00:12:57
                               ETA: 00:22:05

################################################################################
                     [1m Learning iteration 740/2000 [0m                      

                       Computation: 100500 steps/s (collection: 0.821s, learning 0.158s)
             Mean action noise std: 2.51
          Mean value_function loss: 44.7838
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.1478
                       Mean reward: 842.84
               Mean episode length: 245.03
    Episode_Reward/reaching_object: 0.7586
     Episode_Reward/lifting_object: 169.2987
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 0.98s
                      Time elapsed: 00:12:58
                               ETA: 00:22:04

################################################################################
                     [1m Learning iteration 741/2000 [0m                      

                       Computation: 113722 steps/s (collection: 0.774s, learning 0.090s)
             Mean action noise std: 2.52
          Mean value_function loss: 47.2702
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 18.1744
                       Mean reward: 833.62
               Mean episode length: 241.42
    Episode_Reward/reaching_object: 0.7529
     Episode_Reward/lifting_object: 168.5159
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 0.86s
                      Time elapsed: 00:12:59
                               ETA: 00:22:02

################################################################################
                     [1m Learning iteration 742/2000 [0m                      

                       Computation: 108318 steps/s (collection: 0.799s, learning 0.109s)
             Mean action noise std: 2.53
          Mean value_function loss: 43.5848
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.1910
                       Mean reward: 861.60
               Mean episode length: 248.69
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 170.7516
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 0.91s
                      Time elapsed: 00:13:00
                               ETA: 00:22:01

################################################################################
                     [1m Learning iteration 743/2000 [0m                      

                       Computation: 117065 steps/s (collection: 0.748s, learning 0.092s)
             Mean action noise std: 2.54
          Mean value_function loss: 40.1885
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.2244
                       Mean reward: 856.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 170.7129
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 0.84s
                      Time elapsed: 00:13:01
                               ETA: 00:22:00

################################################################################
                     [1m Learning iteration 744/2000 [0m                      

                       Computation: 111236 steps/s (collection: 0.785s, learning 0.099s)
             Mean action noise std: 2.55
          Mean value_function loss: 35.4470
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.2562
                       Mean reward: 861.18
               Mean episode length: 248.43
    Episode_Reward/reaching_object: 0.7659
     Episode_Reward/lifting_object: 170.6053
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 0.88s
                      Time elapsed: 00:13:02
                               ETA: 00:21:58

################################################################################
                     [1m Learning iteration 745/2000 [0m                      

                       Computation: 114146 steps/s (collection: 0.772s, learning 0.089s)
             Mean action noise std: 2.55
          Mean value_function loss: 47.1274
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.2793
                       Mean reward: 854.11
               Mean episode length: 246.03
    Episode_Reward/reaching_object: 0.7516
     Episode_Reward/lifting_object: 168.3262
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 0.86s
                      Time elapsed: 00:13:03
                               ETA: 00:21:57

################################################################################
                     [1m Learning iteration 746/2000 [0m                      

                       Computation: 103652 steps/s (collection: 0.782s, learning 0.166s)
             Mean action noise std: 2.56
          Mean value_function loss: 54.9227
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.2950
                       Mean reward: 861.53
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 171.0088
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 0.95s
                      Time elapsed: 00:13:04
                               ETA: 00:21:56

################################################################################
                     [1m Learning iteration 747/2000 [0m                      

                       Computation: 106785 steps/s (collection: 0.777s, learning 0.144s)
             Mean action noise std: 2.56
          Mean value_function loss: 45.1600
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.3086
                       Mean reward: 859.51
               Mean episode length: 246.77
    Episode_Reward/reaching_object: 0.7558
     Episode_Reward/lifting_object: 169.6508
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 0.92s
                      Time elapsed: 00:13:05
                               ETA: 00:21:55

################################################################################
                     [1m Learning iteration 748/2000 [0m                      

                       Computation: 106620 steps/s (collection: 0.795s, learning 0.127s)
             Mean action noise std: 2.57
          Mean value_function loss: 38.6702
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.3218
                       Mean reward: 827.24
               Mean episode length: 241.37
    Episode_Reward/reaching_object: 0.7568
     Episode_Reward/lifting_object: 170.0039
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 0.92s
                      Time elapsed: 00:13:05
                               ETA: 00:21:53

################################################################################
                     [1m Learning iteration 749/2000 [0m                      

                       Computation: 99433 steps/s (collection: 0.838s, learning 0.151s)
             Mean action noise std: 2.57
          Mean value_function loss: 44.0727
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.3312
                       Mean reward: 864.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 171.5443
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 0.99s
                      Time elapsed: 00:13:06
                               ETA: 00:21:52

################################################################################
                     [1m Learning iteration 750/2000 [0m                      

                       Computation: 105282 steps/s (collection: 0.821s, learning 0.113s)
             Mean action noise std: 2.58
          Mean value_function loss: 39.1804
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.3520
                       Mean reward: 865.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7629
     Episode_Reward/lifting_object: 170.1048
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 0.93s
                      Time elapsed: 00:13:07
                               ETA: 00:21:51

################################################################################
                     [1m Learning iteration 751/2000 [0m                      

                       Computation: 113211 steps/s (collection: 0.772s, learning 0.096s)
             Mean action noise std: 2.58
          Mean value_function loss: 51.3595
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.3730
                       Mean reward: 857.08
               Mean episode length: 248.91
    Episode_Reward/reaching_object: 0.7610
     Episode_Reward/lifting_object: 170.7628
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 0.87s
                      Time elapsed: 00:13:08
                               ETA: 00:21:50

################################################################################
                     [1m Learning iteration 752/2000 [0m                      

                       Computation: 115180 steps/s (collection: 0.757s, learning 0.097s)
             Mean action noise std: 2.59
          Mean value_function loss: 46.7804
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.3972
                       Mean reward: 870.17
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.7614
     Episode_Reward/lifting_object: 171.4738
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 0.85s
                      Time elapsed: 00:13:09
                               ETA: 00:21:48

################################################################################
                     [1m Learning iteration 753/2000 [0m                      

                       Computation: 109949 steps/s (collection: 0.792s, learning 0.102s)
             Mean action noise std: 2.60
          Mean value_function loss: 45.7111
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 18.4190
                       Mean reward: 857.87
               Mean episode length: 247.29
    Episode_Reward/reaching_object: 0.7639
     Episode_Reward/lifting_object: 172.0538
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 0.89s
                      Time elapsed: 00:13:10
                               ETA: 00:21:47

################################################################################
                     [1m Learning iteration 754/2000 [0m                      

                       Computation: 107280 steps/s (collection: 0.809s, learning 0.107s)
             Mean action noise std: 2.60
          Mean value_function loss: 56.5751
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.4311
                       Mean reward: 834.69
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 0.7489
     Episode_Reward/lifting_object: 169.5095
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 0.92s
                      Time elapsed: 00:13:11
                               ETA: 00:21:46

################################################################################
                     [1m Learning iteration 755/2000 [0m                      

                       Computation: 88334 steps/s (collection: 0.948s, learning 0.164s)
             Mean action noise std: 2.61
          Mean value_function loss: 43.5557
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 18.4424
                       Mean reward: 848.77
               Mean episode length: 246.09
    Episode_Reward/reaching_object: 0.7413
     Episode_Reward/lifting_object: 167.6859
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 1.11s
                      Time elapsed: 00:13:12
                               ETA: 00:21:45

################################################################################
                     [1m Learning iteration 756/2000 [0m                      

                       Computation: 93573 steps/s (collection: 0.937s, learning 0.113s)
             Mean action noise std: 2.61
          Mean value_function loss: 53.2004
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.4541
                       Mean reward: 859.75
               Mean episode length: 247.57
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 170.9968
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 1.05s
                      Time elapsed: 00:13:13
                               ETA: 00:21:44

################################################################################
                     [1m Learning iteration 757/2000 [0m                      

                       Computation: 102544 steps/s (collection: 0.874s, learning 0.085s)
             Mean action noise std: 2.62
          Mean value_function loss: 39.5736
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.4727
                       Mean reward: 834.61
               Mean episode length: 245.78
    Episode_Reward/reaching_object: 0.7437
     Episode_Reward/lifting_object: 168.6241
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 0.96s
                      Time elapsed: 00:13:14
                               ETA: 00:21:42

################################################################################
                     [1m Learning iteration 758/2000 [0m                      

                       Computation: 108744 steps/s (collection: 0.814s, learning 0.090s)
             Mean action noise std: 2.62
          Mean value_function loss: 41.6574
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 18.4913
                       Mean reward: 843.51
               Mean episode length: 246.93
    Episode_Reward/reaching_object: 0.7478
     Episode_Reward/lifting_object: 169.9248
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 0.90s
                      Time elapsed: 00:13:15
                               ETA: 00:21:41

################################################################################
                     [1m Learning iteration 759/2000 [0m                      

                       Computation: 111084 steps/s (collection: 0.798s, learning 0.087s)
             Mean action noise std: 2.63
          Mean value_function loss: 46.9636
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.5015
                       Mean reward: 874.71
               Mean episode length: 249.81
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 170.5172
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 0.88s
                      Time elapsed: 00:13:16
                               ETA: 00:21:40

################################################################################
                     [1m Learning iteration 760/2000 [0m                      

                       Computation: 104250 steps/s (collection: 0.823s, learning 0.120s)
             Mean action noise std: 2.64
          Mean value_function loss: 28.8784
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.5181
                       Mean reward: 865.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7462
     Episode_Reward/lifting_object: 168.0576
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 0.94s
                      Time elapsed: 00:13:17
                               ETA: 00:21:39

################################################################################
                     [1m Learning iteration 761/2000 [0m                      

                       Computation: 108233 steps/s (collection: 0.759s, learning 0.149s)
             Mean action noise std: 2.64
          Mean value_function loss: 38.6797
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.5375
                       Mean reward: 851.50
               Mean episode length: 243.61
    Episode_Reward/reaching_object: 0.7514
     Episode_Reward/lifting_object: 170.8895
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 0.91s
                      Time elapsed: 00:13:18
                               ETA: 00:21:37

################################################################################
                     [1m Learning iteration 762/2000 [0m                      

                       Computation: 107839 steps/s (collection: 0.762s, learning 0.150s)
             Mean action noise std: 2.65
          Mean value_function loss: 35.5561
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.5521
                       Mean reward: 857.95
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 171.8100
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 0.91s
                      Time elapsed: 00:13:19
                               ETA: 00:21:36

################################################################################
                     [1m Learning iteration 763/2000 [0m                      

                       Computation: 107629 steps/s (collection: 0.778s, learning 0.136s)
             Mean action noise std: 2.65
          Mean value_function loss: 31.5048
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.5593
                       Mean reward: 865.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7524
     Episode_Reward/lifting_object: 169.3057
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 0.91s
                      Time elapsed: 00:13:19
                               ETA: 00:21:35

################################################################################
                     [1m Learning iteration 764/2000 [0m                      

                       Computation: 108297 steps/s (collection: 0.783s, learning 0.125s)
             Mean action noise std: 2.66
          Mean value_function loss: 36.6498
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.5702
                       Mean reward: 879.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7644
     Episode_Reward/lifting_object: 172.5012
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 0.91s
                      Time elapsed: 00:13:20
                               ETA: 00:21:34

################################################################################
                     [1m Learning iteration 765/2000 [0m                      

                       Computation: 103558 steps/s (collection: 0.857s, learning 0.092s)
             Mean action noise std: 2.66
          Mean value_function loss: 32.5230
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.5904
                       Mean reward: 853.47
               Mean episode length: 249.31
    Episode_Reward/reaching_object: 0.7508
     Episode_Reward/lifting_object: 170.1183
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 0.95s
                      Time elapsed: 00:13:21
                               ETA: 00:21:32

################################################################################
                     [1m Learning iteration 766/2000 [0m                      

                       Computation: 110773 steps/s (collection: 0.779s, learning 0.109s)
             Mean action noise std: 2.67
          Mean value_function loss: 31.4048
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.6069
                       Mean reward: 874.86
               Mean episode length: 249.18
    Episode_Reward/reaching_object: 0.7634
     Episode_Reward/lifting_object: 171.9347
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 0.89s
                      Time elapsed: 00:13:22
                               ETA: 00:21:31

################################################################################
                     [1m Learning iteration 767/2000 [0m                      

                       Computation: 111510 steps/s (collection: 0.780s, learning 0.101s)
             Mean action noise std: 2.68
          Mean value_function loss: 47.6686
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.6308
                       Mean reward: 863.47
               Mean episode length: 246.48
    Episode_Reward/reaching_object: 0.7612
     Episode_Reward/lifting_object: 171.9034
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 0.88s
                      Time elapsed: 00:13:23
                               ETA: 00:21:30

################################################################################
                     [1m Learning iteration 768/2000 [0m                      

                       Computation: 95025 steps/s (collection: 0.944s, learning 0.090s)
             Mean action noise std: 2.68
          Mean value_function loss: 25.1938
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 18.6565
                       Mean reward: 864.40
               Mean episode length: 248.28
    Episode_Reward/reaching_object: 0.7588
     Episode_Reward/lifting_object: 171.2630
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 1.03s
                      Time elapsed: 00:13:24
                               ETA: 00:21:29

################################################################################
                     [1m Learning iteration 769/2000 [0m                      

                       Computation: 114467 steps/s (collection: 0.763s, learning 0.096s)
             Mean action noise std: 2.68
          Mean value_function loss: 48.8301
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.6627
                       Mean reward: 876.87
               Mean episode length: 249.63
    Episode_Reward/reaching_object: 0.7728
     Episode_Reward/lifting_object: 173.3206
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 0.86s
                      Time elapsed: 00:13:25
                               ETA: 00:21:27

################################################################################
                     [1m Learning iteration 770/2000 [0m                      

                       Computation: 113323 steps/s (collection: 0.773s, learning 0.095s)
             Mean action noise std: 2.69
          Mean value_function loss: 39.3119
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.6752
                       Mean reward: 858.85
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 173.1272
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 0.87s
                      Time elapsed: 00:13:26
                               ETA: 00:21:26

################################################################################
                     [1m Learning iteration 771/2000 [0m                      

                       Computation: 114661 steps/s (collection: 0.765s, learning 0.092s)
             Mean action noise std: 2.69
          Mean value_function loss: 39.4205
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.6862
                       Mean reward: 876.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7644
     Episode_Reward/lifting_object: 170.8724
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 0.86s
                      Time elapsed: 00:13:27
                               ETA: 00:21:25

################################################################################
                     [1m Learning iteration 772/2000 [0m                      

                       Computation: 105642 steps/s (collection: 0.772s, learning 0.159s)
             Mean action noise std: 2.69
          Mean value_function loss: 37.2385
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 18.6979
                       Mean reward: 844.58
               Mean episode length: 246.31
    Episode_Reward/reaching_object: 0.7554
     Episode_Reward/lifting_object: 169.7934
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 0.93s
                      Time elapsed: 00:13:28
                               ETA: 00:21:23

################################################################################
                     [1m Learning iteration 773/2000 [0m                      

                       Computation: 111851 steps/s (collection: 0.794s, learning 0.085s)
             Mean action noise std: 2.70
          Mean value_function loss: 37.0863
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.7082
                       Mean reward: 878.62
               Mean episode length: 249.88
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 172.6797
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 0.88s
                      Time elapsed: 00:13:29
                               ETA: 00:21:22

################################################################################
                     [1m Learning iteration 774/2000 [0m                      

                       Computation: 114972 steps/s (collection: 0.767s, learning 0.088s)
             Mean action noise std: 2.70
          Mean value_function loss: 31.5317
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 18.7243
                       Mean reward: 849.35
               Mean episode length: 244.98
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 170.0963
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 0.86s
                      Time elapsed: 00:13:29
                               ETA: 00:21:21

################################################################################
                     [1m Learning iteration 775/2000 [0m                      

                       Computation: 110928 steps/s (collection: 0.782s, learning 0.105s)
             Mean action noise std: 2.70
          Mean value_function loss: 40.1487
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 18.7397
                       Mean reward: 853.77
               Mean episode length: 247.08
    Episode_Reward/reaching_object: 0.7630
     Episode_Reward/lifting_object: 171.9836
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 0.89s
                      Time elapsed: 00:13:30
                               ETA: 00:21:19

################################################################################
                     [1m Learning iteration 776/2000 [0m                      

                       Computation: 101407 steps/s (collection: 0.863s, learning 0.107s)
             Mean action noise std: 2.71
          Mean value_function loss: 25.7754
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.7481
                       Mean reward: 861.58
               Mean episode length: 247.44
    Episode_Reward/reaching_object: 0.7596
     Episode_Reward/lifting_object: 170.4087
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 0.97s
                      Time elapsed: 00:13:31
                               ETA: 00:21:18

################################################################################
                     [1m Learning iteration 777/2000 [0m                      

                       Computation: 105548 steps/s (collection: 0.827s, learning 0.104s)
             Mean action noise std: 2.72
          Mean value_function loss: 30.0916
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.7624
                       Mean reward: 867.29
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7630
     Episode_Reward/lifting_object: 171.3573
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 0.93s
                      Time elapsed: 00:13:32
                               ETA: 00:21:17

################################################################################
                     [1m Learning iteration 778/2000 [0m                      

                       Computation: 113961 steps/s (collection: 0.769s, learning 0.094s)
             Mean action noise std: 2.72
          Mean value_function loss: 39.8153
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.7834
                       Mean reward: 870.34
               Mean episode length: 247.82
    Episode_Reward/reaching_object: 0.7713
     Episode_Reward/lifting_object: 173.5649
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 0.86s
                      Time elapsed: 00:13:33
                               ETA: 00:21:16

################################################################################
                     [1m Learning iteration 779/2000 [0m                      

                       Computation: 102576 steps/s (collection: 0.823s, learning 0.135s)
             Mean action noise std: 2.73
          Mean value_function loss: 41.4746
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.8074
                       Mean reward: 855.57
               Mean episode length: 244.82
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 170.8719
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 0.96s
                      Time elapsed: 00:13:34
                               ETA: 00:21:15

################################################################################
                     [1m Learning iteration 780/2000 [0m                      

                       Computation: 111366 steps/s (collection: 0.788s, learning 0.095s)
             Mean action noise std: 2.73
          Mean value_function loss: 36.5840
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 18.8291
                       Mean reward: 852.14
               Mean episode length: 249.35
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 171.2834
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 0.88s
                      Time elapsed: 00:13:35
                               ETA: 00:21:13

################################################################################
                     [1m Learning iteration 781/2000 [0m                      

                       Computation: 114741 steps/s (collection: 0.771s, learning 0.086s)
             Mean action noise std: 2.74
          Mean value_function loss: 33.0414
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.8327
                       Mean reward: 855.15
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 171.1797
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 0.86s
                      Time elapsed: 00:13:36
                               ETA: 00:21:12

################################################################################
                     [1m Learning iteration 782/2000 [0m                      

                       Computation: 105992 steps/s (collection: 0.826s, learning 0.102s)
             Mean action noise std: 2.75
          Mean value_function loss: 42.7671
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.8516
                       Mean reward: 851.70
               Mean episode length: 245.22
    Episode_Reward/reaching_object: 0.7578
     Episode_Reward/lifting_object: 171.4697
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 0.93s
                      Time elapsed: 00:13:37
                               ETA: 00:21:11

################################################################################
                     [1m Learning iteration 783/2000 [0m                      

                       Computation: 108601 steps/s (collection: 0.780s, learning 0.126s)
             Mean action noise std: 2.75
          Mean value_function loss: 39.2800
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 18.8761
                       Mean reward: 869.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7673
     Episode_Reward/lifting_object: 172.2522
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 0.91s
                      Time elapsed: 00:13:38
                               ETA: 00:21:09

################################################################################
                     [1m Learning iteration 784/2000 [0m                      

                       Computation: 99320 steps/s (collection: 0.787s, learning 0.203s)
             Mean action noise std: 2.76
          Mean value_function loss: 35.3366
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.8912
                       Mean reward: 851.80
               Mean episode length: 245.48
    Episode_Reward/reaching_object: 0.7559
     Episode_Reward/lifting_object: 170.4483
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 0.99s
                      Time elapsed: 00:13:39
                               ETA: 00:21:08

################################################################################
                     [1m Learning iteration 785/2000 [0m                      

                       Computation: 101742 steps/s (collection: 0.838s, learning 0.128s)
             Mean action noise std: 2.76
          Mean value_function loss: 46.5103
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 18.9053
                       Mean reward: 855.45
               Mean episode length: 245.85
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 171.3567
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 0.97s
                      Time elapsed: 00:13:40
                               ETA: 00:21:07

################################################################################
                     [1m Learning iteration 786/2000 [0m                      

                       Computation: 101958 steps/s (collection: 0.799s, learning 0.165s)
             Mean action noise std: 2.77
          Mean value_function loss: 38.7569
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.9167
                       Mean reward: 864.96
               Mean episode length: 248.32
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 170.4041
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 0.96s
                      Time elapsed: 00:13:41
                               ETA: 00:21:06

################################################################################
                     [1m Learning iteration 787/2000 [0m                      

                       Computation: 101328 steps/s (collection: 0.790s, learning 0.180s)
             Mean action noise std: 2.77
          Mean value_function loss: 43.2691
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 18.9363
                       Mean reward: 831.97
               Mean episode length: 246.74
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 170.0297
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 0.97s
                      Time elapsed: 00:13:41
                               ETA: 00:21:05

################################################################################
                     [1m Learning iteration 788/2000 [0m                      

                       Computation: 108926 steps/s (collection: 0.800s, learning 0.102s)
             Mean action noise std: 2.78
          Mean value_function loss: 37.7777
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 18.9633
                       Mean reward: 849.19
               Mean episode length: 248.68
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 170.5415
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 0.90s
                      Time elapsed: 00:13:42
                               ETA: 00:21:04

################################################################################
                     [1m Learning iteration 789/2000 [0m                      

                       Computation: 109183 steps/s (collection: 0.811s, learning 0.090s)
             Mean action noise std: 2.79
          Mean value_function loss: 43.6805
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.9830
                       Mean reward: 871.00
               Mean episode length: 248.97
    Episode_Reward/reaching_object: 0.7602
     Episode_Reward/lifting_object: 171.5866
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 0.90s
                      Time elapsed: 00:13:43
                               ETA: 00:21:02

################################################################################
                     [1m Learning iteration 790/2000 [0m                      

                       Computation: 100719 steps/s (collection: 0.883s, learning 0.093s)
             Mean action noise std: 2.79
          Mean value_function loss: 57.8305
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.0023
                       Mean reward: 860.16
               Mean episode length: 246.26
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 171.1572
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 0.98s
                      Time elapsed: 00:13:44
                               ETA: 00:21:01

################################################################################
                     [1m Learning iteration 791/2000 [0m                      

                       Computation: 108410 steps/s (collection: 0.809s, learning 0.098s)
             Mean action noise std: 2.80
          Mean value_function loss: 42.3646
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 19.0143
                       Mean reward: 873.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7504
     Episode_Reward/lifting_object: 169.3186
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 0.91s
                      Time elapsed: 00:13:45
                               ETA: 00:21:00

################################################################################
                     [1m Learning iteration 792/2000 [0m                      

                       Computation: 115435 steps/s (collection: 0.762s, learning 0.090s)
             Mean action noise std: 2.80
          Mean value_function loss: 41.8917
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.0199
                       Mean reward: 841.26
               Mean episode length: 246.91
    Episode_Reward/reaching_object: 0.7619
     Episode_Reward/lifting_object: 170.6281
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 0.85s
                      Time elapsed: 00:13:46
                               ETA: 00:20:59

################################################################################
                     [1m Learning iteration 793/2000 [0m                      

                       Computation: 111336 steps/s (collection: 0.795s, learning 0.088s)
             Mean action noise std: 2.81
          Mean value_function loss: 44.0844
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 19.0333
                       Mean reward: 867.69
               Mean episode length: 249.50
    Episode_Reward/reaching_object: 0.7675
     Episode_Reward/lifting_object: 172.2109
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 0.88s
                      Time elapsed: 00:13:47
                               ETA: 00:20:57

################################################################################
                     [1m Learning iteration 794/2000 [0m                      

                       Computation: 109062 steps/s (collection: 0.806s, learning 0.095s)
             Mean action noise std: 2.81
          Mean value_function loss: 44.7673
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 19.0499
                       Mean reward: 825.88
               Mean episode length: 244.26
    Episode_Reward/reaching_object: 0.7512
     Episode_Reward/lifting_object: 167.9460
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 0.90s
                      Time elapsed: 00:13:48
                               ETA: 00:20:56

################################################################################
                     [1m Learning iteration 795/2000 [0m                      

                       Computation: 103286 steps/s (collection: 0.770s, learning 0.182s)
             Mean action noise std: 2.81
          Mean value_function loss: 38.0621
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.0568
                       Mean reward: 857.44
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 0.7406
     Episode_Reward/lifting_object: 167.5201
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 0.95s
                      Time elapsed: 00:13:49
                               ETA: 00:20:55

################################################################################
                     [1m Learning iteration 796/2000 [0m                      

                       Computation: 104629 steps/s (collection: 0.836s, learning 0.104s)
             Mean action noise std: 2.82
          Mean value_function loss: 18.4731
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 19.0677
                       Mean reward: 857.24
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.7585
     Episode_Reward/lifting_object: 170.8128
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 0.94s
                      Time elapsed: 00:13:50
                               ETA: 00:20:54

################################################################################
                     [1m Learning iteration 797/2000 [0m                      

                       Computation: 114721 steps/s (collection: 0.771s, learning 0.086s)
             Mean action noise std: 2.82
          Mean value_function loss: 34.9832
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 19.0775
                       Mean reward: 872.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 172.0751
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 0.86s
                      Time elapsed: 00:13:51
                               ETA: 00:20:52

################################################################################
                     [1m Learning iteration 798/2000 [0m                      

                       Computation: 104210 steps/s (collection: 0.771s, learning 0.173s)
             Mean action noise std: 2.83
          Mean value_function loss: 50.2250
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.0889
                       Mean reward: 874.64
               Mean episode length: 249.30
    Episode_Reward/reaching_object: 0.7694
     Episode_Reward/lifting_object: 173.6692
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 0.94s
                      Time elapsed: 00:13:51
                               ETA: 00:20:51

################################################################################
                     [1m Learning iteration 799/2000 [0m                      

                       Computation: 110267 steps/s (collection: 0.772s, learning 0.119s)
             Mean action noise std: 2.83
          Mean value_function loss: 42.1338
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 19.0996
                       Mean reward: 832.52
               Mean episode length: 243.85
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 170.0360
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 0.89s
                      Time elapsed: 00:13:52
                               ETA: 00:20:50

################################################################################
                     [1m Learning iteration 800/2000 [0m                      

                       Computation: 112693 steps/s (collection: 0.772s, learning 0.101s)
             Mean action noise std: 2.84
          Mean value_function loss: 47.4571
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 19.1180
                       Mean reward: 859.40
               Mean episode length: 245.97
    Episode_Reward/reaching_object: 0.7534
     Episode_Reward/lifting_object: 170.1639
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 0.87s
                      Time elapsed: 00:13:53
                               ETA: 00:20:49

################################################################################
                     [1m Learning iteration 801/2000 [0m                      

                       Computation: 98182 steps/s (collection: 0.853s, learning 0.148s)
             Mean action noise std: 2.85
          Mean value_function loss: 43.5088
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.1509
                       Mean reward: 843.13
               Mean episode length: 244.57
    Episode_Reward/reaching_object: 0.7503
     Episode_Reward/lifting_object: 169.1153
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 1.00s
                      Time elapsed: 00:13:54
                               ETA: 00:20:47

################################################################################
                     [1m Learning iteration 802/2000 [0m                      

                       Computation: 104529 steps/s (collection: 0.826s, learning 0.115s)
             Mean action noise std: 2.86
          Mean value_function loss: 39.6640
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 19.1736
                       Mean reward: 865.63
               Mean episode length: 248.79
    Episode_Reward/reaching_object: 0.7526
     Episode_Reward/lifting_object: 170.3205
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 0.94s
                      Time elapsed: 00:13:55
                               ETA: 00:20:46

################################################################################
                     [1m Learning iteration 803/2000 [0m                      

                       Computation: 109547 steps/s (collection: 0.792s, learning 0.106s)
             Mean action noise std: 2.86
          Mean value_function loss: 52.8847
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 19.1839
                       Mean reward: 834.02
               Mean episode length: 245.80
    Episode_Reward/reaching_object: 0.7519
     Episode_Reward/lifting_object: 169.9206
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 0.90s
                      Time elapsed: 00:13:56
                               ETA: 00:20:45

################################################################################
                     [1m Learning iteration 804/2000 [0m                      

                       Computation: 105410 steps/s (collection: 0.823s, learning 0.110s)
             Mean action noise std: 2.86
          Mean value_function loss: 40.6929
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.1937
                       Mean reward: 852.15
               Mean episode length: 246.37
    Episode_Reward/reaching_object: 0.7519
     Episode_Reward/lifting_object: 169.1396
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 0.93s
                      Time elapsed: 00:13:57
                               ETA: 00:20:44

################################################################################
                     [1m Learning iteration 805/2000 [0m                      

                       Computation: 108232 steps/s (collection: 0.803s, learning 0.105s)
             Mean action noise std: 2.87
          Mean value_function loss: 51.0498
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 19.2102
                       Mean reward: 862.19
               Mean episode length: 249.41
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 172.3028
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 0.91s
                      Time elapsed: 00:13:58
                               ETA: 00:20:43

################################################################################
                     [1m Learning iteration 806/2000 [0m                      

                       Computation: 104690 steps/s (collection: 0.836s, learning 0.103s)
             Mean action noise std: 2.88
          Mean value_function loss: 47.9358
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 19.2239
                       Mean reward: 841.99
               Mean episode length: 245.41
    Episode_Reward/reaching_object: 0.7484
     Episode_Reward/lifting_object: 169.4490
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 0.94s
                      Time elapsed: 00:13:59
                               ETA: 00:20:41

################################################################################
                     [1m Learning iteration 807/2000 [0m                      

                       Computation: 108220 steps/s (collection: 0.799s, learning 0.110s)
             Mean action noise std: 2.88
          Mean value_function loss: 43.3623
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 19.2412
                       Mean reward: 861.16
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.7466
     Episode_Reward/lifting_object: 170.3304
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 0.91s
                      Time elapsed: 00:14:00
                               ETA: 00:20:40

################################################################################
                     [1m Learning iteration 808/2000 [0m                      

                       Computation: 105744 steps/s (collection: 0.824s, learning 0.106s)
             Mean action noise std: 2.88
          Mean value_function loss: 45.5584
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 19.2443
                       Mean reward: 861.72
               Mean episode length: 248.88
    Episode_Reward/reaching_object: 0.7502
     Episode_Reward/lifting_object: 170.2759
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 0.93s
                      Time elapsed: 00:14:01
                               ETA: 00:20:39

################################################################################
                     [1m Learning iteration 809/2000 [0m                      

                       Computation: 109232 steps/s (collection: 0.788s, learning 0.112s)
             Mean action noise std: 2.89
          Mean value_function loss: 47.8805
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 19.2520
                       Mean reward: 850.08
               Mean episode length: 246.33
    Episode_Reward/reaching_object: 0.7520
     Episode_Reward/lifting_object: 171.1848
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 0.90s
                      Time elapsed: 00:14:02
                               ETA: 00:20:38

################################################################################
                     [1m Learning iteration 810/2000 [0m                      

                       Computation: 107006 steps/s (collection: 0.778s, learning 0.141s)
             Mean action noise std: 2.89
          Mean value_function loss: 63.4804
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.2664
                       Mean reward: 847.17
               Mean episode length: 245.57
    Episode_Reward/reaching_object: 0.7427
     Episode_Reward/lifting_object: 169.9527
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 0.92s
                      Time elapsed: 00:14:03
                               ETA: 00:20:36

################################################################################
                     [1m Learning iteration 811/2000 [0m                      

                       Computation: 103575 steps/s (collection: 0.822s, learning 0.127s)
             Mean action noise std: 2.90
          Mean value_function loss: 29.8084
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.2806
                       Mean reward: 838.07
               Mean episode length: 246.39
    Episode_Reward/reaching_object: 0.7468
     Episode_Reward/lifting_object: 169.5281
      Episode_Reward/object_height: 0.0615
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 0.95s
                      Time elapsed: 00:14:03
                               ETA: 00:20:35

################################################################################
                     [1m Learning iteration 812/2000 [0m                      

                       Computation: 98439 steps/s (collection: 0.865s, learning 0.134s)
             Mean action noise std: 2.91
          Mean value_function loss: 29.8275
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 19.3085
                       Mean reward: 860.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7546
     Episode_Reward/lifting_object: 171.8487
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 1.00s
                      Time elapsed: 00:14:04
                               ETA: 00:20:34

################################################################################
                     [1m Learning iteration 813/2000 [0m                      

                       Computation: 97069 steps/s (collection: 0.787s, learning 0.225s)
             Mean action noise std: 2.92
          Mean value_function loss: 39.3314
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 19.3269
                       Mean reward: 866.06
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 172.4684
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 1.01s
                      Time elapsed: 00:14:05
                               ETA: 00:20:33

################################################################################
                     [1m Learning iteration 814/2000 [0m                      

                       Computation: 99308 steps/s (collection: 0.846s, learning 0.144s)
             Mean action noise std: 2.92
          Mean value_function loss: 41.7925
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 19.3450
                       Mean reward: 861.09
               Mean episode length: 249.91
    Episode_Reward/reaching_object: 0.7472
     Episode_Reward/lifting_object: 169.6906
      Episode_Reward/object_height: 0.0615
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 0.99s
                      Time elapsed: 00:14:06
                               ETA: 00:20:32

################################################################################
                     [1m Learning iteration 815/2000 [0m                      

                       Computation: 108834 steps/s (collection: 0.788s, learning 0.116s)
             Mean action noise std: 2.93
          Mean value_function loss: 48.3087
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 19.3623
                       Mean reward: 869.29
               Mean episode length: 249.42
    Episode_Reward/reaching_object: 0.7591
     Episode_Reward/lifting_object: 172.2546
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 0.90s
                      Time elapsed: 00:14:07
                               ETA: 00:20:31

################################################################################
                     [1m Learning iteration 816/2000 [0m                      

                       Computation: 113344 steps/s (collection: 0.776s, learning 0.091s)
             Mean action noise std: 2.94
          Mean value_function loss: 30.1859
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 19.3820
                       Mean reward: 863.42
               Mean episode length: 247.63
    Episode_Reward/reaching_object: 0.7470
     Episode_Reward/lifting_object: 169.2999
      Episode_Reward/object_height: 0.0615
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 0.87s
                      Time elapsed: 00:14:08
                               ETA: 00:20:30

################################################################################
                     [1m Learning iteration 817/2000 [0m                      

                       Computation: 110957 steps/s (collection: 0.797s, learning 0.089s)
             Mean action noise std: 2.94
          Mean value_function loss: 40.9992
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 19.4011
                       Mean reward: 857.58
               Mean episode length: 245.84
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 172.1484
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 0.89s
                      Time elapsed: 00:14:09
                               ETA: 00:20:28

################################################################################
                     [1m Learning iteration 818/2000 [0m                      

                       Computation: 107604 steps/s (collection: 0.818s, learning 0.096s)
             Mean action noise std: 2.95
          Mean value_function loss: 49.5705
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 19.4132
                       Mean reward: 879.77
               Mean episode length: 249.85
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 170.9832
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 0.91s
                      Time elapsed: 00:14:10
                               ETA: 00:20:27

################################################################################
                     [1m Learning iteration 819/2000 [0m                      

                       Computation: 113855 steps/s (collection: 0.773s, learning 0.090s)
             Mean action noise std: 2.95
          Mean value_function loss: 43.1989
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.4200
                       Mean reward: 863.84
               Mean episode length: 247.46
    Episode_Reward/reaching_object: 0.7547
     Episode_Reward/lifting_object: 170.6160
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 0.86s
                      Time elapsed: 00:14:11
                               ETA: 00:20:26

################################################################################
                     [1m Learning iteration 820/2000 [0m                      

                       Computation: 108214 steps/s (collection: 0.814s, learning 0.094s)
             Mean action noise std: 2.95
          Mean value_function loss: 31.9738
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.4311
                       Mean reward: 855.14
               Mean episode length: 246.12
    Episode_Reward/reaching_object: 0.7634
     Episode_Reward/lifting_object: 172.5419
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 0.91s
                      Time elapsed: 00:14:12
                               ETA: 00:20:25

################################################################################
                     [1m Learning iteration 821/2000 [0m                      

                       Computation: 109805 steps/s (collection: 0.786s, learning 0.109s)
             Mean action noise std: 2.96
          Mean value_function loss: 34.9864
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 19.4409
                       Mean reward: 858.04
               Mean episode length: 247.25
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 171.3293
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 0.90s
                      Time elapsed: 00:14:13
                               ETA: 00:20:23

################################################################################
                     [1m Learning iteration 822/2000 [0m                      

                       Computation: 105389 steps/s (collection: 0.806s, learning 0.127s)
             Mean action noise std: 2.97
          Mean value_function loss: 39.3460
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.4634
                       Mean reward: 866.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7555
     Episode_Reward/lifting_object: 171.5445
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 0.93s
                      Time elapsed: 00:14:14
                               ETA: 00:20:22

################################################################################
                     [1m Learning iteration 823/2000 [0m                      

                       Computation: 112318 steps/s (collection: 0.781s, learning 0.094s)
             Mean action noise std: 2.97
          Mean value_function loss: 30.5848
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.4840
                       Mean reward: 882.02
               Mean episode length: 249.56
    Episode_Reward/reaching_object: 0.7596
     Episode_Reward/lifting_object: 170.1542
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 0.88s
                      Time elapsed: 00:14:15
                               ETA: 00:20:21

################################################################################
                     [1m Learning iteration 824/2000 [0m                      

                       Computation: 109747 steps/s (collection: 0.758s, learning 0.137s)
             Mean action noise std: 2.98
          Mean value_function loss: 36.0699
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 19.5035
                       Mean reward: 872.04
               Mean episode length: 249.77
    Episode_Reward/reaching_object: 0.7614
     Episode_Reward/lifting_object: 172.6826
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 0.90s
                      Time elapsed: 00:14:15
                               ETA: 00:20:20

################################################################################
                     [1m Learning iteration 825/2000 [0m                      

                       Computation: 102668 steps/s (collection: 0.839s, learning 0.119s)
             Mean action noise std: 2.99
          Mean value_function loss: 30.1060
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 19.5190
                       Mean reward: 870.37
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.7587
     Episode_Reward/lifting_object: 172.0108
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 0.96s
                      Time elapsed: 00:14:16
                               ETA: 00:20:18

################################################################################
                     [1m Learning iteration 826/2000 [0m                      

                       Computation: 106695 steps/s (collection: 0.809s, learning 0.112s)
             Mean action noise std: 2.99
          Mean value_function loss: 39.4073
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 19.5335
                       Mean reward: 877.20
               Mean episode length: 249.93
    Episode_Reward/reaching_object: 0.7586
     Episode_Reward/lifting_object: 171.9896
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 0.92s
                      Time elapsed: 00:14:17
                               ETA: 00:20:17

################################################################################
                     [1m Learning iteration 827/2000 [0m                      

                       Computation: 107294 steps/s (collection: 0.826s, learning 0.091s)
             Mean action noise std: 3.00
          Mean value_function loss: 38.8548
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 19.5530
                       Mean reward: 851.97
               Mean episode length: 247.31
    Episode_Reward/reaching_object: 0.7582
     Episode_Reward/lifting_object: 171.8654
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 0.92s
                      Time elapsed: 00:14:18
                               ETA: 00:20:16

################################################################################
                     [1m Learning iteration 828/2000 [0m                      

                       Computation: 109954 steps/s (collection: 0.788s, learning 0.106s)
             Mean action noise std: 3.00
          Mean value_function loss: 40.4914
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 19.5619
                       Mean reward: 861.68
               Mean episode length: 246.93
    Episode_Reward/reaching_object: 0.7532
     Episode_Reward/lifting_object: 171.0691
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 0.89s
                      Time elapsed: 00:14:19
                               ETA: 00:20:15

################################################################################
                     [1m Learning iteration 829/2000 [0m                      

                       Computation: 108178 steps/s (collection: 0.816s, learning 0.092s)
             Mean action noise std: 3.01
          Mean value_function loss: 43.9195
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.5686
                       Mean reward: 857.94
               Mean episode length: 247.78
    Episode_Reward/reaching_object: 0.7463
     Episode_Reward/lifting_object: 170.5799
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 0.91s
                      Time elapsed: 00:14:20
                               ETA: 00:20:14

################################################################################
                     [1m Learning iteration 830/2000 [0m                      

                       Computation: 108368 steps/s (collection: 0.787s, learning 0.121s)
             Mean action noise std: 3.01
          Mean value_function loss: 33.2730
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.5781
                       Mean reward: 868.23
               Mean episode length: 249.01
    Episode_Reward/reaching_object: 0.7551
     Episode_Reward/lifting_object: 171.6373
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 0.91s
                      Time elapsed: 00:14:21
                               ETA: 00:20:12

################################################################################
                     [1m Learning iteration 831/2000 [0m                      

                       Computation: 110985 steps/s (collection: 0.796s, learning 0.090s)
             Mean action noise std: 3.02
          Mean value_function loss: 27.1110
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.5968
                       Mean reward: 877.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7564
     Episode_Reward/lifting_object: 171.5019
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 0.89s
                      Time elapsed: 00:14:22
                               ETA: 00:20:11

################################################################################
                     [1m Learning iteration 832/2000 [0m                      

                       Computation: 108266 steps/s (collection: 0.801s, learning 0.107s)
             Mean action noise std: 3.03
          Mean value_function loss: 38.6413
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.6268
                       Mean reward: 866.41
               Mean episode length: 248.64
    Episode_Reward/reaching_object: 0.7669
     Episode_Reward/lifting_object: 172.8842
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 0.91s
                      Time elapsed: 00:14:23
                               ETA: 00:20:10

################################################################################
                     [1m Learning iteration 833/2000 [0m                      

                       Computation: 111055 steps/s (collection: 0.779s, learning 0.106s)
             Mean action noise std: 3.04
          Mean value_function loss: 39.2898
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 19.6512
                       Mean reward: 866.54
               Mean episode length: 247.90
    Episode_Reward/reaching_object: 0.7539
     Episode_Reward/lifting_object: 169.9383
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 0.89s
                      Time elapsed: 00:14:24
                               ETA: 00:20:09

################################################################################
                     [1m Learning iteration 834/2000 [0m                      

                       Computation: 105641 steps/s (collection: 0.791s, learning 0.140s)
             Mean action noise std: 3.04
          Mean value_function loss: 38.7123
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 19.6692
                       Mean reward: 859.96
               Mean episode length: 248.81
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 171.9129
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.0417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 0.93s
                      Time elapsed: 00:14:25
                               ETA: 00:20:07

################################################################################
                     [1m Learning iteration 835/2000 [0m                      

                       Computation: 109310 steps/s (collection: 0.780s, learning 0.119s)
             Mean action noise std: 3.05
          Mean value_function loss: 46.4219
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.6869
                       Mean reward: 873.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 171.3763
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 0.90s
                      Time elapsed: 00:14:25
                               ETA: 00:20:06

################################################################################
                     [1m Learning iteration 836/2000 [0m                      

                       Computation: 105653 steps/s (collection: 0.791s, learning 0.139s)
             Mean action noise std: 3.06
          Mean value_function loss: 29.9546
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 19.7002
                       Mean reward: 852.07
               Mean episode length: 246.19
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 170.9130
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 0.93s
                      Time elapsed: 00:14:26
                               ETA: 00:20:05

################################################################################
                     [1m Learning iteration 837/2000 [0m                      

                       Computation: 101748 steps/s (collection: 0.849s, learning 0.118s)
             Mean action noise std: 3.06
          Mean value_function loss: 33.3412
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 19.7146
                       Mean reward: 837.24
               Mean episode length: 244.80
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 171.0134
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 0.97s
                      Time elapsed: 00:14:27
                               ETA: 00:20:04

################################################################################
                     [1m Learning iteration 838/2000 [0m                      

                       Computation: 97596 steps/s (collection: 0.864s, learning 0.143s)
             Mean action noise std: 3.06
          Mean value_function loss: 33.7718
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 19.7200
                       Mean reward: 865.38
               Mean episode length: 249.51
    Episode_Reward/reaching_object: 0.7687
     Episode_Reward/lifting_object: 172.0233
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 1.01s
                      Time elapsed: 00:14:28
                               ETA: 00:20:03

################################################################################
                     [1m Learning iteration 839/2000 [0m                      

                       Computation: 110420 steps/s (collection: 0.788s, learning 0.102s)
             Mean action noise std: 3.07
          Mean value_function loss: 38.9910
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 19.7355
                       Mean reward: 872.21
               Mean episode length: 248.52
    Episode_Reward/reaching_object: 0.7507
     Episode_Reward/lifting_object: 169.0745
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 0.89s
                      Time elapsed: 00:14:29
                               ETA: 00:20:02

################################################################################
                     [1m Learning iteration 840/2000 [0m                      

                       Computation: 112252 steps/s (collection: 0.783s, learning 0.093s)
             Mean action noise std: 3.07
          Mean value_function loss: 33.3683
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 19.7453
                       Mean reward: 852.50
               Mean episode length: 247.02
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 170.3530
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 0.88s
                      Time elapsed: 00:14:30
                               ETA: 00:20:00

################################################################################
                     [1m Learning iteration 841/2000 [0m                      

                       Computation: 111612 steps/s (collection: 0.786s, learning 0.094s)
             Mean action noise std: 3.08
          Mean value_function loss: 32.7502
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 19.7512
                       Mean reward: 866.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7602
     Episode_Reward/lifting_object: 171.7203
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 0.88s
                      Time elapsed: 00:14:31
                               ETA: 00:19:59

################################################################################
                     [1m Learning iteration 842/2000 [0m                      

                       Computation: 110457 steps/s (collection: 0.773s, learning 0.117s)
             Mean action noise std: 3.08
          Mean value_function loss: 31.1909
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 19.7594
                       Mean reward: 861.47
               Mean episode length: 249.34
    Episode_Reward/reaching_object: 0.7615
     Episode_Reward/lifting_object: 172.3775
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 0.89s
                      Time elapsed: 00:14:32
                               ETA: 00:19:58

################################################################################
                     [1m Learning iteration 843/2000 [0m                      

                       Computation: 101504 steps/s (collection: 0.834s, learning 0.134s)
             Mean action noise std: 3.09
          Mean value_function loss: 38.0238
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 19.7727
                       Mean reward: 868.88
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.7511
     Episode_Reward/lifting_object: 170.2493
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 0.97s
                      Time elapsed: 00:14:33
                               ETA: 00:19:57

################################################################################
                     [1m Learning iteration 844/2000 [0m                      

                       Computation: 108399 steps/s (collection: 0.796s, learning 0.111s)
             Mean action noise std: 3.09
          Mean value_function loss: 36.8041
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 19.7905
                       Mean reward: 860.95
               Mean episode length: 249.04
    Episode_Reward/reaching_object: 0.7541
     Episode_Reward/lifting_object: 171.0844
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 0.91s
                      Time elapsed: 00:14:34
                               ETA: 00:19:56

################################################################################
                     [1m Learning iteration 845/2000 [0m                      

                       Computation: 107291 steps/s (collection: 0.798s, learning 0.118s)
             Mean action noise std: 3.10
          Mean value_function loss: 30.0602
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.8011
                       Mean reward: 870.76
               Mean episode length: 248.81
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 173.0047
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 0.92s
                      Time elapsed: 00:14:35
                               ETA: 00:19:54

################################################################################
                     [1m Learning iteration 846/2000 [0m                      

                       Computation: 107983 steps/s (collection: 0.818s, learning 0.093s)
             Mean action noise std: 3.10
          Mean value_function loss: 30.7311
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.8142
                       Mean reward: 853.98
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 172.4237
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 0.91s
                      Time elapsed: 00:14:36
                               ETA: 00:19:53

################################################################################
                     [1m Learning iteration 847/2000 [0m                      

                       Computation: 114918 steps/s (collection: 0.762s, learning 0.093s)
             Mean action noise std: 3.11
          Mean value_function loss: 24.9157
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 19.8271
                       Mean reward: 876.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 173.4129
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 0.86s
                      Time elapsed: 00:14:36
                               ETA: 00:19:52

################################################################################
                     [1m Learning iteration 848/2000 [0m                      

                       Computation: 108821 steps/s (collection: 0.758s, learning 0.146s)
             Mean action noise std: 3.11
          Mean value_function loss: 32.3794
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 19.8404
                       Mean reward: 865.68
               Mean episode length: 249.86
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 171.3424
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 0.90s
                      Time elapsed: 00:14:37
                               ETA: 00:19:51

################################################################################
                     [1m Learning iteration 849/2000 [0m                      

                       Computation: 111535 steps/s (collection: 0.787s, learning 0.095s)
             Mean action noise std: 3.12
          Mean value_function loss: 34.4459
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.8485
                       Mean reward: 858.68
               Mean episode length: 247.92
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 171.8401
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 0.88s
                      Time elapsed: 00:14:38
                               ETA: 00:19:49

################################################################################
                     [1m Learning iteration 850/2000 [0m                      

                       Computation: 111815 steps/s (collection: 0.794s, learning 0.085s)
             Mean action noise std: 3.12
          Mean value_function loss: 45.9026
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 19.8567
                       Mean reward: 869.59
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7588
     Episode_Reward/lifting_object: 170.9263
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 0.88s
                      Time elapsed: 00:14:39
                               ETA: 00:19:48

################################################################################
                     [1m Learning iteration 851/2000 [0m                      

                       Computation: 103526 steps/s (collection: 0.865s, learning 0.085s)
             Mean action noise std: 3.12
          Mean value_function loss: 29.6079
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.8619
                       Mean reward: 856.69
               Mean episode length: 245.45
    Episode_Reward/reaching_object: 0.7639
     Episode_Reward/lifting_object: 172.0127
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 0.95s
                      Time elapsed: 00:14:40
                               ETA: 00:19:47

################################################################################
                     [1m Learning iteration 852/2000 [0m                      

                       Computation: 100745 steps/s (collection: 0.887s, learning 0.089s)
             Mean action noise std: 3.13
          Mean value_function loss: 37.9062
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.8737
                       Mean reward: 871.09
               Mean episode length: 249.34
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 171.4081
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 0.98s
                      Time elapsed: 00:14:41
                               ETA: 00:19:46

################################################################################
                     [1m Learning iteration 853/2000 [0m                      

                       Computation: 113674 steps/s (collection: 0.768s, learning 0.096s)
             Mean action noise std: 3.13
          Mean value_function loss: 34.8313
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 19.8886
                       Mean reward: 874.60
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7668
     Episode_Reward/lifting_object: 172.6250
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 0.86s
                      Time elapsed: 00:14:42
                               ETA: 00:19:45

################################################################################
                     [1m Learning iteration 854/2000 [0m                      

                       Computation: 109788 steps/s (collection: 0.807s, learning 0.088s)
             Mean action noise std: 3.13
          Mean value_function loss: 34.5868
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 19.8939
                       Mean reward: 868.69
               Mean episode length: 246.89
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 172.4709
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 0.90s
                      Time elapsed: 00:14:43
                               ETA: 00:19:43

################################################################################
                     [1m Learning iteration 855/2000 [0m                      

                       Computation: 110822 steps/s (collection: 0.792s, learning 0.096s)
             Mean action noise std: 3.14
          Mean value_function loss: 27.3408
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 19.9105
                       Mean reward: 856.71
               Mean episode length: 247.72
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 170.7521
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 0.89s
                      Time elapsed: 00:14:44
                               ETA: 00:19:42

################################################################################
                     [1m Learning iteration 856/2000 [0m                      

                       Computation: 108476 steps/s (collection: 0.800s, learning 0.106s)
             Mean action noise std: 3.14
          Mean value_function loss: 33.8916
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 19.9200
                       Mean reward: 839.82
               Mean episode length: 246.76
    Episode_Reward/reaching_object: 0.7641
     Episode_Reward/lifting_object: 171.7165
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 0.91s
                      Time elapsed: 00:14:45
                               ETA: 00:19:41

################################################################################
                     [1m Learning iteration 857/2000 [0m                      

                       Computation: 104853 steps/s (collection: 0.796s, learning 0.141s)
             Mean action noise std: 3.14
          Mean value_function loss: 35.6557
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 19.9225
                       Mean reward: 873.43
               Mean episode length: 249.09
    Episode_Reward/reaching_object: 0.7609
     Episode_Reward/lifting_object: 171.2301
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 0.94s
                      Time elapsed: 00:14:46
                               ETA: 00:19:40

################################################################################
                     [1m Learning iteration 858/2000 [0m                      

                       Computation: 107098 steps/s (collection: 0.780s, learning 0.138s)
             Mean action noise std: 3.15
          Mean value_function loss: 29.6536
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 19.9307
                       Mean reward: 879.34
               Mean episode length: 248.91
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 171.8524
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 0.92s
                      Time elapsed: 00:14:46
                               ETA: 00:19:39

################################################################################
                     [1m Learning iteration 859/2000 [0m                      

                       Computation: 106981 steps/s (collection: 0.816s, learning 0.103s)
             Mean action noise std: 3.15
          Mean value_function loss: 43.8524
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 19.9452
                       Mean reward: 871.77
               Mean episode length: 248.91
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 172.1052
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 0.92s
                      Time elapsed: 00:14:47
                               ETA: 00:19:37

################################################################################
                     [1m Learning iteration 860/2000 [0m                      

                       Computation: 105754 steps/s (collection: 0.791s, learning 0.138s)
             Mean action noise std: 3.16
          Mean value_function loss: 35.1112
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.9557
                       Mean reward: 850.71
               Mean episode length: 246.21
    Episode_Reward/reaching_object: 0.7557
     Episode_Reward/lifting_object: 170.4958
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 0.93s
                      Time elapsed: 00:14:48
                               ETA: 00:19:36

################################################################################
                     [1m Learning iteration 861/2000 [0m                      

                       Computation: 108530 steps/s (collection: 0.799s, learning 0.107s)
             Mean action noise std: 3.16
          Mean value_function loss: 30.3703
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.9640
                       Mean reward: 853.45
               Mean episode length: 247.11
    Episode_Reward/reaching_object: 0.7479
     Episode_Reward/lifting_object: 168.4267
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 0.91s
                      Time elapsed: 00:14:49
                               ETA: 00:19:35

################################################################################
                     [1m Learning iteration 862/2000 [0m                      

                       Computation: 108096 steps/s (collection: 0.787s, learning 0.123s)
             Mean action noise std: 3.17
          Mean value_function loss: 33.7691
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.9814
                       Mean reward: 850.39
               Mean episode length: 247.71
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 172.0610
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 0.91s
                      Time elapsed: 00:14:50
                               ETA: 00:19:34

################################################################################
                     [1m Learning iteration 863/2000 [0m                      

                       Computation: 105775 steps/s (collection: 0.836s, learning 0.094s)
             Mean action noise std: 3.17
          Mean value_function loss: 33.8877
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.0009
                       Mean reward: 840.63
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7548
     Episode_Reward/lifting_object: 171.2539
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 0.93s
                      Time elapsed: 00:14:51
                               ETA: 00:19:33

################################################################################
                     [1m Learning iteration 864/2000 [0m                      

                       Computation: 109484 steps/s (collection: 0.786s, learning 0.112s)
             Mean action noise std: 3.18
          Mean value_function loss: 34.3022
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 20.0095
                       Mean reward: 875.19
               Mean episode length: 249.67
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 173.8300
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 0.90s
                      Time elapsed: 00:14:52
                               ETA: 00:19:32

################################################################################
                     [1m Learning iteration 865/2000 [0m                      

                       Computation: 107706 steps/s (collection: 0.817s, learning 0.096s)
             Mean action noise std: 3.18
          Mean value_function loss: 28.9370
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.0254
                       Mean reward: 872.50
               Mean episode length: 249.45
    Episode_Reward/reaching_object: 0.7669
     Episode_Reward/lifting_object: 172.6508
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 0.91s
                      Time elapsed: 00:14:53
                               ETA: 00:19:30

################################################################################
                     [1m Learning iteration 866/2000 [0m                      

                       Computation: 106327 steps/s (collection: 0.834s, learning 0.091s)
             Mean action noise std: 3.19
          Mean value_function loss: 36.1929
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.0369
                       Mean reward: 863.28
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.7678
     Episode_Reward/lifting_object: 173.3532
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 0.92s
                      Time elapsed: 00:14:54
                               ETA: 00:19:29

################################################################################
                     [1m Learning iteration 867/2000 [0m                      

                       Computation: 102718 steps/s (collection: 0.864s, learning 0.093s)
             Mean action noise std: 3.19
          Mean value_function loss: 38.1753
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 20.0477
                       Mean reward: 867.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 172.1745
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 0.96s
                      Time elapsed: 00:14:55
                               ETA: 00:19:28

################################################################################
                     [1m Learning iteration 868/2000 [0m                      

                       Computation: 105506 steps/s (collection: 0.842s, learning 0.090s)
             Mean action noise std: 3.20
          Mean value_function loss: 40.2140
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 20.0695
                       Mean reward: 851.42
               Mean episode length: 249.16
    Episode_Reward/reaching_object: 0.7557
     Episode_Reward/lifting_object: 169.8951
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 0.93s
                      Time elapsed: 00:14:56
                               ETA: 00:19:27

################################################################################
                     [1m Learning iteration 869/2000 [0m                      

                       Computation: 102684 steps/s (collection: 0.867s, learning 0.091s)
             Mean action noise std: 3.21
          Mean value_function loss: 46.4265
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 20.0822
                       Mean reward: 862.27
               Mean episode length: 247.62
    Episode_Reward/reaching_object: 0.7630
     Episode_Reward/lifting_object: 171.6259
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 0.96s
                      Time elapsed: 00:14:57
                               ETA: 00:19:26

################################################################################
                     [1m Learning iteration 870/2000 [0m                      

                       Computation: 106367 steps/s (collection: 0.798s, learning 0.126s)
             Mean action noise std: 3.21
          Mean value_function loss: 27.2629
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 20.0886
                       Mean reward: 860.71
               Mean episode length: 246.62
    Episode_Reward/reaching_object: 0.7553
     Episode_Reward/lifting_object: 170.2650
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 0.92s
                      Time elapsed: 00:14:58
                               ETA: 00:19:25

################################################################################
                     [1m Learning iteration 871/2000 [0m                      

                       Computation: 106797 steps/s (collection: 0.778s, learning 0.142s)
             Mean action noise std: 3.21
          Mean value_function loss: 35.4095
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 20.1009
                       Mean reward: 877.01
               Mean episode length: 248.89
    Episode_Reward/reaching_object: 0.7739
     Episode_Reward/lifting_object: 174.1789
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 0.92s
                      Time elapsed: 00:14:58
                               ETA: 00:19:23

################################################################################
                     [1m Learning iteration 872/2000 [0m                      

                       Computation: 104601 steps/s (collection: 0.803s, learning 0.137s)
             Mean action noise std: 3.22
          Mean value_function loss: 28.6473
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 20.1167
                       Mean reward: 855.99
               Mean episode length: 247.66
    Episode_Reward/reaching_object: 0.7502
     Episode_Reward/lifting_object: 170.3141
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 0.94s
                      Time elapsed: 00:14:59
                               ETA: 00:19:22

################################################################################
                     [1m Learning iteration 873/2000 [0m                      

                       Computation: 114190 steps/s (collection: 0.766s, learning 0.095s)
             Mean action noise std: 3.23
          Mean value_function loss: 32.5451
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 20.1327
                       Mean reward: 866.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7578
     Episode_Reward/lifting_object: 171.3304
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 0.86s
                      Time elapsed: 00:15:00
                               ETA: 00:19:21

################################################################################
                     [1m Learning iteration 874/2000 [0m                      

                       Computation: 115952 steps/s (collection: 0.757s, learning 0.091s)
             Mean action noise std: 3.24
          Mean value_function loss: 34.7827
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 20.1526
                       Mean reward: 857.80
               Mean episode length: 246.72
    Episode_Reward/reaching_object: 0.7546
     Episode_Reward/lifting_object: 171.0236
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 0.85s
                      Time elapsed: 00:15:01
                               ETA: 00:19:20

################################################################################
                     [1m Learning iteration 875/2000 [0m                      

                       Computation: 106939 steps/s (collection: 0.832s, learning 0.087s)
             Mean action noise std: 3.24
          Mean value_function loss: 33.4212
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.1647
                       Mean reward: 864.61
               Mean episode length: 248.85
    Episode_Reward/reaching_object: 0.7566
     Episode_Reward/lifting_object: 171.1652
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 0.92s
                      Time elapsed: 00:15:02
                               ETA: 00:19:19

################################################################################
                     [1m Learning iteration 876/2000 [0m                      

                       Computation: 115745 steps/s (collection: 0.763s, learning 0.087s)
             Mean action noise std: 3.25
          Mean value_function loss: 38.9724
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 20.1790
                       Mean reward: 859.42
               Mean episode length: 245.24
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 172.9982
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 0.85s
                      Time elapsed: 00:15:03
                               ETA: 00:19:17

################################################################################
                     [1m Learning iteration 877/2000 [0m                      

                       Computation: 112018 steps/s (collection: 0.788s, learning 0.090s)
             Mean action noise std: 3.25
          Mean value_function loss: 26.2985
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.1912
                       Mean reward: 860.20
               Mean episode length: 247.73
    Episode_Reward/reaching_object: 0.7576
     Episode_Reward/lifting_object: 171.7433
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 0.88s
                      Time elapsed: 00:15:04
                               ETA: 00:19:16

################################################################################
                     [1m Learning iteration 878/2000 [0m                      

                       Computation: 112948 steps/s (collection: 0.782s, learning 0.088s)
             Mean action noise std: 3.26
          Mean value_function loss: 27.0845
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 20.2085
                       Mean reward: 866.56
               Mean episode length: 249.18
    Episode_Reward/reaching_object: 0.7660
     Episode_Reward/lifting_object: 172.7215
      Episode_Reward/object_height: 0.0615
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 0.87s
                      Time elapsed: 00:15:05
                               ETA: 00:19:15

################################################################################
                     [1m Learning iteration 879/2000 [0m                      

                       Computation: 98766 steps/s (collection: 0.823s, learning 0.173s)
             Mean action noise std: 3.27
          Mean value_function loss: 29.6049
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 20.2226
                       Mean reward: 879.07
               Mean episode length: 249.45
    Episode_Reward/reaching_object: 0.7663
     Episode_Reward/lifting_object: 172.4139
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 1.00s
                      Time elapsed: 00:15:06
                               ETA: 00:19:14

################################################################################
                     [1m Learning iteration 880/2000 [0m                      

                       Computation: 111099 steps/s (collection: 0.783s, learning 0.102s)
             Mean action noise std: 3.27
          Mean value_function loss: 26.4127
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 20.2452
                       Mean reward: 877.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7683
     Episode_Reward/lifting_object: 172.2839
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 0.88s
                      Time elapsed: 00:15:06
                               ETA: 00:19:13

################################################################################
                     [1m Learning iteration 881/2000 [0m                      

                       Computation: 106318 steps/s (collection: 0.754s, learning 0.171s)
             Mean action noise std: 3.28
          Mean value_function loss: 35.8707
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.2601
                       Mean reward: 867.34
               Mean episode length: 249.16
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 173.4830
      Episode_Reward/object_height: 0.0615
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 0.92s
                      Time elapsed: 00:15:07
                               ETA: 00:19:11

################################################################################
                     [1m Learning iteration 882/2000 [0m                      

                       Computation: 116168 steps/s (collection: 0.759s, learning 0.088s)
             Mean action noise std: 3.29
          Mean value_function loss: 46.2219
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 20.2773
                       Mean reward: 872.33
               Mean episode length: 249.66
    Episode_Reward/reaching_object: 0.7659
     Episode_Reward/lifting_object: 172.1127
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 0.85s
                      Time elapsed: 00:15:08
                               ETA: 00:19:10

################################################################################
                     [1m Learning iteration 883/2000 [0m                      

                       Computation: 114604 steps/s (collection: 0.769s, learning 0.089s)
             Mean action noise std: 3.29
          Mean value_function loss: 31.0021
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 20.2919
                       Mean reward: 862.24
               Mean episode length: 247.77
    Episode_Reward/reaching_object: 0.7533
     Episode_Reward/lifting_object: 169.2198
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 0.86s
                      Time elapsed: 00:15:09
                               ETA: 00:19:09

################################################################################
                     [1m Learning iteration 884/2000 [0m                      

                       Computation: 107844 steps/s (collection: 0.822s, learning 0.089s)
             Mean action noise std: 3.30
          Mean value_function loss: 32.1513
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.3051
                       Mean reward: 875.80
               Mean episode length: 249.52
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 173.2825
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 0.91s
                      Time elapsed: 00:15:10
                               ETA: 00:19:08

################################################################################
                     [1m Learning iteration 885/2000 [0m                      

                       Computation: 110464 steps/s (collection: 0.800s, learning 0.090s)
             Mean action noise std: 3.30
          Mean value_function loss: 33.7382
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 20.3211
                       Mean reward: 869.87
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.7602
     Episode_Reward/lifting_object: 171.4702
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 0.89s
                      Time elapsed: 00:15:11
                               ETA: 00:19:06

################################################################################
                     [1m Learning iteration 886/2000 [0m                      

                       Computation: 113623 steps/s (collection: 0.775s, learning 0.090s)
             Mean action noise std: 3.31
          Mean value_function loss: 32.6209
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.3330
                       Mean reward: 879.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 172.0000
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 0.87s
                      Time elapsed: 00:15:12
                               ETA: 00:19:05

################################################################################
                     [1m Learning iteration 887/2000 [0m                      

                       Computation: 107185 steps/s (collection: 0.817s, learning 0.101s)
             Mean action noise std: 3.31
          Mean value_function loss: 37.4385
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 20.3396
                       Mean reward: 848.32
               Mean episode length: 246.69
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 171.6618
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 0.92s
                      Time elapsed: 00:15:13
                               ETA: 00:19:04

################################################################################
                     [1m Learning iteration 888/2000 [0m                      

                       Computation: 110332 steps/s (collection: 0.778s, learning 0.113s)
             Mean action noise std: 3.32
          Mean value_function loss: 25.6242
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 20.3488
                       Mean reward: 856.33
               Mean episode length: 247.45
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 171.6781
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 0.89s
                      Time elapsed: 00:15:14
                               ETA: 00:19:03

################################################################################
                     [1m Learning iteration 889/2000 [0m                      

                       Computation: 97452 steps/s (collection: 0.901s, learning 0.108s)
             Mean action noise std: 3.32
          Mean value_function loss: 41.2305
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 20.3557
                       Mean reward: 844.96
               Mean episode length: 246.21
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 171.6170
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 1.01s
                      Time elapsed: 00:15:15
                               ETA: 00:19:02

################################################################################
                     [1m Learning iteration 890/2000 [0m                      

                       Computation: 108721 steps/s (collection: 0.803s, learning 0.102s)
             Mean action noise std: 3.32
          Mean value_function loss: 34.8843
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 20.3620
                       Mean reward: 830.59
               Mean episode length: 242.04
    Episode_Reward/reaching_object: 0.7640
     Episode_Reward/lifting_object: 171.1526
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 0.90s
                      Time elapsed: 00:15:16
                               ETA: 00:19:01

################################################################################
                     [1m Learning iteration 891/2000 [0m                      

                       Computation: 111942 steps/s (collection: 0.759s, learning 0.119s)
             Mean action noise std: 3.33
          Mean value_function loss: 40.2966
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 20.3770
                       Mean reward: 856.69
               Mean episode length: 246.25
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 170.5371
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 0.88s
                      Time elapsed: 00:15:16
                               ETA: 00:18:59

################################################################################
                     [1m Learning iteration 892/2000 [0m                      

                       Computation: 112548 steps/s (collection: 0.777s, learning 0.096s)
             Mean action noise std: 3.33
          Mean value_function loss: 40.2587
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.3892
                       Mean reward: 852.54
               Mean episode length: 245.45
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 170.6734
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 0.87s
                      Time elapsed: 00:15:17
                               ETA: 00:18:58

################################################################################
                     [1m Learning iteration 893/2000 [0m                      

                       Computation: 114346 steps/s (collection: 0.773s, learning 0.087s)
             Mean action noise std: 3.34
          Mean value_function loss: 37.5182
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 20.4005
                       Mean reward: 844.39
               Mean episode length: 246.86
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 171.3760
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 0.86s
                      Time elapsed: 00:15:18
                               ETA: 00:18:57

################################################################################
                     [1m Learning iteration 894/2000 [0m                      

                       Computation: 108730 steps/s (collection: 0.791s, learning 0.113s)
             Mean action noise std: 3.35
          Mean value_function loss: 35.0469
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 20.4234
                       Mean reward: 867.07
               Mean episode length: 249.03
    Episode_Reward/reaching_object: 0.7612
     Episode_Reward/lifting_object: 171.3344
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 0.90s
                      Time elapsed: 00:15:19
                               ETA: 00:18:56

################################################################################
                     [1m Learning iteration 895/2000 [0m                      

                       Computation: 111281 steps/s (collection: 0.793s, learning 0.091s)
             Mean action noise std: 3.35
          Mean value_function loss: 32.1905
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.4359
                       Mean reward: 868.06
               Mean episode length: 248.93
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 171.6131
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 0.88s
                      Time elapsed: 00:15:20
                               ETA: 00:18:55

################################################################################
                     [1m Learning iteration 896/2000 [0m                      

                       Computation: 113400 steps/s (collection: 0.777s, learning 0.090s)
             Mean action noise std: 3.36
          Mean value_function loss: 35.1690
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 20.4574
                       Mean reward: 851.05
               Mean episode length: 248.72
    Episode_Reward/reaching_object: 0.7609
     Episode_Reward/lifting_object: 171.7512
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 0.87s
                      Time elapsed: 00:15:21
                               ETA: 00:18:53

################################################################################
                     [1m Learning iteration 897/2000 [0m                      

                       Computation: 111984 steps/s (collection: 0.789s, learning 0.089s)
             Mean action noise std: 3.37
          Mean value_function loss: 45.4830
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 20.4716
                       Mean reward: 868.60
               Mean episode length: 249.28
    Episode_Reward/reaching_object: 0.7619
     Episode_Reward/lifting_object: 171.9096
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 0.88s
                      Time elapsed: 00:15:22
                               ETA: 00:18:52

################################################################################
                     [1m Learning iteration 898/2000 [0m                      

                       Computation: 110314 steps/s (collection: 0.789s, learning 0.103s)
             Mean action noise std: 3.37
          Mean value_function loss: 50.3744
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 20.4849
                       Mean reward: 852.40
               Mean episode length: 243.94
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 172.3172
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 0.89s
                      Time elapsed: 00:15:23
                               ETA: 00:18:51

################################################################################
                     [1m Learning iteration 899/2000 [0m                      

                       Computation: 108390 steps/s (collection: 0.793s, learning 0.114s)
             Mean action noise std: 3.38
          Mean value_function loss: 30.6118
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 20.5015
                       Mean reward: 855.83
               Mean episode length: 245.12
    Episode_Reward/reaching_object: 0.7422
     Episode_Reward/lifting_object: 169.6164
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 0.91s
                      Time elapsed: 00:15:23
                               ETA: 00:18:50

################################################################################
                     [1m Learning iteration 900/2000 [0m                      

                       Computation: 105286 steps/s (collection: 0.774s, learning 0.160s)
             Mean action noise std: 3.38
          Mean value_function loss: 31.5843
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 20.5101
                       Mean reward: 872.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7624
     Episode_Reward/lifting_object: 172.5876
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 0.93s
                      Time elapsed: 00:15:24
                               ETA: 00:18:49

################################################################################
                     [1m Learning iteration 901/2000 [0m                      

                       Computation: 105306 steps/s (collection: 0.779s, learning 0.155s)
             Mean action noise std: 3.39
          Mean value_function loss: 38.4927
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 20.5118
                       Mean reward: 875.40
               Mean episode length: 249.66
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 171.6378
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 0.93s
                      Time elapsed: 00:15:25
                               ETA: 00:18:48

################################################################################
                     [1m Learning iteration 902/2000 [0m                      

                       Computation: 114720 steps/s (collection: 0.769s, learning 0.088s)
             Mean action noise std: 3.39
          Mean value_function loss: 39.0219
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 20.5196
                       Mean reward: 867.57
               Mean episode length: 247.78
    Episode_Reward/reaching_object: 0.7544
     Episode_Reward/lifting_object: 170.8358
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 0.86s
                      Time elapsed: 00:15:26
                               ETA: 00:18:46

################################################################################
                     [1m Learning iteration 903/2000 [0m                      

                       Computation: 113284 steps/s (collection: 0.777s, learning 0.091s)
             Mean action noise std: 3.40
          Mean value_function loss: 42.2063
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 20.5373
                       Mean reward: 858.49
               Mean episode length: 247.85
    Episode_Reward/reaching_object: 0.7561
     Episode_Reward/lifting_object: 170.1184
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 0.87s
                      Time elapsed: 00:15:27
                               ETA: 00:18:45

################################################################################
                     [1m Learning iteration 904/2000 [0m                      

                       Computation: 108105 steps/s (collection: 0.814s, learning 0.095s)
             Mean action noise std: 3.41
          Mean value_function loss: 30.3379
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 20.5569
                       Mean reward: 867.04
               Mean episode length: 247.72
    Episode_Reward/reaching_object: 0.7500
     Episode_Reward/lifting_object: 170.3489
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 0.91s
                      Time elapsed: 00:15:28
                               ETA: 00:18:44

################################################################################
                     [1m Learning iteration 905/2000 [0m                      

                       Computation: 108949 steps/s (collection: 0.801s, learning 0.101s)
             Mean action noise std: 3.41
          Mean value_function loss: 36.5472
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.5689
                       Mean reward: 847.11
               Mean episode length: 247.27
    Episode_Reward/reaching_object: 0.7536
     Episode_Reward/lifting_object: 171.1478
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 0.90s
                      Time elapsed: 00:15:29
                               ETA: 00:18:43

################################################################################
                     [1m Learning iteration 906/2000 [0m                      

                       Computation: 106286 steps/s (collection: 0.825s, learning 0.100s)
             Mean action noise std: 3.41
          Mean value_function loss: 45.7956
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 20.5763
                       Mean reward: 869.71
               Mean episode length: 248.58
    Episode_Reward/reaching_object: 0.7674
     Episode_Reward/lifting_object: 173.9953
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 0.92s
                      Time elapsed: 00:15:30
                               ETA: 00:18:42

################################################################################
                     [1m Learning iteration 907/2000 [0m                      

                       Computation: 113604 steps/s (collection: 0.770s, learning 0.095s)
             Mean action noise std: 3.42
          Mean value_function loss: 56.4909
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.5835
                       Mean reward: 858.65
               Mean episode length: 247.10
    Episode_Reward/reaching_object: 0.7477
     Episode_Reward/lifting_object: 169.3449
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 0.87s
                      Time elapsed: 00:15:31
                               ETA: 00:18:40

################################################################################
                     [1m Learning iteration 908/2000 [0m                      

                       Computation: 109364 steps/s (collection: 0.789s, learning 0.110s)
             Mean action noise std: 3.42
          Mean value_function loss: 28.2601
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 20.5916
                       Mean reward: 853.70
               Mean episode length: 244.35
    Episode_Reward/reaching_object: 0.7531
     Episode_Reward/lifting_object: 170.3053
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 0.90s
                      Time elapsed: 00:15:32
                               ETA: 00:18:39

################################################################################
                     [1m Learning iteration 909/2000 [0m                      

                       Computation: 106123 steps/s (collection: 0.806s, learning 0.120s)
             Mean action noise std: 3.43
          Mean value_function loss: 50.0687
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.6041
                       Mean reward: 864.37
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.7640
     Episode_Reward/lifting_object: 172.6277
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 0.93s
                      Time elapsed: 00:15:32
                               ETA: 00:18:38

################################################################################
                     [1m Learning iteration 910/2000 [0m                      

                       Computation: 105489 steps/s (collection: 0.806s, learning 0.126s)
             Mean action noise std: 3.44
          Mean value_function loss: 34.0385
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 20.6303
                       Mean reward: 866.10
               Mean episode length: 247.83
    Episode_Reward/reaching_object: 0.7511
     Episode_Reward/lifting_object: 170.4504
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 0.93s
                      Time elapsed: 00:15:33
                               ETA: 00:18:37

################################################################################
                     [1m Learning iteration 911/2000 [0m                      

                       Computation: 104594 steps/s (collection: 0.832s, learning 0.108s)
             Mean action noise std: 3.45
          Mean value_function loss: 33.1801
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 20.6582
                       Mean reward: 877.75
               Mean episode length: 249.76
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 172.3072
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 0.94s
                      Time elapsed: 00:15:34
                               ETA: 00:18:36

################################################################################
                     [1m Learning iteration 912/2000 [0m                      

                       Computation: 107478 steps/s (collection: 0.779s, learning 0.136s)
             Mean action noise std: 3.45
          Mean value_function loss: 33.6861
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 20.6755
                       Mean reward: 834.14
               Mean episode length: 245.29
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 171.0141
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 0.91s
                      Time elapsed: 00:15:35
                               ETA: 00:18:35

################################################################################
                     [1m Learning iteration 913/2000 [0m                      

                       Computation: 112501 steps/s (collection: 0.776s, learning 0.098s)
             Mean action noise std: 3.46
          Mean value_function loss: 28.9317
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 20.6843
                       Mean reward: 864.01
               Mean episode length: 248.43
    Episode_Reward/reaching_object: 0.7609
     Episode_Reward/lifting_object: 170.8859
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 0.87s
                      Time elapsed: 00:15:36
                               ETA: 00:18:33

################################################################################
                     [1m Learning iteration 914/2000 [0m                      

                       Computation: 107327 steps/s (collection: 0.820s, learning 0.096s)
             Mean action noise std: 3.47
          Mean value_function loss: 36.3348
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 20.7003
                       Mean reward: 872.07
               Mean episode length: 248.39
    Episode_Reward/reaching_object: 0.7640
     Episode_Reward/lifting_object: 172.8105
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 0.92s
                      Time elapsed: 00:15:37
                               ETA: 00:18:32

################################################################################
                     [1m Learning iteration 915/2000 [0m                      

                       Computation: 112269 steps/s (collection: 0.783s, learning 0.092s)
             Mean action noise std: 3.47
          Mean value_function loss: 35.7010
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.7103
                       Mean reward: 862.49
               Mean episode length: 247.51
    Episode_Reward/reaching_object: 0.7560
     Episode_Reward/lifting_object: 170.6803
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 0.88s
                      Time elapsed: 00:15:38
                               ETA: 00:18:31

################################################################################
                     [1m Learning iteration 916/2000 [0m                      

                       Computation: 107136 steps/s (collection: 0.792s, learning 0.125s)
             Mean action noise std: 3.48
          Mean value_function loss: 34.5351
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 20.7236
                       Mean reward: 870.71
               Mean episode length: 248.60
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 171.5388
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 0.92s
                      Time elapsed: 00:15:39
                               ETA: 00:18:30

################################################################################
                     [1m Learning iteration 917/2000 [0m                      

                       Computation: 105607 steps/s (collection: 0.839s, learning 0.092s)
             Mean action noise std: 3.49
          Mean value_function loss: 33.2540
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 20.7394
                       Mean reward: 861.02
               Mean episode length: 247.45
    Episode_Reward/reaching_object: 0.7621
     Episode_Reward/lifting_object: 171.5625
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 0.93s
                      Time elapsed: 00:15:40
                               ETA: 00:18:29

################################################################################
                     [1m Learning iteration 918/2000 [0m                      

                       Computation: 107586 steps/s (collection: 0.822s, learning 0.092s)
             Mean action noise std: 3.49
          Mean value_function loss: 22.3659
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 20.7616
                       Mean reward: 868.87
               Mean episode length: 249.08
    Episode_Reward/reaching_object: 0.7720
     Episode_Reward/lifting_object: 173.0994
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 0.91s
                      Time elapsed: 00:15:41
                               ETA: 00:18:28

################################################################################
                     [1m Learning iteration 919/2000 [0m                      

                       Computation: 109978 steps/s (collection: 0.796s, learning 0.098s)
             Mean action noise std: 3.50
          Mean value_function loss: 39.4667
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 20.7816
                       Mean reward: 860.00
               Mean episode length: 248.92
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 171.7814
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 0.89s
                      Time elapsed: 00:15:42
                               ETA: 00:18:26

################################################################################
                     [1m Learning iteration 920/2000 [0m                      

                       Computation: 102689 steps/s (collection: 0.839s, learning 0.118s)
             Mean action noise std: 3.51
          Mean value_function loss: 34.8991
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 20.7972
                       Mean reward: 857.69
               Mean episode length: 246.03
    Episode_Reward/reaching_object: 0.7672
     Episode_Reward/lifting_object: 173.2714
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 0.96s
                      Time elapsed: 00:15:43
                               ETA: 00:18:25

################################################################################
                     [1m Learning iteration 921/2000 [0m                      

                       Computation: 109771 steps/s (collection: 0.806s, learning 0.089s)
             Mean action noise std: 3.51
          Mean value_function loss: 44.2021
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.8139
                       Mean reward: 874.03
               Mean episode length: 249.74
    Episode_Reward/reaching_object: 0.7740
     Episode_Reward/lifting_object: 173.4779
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 0.90s
                      Time elapsed: 00:15:43
                               ETA: 00:18:24

################################################################################
                     [1m Learning iteration 922/2000 [0m                      

                       Computation: 110659 steps/s (collection: 0.780s, learning 0.109s)
             Mean action noise std: 3.52
          Mean value_function loss: 45.2387
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 20.8312
                       Mean reward: 851.35
               Mean episode length: 246.40
    Episode_Reward/reaching_object: 0.7692
     Episode_Reward/lifting_object: 171.6533
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 0.89s
                      Time elapsed: 00:15:44
                               ETA: 00:18:23

################################################################################
                     [1m Learning iteration 923/2000 [0m                      

                       Computation: 105540 steps/s (collection: 0.789s, learning 0.143s)
             Mean action noise std: 3.52
          Mean value_function loss: 35.5295
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 20.8458
                       Mean reward: 859.26
               Mean episode length: 247.38
    Episode_Reward/reaching_object: 0.7627
     Episode_Reward/lifting_object: 171.5715
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 0.93s
                      Time elapsed: 00:15:45
                               ETA: 00:18:22

################################################################################
                     [1m Learning iteration 924/2000 [0m                      

                       Computation: 110234 steps/s (collection: 0.802s, learning 0.090s)
             Mean action noise std: 3.53
          Mean value_function loss: 38.5413
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 20.8596
                       Mean reward: 854.12
               Mean episode length: 246.13
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 170.3804
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 0.89s
                      Time elapsed: 00:15:46
                               ETA: 00:18:21

################################################################################
                     [1m Learning iteration 925/2000 [0m                      

                       Computation: 108280 steps/s (collection: 0.795s, learning 0.113s)
             Mean action noise std: 3.53
          Mean value_function loss: 33.3246
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.8658
                       Mean reward: 859.67
               Mean episode length: 247.32
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 169.9774
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 0.91s
                      Time elapsed: 00:15:47
                               ETA: 00:18:20

################################################################################
                     [1m Learning iteration 926/2000 [0m                      

                       Computation: 108670 steps/s (collection: 0.813s, learning 0.092s)
             Mean action noise std: 3.53
          Mean value_function loss: 26.5210
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 20.8678
                       Mean reward: 870.28
               Mean episode length: 249.30
    Episode_Reward/reaching_object: 0.7692
     Episode_Reward/lifting_object: 172.5180
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 0.90s
                      Time elapsed: 00:15:48
                               ETA: 00:18:18

################################################################################
                     [1m Learning iteration 927/2000 [0m                      

                       Computation: 108629 steps/s (collection: 0.811s, learning 0.094s)
             Mean action noise std: 3.54
          Mean value_function loss: 37.6804
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 20.8736
                       Mean reward: 866.60
               Mean episode length: 248.86
    Episode_Reward/reaching_object: 0.7668
     Episode_Reward/lifting_object: 172.1434
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 0.90s
                      Time elapsed: 00:15:49
                               ETA: 00:18:17

################################################################################
                     [1m Learning iteration 928/2000 [0m                      

                       Computation: 108206 steps/s (collection: 0.805s, learning 0.103s)
             Mean action noise std: 3.54
          Mean value_function loss: 41.1471
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 20.8881
                       Mean reward: 853.48
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.7619
     Episode_Reward/lifting_object: 170.6745
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 0.91s
                      Time elapsed: 00:15:50
                               ETA: 00:18:16

################################################################################
                     [1m Learning iteration 929/2000 [0m                      

                       Computation: 107510 steps/s (collection: 0.806s, learning 0.109s)
             Mean action noise std: 3.55
          Mean value_function loss: 34.9862
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 20.9054
                       Mean reward: 853.90
               Mean episode length: 247.72
    Episode_Reward/reaching_object: 0.7546
     Episode_Reward/lifting_object: 169.5102
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 0.91s
                      Time elapsed: 00:15:51
                               ETA: 00:18:15

################################################################################
                     [1m Learning iteration 930/2000 [0m                      

                       Computation: 103003 steps/s (collection: 0.841s, learning 0.113s)
             Mean action noise std: 3.56
          Mean value_function loss: 45.8354
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 20.9209
                       Mean reward: 835.54
               Mean episode length: 244.10
    Episode_Reward/reaching_object: 0.7539
     Episode_Reward/lifting_object: 169.6953
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 0.95s
                      Time elapsed: 00:15:52
                               ETA: 00:18:14

################################################################################
                     [1m Learning iteration 931/2000 [0m                      

                       Computation: 105142 steps/s (collection: 0.833s, learning 0.102s)
             Mean action noise std: 3.56
          Mean value_function loss: 44.4103
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 20.9339
                       Mean reward: 864.85
               Mean episode length: 246.49
    Episode_Reward/reaching_object: 0.7576
     Episode_Reward/lifting_object: 171.1063
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 0.93s
                      Time elapsed: 00:15:53
                               ETA: 00:18:13

################################################################################
                     [1m Learning iteration 932/2000 [0m                      

                       Computation: 106095 steps/s (collection: 0.812s, learning 0.115s)
             Mean action noise std: 3.56
          Mean value_function loss: 37.8490
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 20.9422
                       Mean reward: 854.60
               Mean episode length: 246.98
    Episode_Reward/reaching_object: 0.7488
     Episode_Reward/lifting_object: 169.2443
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 0.93s
                      Time elapsed: 00:15:53
                               ETA: 00:18:12

################################################################################
                     [1m Learning iteration 933/2000 [0m                      

                       Computation: 109808 steps/s (collection: 0.795s, learning 0.100s)
             Mean action noise std: 3.57
          Mean value_function loss: 24.6495
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 20.9588
                       Mean reward: 854.53
               Mean episode length: 246.51
    Episode_Reward/reaching_object: 0.7582
     Episode_Reward/lifting_object: 171.9996
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 0.90s
                      Time elapsed: 00:15:54
                               ETA: 00:18:10

################################################################################
                     [1m Learning iteration 934/2000 [0m                      

                       Computation: 107724 steps/s (collection: 0.776s, learning 0.136s)
             Mean action noise std: 3.58
          Mean value_function loss: 29.6498
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.9865
                       Mean reward: 861.57
               Mean episode length: 246.78
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 172.4377
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 0.91s
                      Time elapsed: 00:15:55
                               ETA: 00:18:09

################################################################################
                     [1m Learning iteration 935/2000 [0m                      

                       Computation: 110210 steps/s (collection: 0.793s, learning 0.099s)
             Mean action noise std: 3.59
          Mean value_function loss: 40.9416
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 21.0074
                       Mean reward: 861.83
               Mean episode length: 245.93
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 172.7480
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 0.89s
                      Time elapsed: 00:15:56
                               ETA: 00:18:08

################################################################################
                     [1m Learning iteration 936/2000 [0m                      

                       Computation: 111471 steps/s (collection: 0.781s, learning 0.101s)
             Mean action noise std: 3.59
          Mean value_function loss: 30.9537
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 21.0212
                       Mean reward: 872.07
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 172.4938
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 0.88s
                      Time elapsed: 00:15:57
                               ETA: 00:18:07

################################################################################
                     [1m Learning iteration 937/2000 [0m                      

                       Computation: 110013 steps/s (collection: 0.802s, learning 0.092s)
             Mean action noise std: 3.60
          Mean value_function loss: 36.4641
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 21.0313
                       Mean reward: 868.45
               Mean episode length: 248.63
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 172.5436
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 0.89s
                      Time elapsed: 00:15:58
                               ETA: 00:18:06

################################################################################
                     [1m Learning iteration 938/2000 [0m                      

                       Computation: 109677 steps/s (collection: 0.801s, learning 0.095s)
             Mean action noise std: 3.60
          Mean value_function loss: 44.9847
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 21.0405
                       Mean reward: 866.54
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7673
     Episode_Reward/lifting_object: 172.6639
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 0.90s
                      Time elapsed: 00:15:59
                               ETA: 00:18:05

################################################################################
                     [1m Learning iteration 939/2000 [0m                      

                       Computation: 103970 steps/s (collection: 0.826s, learning 0.119s)
             Mean action noise std: 3.61
          Mean value_function loss: 37.4731
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 21.0488
                       Mean reward: 868.22
               Mean episode length: 248.96
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 172.8673
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 0.95s
                      Time elapsed: 00:16:00
                               ETA: 00:18:03

################################################################################
                     [1m Learning iteration 940/2000 [0m                      

                       Computation: 104620 steps/s (collection: 0.841s, learning 0.099s)
             Mean action noise std: 3.62
          Mean value_function loss: 39.4060
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 21.0613
                       Mean reward: 871.04
               Mean episode length: 249.64
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 170.6655
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 0.94s
                      Time elapsed: 00:16:01
                               ETA: 00:18:02

################################################################################
                     [1m Learning iteration 941/2000 [0m                      

                       Computation: 105284 steps/s (collection: 0.817s, learning 0.117s)
             Mean action noise std: 3.62
          Mean value_function loss: 43.9556
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 21.0734
                       Mean reward: 870.69
               Mean episode length: 248.39
    Episode_Reward/reaching_object: 0.7502
     Episode_Reward/lifting_object: 170.2808
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 0.93s
                      Time elapsed: 00:16:02
                               ETA: 00:18:01

################################################################################
                     [1m Learning iteration 942/2000 [0m                      

                       Computation: 107008 steps/s (collection: 0.812s, learning 0.107s)
             Mean action noise std: 3.63
          Mean value_function loss: 29.9582
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 21.0823
                       Mean reward: 881.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7628
     Episode_Reward/lifting_object: 171.7129
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 0.92s
                      Time elapsed: 00:16:03
                               ETA: 00:18:00

################################################################################
                     [1m Learning iteration 943/2000 [0m                      

                       Computation: 104805 steps/s (collection: 0.800s, learning 0.138s)
             Mean action noise std: 3.63
          Mean value_function loss: 44.0421
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 21.0938
                       Mean reward: 855.33
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.7542
     Episode_Reward/lifting_object: 169.9671
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 0.94s
                      Time elapsed: 00:16:04
                               ETA: 00:17:59

################################################################################
                     [1m Learning iteration 944/2000 [0m                      

                       Computation: 98894 steps/s (collection: 0.820s, learning 0.174s)
             Mean action noise std: 3.64
          Mean value_function loss: 52.7009
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 21.1047
                       Mean reward: 853.98
               Mean episode length: 245.64
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 171.6443
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 0.99s
                      Time elapsed: 00:16:05
                               ETA: 00:17:58

################################################################################
                     [1m Learning iteration 945/2000 [0m                      

                       Computation: 100526 steps/s (collection: 0.875s, learning 0.103s)
             Mean action noise std: 3.64
          Mean value_function loss: 37.1922
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 21.1152
                       Mean reward: 852.77
               Mean episode length: 243.90
    Episode_Reward/reaching_object: 0.7508
     Episode_Reward/lifting_object: 171.1665
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 0.98s
                      Time elapsed: 00:16:06
                               ETA: 00:17:57

################################################################################
                     [1m Learning iteration 946/2000 [0m                      

                       Computation: 102640 steps/s (collection: 0.824s, learning 0.134s)
             Mean action noise std: 3.65
          Mean value_function loss: 35.8612
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 21.1254
                       Mean reward: 855.28
               Mean episode length: 248.62
    Episode_Reward/reaching_object: 0.7586
     Episode_Reward/lifting_object: 171.1040
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 0.96s
                      Time elapsed: 00:16:06
                               ETA: 00:17:56

################################################################################
                     [1m Learning iteration 947/2000 [0m                      

                       Computation: 109066 steps/s (collection: 0.805s, learning 0.096s)
             Mean action noise std: 3.66
          Mean value_function loss: 36.7099
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 21.1436
                       Mean reward: 853.22
               Mean episode length: 245.24
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 171.1492
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 0.90s
                      Time elapsed: 00:16:07
                               ETA: 00:17:55

################################################################################
                     [1m Learning iteration 948/2000 [0m                      

                       Computation: 103078 steps/s (collection: 0.812s, learning 0.142s)
             Mean action noise std: 3.67
          Mean value_function loss: 34.8293
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 21.1699
                       Mean reward: 870.52
               Mean episode length: 248.49
    Episode_Reward/reaching_object: 0.7535
     Episode_Reward/lifting_object: 170.7604
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 0.95s
                      Time elapsed: 00:16:08
                               ETA: 00:17:53

################################################################################
                     [1m Learning iteration 949/2000 [0m                      

                       Computation: 106416 steps/s (collection: 0.789s, learning 0.134s)
             Mean action noise std: 3.67
          Mean value_function loss: 29.6256
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 21.1861
                       Mean reward: 848.68
               Mean episode length: 246.23
    Episode_Reward/reaching_object: 0.7610
     Episode_Reward/lifting_object: 171.5861
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 0.92s
                      Time elapsed: 00:16:09
                               ETA: 00:17:52

################################################################################
                     [1m Learning iteration 950/2000 [0m                      

                       Computation: 110962 steps/s (collection: 0.796s, learning 0.090s)
             Mean action noise std: 3.68
          Mean value_function loss: 40.5550
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 21.1932
                       Mean reward: 867.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7606
     Episode_Reward/lifting_object: 171.5801
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 0.89s
                      Time elapsed: 00:16:10
                               ETA: 00:17:51

################################################################################
                     [1m Learning iteration 951/2000 [0m                      

                       Computation: 105702 steps/s (collection: 0.824s, learning 0.106s)
             Mean action noise std: 3.68
          Mean value_function loss: 49.8170
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 21.1942
                       Mean reward: 846.60
               Mean episode length: 243.44
    Episode_Reward/reaching_object: 0.7486
     Episode_Reward/lifting_object: 170.3233
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 0.93s
                      Time elapsed: 00:16:11
                               ETA: 00:17:50

################################################################################
                     [1m Learning iteration 952/2000 [0m                      

                       Computation: 108334 steps/s (collection: 0.808s, learning 0.100s)
             Mean action noise std: 3.68
          Mean value_function loss: 29.8632
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 21.2008
                       Mean reward: 871.27
               Mean episode length: 248.68
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 171.6313
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 0.91s
                      Time elapsed: 00:16:12
                               ETA: 00:17:49

################################################################################
                     [1m Learning iteration 953/2000 [0m                      

                       Computation: 103212 steps/s (collection: 0.855s, learning 0.097s)
             Mean action noise std: 3.69
          Mean value_function loss: 32.5852
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 21.2126
                       Mean reward: 876.61
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.7590
     Episode_Reward/lifting_object: 173.5188
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 0.95s
                      Time elapsed: 00:16:13
                               ETA: 00:17:48

################################################################################
                     [1m Learning iteration 954/2000 [0m                      

                       Computation: 107086 steps/s (collection: 0.819s, learning 0.099s)
             Mean action noise std: 3.69
          Mean value_function loss: 38.9625
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 21.2248
                       Mean reward: 868.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7611
     Episode_Reward/lifting_object: 171.9619
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 0.92s
                      Time elapsed: 00:16:14
                               ETA: 00:17:47

################################################################################
                     [1m Learning iteration 955/2000 [0m                      

                       Computation: 107206 steps/s (collection: 0.821s, learning 0.096s)
             Mean action noise std: 3.70
          Mean value_function loss: 25.1494
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 21.2337
                       Mean reward: 852.44
               Mean episode length: 245.55
    Episode_Reward/reaching_object: 0.7601
     Episode_Reward/lifting_object: 171.2061
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 0.92s
                      Time elapsed: 00:16:15
                               ETA: 00:17:46

################################################################################
                     [1m Learning iteration 956/2000 [0m                      

                       Computation: 109258 steps/s (collection: 0.803s, learning 0.097s)
             Mean action noise std: 3.70
          Mean value_function loss: 27.9370
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 21.2470
                       Mean reward: 883.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7669
     Episode_Reward/lifting_object: 173.5914
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 0.90s
                      Time elapsed: 00:16:16
                               ETA: 00:17:44

################################################################################
                     [1m Learning iteration 957/2000 [0m                      

                       Computation: 105283 steps/s (collection: 0.828s, learning 0.106s)
             Mean action noise std: 3.71
          Mean value_function loss: 25.9127
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 21.2603
                       Mean reward: 851.75
               Mean episode length: 249.62
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 172.9084
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 0.93s
                      Time elapsed: 00:16:17
                               ETA: 00:17:43

################################################################################
                     [1m Learning iteration 958/2000 [0m                      

                       Computation: 105561 steps/s (collection: 0.801s, learning 0.131s)
             Mean action noise std: 3.71
          Mean value_function loss: 33.1390
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 21.2726
                       Mean reward: 850.26
               Mean episode length: 247.83
    Episode_Reward/reaching_object: 0.7559
     Episode_Reward/lifting_object: 170.9521
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 0.93s
                      Time elapsed: 00:16:18
                               ETA: 00:17:42

################################################################################
                     [1m Learning iteration 959/2000 [0m                      

                       Computation: 107449 steps/s (collection: 0.802s, learning 0.113s)
             Mean action noise std: 3.72
          Mean value_function loss: 31.9460
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 21.2822
                       Mean reward: 879.30
               Mean episode length: 249.86
    Episode_Reward/reaching_object: 0.7678
     Episode_Reward/lifting_object: 173.4491
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 0.91s
                      Time elapsed: 00:16:18
                               ETA: 00:17:41

################################################################################
                     [1m Learning iteration 960/2000 [0m                      

                       Computation: 98816 steps/s (collection: 0.825s, learning 0.170s)
             Mean action noise std: 3.72
          Mean value_function loss: 27.4144
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 21.2948
                       Mean reward: 872.57
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7546
     Episode_Reward/lifting_object: 170.6851
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 0.99s
                      Time elapsed: 00:16:19
                               ETA: 00:17:40

################################################################################
                     [1m Learning iteration 961/2000 [0m                      

                       Computation: 107404 steps/s (collection: 0.793s, learning 0.122s)
             Mean action noise std: 3.73
          Mean value_function loss: 24.7146
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 21.3065
                       Mean reward: 852.95
               Mean episode length: 246.58
    Episode_Reward/reaching_object: 0.7583
     Episode_Reward/lifting_object: 171.6147
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 0.92s
                      Time elapsed: 00:16:20
                               ETA: 00:17:39

################################################################################
                     [1m Learning iteration 962/2000 [0m                      

                       Computation: 102260 steps/s (collection: 0.785s, learning 0.177s)
             Mean action noise std: 3.74
          Mean value_function loss: 28.3141
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 21.3264
                       Mean reward: 867.30
               Mean episode length: 246.90
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 171.8801
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 0.96s
                      Time elapsed: 00:16:21
                               ETA: 00:17:38

################################################################################
                     [1m Learning iteration 963/2000 [0m                      

                       Computation: 104323 steps/s (collection: 0.806s, learning 0.136s)
             Mean action noise std: 3.74
          Mean value_function loss: 36.3014
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 21.3366
                       Mean reward: 866.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7707
     Episode_Reward/lifting_object: 173.7875
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 0.94s
                      Time elapsed: 00:16:22
                               ETA: 00:17:37

################################################################################
                     [1m Learning iteration 964/2000 [0m                      

                       Computation: 109907 steps/s (collection: 0.784s, learning 0.110s)
             Mean action noise std: 3.74
          Mean value_function loss: 35.1209
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 21.3409
                       Mean reward: 871.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7627
     Episode_Reward/lifting_object: 172.0378
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 0.89s
                      Time elapsed: 00:16:23
                               ETA: 00:17:36

################################################################################
                     [1m Learning iteration 965/2000 [0m                      

                       Computation: 105828 steps/s (collection: 0.829s, learning 0.100s)
             Mean action noise std: 3.75
          Mean value_function loss: 35.6615
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 21.3493
                       Mean reward: 865.08
               Mean episode length: 247.59
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 172.3452
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 0.93s
                      Time elapsed: 00:16:24
                               ETA: 00:17:34

################################################################################
                     [1m Learning iteration 966/2000 [0m                      

                       Computation: 103358 steps/s (collection: 0.838s, learning 0.113s)
             Mean action noise std: 3.76
          Mean value_function loss: 32.6738
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 21.3658
                       Mean reward: 862.30
               Mean episode length: 247.12
    Episode_Reward/reaching_object: 0.7536
     Episode_Reward/lifting_object: 171.6368
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 0.95s
                      Time elapsed: 00:16:25
                               ETA: 00:17:33

################################################################################
                     [1m Learning iteration 967/2000 [0m                      

                       Computation: 106777 steps/s (collection: 0.828s, learning 0.093s)
             Mean action noise std: 3.76
          Mean value_function loss: 28.0182
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 21.3796
                       Mean reward: 875.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 172.6698
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 0.92s
                      Time elapsed: 00:16:26
                               ETA: 00:17:32

################################################################################
                     [1m Learning iteration 968/2000 [0m                      

                       Computation: 101020 steps/s (collection: 0.857s, learning 0.116s)
             Mean action noise std: 3.77
          Mean value_function loss: 35.2138
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 21.3887
                       Mean reward: 868.64
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 0.7552
     Episode_Reward/lifting_object: 171.1451
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 0.97s
                      Time elapsed: 00:16:27
                               ETA: 00:17:31

################################################################################
                     [1m Learning iteration 969/2000 [0m                      

                       Computation: 104582 steps/s (collection: 0.844s, learning 0.096s)
             Mean action noise std: 3.77
          Mean value_function loss: 37.0791
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 21.3959
                       Mean reward: 875.88
               Mean episode length: 247.70
    Episode_Reward/reaching_object: 0.7609
     Episode_Reward/lifting_object: 171.7504
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 0.94s
                      Time elapsed: 00:16:28
                               ETA: 00:17:30

################################################################################
                     [1m Learning iteration 970/2000 [0m                      

                       Computation: 103269 steps/s (collection: 0.846s, learning 0.105s)
             Mean action noise std: 3.78
          Mean value_function loss: 27.8091
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 21.4108
                       Mean reward: 869.43
               Mean episode length: 249.64
    Episode_Reward/reaching_object: 0.7621
     Episode_Reward/lifting_object: 172.5142
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 0.95s
                      Time elapsed: 00:16:29
                               ETA: 00:17:29

################################################################################
                     [1m Learning iteration 971/2000 [0m                      

                       Computation: 108868 steps/s (collection: 0.804s, learning 0.099s)
             Mean action noise std: 3.78
          Mean value_function loss: 38.3048
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 21.4207
                       Mean reward: 854.32
               Mean episode length: 244.57
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 172.3020
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 0.90s
                      Time elapsed: 00:16:30
                               ETA: 00:17:28

################################################################################
                     [1m Learning iteration 972/2000 [0m                      

                       Computation: 108017 steps/s (collection: 0.811s, learning 0.099s)
             Mean action noise std: 3.79
          Mean value_function loss: 37.0589
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 21.4408
                       Mean reward: 855.97
               Mean episode length: 248.73
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 171.9023
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 0.91s
                      Time elapsed: 00:16:31
                               ETA: 00:17:27

################################################################################
                     [1m Learning iteration 973/2000 [0m                      

                       Computation: 106120 steps/s (collection: 0.812s, learning 0.115s)
             Mean action noise std: 3.80
          Mean value_function loss: 32.9835
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 21.4557
                       Mean reward: 872.26
               Mean episode length: 249.69
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 172.6889
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 0.93s
                      Time elapsed: 00:16:32
                               ETA: 00:17:26

################################################################################
                     [1m Learning iteration 974/2000 [0m                      

                       Computation: 97789 steps/s (collection: 0.821s, learning 0.185s)
             Mean action noise std: 3.81
          Mean value_function loss: 27.4486
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 21.4680
                       Mean reward: 857.01
               Mean episode length: 244.53
    Episode_Reward/reaching_object: 0.7562
     Episode_Reward/lifting_object: 171.9406
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 1.01s
                      Time elapsed: 00:16:33
                               ETA: 00:17:25

################################################################################
                     [1m Learning iteration 975/2000 [0m                      

                       Computation: 107977 steps/s (collection: 0.821s, learning 0.090s)
             Mean action noise std: 3.81
          Mean value_function loss: 36.7914
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 21.4854
                       Mean reward: 860.44
               Mean episode length: 245.59
    Episode_Reward/reaching_object: 0.7548
     Episode_Reward/lifting_object: 172.5831
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 0.91s
                      Time elapsed: 00:16:33
                               ETA: 00:17:23

################################################################################
                     [1m Learning iteration 976/2000 [0m                      

                       Computation: 104750 steps/s (collection: 0.806s, learning 0.132s)
             Mean action noise std: 3.82
          Mean value_function loss: 22.6376
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 21.5062
                       Mean reward: 870.90
               Mean episode length: 248.69
    Episode_Reward/reaching_object: 0.7548
     Episode_Reward/lifting_object: 172.3905
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 0.94s
                      Time elapsed: 00:16:34
                               ETA: 00:17:22

################################################################################
                     [1m Learning iteration 977/2000 [0m                      

                       Computation: 110255 steps/s (collection: 0.798s, learning 0.094s)
             Mean action noise std: 3.82
          Mean value_function loss: 30.7890
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 21.5205
                       Mean reward: 870.09
               Mean episode length: 248.95
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 173.5807
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 0.89s
                      Time elapsed: 00:16:35
                               ETA: 00:17:21

################################################################################
                     [1m Learning iteration 978/2000 [0m                      

                       Computation: 99835 steps/s (collection: 0.798s, learning 0.186s)
             Mean action noise std: 3.83
          Mean value_function loss: 26.0137
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 21.5221
                       Mean reward: 835.47
               Mean episode length: 247.03
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 171.2506
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 0.98s
                      Time elapsed: 00:16:36
                               ETA: 00:17:20

################################################################################
                     [1m Learning iteration 979/2000 [0m                      

                       Computation: 103799 steps/s (collection: 0.821s, learning 0.126s)
             Mean action noise std: 3.83
          Mean value_function loss: 25.1987
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 21.5265
                       Mean reward: 860.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7672
     Episode_Reward/lifting_object: 172.4647
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 0.95s
                      Time elapsed: 00:16:37
                               ETA: 00:17:19

################################################################################
                     [1m Learning iteration 980/2000 [0m                      

                       Computation: 108536 steps/s (collection: 0.794s, learning 0.112s)
             Mean action noise std: 3.84
          Mean value_function loss: 23.3060
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 21.5388
                       Mean reward: 865.85
               Mean episode length: 248.79
    Episode_Reward/reaching_object: 0.7682
     Episode_Reward/lifting_object: 172.8045
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 0.91s
                      Time elapsed: 00:16:38
                               ETA: 00:17:18

################################################################################
                     [1m Learning iteration 981/2000 [0m                      

                       Computation: 96068 steps/s (collection: 0.885s, learning 0.138s)
             Mean action noise std: 3.84
          Mean value_function loss: 29.0642
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 21.5575
                       Mean reward: 873.56
               Mean episode length: 249.48
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 172.0693
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 1.02s
                      Time elapsed: 00:16:39
                               ETA: 00:17:17

################################################################################
                     [1m Learning iteration 982/2000 [0m                      

                       Computation: 86399 steps/s (collection: 1.042s, learning 0.096s)
             Mean action noise std: 3.85
          Mean value_function loss: 26.4654
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 21.5664
                       Mean reward: 877.74
               Mean episode length: 249.30
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 173.0030
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 1.14s
                      Time elapsed: 00:16:40
                               ETA: 00:17:16

################################################################################
                     [1m Learning iteration 983/2000 [0m                      

                       Computation: 98675 steps/s (collection: 0.893s, learning 0.104s)
             Mean action noise std: 3.85
          Mean value_function loss: 27.9728
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 21.5789
                       Mean reward: 871.96
               Mean episode length: 249.72
    Episode_Reward/reaching_object: 0.7664
     Episode_Reward/lifting_object: 173.0437
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 1.00s
                      Time elapsed: 00:16:41
                               ETA: 00:17:15

################################################################################
                     [1m Learning iteration 984/2000 [0m                      

                       Computation: 101115 steps/s (collection: 0.837s, learning 0.135s)
             Mean action noise std: 3.86
          Mean value_function loss: 31.5757
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 21.5889
                       Mean reward: 846.10
               Mean episode length: 245.45
    Episode_Reward/reaching_object: 0.7627
     Episode_Reward/lifting_object: 171.3128
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 0.97s
                      Time elapsed: 00:16:42
                               ETA: 00:17:14

################################################################################
                     [1m Learning iteration 985/2000 [0m                      

                       Computation: 104076 steps/s (collection: 0.793s, learning 0.152s)
             Mean action noise std: 3.86
          Mean value_function loss: 33.8280
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 21.6001
                       Mean reward: 861.93
               Mean episode length: 249.07
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 172.4046
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 0.94s
                      Time elapsed: 00:16:43
                               ETA: 00:17:13

################################################################################
                     [1m Learning iteration 986/2000 [0m                      

                       Computation: 105028 steps/s (collection: 0.805s, learning 0.131s)
             Mean action noise std: 3.87
          Mean value_function loss: 32.2817
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 21.6066
                       Mean reward: 866.71
               Mean episode length: 249.81
    Episode_Reward/reaching_object: 0.7624
     Episode_Reward/lifting_object: 171.0021
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 0.94s
                      Time elapsed: 00:16:44
                               ETA: 00:17:12

################################################################################
                     [1m Learning iteration 987/2000 [0m                      

                       Computation: 108345 steps/s (collection: 0.808s, learning 0.100s)
             Mean action noise std: 3.88
          Mean value_function loss: 37.7759
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 21.6208
                       Mean reward: 872.33
               Mean episode length: 249.25
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 172.2134
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 0.91s
                      Time elapsed: 00:16:45
                               ETA: 00:17:10

################################################################################
                     [1m Learning iteration 988/2000 [0m                      

                       Computation: 106175 steps/s (collection: 0.802s, learning 0.124s)
             Mean action noise std: 3.88
          Mean value_function loss: 38.9446
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 21.6448
                       Mean reward: 846.19
               Mean episode length: 244.41
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 171.4671
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 0.93s
                      Time elapsed: 00:16:46
                               ETA: 00:17:09

################################################################################
                     [1m Learning iteration 989/2000 [0m                      

                       Computation: 109111 steps/s (collection: 0.808s, learning 0.093s)
             Mean action noise std: 3.89
          Mean value_function loss: 27.7328
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 21.6613
                       Mean reward: 861.22
               Mean episode length: 249.61
    Episode_Reward/reaching_object: 0.7597
     Episode_Reward/lifting_object: 171.7767
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 0.90s
                      Time elapsed: 00:16:47
                               ETA: 00:17:08

################################################################################
                     [1m Learning iteration 990/2000 [0m                      

                       Computation: 107742 steps/s (collection: 0.813s, learning 0.099s)
             Mean action noise std: 3.89
          Mean value_function loss: 42.0531
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 21.6713
                       Mean reward: 877.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 172.8693
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 0.91s
                      Time elapsed: 00:16:48
                               ETA: 00:17:07

################################################################################
                     [1m Learning iteration 991/2000 [0m                      

                       Computation: 99189 steps/s (collection: 0.847s, learning 0.144s)
             Mean action noise std: 3.90
          Mean value_function loss: 33.2081
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 21.6741
                       Mean reward: 877.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7559
     Episode_Reward/lifting_object: 171.2880
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 0.99s
                      Time elapsed: 00:16:49
                               ETA: 00:17:06

################################################################################
                     [1m Learning iteration 992/2000 [0m                      

                       Computation: 87467 steps/s (collection: 0.947s, learning 0.177s)
             Mean action noise std: 3.90
          Mean value_function loss: 28.4939
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 21.6817
                       Mean reward: 863.29
               Mean episode length: 248.64
    Episode_Reward/reaching_object: 0.7691
     Episode_Reward/lifting_object: 172.6165
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 1.12s
                      Time elapsed: 00:16:50
                               ETA: 00:17:05

################################################################################
                     [1m Learning iteration 993/2000 [0m                      

                       Computation: 104117 steps/s (collection: 0.840s, learning 0.104s)
             Mean action noise std: 3.91
          Mean value_function loss: 39.0773
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 21.6905
                       Mean reward: 885.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7683
     Episode_Reward/lifting_object: 173.3337
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 0.94s
                      Time elapsed: 00:16:51
                               ETA: 00:17:04

################################################################################
                     [1m Learning iteration 994/2000 [0m                      

                       Computation: 95308 steps/s (collection: 0.870s, learning 0.161s)
             Mean action noise std: 3.91
          Mean value_function loss: 36.7341
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 21.6994
                       Mean reward: 842.32
               Mean episode length: 245.04
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 169.7326
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 1.03s
                      Time elapsed: 00:16:52
                               ETA: 00:17:03

################################################################################
                     [1m Learning iteration 995/2000 [0m                      

                       Computation: 96455 steps/s (collection: 0.916s, learning 0.104s)
             Mean action noise std: 3.92
          Mean value_function loss: 28.4940
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 21.7108
                       Mean reward: 852.74
               Mean episode length: 248.89
    Episode_Reward/reaching_object: 0.7518
     Episode_Reward/lifting_object: 169.3169
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 1.02s
                      Time elapsed: 00:16:53
                               ETA: 00:17:02

################################################################################
                     [1m Learning iteration 996/2000 [0m                      

                       Computation: 105976 steps/s (collection: 0.828s, learning 0.100s)
             Mean action noise std: 3.93
          Mean value_function loss: 35.1854
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 21.7313
                       Mean reward: 868.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7627
     Episode_Reward/lifting_object: 172.4183
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 0.93s
                      Time elapsed: 00:16:54
                               ETA: 00:17:01

################################################################################
                     [1m Learning iteration 997/2000 [0m                      

                       Computation: 99024 steps/s (collection: 0.886s, learning 0.107s)
             Mean action noise std: 3.94
          Mean value_function loss: 33.8845
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 21.7573
                       Mean reward: 878.34
               Mean episode length: 249.66
    Episode_Reward/reaching_object: 0.7641
     Episode_Reward/lifting_object: 173.0947
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 0.99s
                      Time elapsed: 00:16:55
                               ETA: 00:17:00

################################################################################
                     [1m Learning iteration 998/2000 [0m                      

                       Computation: 109130 steps/s (collection: 0.800s, learning 0.101s)
             Mean action noise std: 3.95
          Mean value_function loss: 42.5013
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 21.7753
                       Mean reward: 864.59
               Mean episode length: 247.85
    Episode_Reward/reaching_object: 0.7618
     Episode_Reward/lifting_object: 172.7561
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 0.90s
                      Time elapsed: 00:16:56
                               ETA: 00:16:59

################################################################################
                     [1m Learning iteration 999/2000 [0m                      

                       Computation: 102066 steps/s (collection: 0.861s, learning 0.102s)
             Mean action noise std: 3.95
          Mean value_function loss: 40.9118
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 21.7905
                       Mean reward: 852.89
               Mean episode length: 248.59
    Episode_Reward/reaching_object: 0.7624
     Episode_Reward/lifting_object: 172.5310
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 0.96s
                      Time elapsed: 00:16:57
                               ETA: 00:16:58

################################################################################
                     [1m Learning iteration 1000/2000 [0m                     

                       Computation: 31626 steps/s (collection: 2.998s, learning 0.110s)
             Mean action noise std: 3.96
          Mean value_function loss: 38.3590
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 21.8042
                       Mean reward: 860.18
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7552
     Episode_Reward/lifting_object: 169.5049
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 3.11s
                      Time elapsed: 00:17:00
                               ETA: 00:16:59

################################################################################
                     [1m Learning iteration 1001/2000 [0m                     

                       Computation: 30355 steps/s (collection: 3.112s, learning 0.127s)
             Mean action noise std: 3.97
          Mean value_function loss: 32.6019
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 21.8203
                       Mean reward: 865.92
               Mean episode length: 247.63
    Episode_Reward/reaching_object: 0.7639
     Episode_Reward/lifting_object: 171.9553
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 3.24s
                      Time elapsed: 00:17:03
                               ETA: 00:17:00

################################################################################
                     [1m Learning iteration 1002/2000 [0m                     

                       Computation: 29930 steps/s (collection: 3.147s, learning 0.137s)
             Mean action noise std: 3.97
          Mean value_function loss: 39.1785
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 21.8377
                       Mean reward: 866.30
               Mean episode length: 246.44
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 170.5713
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 3.28s
                      Time elapsed: 00:17:06
                               ETA: 00:17:01

################################################################################
                     [1m Learning iteration 1003/2000 [0m                     

                       Computation: 29536 steps/s (collection: 3.197s, learning 0.131s)
             Mean action noise std: 3.98
          Mean value_function loss: 30.7079
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 21.8462
                       Mean reward: 868.42
               Mean episode length: 248.53
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 171.5036
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 3.33s
                      Time elapsed: 00:17:10
                               ETA: 00:17:02

################################################################################
                     [1m Learning iteration 1004/2000 [0m                     

                       Computation: 28673 steps/s (collection: 3.230s, learning 0.198s)
             Mean action noise std: 3.98
          Mean value_function loss: 33.6766
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 21.8594
                       Mean reward: 851.91
               Mean episode length: 246.10
    Episode_Reward/reaching_object: 0.7614
     Episode_Reward/lifting_object: 169.8949
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 3.43s
                      Time elapsed: 00:17:13
                               ETA: 00:17:04

################################################################################
                     [1m Learning iteration 1005/2000 [0m                     

                       Computation: 29108 steps/s (collection: 3.230s, learning 0.147s)
             Mean action noise std: 3.98
          Mean value_function loss: 39.6836
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 21.8669
                       Mean reward: 861.01
               Mean episode length: 246.63
    Episode_Reward/reaching_object: 0.7675
     Episode_Reward/lifting_object: 171.8733
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 3.38s
                      Time elapsed: 00:17:16
                               ETA: 00:17:05

################################################################################
                     [1m Learning iteration 1006/2000 [0m                     

                       Computation: 29703 steps/s (collection: 3.167s, learning 0.142s)
             Mean action noise std: 3.99
          Mean value_function loss: 52.1360
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 21.8703
                       Mean reward: 848.96
               Mean episode length: 246.99
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 169.3243
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 3.31s
                      Time elapsed: 00:17:20
                               ETA: 00:17:06

################################################################################
                     [1m Learning iteration 1007/2000 [0m                     

                       Computation: 30145 steps/s (collection: 3.143s, learning 0.118s)
             Mean action noise std: 3.99
          Mean value_function loss: 51.1630
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 21.8766
                       Mean reward: 843.05
               Mean episode length: 247.21
    Episode_Reward/reaching_object: 0.7621
     Episode_Reward/lifting_object: 170.4765
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 3.26s
                      Time elapsed: 00:17:23
                               ETA: 00:17:07

################################################################################
                     [1m Learning iteration 1008/2000 [0m                     

                       Computation: 18810 steps/s (collection: 5.081s, learning 0.145s)
             Mean action noise std: 4.00
          Mean value_function loss: 55.9576
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 21.8867
                       Mean reward: 868.79
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 170.2362
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 5.23s
                      Time elapsed: 00:17:28
                               ETA: 00:17:11

################################################################################
                     [1m Learning iteration 1009/2000 [0m                     

                       Computation: 89932 steps/s (collection: 0.901s, learning 0.192s)
             Mean action noise std: 4.01
          Mean value_function loss: 58.2231
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 21.9019
                       Mean reward: 853.38
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.7469
     Episode_Reward/lifting_object: 168.5265
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 1.09s
                      Time elapsed: 00:17:29
                               ETA: 00:17:10

################################################################################
                     [1m Learning iteration 1010/2000 [0m                     

                       Computation: 79010 steps/s (collection: 1.077s, learning 0.168s)
             Mean action noise std: 4.02
          Mean value_function loss: 46.9951
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 21.9167
                       Mean reward: 850.94
               Mean episode length: 243.42
    Episode_Reward/reaching_object: 0.7338
     Episode_Reward/lifting_object: 167.3957
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 1.24s
                      Time elapsed: 00:17:31
                               ETA: 00:17:09

################################################################################
                     [1m Learning iteration 1011/2000 [0m                     

                       Computation: 79582 steps/s (collection: 1.120s, learning 0.116s)
             Mean action noise std: 4.02
          Mean value_function loss: 41.7854
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 21.9312
                       Mean reward: 859.21
               Mean episode length: 246.59
    Episode_Reward/reaching_object: 0.7564
     Episode_Reward/lifting_object: 170.7272
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 1.24s
                      Time elapsed: 00:17:32
                               ETA: 00:17:08

################################################################################
                     [1m Learning iteration 1012/2000 [0m                     

                       Computation: 77451 steps/s (collection: 1.082s, learning 0.187s)
             Mean action noise std: 4.03
          Mean value_function loss: 51.7477
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 21.9466
                       Mean reward: 850.98
               Mean episode length: 246.11
    Episode_Reward/reaching_object: 0.7558
     Episode_Reward/lifting_object: 171.9604
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 1.27s
                      Time elapsed: 00:17:33
                               ETA: 00:17:07

################################################################################
                     [1m Learning iteration 1013/2000 [0m                     

                       Computation: 74933 steps/s (collection: 1.149s, learning 0.163s)
             Mean action noise std: 4.04
          Mean value_function loss: 39.2800
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 21.9708
                       Mean reward: 845.99
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7384
     Episode_Reward/lifting_object: 167.3400
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 1.31s
                      Time elapsed: 00:17:34
                               ETA: 00:17:06

################################################################################
                     [1m Learning iteration 1014/2000 [0m                     

                       Computation: 89680 steps/s (collection: 0.927s, learning 0.170s)
             Mean action noise std: 4.05
          Mean value_function loss: 41.5259
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 21.9868
                       Mean reward: 866.91
               Mean episode length: 247.69
    Episode_Reward/reaching_object: 0.7517
     Episode_Reward/lifting_object: 170.8549
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 1.10s
                      Time elapsed: 00:17:35
                               ETA: 00:17:05

################################################################################
                     [1m Learning iteration 1015/2000 [0m                     

                       Computation: 80363 steps/s (collection: 1.015s, learning 0.209s)
             Mean action noise std: 4.06
          Mean value_function loss: 28.1397
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 22.0018
                       Mean reward: 852.78
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7383
     Episode_Reward/lifting_object: 167.8787
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 1.22s
                      Time elapsed: 00:17:37
                               ETA: 00:17:04

################################################################################
                     [1m Learning iteration 1016/2000 [0m                     

                       Computation: 90942 steps/s (collection: 0.977s, learning 0.104s)
             Mean action noise std: 4.06
          Mean value_function loss: 44.3627
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 22.0131
                       Mean reward: 870.31
               Mean episode length: 249.79
    Episode_Reward/reaching_object: 0.7537
     Episode_Reward/lifting_object: 172.4254
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 1.08s
                      Time elapsed: 00:17:38
                               ETA: 00:17:03

################################################################################
                     [1m Learning iteration 1017/2000 [0m                     

                       Computation: 96389 steps/s (collection: 0.913s, learning 0.107s)
             Mean action noise std: 4.06
          Mean value_function loss: 54.2074
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 22.0187
                       Mean reward: 839.16
               Mean episode length: 244.33
    Episode_Reward/reaching_object: 0.7502
     Episode_Reward/lifting_object: 170.9541
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 1.02s
                      Time elapsed: 00:17:39
                               ETA: 00:17:02

################################################################################
                     [1m Learning iteration 1018/2000 [0m                     

                       Computation: 105585 steps/s (collection: 0.823s, learning 0.108s)
             Mean action noise std: 4.07
          Mean value_function loss: 40.4462
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 22.0246
                       Mean reward: 855.94
               Mean episode length: 248.63
    Episode_Reward/reaching_object: 0.7551
     Episode_Reward/lifting_object: 171.1617
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 0.93s
                      Time elapsed: 00:17:40
                               ETA: 00:17:01

################################################################################
                     [1m Learning iteration 1019/2000 [0m                     

                       Computation: 105748 steps/s (collection: 0.835s, learning 0.095s)
             Mean action noise std: 4.07
          Mean value_function loss: 44.8670
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 22.0360
                       Mean reward: 857.00
               Mean episode length: 247.51
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 171.8183
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 0.93s
                      Time elapsed: 00:17:41
                               ETA: 00:17:00

################################################################################
                     [1m Learning iteration 1020/2000 [0m                     

                       Computation: 99828 steps/s (collection: 0.894s, learning 0.091s)
             Mean action noise std: 4.08
          Mean value_function loss: 34.6421
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 22.0425
                       Mean reward: 862.93
               Mean episode length: 245.26
    Episode_Reward/reaching_object: 0.7564
     Episode_Reward/lifting_object: 171.2359
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 0.98s
                      Time elapsed: 00:17:42
                               ETA: 00:16:59

################################################################################
                     [1m Learning iteration 1021/2000 [0m                     

                       Computation: 91016 steps/s (collection: 0.928s, learning 0.152s)
             Mean action noise std: 4.08
          Mean value_function loss: 41.8672
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 22.0478
                       Mean reward: 850.64
               Mean episode length: 247.93
    Episode_Reward/reaching_object: 0.7546
     Episode_Reward/lifting_object: 171.1129
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 1.08s
                      Time elapsed: 00:17:43
                               ETA: 00:16:58

################################################################################
                     [1m Learning iteration 1022/2000 [0m                     

                       Computation: 95687 steps/s (collection: 0.914s, learning 0.113s)
             Mean action noise std: 4.08
          Mean value_function loss: 32.8982
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 22.0498
                       Mean reward: 857.26
               Mean episode length: 246.74
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 171.1526
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 1.03s
                      Time elapsed: 00:17:44
                               ETA: 00:16:57

################################################################################
                     [1m Learning iteration 1023/2000 [0m                     

                       Computation: 102919 steps/s (collection: 0.850s, learning 0.106s)
             Mean action noise std: 4.08
          Mean value_function loss: 28.4605
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 22.0562
                       Mean reward: 865.62
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 0.7563
     Episode_Reward/lifting_object: 170.4316
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 0.96s
                      Time elapsed: 00:17:45
                               ETA: 00:16:56

################################################################################
                     [1m Learning iteration 1024/2000 [0m                     

                       Computation: 99332 steps/s (collection: 0.890s, learning 0.100s)
             Mean action noise std: 4.09
          Mean value_function loss: 30.9763
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.0639
                       Mean reward: 854.18
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 170.5093
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 0.99s
                      Time elapsed: 00:17:46
                               ETA: 00:16:55

################################################################################
                     [1m Learning iteration 1025/2000 [0m                     

                       Computation: 100701 steps/s (collection: 0.875s, learning 0.102s)
             Mean action noise std: 4.09
          Mean value_function loss: 34.6727
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 22.0784
                       Mean reward: 856.82
               Mean episode length: 248.90
    Episode_Reward/reaching_object: 0.7606
     Episode_Reward/lifting_object: 170.5096
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 0.98s
                      Time elapsed: 00:17:47
                               ETA: 00:16:54

################################################################################
                     [1m Learning iteration 1026/2000 [0m                     

                       Computation: 104817 steps/s (collection: 0.839s, learning 0.099s)
             Mean action noise std: 4.10
          Mean value_function loss: 30.2135
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 22.0855
                       Mean reward: 870.59
               Mean episode length: 249.24
    Episode_Reward/reaching_object: 0.7689
     Episode_Reward/lifting_object: 171.9943
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 0.94s
                      Time elapsed: 00:17:48
                               ETA: 00:16:53

################################################################################
                     [1m Learning iteration 1027/2000 [0m                     

                       Computation: 103230 steps/s (collection: 0.853s, learning 0.100s)
             Mean action noise std: 4.10
          Mean value_function loss: 32.9575
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 22.0899
                       Mean reward: 871.60
               Mean episode length: 249.03
    Episode_Reward/reaching_object: 0.7671
     Episode_Reward/lifting_object: 173.9090
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 0.95s
                      Time elapsed: 00:17:49
                               ETA: 00:16:51

################################################################################
                     [1m Learning iteration 1028/2000 [0m                     

                       Computation: 101415 steps/s (collection: 0.843s, learning 0.126s)
             Mean action noise std: 4.11
          Mean value_function loss: 29.0840
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 22.1012
                       Mean reward: 870.96
               Mean episode length: 248.69
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 172.1442
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 0.97s
                      Time elapsed: 00:17:50
                               ETA: 00:16:50

################################################################################
                     [1m Learning iteration 1029/2000 [0m                     

                       Computation: 104961 steps/s (collection: 0.824s, learning 0.112s)
             Mean action noise std: 4.11
          Mean value_function loss: 41.6268
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 22.1080
                       Mean reward: 875.76
               Mean episode length: 249.84
    Episode_Reward/reaching_object: 0.7660
     Episode_Reward/lifting_object: 173.0386
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 0.94s
                      Time elapsed: 00:17:50
                               ETA: 00:16:49

################################################################################
                     [1m Learning iteration 1030/2000 [0m                     

                       Computation: 99136 steps/s (collection: 0.819s, learning 0.173s)
             Mean action noise std: 4.11
          Mean value_function loss: 32.1457
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 22.1177
                       Mean reward: 861.67
               Mean episode length: 246.96
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 170.4625
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 0.99s
                      Time elapsed: 00:17:51
                               ETA: 00:16:48

################################################################################
                     [1m Learning iteration 1031/2000 [0m                     

                       Computation: 107616 steps/s (collection: 0.812s, learning 0.102s)
             Mean action noise std: 4.12
          Mean value_function loss: 31.9635
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 22.1294
                       Mean reward: 865.70
               Mean episode length: 247.61
    Episode_Reward/reaching_object: 0.7640
     Episode_Reward/lifting_object: 172.5833
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 0.91s
                      Time elapsed: 00:17:52
                               ETA: 00:16:47

################################################################################
                     [1m Learning iteration 1032/2000 [0m                     

                       Computation: 111345 steps/s (collection: 0.778s, learning 0.105s)
             Mean action noise std: 4.12
          Mean value_function loss: 28.8949
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 22.1359
                       Mean reward: 869.16
               Mean episode length: 248.39
    Episode_Reward/reaching_object: 0.7672
     Episode_Reward/lifting_object: 173.2946
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 0.88s
                      Time elapsed: 00:17:53
                               ETA: 00:16:46

################################################################################
                     [1m Learning iteration 1033/2000 [0m                     

                       Computation: 101020 steps/s (collection: 0.838s, learning 0.135s)
             Mean action noise std: 4.12
          Mean value_function loss: 31.2963
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 22.1407
                       Mean reward: 861.59
               Mean episode length: 245.88
    Episode_Reward/reaching_object: 0.7554
     Episode_Reward/lifting_object: 171.9199
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 0.97s
                      Time elapsed: 00:17:54
                               ETA: 00:16:45

################################################################################
                     [1m Learning iteration 1034/2000 [0m                     

                       Computation: 106401 steps/s (collection: 0.810s, learning 0.114s)
             Mean action noise std: 4.13
          Mean value_function loss: 22.7780
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 22.1485
                       Mean reward: 876.16
               Mean episode length: 249.00
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 173.6356
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 0.92s
                      Time elapsed: 00:17:55
                               ETA: 00:16:43

################################################################################
                     [1m Learning iteration 1035/2000 [0m                     

                       Computation: 97394 steps/s (collection: 0.868s, learning 0.142s)
             Mean action noise std: 4.13
          Mean value_function loss: 34.1413
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 22.1601
                       Mean reward: 862.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7612
     Episode_Reward/lifting_object: 172.5760
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 1.01s
                      Time elapsed: 00:17:56
                               ETA: 00:16:42

################################################################################
                     [1m Learning iteration 1036/2000 [0m                     

                       Computation: 104212 steps/s (collection: 0.845s, learning 0.099s)
             Mean action noise std: 4.14
          Mean value_function loss: 40.7579
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 22.1666
                       Mean reward: 875.83
               Mean episode length: 249.56
    Episode_Reward/reaching_object: 0.7568
     Episode_Reward/lifting_object: 171.3407
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 0.94s
                      Time elapsed: 00:17:57
                               ETA: 00:16:41

################################################################################
                     [1m Learning iteration 1037/2000 [0m                     

                       Computation: 100613 steps/s (collection: 0.884s, learning 0.093s)
             Mean action noise std: 4.14
          Mean value_function loss: 33.4224
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 22.1765
                       Mean reward: 857.49
               Mean episode length: 247.18
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 170.9919
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 0.98s
                      Time elapsed: 00:17:58
                               ETA: 00:16:40

################################################################################
                     [1m Learning iteration 1038/2000 [0m                     

                       Computation: 102174 steps/s (collection: 0.829s, learning 0.134s)
             Mean action noise std: 4.14
          Mean value_function loss: 41.3641
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 22.1763
                       Mean reward: 858.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7578
     Episode_Reward/lifting_object: 171.3769
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 0.96s
                      Time elapsed: 00:17:59
                               ETA: 00:16:39

################################################################################
                     [1m Learning iteration 1039/2000 [0m                     

                       Computation: 92013 steps/s (collection: 0.939s, learning 0.130s)
             Mean action noise std: 4.14
          Mean value_function loss: 42.7303
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 22.1749
                       Mean reward: 846.63
               Mean episode length: 247.33
    Episode_Reward/reaching_object: 0.7577
     Episode_Reward/lifting_object: 170.6689
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 1.07s
                      Time elapsed: 00:18:00
                               ETA: 00:16:38

################################################################################
                     [1m Learning iteration 1040/2000 [0m                     

                       Computation: 104277 steps/s (collection: 0.850s, learning 0.093s)
             Mean action noise std: 4.15
          Mean value_function loss: 31.0992
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 22.1816
                       Mean reward: 875.31
               Mean episode length: 249.90
    Episode_Reward/reaching_object: 0.7590
     Episode_Reward/lifting_object: 171.8409
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 0.94s
                      Time elapsed: 00:18:01
                               ETA: 00:16:37

################################################################################
                     [1m Learning iteration 1041/2000 [0m                     

                       Computation: 97813 steps/s (collection: 0.909s, learning 0.097s)
             Mean action noise std: 4.15
          Mean value_function loss: 26.4790
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 22.1958
                       Mean reward: 858.32
               Mean episode length: 245.60
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 171.1673
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 1.01s
                      Time elapsed: 00:18:02
                               ETA: 00:16:36

################################################################################
                     [1m Learning iteration 1042/2000 [0m                     

                       Computation: 104145 steps/s (collection: 0.831s, learning 0.113s)
             Mean action noise std: 4.16
          Mean value_function loss: 38.8794
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 22.2079
                       Mean reward: 865.87
               Mean episode length: 249.13
    Episode_Reward/reaching_object: 0.7594
     Episode_Reward/lifting_object: 170.9056
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 0.94s
                      Time elapsed: 00:18:03
                               ETA: 00:16:35

################################################################################
                     [1m Learning iteration 1043/2000 [0m                     

                       Computation: 94815 steps/s (collection: 0.876s, learning 0.161s)
             Mean action noise std: 4.16
          Mean value_function loss: 29.0172
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 22.2157
                       Mean reward: 865.43
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 173.0849
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 1.04s
                      Time elapsed: 00:18:04
                               ETA: 00:16:34

################################################################################
                     [1m Learning iteration 1044/2000 [0m                     

                       Computation: 106232 steps/s (collection: 0.830s, learning 0.095s)
             Mean action noise std: 4.17
          Mean value_function loss: 36.8870
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 22.2195
                       Mean reward: 866.13
               Mean episode length: 246.64
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 171.9863
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 0.93s
                      Time elapsed: 00:18:05
                               ETA: 00:16:33

################################################################################
                     [1m Learning iteration 1045/2000 [0m                     

                       Computation: 103467 steps/s (collection: 0.856s, learning 0.095s)
             Mean action noise std: 4.17
          Mean value_function loss: 34.8163
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 22.2300
                       Mean reward: 871.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7601
     Episode_Reward/lifting_object: 172.1198
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 0.95s
                      Time elapsed: 00:18:06
                               ETA: 00:16:31

################################################################################
                     [1m Learning iteration 1046/2000 [0m                     

                       Computation: 109319 steps/s (collection: 0.808s, learning 0.091s)
             Mean action noise std: 4.18
          Mean value_function loss: 49.3076
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 22.2412
                       Mean reward: 834.57
               Mean episode length: 245.83
    Episode_Reward/reaching_object: 0.7553
     Episode_Reward/lifting_object: 169.3738
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 0.90s
                      Time elapsed: 00:18:07
                               ETA: 00:16:30

################################################################################
                     [1m Learning iteration 1047/2000 [0m                     

                       Computation: 105713 steps/s (collection: 0.836s, learning 0.094s)
             Mean action noise std: 4.18
          Mean value_function loss: 36.3280
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 22.2524
                       Mean reward: 838.06
               Mean episode length: 243.33
    Episode_Reward/reaching_object: 0.7573
     Episode_Reward/lifting_object: 170.8358
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 0.93s
                      Time elapsed: 00:18:08
                               ETA: 00:16:29

################################################################################
                     [1m Learning iteration 1048/2000 [0m                     

                       Computation: 109236 steps/s (collection: 0.794s, learning 0.106s)
             Mean action noise std: 4.19
          Mean value_function loss: 25.1202
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 22.2691
                       Mean reward: 876.93
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 173.0156
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 0.90s
                      Time elapsed: 00:18:09
                               ETA: 00:16:28

################################################################################
                     [1m Learning iteration 1049/2000 [0m                     

                       Computation: 90875 steps/s (collection: 0.911s, learning 0.171s)
             Mean action noise std: 4.20
          Mean value_function loss: 25.7187
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 22.2895
                       Mean reward: 877.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 173.4350
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 1.08s
                      Time elapsed: 00:18:10
                               ETA: 00:16:27

################################################################################
                     [1m Learning iteration 1050/2000 [0m                     

                       Computation: 98536 steps/s (collection: 0.874s, learning 0.124s)
             Mean action noise std: 4.21
          Mean value_function loss: 28.4347
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 22.2998
                       Mean reward: 864.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7597
     Episode_Reward/lifting_object: 172.1222
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 1.00s
                      Time elapsed: 00:18:11
                               ETA: 00:16:26

################################################################################
                     [1m Learning iteration 1051/2000 [0m                     

                       Computation: 99197 steps/s (collection: 0.829s, learning 0.162s)
             Mean action noise std: 4.21
          Mean value_function loss: 35.3269
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 22.3100
                       Mean reward: 867.67
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7685
     Episode_Reward/lifting_object: 172.7064
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 0.99s
                      Time elapsed: 00:18:12
                               ETA: 00:16:25

################################################################################
                     [1m Learning iteration 1052/2000 [0m                     

                       Computation: 103235 steps/s (collection: 0.852s, learning 0.100s)
             Mean action noise std: 4.22
          Mean value_function loss: 37.8743
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 22.3239
                       Mean reward: 857.68
               Mean episode length: 247.14
    Episode_Reward/reaching_object: 0.7564
     Episode_Reward/lifting_object: 171.4065
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 0.95s
                      Time elapsed: 00:18:13
                               ETA: 00:16:24

################################################################################
                     [1m Learning iteration 1053/2000 [0m                     

                       Computation: 84686 steps/s (collection: 0.963s, learning 0.197s)
             Mean action noise std: 4.22
          Mean value_function loss: 38.5196
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 22.3297
                       Mean reward: 864.16
               Mean episode length: 248.78
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 172.5344
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 1.16s
                      Time elapsed: 00:18:14
                               ETA: 00:16:23

################################################################################
                     [1m Learning iteration 1054/2000 [0m                     

                       Computation: 107525 steps/s (collection: 0.808s, learning 0.107s)
             Mean action noise std: 4.23
          Mean value_function loss: 38.9151
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 22.3374
                       Mean reward: 852.23
               Mean episode length: 246.80
    Episode_Reward/reaching_object: 0.7615
     Episode_Reward/lifting_object: 171.8479
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 0.91s
                      Time elapsed: 00:18:15
                               ETA: 00:16:22

################################################################################
                     [1m Learning iteration 1055/2000 [0m                     

                       Computation: 106916 steps/s (collection: 0.815s, learning 0.105s)
             Mean action noise std: 4.24
          Mean value_function loss: 37.2425
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 22.3501
                       Mean reward: 866.65
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7609
     Episode_Reward/lifting_object: 171.1723
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 0.92s
                      Time elapsed: 00:18:16
                               ETA: 00:16:20

################################################################################
                     [1m Learning iteration 1056/2000 [0m                     

                       Computation: 106723 steps/s (collection: 0.796s, learning 0.125s)
             Mean action noise std: 4.24
          Mean value_function loss: 25.8605
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 22.3634
                       Mean reward: 863.91
               Mean episode length: 249.45
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 172.2931
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 0.92s
                      Time elapsed: 00:18:17
                               ETA: 00:16:19

################################################################################
                     [1m Learning iteration 1057/2000 [0m                     

                       Computation: 106100 steps/s (collection: 0.827s, learning 0.100s)
             Mean action noise std: 4.25
          Mean value_function loss: 37.2114
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 22.3763
                       Mean reward: 862.02
               Mean episode length: 249.74
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 172.2027
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 0.93s
                      Time elapsed: 00:18:18
                               ETA: 00:16:18

################################################################################
                     [1m Learning iteration 1058/2000 [0m                     

                       Computation: 108524 steps/s (collection: 0.795s, learning 0.111s)
             Mean action noise std: 4.26
          Mean value_function loss: 39.4298
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 22.3942
                       Mean reward: 861.53
               Mean episode length: 246.60
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 172.6690
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 0.91s
                      Time elapsed: 00:18:18
                               ETA: 00:16:17

################################################################################
                     [1m Learning iteration 1059/2000 [0m                     

                       Computation: 99864 steps/s (collection: 0.831s, learning 0.153s)
             Mean action noise std: 4.27
          Mean value_function loss: 38.4423
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 22.4093
                       Mean reward: 864.77
               Mean episode length: 247.69
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 172.9134
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 0.98s
                      Time elapsed: 00:18:19
                               ETA: 00:16:16

################################################################################
                     [1m Learning iteration 1060/2000 [0m                     

                       Computation: 108059 steps/s (collection: 0.805s, learning 0.105s)
             Mean action noise std: 4.27
          Mean value_function loss: 30.9626
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 22.4174
                       Mean reward: 840.37
               Mean episode length: 245.31
    Episode_Reward/reaching_object: 0.7496
     Episode_Reward/lifting_object: 169.2971
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 0.91s
                      Time elapsed: 00:18:20
                               ETA: 00:16:15

################################################################################
                     [1m Learning iteration 1061/2000 [0m                     

                       Computation: 107367 steps/s (collection: 0.779s, learning 0.137s)
             Mean action noise std: 4.27
          Mean value_function loss: 39.2352
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.4268
                       Mean reward: 845.22
               Mean episode length: 247.32
    Episode_Reward/reaching_object: 0.7608
     Episode_Reward/lifting_object: 171.2926
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 0.92s
                      Time elapsed: 00:18:21
                               ETA: 00:16:14

################################################################################
                     [1m Learning iteration 1062/2000 [0m                     

                       Computation: 103703 steps/s (collection: 0.785s, learning 0.163s)
             Mean action noise std: 4.28
          Mean value_function loss: 22.6546
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 22.4348
                       Mean reward: 866.19
               Mean episode length: 249.49
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 170.6510
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 0.95s
                      Time elapsed: 00:18:22
                               ETA: 00:16:13

################################################################################
                     [1m Learning iteration 1063/2000 [0m                     

                       Computation: 101798 steps/s (collection: 0.807s, learning 0.159s)
             Mean action noise std: 4.28
          Mean value_function loss: 28.0726
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 22.4466
                       Mean reward: 856.80
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.7561
     Episode_Reward/lifting_object: 170.9291
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 0.97s
                      Time elapsed: 00:18:23
                               ETA: 00:16:11

################################################################################
                     [1m Learning iteration 1064/2000 [0m                     

                       Computation: 109891 steps/s (collection: 0.785s, learning 0.110s)
             Mean action noise std: 4.29
          Mean value_function loss: 39.3569
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.4605
                       Mean reward: 866.04
               Mean episode length: 246.76
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 173.3397
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 0.89s
                      Time elapsed: 00:18:24
                               ETA: 00:16:10

################################################################################
                     [1m Learning iteration 1065/2000 [0m                     

                       Computation: 104858 steps/s (collection: 0.827s, learning 0.111s)
             Mean action noise std: 4.30
          Mean value_function loss: 29.1183
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.4717
                       Mean reward: 860.79
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 0.7659
     Episode_Reward/lifting_object: 171.6371
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 0.94s
                      Time elapsed: 00:18:25
                               ETA: 00:16:09

################################################################################
                     [1m Learning iteration 1066/2000 [0m                     

                       Computation: 99669 steps/s (collection: 0.882s, learning 0.104s)
             Mean action noise std: 4.30
          Mean value_function loss: 42.8699
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 22.4814
                       Mean reward: 867.05
               Mean episode length: 249.29
    Episode_Reward/reaching_object: 0.7660
     Episode_Reward/lifting_object: 171.7926
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 0.99s
                      Time elapsed: 00:18:26
                               ETA: 00:16:08

################################################################################
                     [1m Learning iteration 1067/2000 [0m                     

                       Computation: 88617 steps/s (collection: 0.891s, learning 0.218s)
             Mean action noise std: 4.31
          Mean value_function loss: 36.9459
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 22.4986
                       Mean reward: 857.79
               Mean episode length: 247.61
    Episode_Reward/reaching_object: 0.7583
     Episode_Reward/lifting_object: 170.8027
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 1.11s
                      Time elapsed: 00:18:27
                               ETA: 00:16:07

################################################################################
                     [1m Learning iteration 1068/2000 [0m                     

                       Computation: 103612 steps/s (collection: 0.822s, learning 0.127s)
             Mean action noise std: 4.32
          Mean value_function loss: 42.9568
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 22.5224
                       Mean reward: 878.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7724
     Episode_Reward/lifting_object: 173.3743
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 0.95s
                      Time elapsed: 00:18:28
                               ETA: 00:16:06

################################################################################
                     [1m Learning iteration 1069/2000 [0m                     

                       Computation: 109972 steps/s (collection: 0.804s, learning 0.090s)
             Mean action noise std: 4.32
          Mean value_function loss: 38.3351
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 22.5286
                       Mean reward: 869.48
               Mean episode length: 248.58
    Episode_Reward/reaching_object: 0.7646
     Episode_Reward/lifting_object: 173.5137
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 0.89s
                      Time elapsed: 00:18:29
                               ETA: 00:16:05

################################################################################
                     [1m Learning iteration 1070/2000 [0m                     

                       Computation: 112604 steps/s (collection: 0.777s, learning 0.096s)
             Mean action noise std: 4.33
          Mean value_function loss: 38.8250
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.5407
                       Mean reward: 869.63
               Mean episode length: 247.57
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 171.9405
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 0.87s
                      Time elapsed: 00:18:30
                               ETA: 00:16:04

################################################################################
                     [1m Learning iteration 1071/2000 [0m                     

                       Computation: 111470 steps/s (collection: 0.793s, learning 0.089s)
             Mean action noise std: 4.33
          Mean value_function loss: 41.1674
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 22.5495
                       Mean reward: 857.50
               Mean episode length: 247.26
    Episode_Reward/reaching_object: 0.7597
     Episode_Reward/lifting_object: 170.2474
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 0.88s
                      Time elapsed: 00:18:31
                               ETA: 00:16:02

################################################################################
                     [1m Learning iteration 1072/2000 [0m                     

                       Computation: 111619 steps/s (collection: 0.785s, learning 0.096s)
             Mean action noise std: 4.34
          Mean value_function loss: 43.5754
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 22.5569
                       Mean reward: 873.93
               Mean episode length: 249.99
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 172.5909
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 0.88s
                      Time elapsed: 00:18:32
                               ETA: 00:16:01

################################################################################
                     [1m Learning iteration 1073/2000 [0m                     

                       Computation: 95480 steps/s (collection: 0.897s, learning 0.132s)
             Mean action noise std: 4.35
          Mean value_function loss: 42.5725
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 22.5704
                       Mean reward: 857.05
               Mean episode length: 249.09
    Episode_Reward/reaching_object: 0.7557
     Episode_Reward/lifting_object: 170.2989
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 1.03s
                      Time elapsed: 00:18:33
                               ETA: 00:16:00

################################################################################
                     [1m Learning iteration 1074/2000 [0m                     

                       Computation: 114792 steps/s (collection: 0.767s, learning 0.090s)
             Mean action noise std: 4.35
          Mean value_function loss: 44.0635
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 22.5846
                       Mean reward: 871.50
               Mean episode length: 249.57
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 171.9784
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 0.86s
                      Time elapsed: 00:18:33
                               ETA: 00:15:59

################################################################################
                     [1m Learning iteration 1075/2000 [0m                     

                       Computation: 108040 steps/s (collection: 0.785s, learning 0.125s)
             Mean action noise std: 4.36
          Mean value_function loss: 38.8483
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.6000
                       Mean reward: 857.24
               Mean episode length: 248.67
    Episode_Reward/reaching_object: 0.7644
     Episode_Reward/lifting_object: 171.8674
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 0.91s
                      Time elapsed: 00:18:34
                               ETA: 00:15:58

################################################################################
                     [1m Learning iteration 1076/2000 [0m                     

                       Computation: 113733 steps/s (collection: 0.761s, learning 0.103s)
             Mean action noise std: 4.36
          Mean value_function loss: 34.5626
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 22.6120
                       Mean reward: 869.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7651
     Episode_Reward/lifting_object: 172.2379
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 0.86s
                      Time elapsed: 00:18:35
                               ETA: 00:15:57

################################################################################
                     [1m Learning iteration 1077/2000 [0m                     

                       Computation: 111829 steps/s (collection: 0.778s, learning 0.102s)
             Mean action noise std: 4.37
          Mean value_function loss: 31.5655
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 22.6133
                       Mean reward: 874.65
               Mean episode length: 249.50
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 172.2491
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 0.88s
                      Time elapsed: 00:18:36
                               ETA: 00:15:56

################################################################################
                     [1m Learning iteration 1078/2000 [0m                     

                       Computation: 111264 steps/s (collection: 0.775s, learning 0.109s)
             Mean action noise std: 4.37
          Mean value_function loss: 38.9329
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 22.6179
                       Mean reward: 846.08
               Mean episode length: 249.38
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 171.6471
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 0.88s
                      Time elapsed: 00:18:37
                               ETA: 00:15:54

################################################################################
                     [1m Learning iteration 1079/2000 [0m                     

                       Computation: 108384 steps/s (collection: 0.816s, learning 0.091s)
             Mean action noise std: 4.38
          Mean value_function loss: 38.4686
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 22.6290
                       Mean reward: 852.81
               Mean episode length: 246.99
    Episode_Reward/reaching_object: 0.7576
     Episode_Reward/lifting_object: 171.0439
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 0.91s
                      Time elapsed: 00:18:38
                               ETA: 00:15:53

################################################################################
                     [1m Learning iteration 1080/2000 [0m                     

                       Computation: 113083 steps/s (collection: 0.782s, learning 0.087s)
             Mean action noise std: 4.39
          Mean value_function loss: 31.9125
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 22.6477
                       Mean reward: 864.59
               Mean episode length: 249.44
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 170.6546
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 0.87s
                      Time elapsed: 00:18:39
                               ETA: 00:15:52

################################################################################
                     [1m Learning iteration 1081/2000 [0m                     

                       Computation: 110086 steps/s (collection: 0.803s, learning 0.090s)
             Mean action noise std: 4.39
          Mean value_function loss: 35.3115
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 22.6571
                       Mean reward: 861.73
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 171.4326
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 0.89s
                      Time elapsed: 00:18:40
                               ETA: 00:15:51

################################################################################
                     [1m Learning iteration 1082/2000 [0m                     

                       Computation: 111583 steps/s (collection: 0.784s, learning 0.097s)
             Mean action noise std: 4.40
          Mean value_function loss: 26.6571
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 22.6707
                       Mean reward: 853.69
               Mean episode length: 249.53
    Episode_Reward/reaching_object: 0.7546
     Episode_Reward/lifting_object: 170.2321
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 0.88s
                      Time elapsed: 00:18:41
                               ETA: 00:15:50

################################################################################
                     [1m Learning iteration 1083/2000 [0m                     

                       Computation: 106879 steps/s (collection: 0.823s, learning 0.097s)
             Mean action noise std: 4.41
          Mean value_function loss: 28.7163
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 22.6848
                       Mean reward: 855.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7585
     Episode_Reward/lifting_object: 169.6341
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 0.92s
                      Time elapsed: 00:18:41
                               ETA: 00:15:49

################################################################################
                     [1m Learning iteration 1084/2000 [0m                     

                       Computation: 113796 steps/s (collection: 0.764s, learning 0.100s)
             Mean action noise std: 4.42
          Mean value_function loss: 37.3956
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 22.6978
                       Mean reward: 858.15
               Mean episode length: 248.92
    Episode_Reward/reaching_object: 0.7691
     Episode_Reward/lifting_object: 172.6411
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 0.86s
                      Time elapsed: 00:18:42
                               ETA: 00:15:47

################################################################################
                     [1m Learning iteration 1085/2000 [0m                     

                       Computation: 113730 steps/s (collection: 0.739s, learning 0.126s)
             Mean action noise std: 4.42
          Mean value_function loss: 34.7785
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 22.7095
                       Mean reward: 858.22
               Mean episode length: 246.61
    Episode_Reward/reaching_object: 0.7586
     Episode_Reward/lifting_object: 171.7104
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 0.86s
                      Time elapsed: 00:18:43
                               ETA: 00:15:46

################################################################################
                     [1m Learning iteration 1086/2000 [0m                     

                       Computation: 112282 steps/s (collection: 0.773s, learning 0.103s)
             Mean action noise std: 4.42
          Mean value_function loss: 37.6215
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 22.7151
                       Mean reward: 851.88
               Mean episode length: 244.99
    Episode_Reward/reaching_object: 0.7596
     Episode_Reward/lifting_object: 171.8868
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 0.88s
                      Time elapsed: 00:18:44
                               ETA: 00:15:45

################################################################################
                     [1m Learning iteration 1087/2000 [0m                     

                       Computation: 113804 steps/s (collection: 0.764s, learning 0.100s)
             Mean action noise std: 4.43
          Mean value_function loss: 41.2808
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.7271
                       Mean reward: 837.08
               Mean episode length: 246.46
    Episode_Reward/reaching_object: 0.7474
     Episode_Reward/lifting_object: 167.8821
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 0.86s
                      Time elapsed: 00:18:45
                               ETA: 00:15:44

################################################################################
                     [1m Learning iteration 1088/2000 [0m                     

                       Computation: 111334 steps/s (collection: 0.790s, learning 0.093s)
             Mean action noise std: 4.45
          Mean value_function loss: 24.0734
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 22.7466
                       Mean reward: 873.91
               Mean episode length: 248.93
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 172.7264
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 0.88s
                      Time elapsed: 00:18:46
                               ETA: 00:15:43

################################################################################
                     [1m Learning iteration 1089/2000 [0m                     

                       Computation: 113024 steps/s (collection: 0.779s, learning 0.091s)
             Mean action noise std: 4.45
          Mean value_function loss: 38.2638
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 22.7644
                       Mean reward: 874.02
               Mean episode length: 249.73
    Episode_Reward/reaching_object: 0.7708
     Episode_Reward/lifting_object: 173.6654
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 0.87s
                      Time elapsed: 00:18:47
                               ETA: 00:15:42

################################################################################
                     [1m Learning iteration 1090/2000 [0m                     

                       Computation: 111350 steps/s (collection: 0.786s, learning 0.097s)
             Mean action noise std: 4.46
          Mean value_function loss: 30.7808
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 22.7810
                       Mean reward: 854.49
               Mean episode length: 247.09
    Episode_Reward/reaching_object: 0.7634
     Episode_Reward/lifting_object: 173.0204
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 0.88s
                      Time elapsed: 00:18:48
                               ETA: 00:15:40

################################################################################
                     [1m Learning iteration 1091/2000 [0m                     

                       Computation: 103707 steps/s (collection: 0.836s, learning 0.112s)
             Mean action noise std: 4.47
          Mean value_function loss: 31.3018
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 22.7957
                       Mean reward: 845.43
               Mean episode length: 244.36
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 171.7356
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 0.95s
                      Time elapsed: 00:18:49
                               ETA: 00:15:39

################################################################################
                     [1m Learning iteration 1092/2000 [0m                     

                       Computation: 111965 steps/s (collection: 0.780s, learning 0.098s)
             Mean action noise std: 4.47
          Mean value_function loss: 35.7888
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 22.8040
                       Mean reward: 866.16
               Mean episode length: 249.78
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 171.9696
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 0.88s
                      Time elapsed: 00:18:49
                               ETA: 00:15:38

################################################################################
                     [1m Learning iteration 1093/2000 [0m                     

                       Computation: 114225 steps/s (collection: 0.766s, learning 0.095s)
             Mean action noise std: 4.48
          Mean value_function loss: 40.1411
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 22.8160
                       Mean reward: 861.15
               Mean episode length: 245.33
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 172.4523
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 0.86s
                      Time elapsed: 00:18:50
                               ETA: 00:15:37

################################################################################
                     [1m Learning iteration 1094/2000 [0m                     

                       Computation: 116919 steps/s (collection: 0.746s, learning 0.094s)
             Mean action noise std: 4.48
          Mean value_function loss: 42.5179
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 22.8247
                       Mean reward: 869.77
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7674
     Episode_Reward/lifting_object: 173.5267
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 0.84s
                      Time elapsed: 00:18:51
                               ETA: 00:15:36

################################################################################
                     [1m Learning iteration 1095/2000 [0m                     

                       Computation: 113886 steps/s (collection: 0.773s, learning 0.090s)
             Mean action noise std: 4.49
          Mean value_function loss: 35.7861
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 22.8356
                       Mean reward: 860.55
               Mean episode length: 246.36
    Episode_Reward/reaching_object: 0.7666
     Episode_Reward/lifting_object: 172.1303
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 0.86s
                      Time elapsed: 00:18:52
                               ETA: 00:15:35

################################################################################
                     [1m Learning iteration 1096/2000 [0m                     

                       Computation: 113618 steps/s (collection: 0.775s, learning 0.090s)
             Mean action noise std: 4.49
          Mean value_function loss: 35.3926
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 22.8444
                       Mean reward: 859.49
               Mean episode length: 248.60
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 171.3345
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 0.87s
                      Time elapsed: 00:18:53
                               ETA: 00:15:33

################################################################################
                     [1m Learning iteration 1097/2000 [0m                     

                       Computation: 112164 steps/s (collection: 0.780s, learning 0.096s)
             Mean action noise std: 4.50
          Mean value_function loss: 31.4779
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 22.8479
                       Mean reward: 864.95
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 172.1213
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0523
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 0.88s
                      Time elapsed: 00:18:54
                               ETA: 00:15:32

################################################################################
                     [1m Learning iteration 1098/2000 [0m                     

                       Computation: 111273 steps/s (collection: 0.794s, learning 0.090s)
             Mean action noise std: 4.51
          Mean value_function loss: 38.4424
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 22.8542
                       Mean reward: 847.69
               Mean episode length: 247.23
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 170.8292
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 0.88s
                      Time elapsed: 00:18:55
                               ETA: 00:15:31

################################################################################
                     [1m Learning iteration 1099/2000 [0m                     

                       Computation: 113787 steps/s (collection: 0.774s, learning 0.090s)
             Mean action noise std: 4.52
          Mean value_function loss: 37.8756
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 22.8782
                       Mean reward: 867.53
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 172.2262
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 0.86s
                      Time elapsed: 00:18:55
                               ETA: 00:15:30

################################################################################
                     [1m Learning iteration 1100/2000 [0m                     

                       Computation: 106528 steps/s (collection: 0.810s, learning 0.113s)
             Mean action noise std: 4.52
          Mean value_function loss: 46.3638
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 22.8930
                       Mean reward: 854.00
               Mean episode length: 245.71
    Episode_Reward/reaching_object: 0.7606
     Episode_Reward/lifting_object: 171.4022
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 0.92s
                      Time elapsed: 00:18:56
                               ETA: 00:15:29

################################################################################
                     [1m Learning iteration 1101/2000 [0m                     

                       Computation: 105304 steps/s (collection: 0.837s, learning 0.097s)
             Mean action noise std: 4.53
          Mean value_function loss: 32.7958
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 22.9009
                       Mean reward: 867.27
               Mean episode length: 249.01
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 172.1239
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 0.93s
                      Time elapsed: 00:18:57
                               ETA: 00:15:28

################################################################################
                     [1m Learning iteration 1102/2000 [0m                     

                       Computation: 96760 steps/s (collection: 0.880s, learning 0.136s)
             Mean action noise std: 4.53
          Mean value_function loss: 41.3092
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 22.9117
                       Mean reward: 855.61
               Mean episode length: 246.33
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 170.6508
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 1.02s
                      Time elapsed: 00:18:58
                               ETA: 00:15:27

################################################################################
                     [1m Learning iteration 1103/2000 [0m                     

                       Computation: 106731 steps/s (collection: 0.831s, learning 0.090s)
             Mean action noise std: 4.54
          Mean value_function loss: 37.4246
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 22.9201
                       Mean reward: 852.43
               Mean episode length: 246.75
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 171.0444
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 0.92s
                      Time elapsed: 00:18:59
                               ETA: 00:15:26

################################################################################
                     [1m Learning iteration 1104/2000 [0m                     

                       Computation: 104888 steps/s (collection: 0.795s, learning 0.142s)
             Mean action noise std: 4.54
          Mean value_function loss: 39.3752
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 22.9280
                       Mean reward: 855.85
               Mean episode length: 246.65
    Episode_Reward/reaching_object: 0.7610
     Episode_Reward/lifting_object: 171.3328
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0530
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 0.94s
                      Time elapsed: 00:19:00
                               ETA: 00:15:24

################################################################################
                     [1m Learning iteration 1105/2000 [0m                     

                       Computation: 87322 steps/s (collection: 0.853s, learning 0.273s)
             Mean action noise std: 4.55
          Mean value_function loss: 40.4595
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 22.9407
                       Mean reward: 876.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7702
     Episode_Reward/lifting_object: 172.9599
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 1.13s
                      Time elapsed: 00:19:01
                               ETA: 00:15:23

################################################################################
                     [1m Learning iteration 1106/2000 [0m                     

                       Computation: 84633 steps/s (collection: 0.875s, learning 0.287s)
             Mean action noise std: 4.56
          Mean value_function loss: 36.4620
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 22.9576
                       Mean reward: 860.96
               Mean episode length: 246.81
    Episode_Reward/reaching_object: 0.7515
     Episode_Reward/lifting_object: 169.8070
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0533
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 1.16s
                      Time elapsed: 00:19:02
                               ETA: 00:15:23

################################################################################
                     [1m Learning iteration 1107/2000 [0m                     

                       Computation: 86150 steps/s (collection: 0.962s, learning 0.179s)
             Mean action noise std: 4.57
          Mean value_function loss: 36.6807
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 22.9752
                       Mean reward: 850.31
               Mean episode length: 246.47
    Episode_Reward/reaching_object: 0.7666
     Episode_Reward/lifting_object: 171.9340
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 1.14s
                      Time elapsed: 00:19:04
                               ETA: 00:15:22

################################################################################
                     [1m Learning iteration 1108/2000 [0m                     

                       Computation: 93251 steps/s (collection: 0.932s, learning 0.122s)
             Mean action noise std: 4.57
          Mean value_function loss: 32.0109
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 22.9834
                       Mean reward: 879.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 172.4456
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 1.05s
                      Time elapsed: 00:19:05
                               ETA: 00:15:21

################################################################################
                     [1m Learning iteration 1109/2000 [0m                     

                       Computation: 97781 steps/s (collection: 0.893s, learning 0.112s)
             Mean action noise std: 4.58
          Mean value_function loss: 31.0273
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 22.9912
                       Mean reward: 869.33
               Mean episode length: 249.01
    Episode_Reward/reaching_object: 0.7701
     Episode_Reward/lifting_object: 173.2405
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 1.01s
                      Time elapsed: 00:19:06
                               ETA: 00:15:20

################################################################################
                     [1m Learning iteration 1110/2000 [0m                     

                       Computation: 104934 steps/s (collection: 0.828s, learning 0.109s)
             Mean action noise std: 4.58
          Mean value_function loss: 47.0224
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 22.9987
                       Mean reward: 862.48
               Mean episode length: 247.73
    Episode_Reward/reaching_object: 0.7640
     Episode_Reward/lifting_object: 172.2374
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 0.94s
                      Time elapsed: 00:19:07
                               ETA: 00:15:18

################################################################################
                     [1m Learning iteration 1111/2000 [0m                     

                       Computation: 105541 steps/s (collection: 0.829s, learning 0.103s)
             Mean action noise std: 4.59
          Mean value_function loss: 56.5147
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 23.0072
                       Mean reward: 817.37
               Mean episode length: 244.90
    Episode_Reward/reaching_object: 0.7561
     Episode_Reward/lifting_object: 169.5244
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 0.93s
                      Time elapsed: 00:19:08
                               ETA: 00:15:17

################################################################################
                     [1m Learning iteration 1112/2000 [0m                     

                       Computation: 109007 steps/s (collection: 0.805s, learning 0.097s)
             Mean action noise std: 4.60
          Mean value_function loss: 46.6363
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 23.0263
                       Mean reward: 873.95
               Mean episode length: 247.72
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 170.7444
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 0.90s
                      Time elapsed: 00:19:08
                               ETA: 00:15:16

################################################################################
                     [1m Learning iteration 1113/2000 [0m                     

                       Computation: 111478 steps/s (collection: 0.777s, learning 0.105s)
             Mean action noise std: 4.60
          Mean value_function loss: 44.8000
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 23.0376
                       Mean reward: 872.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7549
     Episode_Reward/lifting_object: 169.7117
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 0.88s
                      Time elapsed: 00:19:09
                               ETA: 00:15:15

################################################################################
                     [1m Learning iteration 1114/2000 [0m                     

                       Computation: 107161 steps/s (collection: 0.799s, learning 0.119s)
             Mean action noise std: 4.60
          Mean value_function loss: 46.5387
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 23.0424
                       Mean reward: 865.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 169.5429
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 0.92s
                      Time elapsed: 00:19:10
                               ETA: 00:15:14

################################################################################
                     [1m Learning iteration 1115/2000 [0m                     

                       Computation: 101533 steps/s (collection: 0.851s, learning 0.117s)
             Mean action noise std: 4.61
          Mean value_function loss: 58.6723
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 23.0514
                       Mean reward: 848.15
               Mean episode length: 244.21
    Episode_Reward/reaching_object: 0.7553
     Episode_Reward/lifting_object: 171.3424
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 0.97s
                      Time elapsed: 00:19:11
                               ETA: 00:15:13

################################################################################
                     [1m Learning iteration 1116/2000 [0m                     

                       Computation: 105462 steps/s (collection: 0.834s, learning 0.098s)
             Mean action noise std: 4.62
          Mean value_function loss: 49.5083
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 23.0662
                       Mean reward: 851.78
               Mean episode length: 247.00
    Episode_Reward/reaching_object: 0.7531
     Episode_Reward/lifting_object: 168.9454
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 0.93s
                      Time elapsed: 00:19:12
                               ETA: 00:15:12

################################################################################
                     [1m Learning iteration 1117/2000 [0m                     

                       Computation: 106756 steps/s (collection: 0.826s, learning 0.095s)
             Mean action noise std: 4.63
          Mean value_function loss: 41.3072
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 23.0828
                       Mean reward: 856.45
               Mean episode length: 245.87
    Episode_Reward/reaching_object: 0.7521
     Episode_Reward/lifting_object: 170.0017
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0549
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 0.92s
                      Time elapsed: 00:19:13
                               ETA: 00:15:11

################################################################################
                     [1m Learning iteration 1118/2000 [0m                     

                       Computation: 113004 steps/s (collection: 0.776s, learning 0.094s)
             Mean action noise std: 4.64
          Mean value_function loss: 52.7122
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 23.0976
                       Mean reward: 855.87
               Mean episode length: 246.17
    Episode_Reward/reaching_object: 0.7600
     Episode_Reward/lifting_object: 171.3115
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 0.87s
                      Time elapsed: 00:19:14
                               ETA: 00:15:09

################################################################################
                     [1m Learning iteration 1119/2000 [0m                     

                       Computation: 108509 steps/s (collection: 0.809s, learning 0.097s)
             Mean action noise std: 4.65
          Mean value_function loss: 40.6170
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 23.1171
                       Mean reward: 869.47
               Mean episode length: 247.46
    Episode_Reward/reaching_object: 0.7505
     Episode_Reward/lifting_object: 170.6889
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 0.91s
                      Time elapsed: 00:19:15
                               ETA: 00:15:08

################################################################################
                     [1m Learning iteration 1120/2000 [0m                     

                       Computation: 99291 steps/s (collection: 0.888s, learning 0.102s)
             Mean action noise std: 4.65
          Mean value_function loss: 38.6918
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 23.1260
                       Mean reward: 860.78
               Mean episode length: 247.80
    Episode_Reward/reaching_object: 0.7610
     Episode_Reward/lifting_object: 172.6809
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 0.99s
                      Time elapsed: 00:19:16
                               ETA: 00:15:07

################################################################################
                     [1m Learning iteration 1121/2000 [0m                     

                       Computation: 113592 steps/s (collection: 0.774s, learning 0.091s)
             Mean action noise std: 4.66
          Mean value_function loss: 39.2068
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 23.1398
                       Mean reward: 857.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7560
     Episode_Reward/lifting_object: 170.9398
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 0.87s
                      Time elapsed: 00:19:17
                               ETA: 00:15:06

################################################################################
                     [1m Learning iteration 1122/2000 [0m                     

                       Computation: 111375 steps/s (collection: 0.788s, learning 0.095s)
             Mean action noise std: 4.67
          Mean value_function loss: 39.4059
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 23.1533
                       Mean reward: 836.88
               Mean episode length: 247.02
    Episode_Reward/reaching_object: 0.7528
     Episode_Reward/lifting_object: 170.5582
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 0.88s
                      Time elapsed: 00:19:18
                               ETA: 00:15:05

################################################################################
                     [1m Learning iteration 1123/2000 [0m                     

                       Computation: 106351 steps/s (collection: 0.827s, learning 0.097s)
             Mean action noise std: 4.68
          Mean value_function loss: 36.4786
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 23.1658
                       Mean reward: 875.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7572
     Episode_Reward/lifting_object: 170.8129
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 0.92s
                      Time elapsed: 00:19:18
                               ETA: 00:15:04

################################################################################
                     [1m Learning iteration 1124/2000 [0m                     

                       Computation: 109892 steps/s (collection: 0.805s, learning 0.090s)
             Mean action noise std: 4.68
          Mean value_function loss: 42.3005
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 23.1792
                       Mean reward: 851.23
               Mean episode length: 247.16
    Episode_Reward/reaching_object: 0.7527
     Episode_Reward/lifting_object: 169.5124
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 0.89s
                      Time elapsed: 00:19:19
                               ETA: 00:15:03

################################################################################
                     [1m Learning iteration 1125/2000 [0m                     

                       Computation: 105146 steps/s (collection: 0.812s, learning 0.123s)
             Mean action noise std: 4.69
          Mean value_function loss: 30.7133
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 23.1870
                       Mean reward: 873.85
               Mean episode length: 249.31
    Episode_Reward/reaching_object: 0.7659
     Episode_Reward/lifting_object: 173.2230
      Episode_Reward/object_height: 0.0615
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 0.93s
                      Time elapsed: 00:19:20
                               ETA: 00:15:02

################################################################################
                     [1m Learning iteration 1126/2000 [0m                     

                       Computation: 106002 steps/s (collection: 0.824s, learning 0.104s)
             Mean action noise std: 4.70
          Mean value_function loss: 31.2984
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 23.2002
                       Mean reward: 870.05
               Mean episode length: 249.07
    Episode_Reward/reaching_object: 0.7615
     Episode_Reward/lifting_object: 172.0278
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 0.93s
                      Time elapsed: 00:19:21
                               ETA: 00:15:00

################################################################################
                     [1m Learning iteration 1127/2000 [0m                     

                       Computation: 108217 steps/s (collection: 0.817s, learning 0.092s)
             Mean action noise std: 4.71
          Mean value_function loss: 40.1791
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 23.2171
                       Mean reward: 848.45
               Mean episode length: 245.76
    Episode_Reward/reaching_object: 0.7675
     Episode_Reward/lifting_object: 172.3238
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 0.91s
                      Time elapsed: 00:19:22
                               ETA: 00:14:59

################################################################################
                     [1m Learning iteration 1128/2000 [0m                     

                       Computation: 96131 steps/s (collection: 0.894s, learning 0.129s)
             Mean action noise std: 4.71
          Mean value_function loss: 42.7078
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 23.2315
                       Mean reward: 860.48
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 171.4820
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 1.02s
                      Time elapsed: 00:19:23
                               ETA: 00:14:58

################################################################################
                     [1m Learning iteration 1129/2000 [0m                     

                       Computation: 88252 steps/s (collection: 0.981s, learning 0.133s)
             Mean action noise std: 4.72
          Mean value_function loss: 35.9389
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 23.2465
                       Mean reward: 867.07
               Mean episode length: 248.70
    Episode_Reward/reaching_object: 0.7561
     Episode_Reward/lifting_object: 171.3364
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 1.11s
                      Time elapsed: 00:19:24
                               ETA: 00:14:57

################################################################################
                     [1m Learning iteration 1130/2000 [0m                     

                       Computation: 101001 steps/s (collection: 0.864s, learning 0.109s)
             Mean action noise std: 4.73
          Mean value_function loss: 38.7881
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 23.2597
                       Mean reward: 861.30
               Mean episode length: 246.88
    Episode_Reward/reaching_object: 0.7683
     Episode_Reward/lifting_object: 172.8007
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 0.97s
                      Time elapsed: 00:19:25
                               ETA: 00:14:56

################################################################################
                     [1m Learning iteration 1131/2000 [0m                     

                       Computation: 109482 steps/s (collection: 0.780s, learning 0.118s)
             Mean action noise std: 4.73
          Mean value_function loss: 31.3979
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 23.2690
                       Mean reward: 851.49
               Mean episode length: 247.44
    Episode_Reward/reaching_object: 0.7554
     Episode_Reward/lifting_object: 170.4751
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 0.90s
                      Time elapsed: 00:19:26
                               ETA: 00:14:55

################################################################################
                     [1m Learning iteration 1132/2000 [0m                     

                       Computation: 110458 steps/s (collection: 0.787s, learning 0.103s)
             Mean action noise std: 4.74
          Mean value_function loss: 25.6445
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 23.2741
                       Mean reward: 873.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7675
     Episode_Reward/lifting_object: 172.9758
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 0.89s
                      Time elapsed: 00:19:27
                               ETA: 00:14:54

################################################################################
                     [1m Learning iteration 1133/2000 [0m                     

                       Computation: 113641 steps/s (collection: 0.766s, learning 0.099s)
             Mean action noise std: 4.75
          Mean value_function loss: 29.8586
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 23.2898
                       Mean reward: 872.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7576
     Episode_Reward/lifting_object: 170.6038
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 0.87s
                      Time elapsed: 00:19:28
                               ETA: 00:14:53

################################################################################
                     [1m Learning iteration 1134/2000 [0m                     

                       Computation: 102207 steps/s (collection: 0.856s, learning 0.106s)
             Mean action noise std: 4.76
          Mean value_function loss: 39.2197
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 23.3072
                       Mean reward: 866.38
               Mean episode length: 249.72
    Episode_Reward/reaching_object: 0.7686
     Episode_Reward/lifting_object: 172.8561
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 0.96s
                      Time elapsed: 00:19:29
                               ETA: 00:14:52

################################################################################
                     [1m Learning iteration 1135/2000 [0m                     

                       Computation: 107007 steps/s (collection: 0.819s, learning 0.099s)
             Mean action noise std: 4.76
          Mean value_function loss: 39.8903
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 23.3212
                       Mean reward: 881.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 173.1317
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0582
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 0.92s
                      Time elapsed: 00:19:30
                               ETA: 00:14:51

################################################################################
                     [1m Learning iteration 1136/2000 [0m                     

                       Computation: 109726 steps/s (collection: 0.803s, learning 0.093s)
             Mean action noise std: 4.77
          Mean value_function loss: 45.7576
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 23.3363
                       Mean reward: 857.43
               Mean episode length: 248.85
    Episode_Reward/reaching_object: 0.7674
     Episode_Reward/lifting_object: 172.3644
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 0.90s
                      Time elapsed: 00:19:31
                               ETA: 00:14:49

################################################################################
                     [1m Learning iteration 1137/2000 [0m                     

                       Computation: 112363 steps/s (collection: 0.779s, learning 0.096s)
             Mean action noise std: 4.78
          Mean value_function loss: 38.0228
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 23.3538
                       Mean reward: 864.13
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 170.4983
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 0.87s
                      Time elapsed: 00:19:32
                               ETA: 00:14:48

################################################################################
                     [1m Learning iteration 1138/2000 [0m                     

                       Computation: 113715 steps/s (collection: 0.775s, learning 0.090s)
             Mean action noise std: 4.78
          Mean value_function loss: 35.7414
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 23.3620
                       Mean reward: 854.00
               Mean episode length: 248.94
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: 173.3857
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 0.86s
                      Time elapsed: 00:19:32
                               ETA: 00:14:47

################################################################################
                     [1m Learning iteration 1139/2000 [0m                     

                       Computation: 112676 steps/s (collection: 0.767s, learning 0.106s)
             Mean action noise std: 4.79
          Mean value_function loss: 48.8069
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 23.3751
                       Mean reward: 865.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7627
     Episode_Reward/lifting_object: 172.1118
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 0.87s
                      Time elapsed: 00:19:33
                               ETA: 00:14:46

################################################################################
                     [1m Learning iteration 1140/2000 [0m                     

                       Computation: 97242 steps/s (collection: 0.852s, learning 0.159s)
             Mean action noise std: 4.80
          Mean value_function loss: 33.5455
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 23.3959
                       Mean reward: 852.82
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 0.7614
     Episode_Reward/lifting_object: 170.8659
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 1.01s
                      Time elapsed: 00:19:34
                               ETA: 00:14:45

################################################################################
                     [1m Learning iteration 1141/2000 [0m                     

                       Computation: 99173 steps/s (collection: 0.899s, learning 0.092s)
             Mean action noise std: 4.81
          Mean value_function loss: 54.4545
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 23.4163
                       Mean reward: 877.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 172.0661
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0591
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 0.99s
                      Time elapsed: 00:19:35
                               ETA: 00:14:44

################################################################################
                     [1m Learning iteration 1142/2000 [0m                     

                       Computation: 109153 steps/s (collection: 0.796s, learning 0.104s)
             Mean action noise std: 4.82
          Mean value_function loss: 43.3144
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 23.4264
                       Mean reward: 852.70
               Mean episode length: 248.39
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 171.8667
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 0.90s
                      Time elapsed: 00:19:36
                               ETA: 00:14:43

################################################################################
                     [1m Learning iteration 1143/2000 [0m                     

                       Computation: 102630 steps/s (collection: 0.765s, learning 0.193s)
             Mean action noise std: 4.82
          Mean value_function loss: 33.3966
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.4372
                       Mean reward: 872.41
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.7571
     Episode_Reward/lifting_object: 171.0307
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 0.96s
                      Time elapsed: 00:19:37
                               ETA: 00:14:42

################################################################################
                     [1m Learning iteration 1144/2000 [0m                     

                       Computation: 104536 steps/s (collection: 0.804s, learning 0.136s)
             Mean action noise std: 4.83
          Mean value_function loss: 37.0328
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 23.4496
                       Mean reward: 866.18
               Mean episode length: 248.32
    Episode_Reward/reaching_object: 0.7587
     Episode_Reward/lifting_object: 170.2446
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 0.94s
                      Time elapsed: 00:19:38
                               ETA: 00:14:41

################################################################################
                     [1m Learning iteration 1145/2000 [0m                     

                       Computation: 105100 steps/s (collection: 0.789s, learning 0.146s)
             Mean action noise std: 4.83
          Mean value_function loss: 41.0070
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 23.4577
                       Mean reward: 859.71
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 171.8082
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 0.94s
                      Time elapsed: 00:19:39
                               ETA: 00:14:40

################################################################################
                     [1m Learning iteration 1146/2000 [0m                     

                       Computation: 99810 steps/s (collection: 0.823s, learning 0.162s)
             Mean action noise std: 4.84
          Mean value_function loss: 47.5226
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 23.4672
                       Mean reward: 837.66
               Mean episode length: 245.55
    Episode_Reward/reaching_object: 0.7560
     Episode_Reward/lifting_object: 169.8322
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0603
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 0.98s
                      Time elapsed: 00:19:40
                               ETA: 00:14:38

################################################################################
                     [1m Learning iteration 1147/2000 [0m                     

                       Computation: 107161 steps/s (collection: 0.804s, learning 0.114s)
             Mean action noise std: 4.84
          Mean value_function loss: 50.9605
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 23.4688
                       Mean reward: 841.55
               Mean episode length: 249.23
    Episode_Reward/reaching_object: 0.7539
     Episode_Reward/lifting_object: 170.1204
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0603
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 0.92s
                      Time elapsed: 00:19:41
                               ETA: 00:14:37

################################################################################
                     [1m Learning iteration 1148/2000 [0m                     

                       Computation: 111100 steps/s (collection: 0.789s, learning 0.096s)
             Mean action noise std: 4.85
          Mean value_function loss: 48.8125
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 23.4724
                       Mean reward: 829.79
               Mean episode length: 247.23
    Episode_Reward/reaching_object: 0.7364
     Episode_Reward/lifting_object: 167.2696
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0603
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 0.88s
                      Time elapsed: 00:19:42
                               ETA: 00:14:36

################################################################################
                     [1m Learning iteration 1149/2000 [0m                     

                       Computation: 98268 steps/s (collection: 0.894s, learning 0.106s)
             Mean action noise std: 4.86
          Mean value_function loss: 37.4646
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 23.4877
                       Mean reward: 840.66
               Mean episode length: 247.66
    Episode_Reward/reaching_object: 0.7497
     Episode_Reward/lifting_object: 169.2253
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0604
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 1.00s
                      Time elapsed: 00:19:43
                               ETA: 00:14:35

################################################################################
                     [1m Learning iteration 1150/2000 [0m                     

                       Computation: 111285 steps/s (collection: 0.789s, learning 0.094s)
             Mean action noise std: 4.86
          Mean value_function loss: 41.5643
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 23.5033
                       Mean reward: 860.85
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.7549
     Episode_Reward/lifting_object: 170.9272
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 0.88s
                      Time elapsed: 00:19:44
                               ETA: 00:14:34

################################################################################
                     [1m Learning iteration 1151/2000 [0m                     

                       Computation: 101242 steps/s (collection: 0.875s, learning 0.096s)
             Mean action noise std: 4.88
          Mean value_function loss: 36.9356
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 23.5208
                       Mean reward: 860.38
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 171.4670
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 0.97s
                      Time elapsed: 00:19:45
                               ETA: 00:14:33

################################################################################
                     [1m Learning iteration 1152/2000 [0m                     

                       Computation: 112404 steps/s (collection: 0.774s, learning 0.101s)
             Mean action noise std: 4.88
          Mean value_function loss: 48.8940
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 23.5340
                       Mean reward: 862.29
               Mean episode length: 248.78
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 172.0117
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 0.87s
                      Time elapsed: 00:19:46
                               ETA: 00:14:32

################################################################################
                     [1m Learning iteration 1153/2000 [0m                     

                       Computation: 108410 steps/s (collection: 0.816s, learning 0.091s)
             Mean action noise std: 4.88
          Mean value_function loss: 38.2728
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.5372
                       Mean reward: 856.27
               Mean episode length: 246.75
    Episode_Reward/reaching_object: 0.7590
     Episode_Reward/lifting_object: 170.6718
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0615
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 0.91s
                      Time elapsed: 00:19:46
                               ETA: 00:14:31

################################################################################
                     [1m Learning iteration 1154/2000 [0m                     

                       Computation: 91769 steps/s (collection: 0.902s, learning 0.169s)
             Mean action noise std: 4.89
          Mean value_function loss: 34.7247
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 23.5456
                       Mean reward: 857.05
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 170.6773
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 1.07s
                      Time elapsed: 00:19:48
                               ETA: 00:14:30

################################################################################
                     [1m Learning iteration 1155/2000 [0m                     

                       Computation: 108011 steps/s (collection: 0.815s, learning 0.096s)
             Mean action noise std: 4.90
          Mean value_function loss: 28.9875
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 23.5587
                       Mean reward: 869.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7543
     Episode_Reward/lifting_object: 170.7415
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 0.91s
                      Time elapsed: 00:19:48
                               ETA: 00:14:29

################################################################################
                     [1m Learning iteration 1156/2000 [0m                     

                       Computation: 102159 steps/s (collection: 0.836s, learning 0.126s)
             Mean action noise std: 4.91
          Mean value_function loss: 35.3603
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 23.5740
                       Mean reward: 852.72
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 172.3304
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 0.96s
                      Time elapsed: 00:19:49
                               ETA: 00:14:28

################################################################################
                     [1m Learning iteration 1157/2000 [0m                     

                       Computation: 107470 steps/s (collection: 0.783s, learning 0.132s)
             Mean action noise std: 4.92
          Mean value_function loss: 30.1356
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 23.5914
                       Mean reward: 856.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7646
     Episode_Reward/lifting_object: 171.6282
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 0.91s
                      Time elapsed: 00:19:50
                               ETA: 00:14:26

################################################################################
                     [1m Learning iteration 1158/2000 [0m                     

                       Computation: 101658 steps/s (collection: 0.806s, learning 0.161s)
             Mean action noise std: 4.92
          Mean value_function loss: 32.9857
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 23.6064
                       Mean reward: 878.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7675
     Episode_Reward/lifting_object: 173.3781
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0629
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 0.97s
                      Time elapsed: 00:19:51
                               ETA: 00:14:25

################################################################################
                     [1m Learning iteration 1159/2000 [0m                     

                       Computation: 106832 steps/s (collection: 0.807s, learning 0.113s)
             Mean action noise std: 4.93
          Mean value_function loss: 31.7851
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 23.6155
                       Mean reward: 850.37
               Mean episode length: 247.61
    Episode_Reward/reaching_object: 0.7566
     Episode_Reward/lifting_object: 170.2832
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 0.92s
                      Time elapsed: 00:19:52
                               ETA: 00:14:24

################################################################################
                     [1m Learning iteration 1160/2000 [0m                     

                       Computation: 107133 steps/s (collection: 0.796s, learning 0.122s)
             Mean action noise std: 4.94
          Mean value_function loss: 30.3127
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 23.6283
                       Mean reward: 854.35
               Mean episode length: 248.68
    Episode_Reward/reaching_object: 0.7600
     Episode_Reward/lifting_object: 172.2281
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 0.92s
                      Time elapsed: 00:19:53
                               ETA: 00:14:23

################################################################################
                     [1m Learning iteration 1161/2000 [0m                     

                       Computation: 108815 steps/s (collection: 0.783s, learning 0.121s)
             Mean action noise std: 4.95
          Mean value_function loss: 41.4361
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 23.6386
                       Mean reward: 866.94
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 172.5270
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 0.90s
                      Time elapsed: 00:19:54
                               ETA: 00:14:22

################################################################################
                     [1m Learning iteration 1162/2000 [0m                     

                       Computation: 109157 steps/s (collection: 0.801s, learning 0.100s)
             Mean action noise std: 4.95
          Mean value_function loss: 33.7224
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 23.6493
                       Mean reward: 865.41
               Mean episode length: 248.28
    Episode_Reward/reaching_object: 0.7641
     Episode_Reward/lifting_object: 171.6196
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.0625
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 0.90s
                      Time elapsed: 00:19:55
                               ETA: 00:14:21

################################################################################
                     [1m Learning iteration 1163/2000 [0m                     

                       Computation: 100726 steps/s (collection: 0.854s, learning 0.122s)
             Mean action noise std: 4.96
          Mean value_function loss: 37.5135
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 23.6616
                       Mean reward: 872.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 171.5673
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 0.98s
                      Time elapsed: 00:19:56
                               ETA: 00:14:20

################################################################################
                     [1m Learning iteration 1164/2000 [0m                     

                       Computation: 108041 steps/s (collection: 0.792s, learning 0.118s)
             Mean action noise std: 4.97
          Mean value_function loss: 36.1993
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 23.6788
                       Mean reward: 862.16
               Mean episode length: 248.69
    Episode_Reward/reaching_object: 0.7671
     Episode_Reward/lifting_object: 172.0551
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 0.91s
                      Time elapsed: 00:19:57
                               ETA: 00:14:19

################################################################################
                     [1m Learning iteration 1165/2000 [0m                     

                       Computation: 106212 steps/s (collection: 0.814s, learning 0.112s)
             Mean action noise std: 4.98
          Mean value_function loss: 42.7595
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 23.6918
                       Mean reward: 867.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 171.9011
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 0.93s
                      Time elapsed: 00:19:58
                               ETA: 00:14:18

################################################################################
                     [1m Learning iteration 1166/2000 [0m                     

                       Computation: 111566 steps/s (collection: 0.766s, learning 0.115s)
             Mean action noise std: 4.98
          Mean value_function loss: 31.2893
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 23.6978
                       Mean reward: 872.25
               Mean episode length: 249.78
    Episode_Reward/reaching_object: 0.7583
     Episode_Reward/lifting_object: 172.6198
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0626
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 0.88s
                      Time elapsed: 00:19:59
                               ETA: 00:14:16

################################################################################
                     [1m Learning iteration 1167/2000 [0m                     

                       Computation: 108488 steps/s (collection: 0.818s, learning 0.088s)
             Mean action noise std: 4.99
          Mean value_function loss: 38.5669
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 23.7027
                       Mean reward: 875.37
               Mean episode length: 249.30
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 172.8663
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.0625
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 0.91s
                      Time elapsed: 00:20:00
                               ETA: 00:14:15

################################################################################
                     [1m Learning iteration 1168/2000 [0m                     

                       Computation: 108191 steps/s (collection: 0.789s, learning 0.120s)
             Mean action noise std: 4.99
          Mean value_function loss: 43.9495
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 23.7099
                       Mean reward: 864.29
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7550
     Episode_Reward/lifting_object: 171.3708
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 0.91s
                      Time elapsed: 00:20:00
                               ETA: 00:14:14

################################################################################
                     [1m Learning iteration 1169/2000 [0m                     

                       Computation: 106423 steps/s (collection: 0.834s, learning 0.090s)
             Mean action noise std: 5.00
          Mean value_function loss: 44.1107
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 23.7192
                       Mean reward: 843.23
               Mean episode length: 246.59
    Episode_Reward/reaching_object: 0.7490
     Episode_Reward/lifting_object: 169.7935
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 0.92s
                      Time elapsed: 00:20:01
                               ETA: 00:14:13

################################################################################
                     [1m Learning iteration 1170/2000 [0m                     

                       Computation: 90753 steps/s (collection: 0.930s, learning 0.153s)
             Mean action noise std: 5.00
          Mean value_function loss: 43.8040
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 23.7273
                       Mean reward: 850.57
               Mean episode length: 246.58
    Episode_Reward/reaching_object: 0.7508
     Episode_Reward/lifting_object: 169.4984
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 1.08s
                      Time elapsed: 00:20:02
                               ETA: 00:14:12

################################################################################
                     [1m Learning iteration 1171/2000 [0m                     

                       Computation: 98315 steps/s (collection: 0.889s, learning 0.111s)
             Mean action noise std: 5.01
          Mean value_function loss: 35.9911
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 23.7341
                       Mean reward: 839.15
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7477
     Episode_Reward/lifting_object: 168.8106
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0629
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 1.00s
                      Time elapsed: 00:20:03
                               ETA: 00:14:11

################################################################################
                     [1m Learning iteration 1172/2000 [0m                     

                       Computation: 102507 steps/s (collection: 0.852s, learning 0.107s)
             Mean action noise std: 5.01
          Mean value_function loss: 33.8282
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 23.7381
                       Mean reward: 856.57
               Mean episode length: 248.73
    Episode_Reward/reaching_object: 0.7495
     Episode_Reward/lifting_object: 169.8450
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 0.96s
                      Time elapsed: 00:20:04
                               ETA: 00:14:10

################################################################################
                     [1m Learning iteration 1173/2000 [0m                     

                       Computation: 105067 steps/s (collection: 0.840s, learning 0.096s)
             Mean action noise std: 5.01
          Mean value_function loss: 51.0348
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 23.7431
                       Mean reward: 868.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7669
     Episode_Reward/lifting_object: 173.5872
      Episode_Reward/object_height: 0.0615
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 0.94s
                      Time elapsed: 00:20:05
                               ETA: 00:14:09

################################################################################
                     [1m Learning iteration 1174/2000 [0m                     

                       Computation: 109663 steps/s (collection: 0.796s, learning 0.101s)
             Mean action noise std: 5.02
          Mean value_function loss: 38.1161
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 23.7533
                       Mean reward: 848.65
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.7521
     Episode_Reward/lifting_object: 169.6361
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 0.90s
                      Time elapsed: 00:20:06
                               ETA: 00:14:08

################################################################################
                     [1m Learning iteration 1175/2000 [0m                     

                       Computation: 101204 steps/s (collection: 0.856s, learning 0.115s)
             Mean action noise std: 5.02
          Mean value_function loss: 32.5488
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 23.7634
                       Mean reward: 859.47
               Mean episode length: 246.46
    Episode_Reward/reaching_object: 0.7549
     Episode_Reward/lifting_object: 170.3716
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 0.97s
                      Time elapsed: 00:20:07
                               ETA: 00:14:07

################################################################################
                     [1m Learning iteration 1176/2000 [0m                     

                       Computation: 99640 steps/s (collection: 0.814s, learning 0.172s)
             Mean action noise std: 5.03
          Mean value_function loss: 31.7928
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 23.7731
                       Mean reward: 867.85
               Mean episode length: 247.51
    Episode_Reward/reaching_object: 0.7652
     Episode_Reward/lifting_object: 172.2604
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.0643
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 0.99s
                      Time elapsed: 00:20:08
                               ETA: 00:14:06

################################################################################
                     [1m Learning iteration 1177/2000 [0m                     

                       Computation: 108113 steps/s (collection: 0.809s, learning 0.101s)
             Mean action noise std: 5.03
          Mean value_function loss: 41.2426
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 23.7830
                       Mean reward: 864.34
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.7614
     Episode_Reward/lifting_object: 171.6439
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 0.91s
                      Time elapsed: 00:20:09
                               ETA: 00:14:05

################################################################################
                     [1m Learning iteration 1178/2000 [0m                     

                       Computation: 94685 steps/s (collection: 0.881s, learning 0.158s)
             Mean action noise std: 5.04
          Mean value_function loss: 34.4116
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 23.7939
                       Mean reward: 865.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 171.2128
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 1.04s
                      Time elapsed: 00:20:10
                               ETA: 00:14:04

################################################################################
                     [1m Learning iteration 1179/2000 [0m                     

                       Computation: 101628 steps/s (collection: 0.843s, learning 0.124s)
             Mean action noise std: 5.05
          Mean value_function loss: 39.6112
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 23.8083
                       Mean reward: 831.42
               Mean episode length: 246.72
    Episode_Reward/reaching_object: 0.7537
     Episode_Reward/lifting_object: 170.5185
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.0654
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 0.97s
                      Time elapsed: 00:20:11
                               ETA: 00:14:02

################################################################################
                     [1m Learning iteration 1180/2000 [0m                     

                       Computation: 108386 steps/s (collection: 0.794s, learning 0.113s)
             Mean action noise std: 5.05
          Mean value_function loss: 37.7030
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 23.8156
                       Mean reward: 850.85
               Mean episode length: 247.25
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 171.6252
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 0.91s
                      Time elapsed: 00:20:12
                               ETA: 00:14:01

################################################################################
                     [1m Learning iteration 1181/2000 [0m                     

                       Computation: 109978 steps/s (collection: 0.802s, learning 0.092s)
             Mean action noise std: 5.06
          Mean value_function loss: 36.5520
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 23.8273
                       Mean reward: 857.88
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.7550
     Episode_Reward/lifting_object: 170.2871
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 0.89s
                      Time elapsed: 00:20:13
                               ETA: 00:14:00

################################################################################
                     [1m Learning iteration 1182/2000 [0m                     

                       Computation: 108531 steps/s (collection: 0.805s, learning 0.101s)
             Mean action noise std: 5.07
          Mean value_function loss: 30.9785
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 23.8392
                       Mean reward: 866.08
               Mean episode length: 249.65
    Episode_Reward/reaching_object: 0.7566
     Episode_Reward/lifting_object: 169.7066
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.0659
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 0.91s
                      Time elapsed: 00:20:14
                               ETA: 00:13:59

################################################################################
                     [1m Learning iteration 1183/2000 [0m                     

                       Computation: 111175 steps/s (collection: 0.794s, learning 0.090s)
             Mean action noise std: 5.08
          Mean value_function loss: 42.5207
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 23.8507
                       Mean reward: 878.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7630
     Episode_Reward/lifting_object: 172.3049
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.0664
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 0.88s
                      Time elapsed: 00:20:15
                               ETA: 00:13:58

################################################################################
                     [1m Learning iteration 1184/2000 [0m                     

                       Computation: 105990 steps/s (collection: 0.824s, learning 0.103s)
             Mean action noise std: 5.09
          Mean value_function loss: 44.8157
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 23.8672
                       Mean reward: 857.27
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 0.7663
     Episode_Reward/lifting_object: 172.5981
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0667
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 0.93s
                      Time elapsed: 00:20:16
                               ETA: 00:13:57

################################################################################
                     [1m Learning iteration 1185/2000 [0m                     

                       Computation: 107171 steps/s (collection: 0.809s, learning 0.109s)
             Mean action noise std: 5.10
          Mean value_function loss: 43.3348
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 23.8883
                       Mean reward: 877.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 172.0409
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 0.92s
                      Time elapsed: 00:20:17
                               ETA: 00:13:56

################################################################################
                     [1m Learning iteration 1186/2000 [0m                     

                       Computation: 105417 steps/s (collection: 0.821s, learning 0.111s)
             Mean action noise std: 5.11
          Mean value_function loss: 33.9334
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 23.9059
                       Mean reward: 837.96
               Mean episode length: 244.67
    Episode_Reward/reaching_object: 0.7465
     Episode_Reward/lifting_object: 168.6029
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.0666
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 0.93s
                      Time elapsed: 00:20:17
                               ETA: 00:13:55

################################################################################
                     [1m Learning iteration 1187/2000 [0m                     

                       Computation: 107078 steps/s (collection: 0.802s, learning 0.116s)
             Mean action noise std: 5.12
          Mean value_function loss: 33.6650
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 23.9149
                       Mean reward: 872.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 171.8599
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0675
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 0.92s
                      Time elapsed: 00:20:18
                               ETA: 00:13:54

################################################################################
                     [1m Learning iteration 1188/2000 [0m                     

                       Computation: 96346 steps/s (collection: 0.832s, learning 0.188s)
             Mean action noise std: 5.13
          Mean value_function loss: 33.1939
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 23.9287
                       Mean reward: 867.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7646
     Episode_Reward/lifting_object: 172.0421
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 1.02s
                      Time elapsed: 00:20:19
                               ETA: 00:13:53

################################################################################
                     [1m Learning iteration 1189/2000 [0m                     

                       Computation: 88273 steps/s (collection: 0.974s, learning 0.139s)
             Mean action noise std: 5.14
          Mean value_function loss: 48.0172
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 23.9444
                       Mean reward: 857.26
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 172.0658
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 1.11s
                      Time elapsed: 00:20:21
                               ETA: 00:13:52

################################################################################
                     [1m Learning iteration 1190/2000 [0m                     

                       Computation: 96161 steps/s (collection: 0.851s, learning 0.172s)
             Mean action noise std: 5.14
          Mean value_function loss: 42.0254
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 23.9559
                       Mean reward: 855.38
               Mean episode length: 246.80
    Episode_Reward/reaching_object: 0.7559
     Episode_Reward/lifting_object: 170.6758
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.0679
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 1.02s
                      Time elapsed: 00:20:22
                               ETA: 00:13:51

################################################################################
                     [1m Learning iteration 1191/2000 [0m                     

                       Computation: 103596 steps/s (collection: 0.859s, learning 0.090s)
             Mean action noise std: 5.15
          Mean value_function loss: 46.0439
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 23.9664
                       Mean reward: 845.44
               Mean episode length: 247.31
    Episode_Reward/reaching_object: 0.7564
     Episode_Reward/lifting_object: 170.0104
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 0.95s
                      Time elapsed: 00:20:23
                               ETA: 00:13:50

################################################################################
                     [1m Learning iteration 1192/2000 [0m                     

                       Computation: 108358 steps/s (collection: 0.815s, learning 0.092s)
             Mean action noise std: 5.16
          Mean value_function loss: 55.4049
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 23.9780
                       Mean reward: 872.05
               Mean episode length: 249.42
    Episode_Reward/reaching_object: 0.7646
     Episode_Reward/lifting_object: 170.7554
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 0.91s
                      Time elapsed: 00:20:23
                               ETA: 00:13:48

################################################################################
                     [1m Learning iteration 1193/2000 [0m                     

                       Computation: 92452 steps/s (collection: 0.908s, learning 0.155s)
             Mean action noise std: 5.17
          Mean value_function loss: 57.1843
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 23.9937
                       Mean reward: 853.49
               Mean episode length: 246.61
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 171.2530
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.0687
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 1.06s
                      Time elapsed: 00:20:24
                               ETA: 00:13:47

################################################################################
                     [1m Learning iteration 1194/2000 [0m                     

                       Computation: 86678 steps/s (collection: 0.964s, learning 0.170s)
             Mean action noise std: 5.18
          Mean value_function loss: 37.2691
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 24.0039
                       Mean reward: 839.44
               Mean episode length: 244.19
    Episode_Reward/reaching_object: 0.7556
     Episode_Reward/lifting_object: 169.5119
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.0677
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 1.13s
                      Time elapsed: 00:20:26
                               ETA: 00:13:46

################################################################################
                     [1m Learning iteration 1195/2000 [0m                     

                       Computation: 94764 steps/s (collection: 0.909s, learning 0.128s)
             Mean action noise std: 5.18
          Mean value_function loss: 37.6262
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 24.0129
                       Mean reward: 866.59
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 170.6391
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0687
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 1.04s
                      Time elapsed: 00:20:27
                               ETA: 00:13:45

################################################################################
                     [1m Learning iteration 1196/2000 [0m                     

                       Computation: 88242 steps/s (collection: 0.977s, learning 0.137s)
             Mean action noise std: 5.18
          Mean value_function loss: 39.2257
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 24.0181
                       Mean reward: 875.30
               Mean episode length: 249.32
    Episode_Reward/reaching_object: 0.7684
     Episode_Reward/lifting_object: 172.5796
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0694
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 1.11s
                      Time elapsed: 00:20:28
                               ETA: 00:13:44

################################################################################
                     [1m Learning iteration 1197/2000 [0m                     

                       Computation: 85872 steps/s (collection: 1.030s, learning 0.115s)
             Mean action noise std: 5.19
          Mean value_function loss: 61.8749
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 24.0281
                       Mean reward: 856.65
               Mean episode length: 248.73
    Episode_Reward/reaching_object: 0.7541
     Episode_Reward/lifting_object: 170.0044
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.0692
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 1.14s
                      Time elapsed: 00:20:29
                               ETA: 00:13:44

################################################################################
                     [1m Learning iteration 1198/2000 [0m                     

                       Computation: 99968 steps/s (collection: 0.880s, learning 0.103s)
             Mean action noise std: 5.20
          Mean value_function loss: 52.6239
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 24.0364
                       Mean reward: 844.94
               Mean episode length: 247.74
    Episode_Reward/reaching_object: 0.7516
     Episode_Reward/lifting_object: 169.9799
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0694
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 0.98s
                      Time elapsed: 00:20:30
                               ETA: 00:13:42

################################################################################
                     [1m Learning iteration 1199/2000 [0m                     

                       Computation: 110150 steps/s (collection: 0.805s, learning 0.088s)
             Mean action noise std: 5.20
          Mean value_function loss: 46.4651
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 24.0450
                       Mean reward: 847.72
               Mean episode length: 247.89
    Episode_Reward/reaching_object: 0.7575
     Episode_Reward/lifting_object: 170.0984
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.0694
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 0.89s
                      Time elapsed: 00:20:31
                               ETA: 00:13:41

################################################################################
                     [1m Learning iteration 1200/2000 [0m                     

                       Computation: 112705 steps/s (collection: 0.784s, learning 0.088s)
             Mean action noise std: 5.21
          Mean value_function loss: 43.6659
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 24.0589
                       Mean reward: 867.20
               Mean episode length: 248.47
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 171.5281
      Episode_Reward/object_height: 0.0623
        Episode_Reward/action_rate: -0.0699
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 0.87s
                      Time elapsed: 00:20:32
                               ETA: 00:13:40

################################################################################
                     [1m Learning iteration 1201/2000 [0m                     

                       Computation: 110903 steps/s (collection: 0.800s, learning 0.087s)
             Mean action noise std: 5.22
          Mean value_function loss: 55.5547
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 24.0735
                       Mean reward: 855.53
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7628
     Episode_Reward/lifting_object: 170.7285
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0701
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 0.89s
                      Time elapsed: 00:20:33
                               ETA: 00:13:39

################################################################################
                     [1m Learning iteration 1202/2000 [0m                     

                       Computation: 108176 steps/s (collection: 0.817s, learning 0.092s)
             Mean action noise std: 5.23
          Mean value_function loss: 47.6521
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 24.0896
                       Mean reward: 836.89
               Mean episode length: 246.84
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 169.5098
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0697
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 0.91s
                      Time elapsed: 00:20:33
                               ETA: 00:13:38

################################################################################
                     [1m Learning iteration 1203/2000 [0m                     

                       Computation: 107698 steps/s (collection: 0.814s, learning 0.099s)
             Mean action noise std: 5.24
          Mean value_function loss: 45.9234
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 24.0987
                       Mean reward: 855.14
               Mean episode length: 246.75
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 169.4996
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.0702
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 0.91s
                      Time elapsed: 00:20:34
                               ETA: 00:13:37

################################################################################
                     [1m Learning iteration 1204/2000 [0m                     

                       Computation: 108037 steps/s (collection: 0.820s, learning 0.089s)
             Mean action noise std: 5.24
          Mean value_function loss: 47.4233
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 24.1045
                       Mean reward: 838.80
               Mean episode length: 248.45
    Episode_Reward/reaching_object: 0.7572
     Episode_Reward/lifting_object: 169.6634
      Episode_Reward/object_height: 0.0623
        Episode_Reward/action_rate: -0.0703
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 0.91s
                      Time elapsed: 00:20:35
                               ETA: 00:13:36

################################################################################
                     [1m Learning iteration 1205/2000 [0m                     

                       Computation: 110215 steps/s (collection: 0.788s, learning 0.104s)
             Mean action noise std: 5.25
          Mean value_function loss: 45.8987
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 24.1173
                       Mean reward: 838.67
               Mean episode length: 246.51
    Episode_Reward/reaching_object: 0.7472
     Episode_Reward/lifting_object: 168.9138
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.0705
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 0.89s
                      Time elapsed: 00:20:36
                               ETA: 00:13:35

################################################################################
                     [1m Learning iteration 1206/2000 [0m                     

                       Computation: 111183 steps/s (collection: 0.790s, learning 0.094s)
             Mean action noise std: 5.26
          Mean value_function loss: 36.4970
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 24.1296
                       Mean reward: 855.73
               Mean episode length: 249.95
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 171.3454
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0711
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 0.88s
                      Time elapsed: 00:20:37
                               ETA: 00:13:34

################################################################################
                     [1m Learning iteration 1207/2000 [0m                     

                       Computation: 114282 steps/s (collection: 0.763s, learning 0.098s)
             Mean action noise std: 5.26
          Mean value_function loss: 40.9237
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 24.1361
                       Mean reward: 865.56
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7491
     Episode_Reward/lifting_object: 167.9590
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.0708
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 0.86s
                      Time elapsed: 00:20:38
                               ETA: 00:13:32

################################################################################
                     [1m Learning iteration 1208/2000 [0m                     

                       Computation: 108161 steps/s (collection: 0.814s, learning 0.095s)
             Mean action noise std: 5.27
          Mean value_function loss: 39.3391
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 24.1463
                       Mean reward: 842.01
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7663
     Episode_Reward/lifting_object: 171.4355
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0721
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 0.91s
                      Time elapsed: 00:20:39
                               ETA: 00:13:31

################################################################################
                     [1m Learning iteration 1209/2000 [0m                     

                       Computation: 109380 steps/s (collection: 0.800s, learning 0.099s)
             Mean action noise std: 5.27
          Mean value_function loss: 43.5394
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 24.1533
                       Mean reward: 849.34
               Mean episode length: 247.86
    Episode_Reward/reaching_object: 0.7548
     Episode_Reward/lifting_object: 169.0677
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.0714
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 0.90s
                      Time elapsed: 00:20:40
                               ETA: 00:13:30

################################################################################
                     [1m Learning iteration 1210/2000 [0m                     

                       Computation: 104523 steps/s (collection: 0.828s, learning 0.112s)
             Mean action noise std: 5.28
          Mean value_function loss: 39.7029
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 24.1639
                       Mean reward: 840.44
               Mean episode length: 245.60
    Episode_Reward/reaching_object: 0.7530
     Episode_Reward/lifting_object: 169.3806
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0720
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 0.94s
                      Time elapsed: 00:20:41
                               ETA: 00:13:29

################################################################################
                     [1m Learning iteration 1211/2000 [0m                     

                       Computation: 110346 steps/s (collection: 0.800s, learning 0.091s)
             Mean action noise std: 5.29
          Mean value_function loss: 38.9054
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 24.1757
                       Mean reward: 874.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7577
     Episode_Reward/lifting_object: 170.5890
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0728
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 0.89s
                      Time elapsed: 00:20:42
                               ETA: 00:13:28

################################################################################
                     [1m Learning iteration 1212/2000 [0m                     

                       Computation: 108053 steps/s (collection: 0.812s, learning 0.098s)
             Mean action noise std: 5.30
          Mean value_function loss: 36.7841
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 24.1894
                       Mean reward: 864.34
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 171.5719
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0728
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 0.91s
                      Time elapsed: 00:20:42
                               ETA: 00:13:27

################################################################################
                     [1m Learning iteration 1213/2000 [0m                     

                       Computation: 97975 steps/s (collection: 0.903s, learning 0.101s)
             Mean action noise std: 5.31
          Mean value_function loss: 50.6389
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 24.2011
                       Mean reward: 873.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7585
     Episode_Reward/lifting_object: 170.6166
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0733
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 1.00s
                      Time elapsed: 00:20:43
                               ETA: 00:13:26

################################################################################
                     [1m Learning iteration 1214/2000 [0m                     

                       Computation: 104157 steps/s (collection: 0.835s, learning 0.109s)
             Mean action noise std: 5.31
          Mean value_function loss: 52.4197
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 24.2148
                       Mean reward: 844.52
               Mean episode length: 247.04
    Episode_Reward/reaching_object: 0.7560
     Episode_Reward/lifting_object: 169.8157
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.0738
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 0.94s
                      Time elapsed: 00:20:44
                               ETA: 00:13:25

################################################################################
                     [1m Learning iteration 1215/2000 [0m                     

                       Computation: 102637 steps/s (collection: 0.843s, learning 0.115s)
             Mean action noise std: 5.33
          Mean value_function loss: 37.9126
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 24.2272
                       Mean reward: 854.03
               Mean episode length: 247.67
    Episode_Reward/reaching_object: 0.7701
     Episode_Reward/lifting_object: 172.0328
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 0.96s
                      Time elapsed: 00:20:45
                               ETA: 00:13:24

################################################################################
                     [1m Learning iteration 1216/2000 [0m                     

                       Computation: 108452 steps/s (collection: 0.801s, learning 0.105s)
             Mean action noise std: 5.34
          Mean value_function loss: 33.8744
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 24.2498
                       Mean reward: 842.72
               Mean episode length: 248.45
    Episode_Reward/reaching_object: 0.7583
     Episode_Reward/lifting_object: 169.3865
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0741
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 0.91s
                      Time elapsed: 00:20:46
                               ETA: 00:13:23

################################################################################
                     [1m Learning iteration 1217/2000 [0m                     

                       Computation: 107464 steps/s (collection: 0.818s, learning 0.097s)
             Mean action noise std: 5.34
          Mean value_function loss: 36.4956
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 24.2577
                       Mean reward: 855.62
               Mean episode length: 248.65
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 169.4538
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0744
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 0.91s
                      Time elapsed: 00:20:47
                               ETA: 00:13:22

################################################################################
                     [1m Learning iteration 1218/2000 [0m                     

                       Computation: 109991 steps/s (collection: 0.798s, learning 0.096s)
             Mean action noise std: 5.34
          Mean value_function loss: 50.9915
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 24.2604
                       Mean reward: 859.67
               Mean episode length: 249.05
    Episode_Reward/reaching_object: 0.7634
     Episode_Reward/lifting_object: 172.0238
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0749
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 0.89s
                      Time elapsed: 00:20:48
                               ETA: 00:13:20

################################################################################
                     [1m Learning iteration 1219/2000 [0m                     

                       Computation: 104371 steps/s (collection: 0.843s, learning 0.099s)
             Mean action noise std: 5.35
          Mean value_function loss: 41.3296
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 24.2708
                       Mean reward: 844.09
               Mean episode length: 245.79
    Episode_Reward/reaching_object: 0.7536
     Episode_Reward/lifting_object: 169.1925
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0746
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 0.94s
                      Time elapsed: 00:20:49
                               ETA: 00:13:19

################################################################################
                     [1m Learning iteration 1220/2000 [0m                     

                       Computation: 106650 steps/s (collection: 0.830s, learning 0.092s)
             Mean action noise std: 5.35
          Mean value_function loss: 42.7924
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 24.2808
                       Mean reward: 853.62
               Mean episode length: 249.01
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 171.4206
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0753
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 0.92s
                      Time elapsed: 00:20:50
                               ETA: 00:13:18

################################################################################
                     [1m Learning iteration 1221/2000 [0m                     

                       Computation: 111850 steps/s (collection: 0.785s, learning 0.094s)
             Mean action noise std: 5.36
          Mean value_function loss: 47.7035
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 24.2843
                       Mean reward: 852.12
               Mean episode length: 249.21
    Episode_Reward/reaching_object: 0.7698
     Episode_Reward/lifting_object: 171.8988
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.0750
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 0.88s
                      Time elapsed: 00:20:51
                               ETA: 00:13:17

################################################################################
                     [1m Learning iteration 1222/2000 [0m                     

                       Computation: 108043 steps/s (collection: 0.810s, learning 0.100s)
             Mean action noise std: 5.37
          Mean value_function loss: 44.9796
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 24.2924
                       Mean reward: 865.05
               Mean episode length: 249.86
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 170.4997
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0753
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 0.91s
                      Time elapsed: 00:20:52
                               ETA: 00:13:16

################################################################################
                     [1m Learning iteration 1223/2000 [0m                     

                       Computation: 106693 steps/s (collection: 0.817s, learning 0.105s)
             Mean action noise std: 5.37
          Mean value_function loss: 53.7638
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 24.3091
                       Mean reward: 850.07
               Mean episode length: 246.74
    Episode_Reward/reaching_object: 0.7556
     Episode_Reward/lifting_object: 169.3955
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.0755
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 0.92s
                      Time elapsed: 00:20:53
                               ETA: 00:13:15

################################################################################
                     [1m Learning iteration 1224/2000 [0m                     

                       Computation: 107565 steps/s (collection: 0.813s, learning 0.101s)
             Mean action noise std: 5.39
          Mean value_function loss: 43.2709
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 24.3227
                       Mean reward: 851.70
               Mean episode length: 249.21
    Episode_Reward/reaching_object: 0.7553
     Episode_Reward/lifting_object: 169.5817
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0752
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 0.91s
                      Time elapsed: 00:20:54
                               ETA: 00:13:14

################################################################################
                     [1m Learning iteration 1225/2000 [0m                     

                       Computation: 107629 steps/s (collection: 0.821s, learning 0.093s)
             Mean action noise std: 5.39
          Mean value_function loss: 42.1208
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 24.3396
                       Mean reward: 861.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7621
     Episode_Reward/lifting_object: 172.3820
      Episode_Reward/object_height: 0.0647
        Episode_Reward/action_rate: -0.0759
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 0.91s
                      Time elapsed: 00:20:54
                               ETA: 00:13:13

################################################################################
                     [1m Learning iteration 1226/2000 [0m                     

                       Computation: 107306 steps/s (collection: 0.814s, learning 0.102s)
             Mean action noise std: 5.40
          Mean value_function loss: 44.6823
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 24.3452
                       Mean reward: 839.55
               Mean episode length: 247.69
    Episode_Reward/reaching_object: 0.7547
     Episode_Reward/lifting_object: 169.5865
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.0763
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 0.92s
                      Time elapsed: 00:20:55
                               ETA: 00:13:12

################################################################################
                     [1m Learning iteration 1227/2000 [0m                     

                       Computation: 101413 steps/s (collection: 0.874s, learning 0.095s)
             Mean action noise std: 5.41
          Mean value_function loss: 35.9550
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 24.3522
                       Mean reward: 835.94
               Mean episode length: 245.55
    Episode_Reward/reaching_object: 0.7504
     Episode_Reward/lifting_object: 168.8454
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.0758
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 0.97s
                      Time elapsed: 00:20:56
                               ETA: 00:13:11

################################################################################
                     [1m Learning iteration 1228/2000 [0m                     

                       Computation: 109967 steps/s (collection: 0.800s, learning 0.094s)
             Mean action noise std: 5.41
          Mean value_function loss: 38.5276
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 24.3630
                       Mean reward: 879.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7566
     Episode_Reward/lifting_object: 169.3100
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.0766
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 0.89s
                      Time elapsed: 00:20:57
                               ETA: 00:13:10

################################################################################
                     [1m Learning iteration 1229/2000 [0m                     

                       Computation: 107864 steps/s (collection: 0.807s, learning 0.104s)
             Mean action noise std: 5.42
          Mean value_function loss: 28.5380
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 24.3682
                       Mean reward: 854.19
               Mean episode length: 247.83
    Episode_Reward/reaching_object: 0.7523
     Episode_Reward/lifting_object: 169.4645
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.0764
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 0.91s
                      Time elapsed: 00:20:58
                               ETA: 00:13:08

################################################################################
                     [1m Learning iteration 1230/2000 [0m                     

                       Computation: 106694 steps/s (collection: 0.825s, learning 0.096s)
             Mean action noise std: 5.42
          Mean value_function loss: 27.7762
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 24.3731
                       Mean reward: 840.20
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 170.5548
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0769
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 0.92s
                      Time elapsed: 00:20:59
                               ETA: 00:13:07

################################################################################
                     [1m Learning iteration 1231/2000 [0m                     

                       Computation: 106118 steps/s (collection: 0.812s, learning 0.114s)
             Mean action noise std: 5.42
          Mean value_function loss: 35.9976
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 24.3819
                       Mean reward: 878.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 172.2959
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.0773
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 0.93s
                      Time elapsed: 00:21:00
                               ETA: 00:13:06

################################################################################
                     [1m Learning iteration 1232/2000 [0m                     

                       Computation: 95857 steps/s (collection: 0.907s, learning 0.119s)
             Mean action noise std: 5.43
          Mean value_function loss: 30.4568
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 24.3872
                       Mean reward: 858.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 172.2014
      Episode_Reward/object_height: 0.0647
        Episode_Reward/action_rate: -0.0774
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 1.03s
                      Time elapsed: 00:21:01
                               ETA: 00:13:05

################################################################################
                     [1m Learning iteration 1233/2000 [0m                     

                       Computation: 104806 steps/s (collection: 0.831s, learning 0.107s)
             Mean action noise std: 5.43
          Mean value_function loss: 27.5092
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 24.3925
                       Mean reward: 848.32
               Mean episode length: 249.47
    Episode_Reward/reaching_object: 0.7507
     Episode_Reward/lifting_object: 169.5902
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.0782
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 0.94s
                      Time elapsed: 00:21:02
                               ETA: 00:13:04

################################################################################
                     [1m Learning iteration 1234/2000 [0m                     

                       Computation: 99894 steps/s (collection: 0.859s, learning 0.125s)
             Mean action noise std: 5.44
          Mean value_function loss: 38.3014
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 24.4001
                       Mean reward: 872.76
               Mean episode length: 248.65
    Episode_Reward/reaching_object: 0.7723
     Episode_Reward/lifting_object: 172.5346
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.0789
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 0.98s
                      Time elapsed: 00:21:03
                               ETA: 00:13:03

################################################################################
                     [1m Learning iteration 1235/2000 [0m                     

                       Computation: 112317 steps/s (collection: 0.786s, learning 0.090s)
             Mean action noise std: 5.44
          Mean value_function loss: 34.1777
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 24.4056
                       Mean reward: 869.10
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 173.3569
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0793
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 0.88s
                      Time elapsed: 00:21:04
                               ETA: 00:13:02

################################################################################
                     [1m Learning iteration 1236/2000 [0m                     

                       Computation: 107186 steps/s (collection: 0.800s, learning 0.117s)
             Mean action noise std: 5.45
          Mean value_function loss: 34.7408
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 24.4132
                       Mean reward: 858.90
               Mean episode length: 247.42
    Episode_Reward/reaching_object: 0.7576
     Episode_Reward/lifting_object: 171.8182
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.0791
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 0.92s
                      Time elapsed: 00:21:05
                               ETA: 00:13:01

################################################################################
                     [1m Learning iteration 1237/2000 [0m                     

                       Computation: 109132 steps/s (collection: 0.802s, learning 0.099s)
             Mean action noise std: 5.45
          Mean value_function loss: 36.4520
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 24.4249
                       Mean reward: 859.09
               Mean episode length: 248.51
    Episode_Reward/reaching_object: 0.7575
     Episode_Reward/lifting_object: 170.2776
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0797
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 0.90s
                      Time elapsed: 00:21:06
                               ETA: 00:13:00

################################################################################
                     [1m Learning iteration 1238/2000 [0m                     

                       Computation: 108287 steps/s (collection: 0.807s, learning 0.101s)
             Mean action noise std: 5.45
          Mean value_function loss: 29.9975
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 24.4297
                       Mean reward: 859.99
               Mean episode length: 247.46
    Episode_Reward/reaching_object: 0.7621
     Episode_Reward/lifting_object: 170.0729
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.0793
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 0.91s
                      Time elapsed: 00:21:07
                               ETA: 00:12:59

################################################################################
                     [1m Learning iteration 1239/2000 [0m                     

                       Computation: 107036 steps/s (collection: 0.815s, learning 0.103s)
             Mean action noise std: 5.46
          Mean value_function loss: 42.3800
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 24.4386
                       Mean reward: 860.77
               Mean episode length: 247.45
    Episode_Reward/reaching_object: 0.7640
     Episode_Reward/lifting_object: 172.1140
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.0792
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 0.92s
                      Time elapsed: 00:21:07
                               ETA: 00:12:58

################################################################################
                     [1m Learning iteration 1240/2000 [0m                     

                       Computation: 109653 steps/s (collection: 0.799s, learning 0.097s)
             Mean action noise std: 5.47
          Mean value_function loss: 41.5931
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 24.4556
                       Mean reward: 870.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 172.6615
      Episode_Reward/object_height: 0.0648
        Episode_Reward/action_rate: -0.0802
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 0.90s
                      Time elapsed: 00:21:08
                               ETA: 00:12:57

################################################################################
                     [1m Learning iteration 1241/2000 [0m                     

                       Computation: 105198 steps/s (collection: 0.836s, learning 0.099s)
             Mean action noise std: 5.48
          Mean value_function loss: 32.4004
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 24.4640
                       Mean reward: 869.40
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 172.3904
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.0796
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 0.93s
                      Time elapsed: 00:21:09
                               ETA: 00:12:55

################################################################################
                     [1m Learning iteration 1242/2000 [0m                     

                       Computation: 110147 steps/s (collection: 0.790s, learning 0.102s)
             Mean action noise std: 5.49
          Mean value_function loss: 37.5906
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 24.4710
                       Mean reward: 874.09
               Mean episode length: 248.56
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 172.4187
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.0806
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 0.89s
                      Time elapsed: 00:21:10
                               ETA: 00:12:54

################################################################################
                     [1m Learning iteration 1243/2000 [0m                     

                       Computation: 106720 steps/s (collection: 0.809s, learning 0.112s)
             Mean action noise std: 5.50
          Mean value_function loss: 33.2397
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 24.4843
                       Mean reward: 853.34
               Mean episode length: 248.28
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 170.8621
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.0796
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 0.92s
                      Time elapsed: 00:21:11
                               ETA: 00:12:53

################################################################################
                     [1m Learning iteration 1244/2000 [0m                     

                       Computation: 109491 steps/s (collection: 0.799s, learning 0.099s)
             Mean action noise std: 5.50
          Mean value_function loss: 40.8033
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 24.4989
                       Mean reward: 857.32
               Mean episode length: 248.86
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 169.9488
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.0801
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 0.90s
                      Time elapsed: 00:21:12
                               ETA: 00:12:52

################################################################################
                     [1m Learning iteration 1245/2000 [0m                     

                       Computation: 110285 steps/s (collection: 0.787s, learning 0.104s)
             Mean action noise std: 5.51
          Mean value_function loss: 30.2492
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 24.5106
                       Mean reward: 871.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7669
     Episode_Reward/lifting_object: 172.4572
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0803
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 0.89s
                      Time elapsed: 00:21:13
                               ETA: 00:12:51

################################################################################
                     [1m Learning iteration 1246/2000 [0m                     

                       Computation: 103928 steps/s (collection: 0.828s, learning 0.118s)
             Mean action noise std: 5.52
          Mean value_function loss: 41.4898
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 24.5215
                       Mean reward: 862.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7578
     Episode_Reward/lifting_object: 169.8571
      Episode_Reward/object_height: 0.0623
        Episode_Reward/action_rate: -0.0805
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 0.95s
                      Time elapsed: 00:21:14
                               ETA: 00:12:50

################################################################################
                     [1m Learning iteration 1247/2000 [0m                     

                       Computation: 96243 steps/s (collection: 0.895s, learning 0.126s)
             Mean action noise std: 5.53
          Mean value_function loss: 42.1901
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 24.5363
                       Mean reward: 865.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7704
     Episode_Reward/lifting_object: 171.8257
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.0808
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 1.02s
                      Time elapsed: 00:21:15
                               ETA: 00:12:49

################################################################################
                     [1m Learning iteration 1248/2000 [0m                     

                       Computation: 97299 steps/s (collection: 0.875s, learning 0.135s)
             Mean action noise std: 5.54
          Mean value_function loss: 32.3331
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 24.5502
                       Mean reward: 844.59
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.7530
     Episode_Reward/lifting_object: 167.8330
      Episode_Reward/object_height: 0.0615
        Episode_Reward/action_rate: -0.0813
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 1.01s
                      Time elapsed: 00:21:16
                               ETA: 00:12:48

################################################################################
                     [1m Learning iteration 1249/2000 [0m                     

                       Computation: 109490 steps/s (collection: 0.806s, learning 0.092s)
             Mean action noise std: 5.55
          Mean value_function loss: 40.7344
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 24.5626
                       Mean reward: 850.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7673
     Episode_Reward/lifting_object: 170.1438
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.0823
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 0.90s
                      Time elapsed: 00:21:17
                               ETA: 00:12:47

################################################################################
                     [1m Learning iteration 1250/2000 [0m                     

                       Computation: 107666 steps/s (collection: 0.819s, learning 0.094s)
             Mean action noise std: 5.55
          Mean value_function loss: 40.0656
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 24.5727
                       Mean reward: 856.12
               Mean episode length: 245.79
    Episode_Reward/reaching_object: 0.7634
     Episode_Reward/lifting_object: 171.2050
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.0820
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 0.91s
                      Time elapsed: 00:21:18
                               ETA: 00:12:46

################################################################################
                     [1m Learning iteration 1251/2000 [0m                     

                       Computation: 107936 steps/s (collection: 0.820s, learning 0.091s)
             Mean action noise std: 5.56
          Mean value_function loss: 33.0824
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 24.5825
                       Mean reward: 858.91
               Mean episode length: 248.68
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 171.2953
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.0825
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 0.91s
                      Time elapsed: 00:21:19
                               ETA: 00:12:45

################################################################################
                     [1m Learning iteration 1252/2000 [0m                     

                       Computation: 111048 steps/s (collection: 0.792s, learning 0.094s)
             Mean action noise std: 5.56
          Mean value_function loss: 37.0155
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 24.5886
                       Mean reward: 862.70
               Mean episode length: 248.60
    Episode_Reward/reaching_object: 0.7583
     Episode_Reward/lifting_object: 169.9236
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.0818
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 0.89s
                      Time elapsed: 00:21:20
                               ETA: 00:12:44

################################################################################
                     [1m Learning iteration 1253/2000 [0m                     

                       Computation: 107749 steps/s (collection: 0.814s, learning 0.099s)
             Mean action noise std: 5.57
          Mean value_function loss: 34.6484
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 24.5988
                       Mean reward: 862.10
               Mean episode length: 249.37
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 170.5413
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.0828
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 0.91s
                      Time elapsed: 00:21:20
                               ETA: 00:12:43

################################################################################
                     [1m Learning iteration 1254/2000 [0m                     

                       Computation: 107924 steps/s (collection: 0.819s, learning 0.092s)
             Mean action noise std: 5.58
          Mean value_function loss: 29.0660
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 24.6109
                       Mean reward: 863.40
               Mean episode length: 248.65
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 171.3849
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0828
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 0.91s
                      Time elapsed: 00:21:21
                               ETA: 00:12:41

################################################################################
                     [1m Learning iteration 1255/2000 [0m                     

                       Computation: 106568 steps/s (collection: 0.813s, learning 0.110s)
             Mean action noise std: 5.59
          Mean value_function loss: 34.2256
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 24.6196
                       Mean reward: 857.03
               Mean episode length: 248.98
    Episode_Reward/reaching_object: 0.7666
     Episode_Reward/lifting_object: 170.9525
      Episode_Reward/object_height: 0.0615
        Episode_Reward/action_rate: -0.0832
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 0.92s
                      Time elapsed: 00:21:22
                               ETA: 00:12:40

################################################################################
                     [1m Learning iteration 1256/2000 [0m                     

                       Computation: 113026 steps/s (collection: 0.773s, learning 0.096s)
             Mean action noise std: 5.60
          Mean value_function loss: 41.1829
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 24.6281
                       Mean reward: 861.29
               Mean episode length: 248.92
    Episode_Reward/reaching_object: 0.7587
     Episode_Reward/lifting_object: 169.8319
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.0834
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 0.87s
                      Time elapsed: 00:21:23
                               ETA: 00:12:39

################################################################################
                     [1m Learning iteration 1257/2000 [0m                     

                       Computation: 113727 steps/s (collection: 0.771s, learning 0.094s)
             Mean action noise std: 5.60
          Mean value_function loss: 27.0230
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 24.6413
                       Mean reward: 857.64
               Mean episode length: 248.95
    Episode_Reward/reaching_object: 0.7629
     Episode_Reward/lifting_object: 171.0025
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.0833
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 0.86s
                      Time elapsed: 00:21:24
                               ETA: 00:12:38

################################################################################
                     [1m Learning iteration 1258/2000 [0m                     

                       Computation: 110545 steps/s (collection: 0.799s, learning 0.090s)
             Mean action noise std: 5.61
          Mean value_function loss: 30.4084
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 24.6498
                       Mean reward: 856.18
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 171.4108
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.0847
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 0.89s
                      Time elapsed: 00:21:25
                               ETA: 00:12:37

################################################################################
                     [1m Learning iteration 1259/2000 [0m                     

                       Computation: 110055 steps/s (collection: 0.802s, learning 0.091s)
             Mean action noise std: 5.62
          Mean value_function loss: 34.4154
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 24.6593
                       Mean reward: 864.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 173.3326
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0845
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 0.89s
                      Time elapsed: 00:21:26
                               ETA: 00:12:36

################################################################################
                     [1m Learning iteration 1260/2000 [0m                     

                       Computation: 106693 steps/s (collection: 0.825s, learning 0.097s)
             Mean action noise std: 5.62
          Mean value_function loss: 27.5179
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 24.6694
                       Mean reward: 874.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7683
     Episode_Reward/lifting_object: 172.4956
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0849
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 0.92s
                      Time elapsed: 00:21:27
                               ETA: 00:12:35

################################################################################
                     [1m Learning iteration 1261/2000 [0m                     

                       Computation: 109391 steps/s (collection: 0.811s, learning 0.088s)
             Mean action noise std: 5.62
          Mean value_function loss: 32.2684
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 24.6724
                       Mean reward: 857.44
               Mean episode length: 248.63
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 171.4207
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.0858
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 0.90s
                      Time elapsed: 00:21:28
                               ETA: 00:12:34

################################################################################
                     [1m Learning iteration 1262/2000 [0m                     

                       Computation: 111837 steps/s (collection: 0.792s, learning 0.087s)
             Mean action noise std: 5.62
          Mean value_function loss: 31.2698
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 24.6732
                       Mean reward: 872.53
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.7677
     Episode_Reward/lifting_object: 173.0562
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.0857
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 0.88s
                      Time elapsed: 00:21:28
                               ETA: 00:12:33

################################################################################
                     [1m Learning iteration 1263/2000 [0m                     

                       Computation: 109978 steps/s (collection: 0.797s, learning 0.097s)
             Mean action noise std: 5.63
          Mean value_function loss: 37.1029
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 24.6769
                       Mean reward: 862.51
               Mean episode length: 247.63
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 172.4730
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0862
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 0.89s
                      Time elapsed: 00:21:29
                               ETA: 00:12:32

################################################################################
                     [1m Learning iteration 1264/2000 [0m                     

                       Computation: 109911 steps/s (collection: 0.796s, learning 0.098s)
             Mean action noise std: 5.63
          Mean value_function loss: 32.6446
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 24.6844
                       Mean reward: 874.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 171.8056
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.0862
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 0.89s
                      Time elapsed: 00:21:30
                               ETA: 00:12:30

################################################################################
                     [1m Learning iteration 1265/2000 [0m                     

                       Computation: 109501 steps/s (collection: 0.805s, learning 0.093s)
             Mean action noise std: 5.64
          Mean value_function loss: 38.8151
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 24.6945
                       Mean reward: 878.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 171.7080
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.0853
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 0.90s
                      Time elapsed: 00:21:31
                               ETA: 00:12:29

################################################################################
                     [1m Learning iteration 1266/2000 [0m                     

                       Computation: 114105 steps/s (collection: 0.763s, learning 0.098s)
             Mean action noise std: 5.65
          Mean value_function loss: 35.0088
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 24.7035
                       Mean reward: 852.05
               Mean episode length: 246.71
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 171.5443
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.0857
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 0.86s
                      Time elapsed: 00:21:32
                               ETA: 00:12:28

################################################################################
                     [1m Learning iteration 1267/2000 [0m                     

                       Computation: 112015 steps/s (collection: 0.782s, learning 0.096s)
             Mean action noise std: 5.65
          Mean value_function loss: 29.6225
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 24.7134
                       Mean reward: 829.81
               Mean episode length: 244.89
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 170.5498
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0857
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 0.88s
                      Time elapsed: 00:21:33
                               ETA: 00:12:27

################################################################################
                     [1m Learning iteration 1268/2000 [0m                     

                       Computation: 111100 steps/s (collection: 0.785s, learning 0.100s)
             Mean action noise std: 5.65
          Mean value_function loss: 36.2680
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 24.7168
                       Mean reward: 856.33
               Mean episode length: 247.13
    Episode_Reward/reaching_object: 0.7707
     Episode_Reward/lifting_object: 172.4572
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.0858
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 0.88s
                      Time elapsed: 00:21:34
                               ETA: 00:12:26

################################################################################
                     [1m Learning iteration 1269/2000 [0m                     

                       Computation: 110059 steps/s (collection: 0.791s, learning 0.103s)
             Mean action noise std: 5.66
          Mean value_function loss: 35.8201
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 24.7230
                       Mean reward: 863.10
               Mean episode length: 248.45
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 171.2862
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.0861
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 0.89s
                      Time elapsed: 00:21:35
                               ETA: 00:12:25

################################################################################
                     [1m Learning iteration 1270/2000 [0m                     

                       Computation: 109226 steps/s (collection: 0.810s, learning 0.090s)
             Mean action noise std: 5.67
          Mean value_function loss: 34.7405
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 24.7352
                       Mean reward: 849.36
               Mean episode length: 246.57
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 170.2662
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0860
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 0.90s
                      Time elapsed: 00:21:36
                               ETA: 00:12:24

################################################################################
                     [1m Learning iteration 1271/2000 [0m                     

                       Computation: 109077 steps/s (collection: 0.807s, learning 0.094s)
             Mean action noise std: 5.67
          Mean value_function loss: 40.2471
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 24.7413
                       Mean reward: 855.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 171.4268
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0868
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 0.90s
                      Time elapsed: 00:21:36
                               ETA: 00:12:23

################################################################################
                     [1m Learning iteration 1272/2000 [0m                     

                       Computation: 105321 steps/s (collection: 0.826s, learning 0.108s)
             Mean action noise std: 5.68
          Mean value_function loss: 30.2959
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 24.7486
                       Mean reward: 865.12
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 0.7575
     Episode_Reward/lifting_object: 170.4439
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.0867
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 0.93s
                      Time elapsed: 00:21:37
                               ETA: 00:12:22

################################################################################
                     [1m Learning iteration 1273/2000 [0m                     

                       Computation: 106281 steps/s (collection: 0.825s, learning 0.100s)
             Mean action noise std: 5.69
          Mean value_function loss: 31.8947
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 24.7575
                       Mean reward: 847.01
               Mean episode length: 246.25
    Episode_Reward/reaching_object: 0.7629
     Episode_Reward/lifting_object: 171.1375
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0878
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 0.92s
                      Time elapsed: 00:21:38
                               ETA: 00:12:21

################################################################################
                     [1m Learning iteration 1274/2000 [0m                     

                       Computation: 109917 steps/s (collection: 0.802s, learning 0.093s)
             Mean action noise std: 5.69
          Mean value_function loss: 36.8039
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 24.7670
                       Mean reward: 858.52
               Mean episode length: 249.16
    Episode_Reward/reaching_object: 0.7636
     Episode_Reward/lifting_object: 172.3770
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0885
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 0.89s
                      Time elapsed: 00:21:39
                               ETA: 00:12:20

################################################################################
                     [1m Learning iteration 1275/2000 [0m                     

                       Computation: 106196 steps/s (collection: 0.818s, learning 0.108s)
             Mean action noise std: 5.70
          Mean value_function loss: 36.6435
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 24.7783
                       Mean reward: 868.48
               Mean episode length: 249.91
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 171.2362
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.0889
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 0.93s
                      Time elapsed: 00:21:40
                               ETA: 00:12:19

################################################################################
                     [1m Learning iteration 1276/2000 [0m                     

                       Computation: 106751 steps/s (collection: 0.826s, learning 0.095s)
             Mean action noise std: 5.71
          Mean value_function loss: 40.6767
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 24.7861
                       Mean reward: 844.55
               Mean episode length: 249.29
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 170.6492
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0901
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 0.92s
                      Time elapsed: 00:21:41
                               ETA: 00:12:17

################################################################################
                     [1m Learning iteration 1277/2000 [0m                     

                       Computation: 102070 steps/s (collection: 0.848s, learning 0.115s)
             Mean action noise std: 5.72
          Mean value_function loss: 35.2598
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 24.8003
                       Mean reward: 870.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7588
     Episode_Reward/lifting_object: 171.1078
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0902
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 0.96s
                      Time elapsed: 00:21:42
                               ETA: 00:12:16

################################################################################
                     [1m Learning iteration 1278/2000 [0m                     

                       Computation: 104009 steps/s (collection: 0.834s, learning 0.112s)
             Mean action noise std: 5.72
          Mean value_function loss: 27.2938
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 24.8108
                       Mean reward: 860.83
               Mean episode length: 249.07
    Episode_Reward/reaching_object: 0.7612
     Episode_Reward/lifting_object: 170.4698
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0911
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 0.95s
                      Time elapsed: 00:21:43
                               ETA: 00:12:15

################################################################################
                     [1m Learning iteration 1279/2000 [0m                     

                       Computation: 106289 steps/s (collection: 0.823s, learning 0.102s)
             Mean action noise std: 5.73
          Mean value_function loss: 32.0596
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 24.8145
                       Mean reward: 860.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7602
     Episode_Reward/lifting_object: 171.5233
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0919
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 0.92s
                      Time elapsed: 00:21:44
                               ETA: 00:12:14

################################################################################
                     [1m Learning iteration 1280/2000 [0m                     

                       Computation: 105190 steps/s (collection: 0.828s, learning 0.107s)
             Mean action noise std: 5.73
          Mean value_function loss: 30.2438
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 24.8230
                       Mean reward: 870.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7601
     Episode_Reward/lifting_object: 171.6054
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0917
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 0.93s
                      Time elapsed: 00:21:45
                               ETA: 00:12:13

################################################################################
                     [1m Learning iteration 1281/2000 [0m                     

                       Computation: 108620 steps/s (collection: 0.803s, learning 0.102s)
             Mean action noise std: 5.74
          Mean value_function loss: 41.3775
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 24.8364
                       Mean reward: 858.30
               Mean episode length: 249.19
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 171.5183
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.0923
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 0.91s
                      Time elapsed: 00:21:46
                               ETA: 00:12:12

################################################################################
                     [1m Learning iteration 1282/2000 [0m                     

                       Computation: 106297 steps/s (collection: 0.831s, learning 0.094s)
             Mean action noise std: 5.75
          Mean value_function loss: 30.4369
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 24.8452
                       Mean reward: 875.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7664
     Episode_Reward/lifting_object: 172.6247
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.0925
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 0.92s
                      Time elapsed: 00:21:47
                               ETA: 00:12:11

################################################################################
                     [1m Learning iteration 1283/2000 [0m                     

                       Computation: 108818 steps/s (collection: 0.803s, learning 0.100s)
             Mean action noise std: 5.75
          Mean value_function loss: 35.5052
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 24.8532
                       Mean reward: 862.46
               Mean episode length: 246.77
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 171.0971
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.0921
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 0.90s
                      Time elapsed: 00:21:48
                               ETA: 00:12:10

################################################################################
                     [1m Learning iteration 1284/2000 [0m                     

                       Computation: 108183 steps/s (collection: 0.814s, learning 0.094s)
             Mean action noise std: 5.76
          Mean value_function loss: 38.9142
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 24.8587
                       Mean reward: 879.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7701
     Episode_Reward/lifting_object: 172.0697
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0920
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 0.91s
                      Time elapsed: 00:21:48
                               ETA: 00:12:09

################################################################################
                     [1m Learning iteration 1285/2000 [0m                     

                       Computation: 112674 steps/s (collection: 0.780s, learning 0.092s)
             Mean action noise std: 5.77
          Mean value_function loss: 45.3999
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 24.8687
                       Mean reward: 861.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7694
     Episode_Reward/lifting_object: 172.3151
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.0928
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 0.87s
                      Time elapsed: 00:21:49
                               ETA: 00:12:08

################################################################################
                     [1m Learning iteration 1286/2000 [0m                     

                       Computation: 107110 steps/s (collection: 0.827s, learning 0.091s)
             Mean action noise std: 5.78
          Mean value_function loss: 44.6539
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 24.8778
                       Mean reward: 878.06
               Mean episode length: 249.17
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 170.8806
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0918
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 0.92s
                      Time elapsed: 00:21:50
                               ETA: 00:12:07

################################################################################
                     [1m Learning iteration 1287/2000 [0m                     

                       Computation: 107018 steps/s (collection: 0.821s, learning 0.097s)
             Mean action noise std: 5.79
          Mean value_function loss: 34.2065
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 24.8963
                       Mean reward: 865.32
               Mean episode length: 248.00
    Episode_Reward/reaching_object: 0.7641
     Episode_Reward/lifting_object: 171.4726
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.0918
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 0.92s
                      Time elapsed: 00:21:51
                               ETA: 00:12:06

################################################################################
                     [1m Learning iteration 1288/2000 [0m                     

                       Computation: 108784 steps/s (collection: 0.789s, learning 0.114s)
             Mean action noise std: 5.79
          Mean value_function loss: 41.8770
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 24.9034
                       Mean reward: 869.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 172.4909
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0930
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 0.90s
                      Time elapsed: 00:21:52
                               ETA: 00:12:05

################################################################################
                     [1m Learning iteration 1289/2000 [0m                     

                       Computation: 107691 steps/s (collection: 0.806s, learning 0.107s)
             Mean action noise std: 5.80
          Mean value_function loss: 34.1185
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 24.9113
                       Mean reward: 869.24
               Mean episode length: 249.16
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 171.2193
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0930
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 0.91s
                      Time elapsed: 00:21:53
                               ETA: 00:12:03

################################################################################
                     [1m Learning iteration 1290/2000 [0m                     

                       Computation: 108017 steps/s (collection: 0.805s, learning 0.105s)
             Mean action noise std: 5.80
          Mean value_function loss: 44.1141
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 24.9238
                       Mean reward: 853.58
               Mean episode length: 247.71
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 170.3322
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.0930
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 0.91s
                      Time elapsed: 00:21:54
                               ETA: 00:12:02

################################################################################
                     [1m Learning iteration 1291/2000 [0m                     

                       Computation: 106919 steps/s (collection: 0.824s, learning 0.096s)
             Mean action noise std: 5.81
          Mean value_function loss: 33.3124
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 24.9326
                       Mean reward: 865.71
               Mean episode length: 249.45
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 171.7196
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0932
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 0.92s
                      Time elapsed: 00:21:55
                               ETA: 00:12:01

################################################################################
                     [1m Learning iteration 1292/2000 [0m                     

                       Computation: 111006 steps/s (collection: 0.784s, learning 0.102s)
             Mean action noise std: 5.82
          Mean value_function loss: 42.2554
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 24.9466
                       Mean reward: 867.75
               Mean episode length: 249.38
    Episode_Reward/reaching_object: 0.7575
     Episode_Reward/lifting_object: 170.5212
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0935
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 0.89s
                      Time elapsed: 00:21:56
                               ETA: 00:12:00

################################################################################
                     [1m Learning iteration 1293/2000 [0m                     

                       Computation: 109195 steps/s (collection: 0.796s, learning 0.105s)
             Mean action noise std: 5.83
          Mean value_function loss: 43.4987
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 24.9646
                       Mean reward: 862.89
               Mean episode length: 249.66
    Episode_Reward/reaching_object: 0.7543
     Episode_Reward/lifting_object: 170.6641
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0938
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 0.90s
                      Time elapsed: 00:21:57
                               ETA: 00:11:59

################################################################################
                     [1m Learning iteration 1294/2000 [0m                     

                       Computation: 105700 steps/s (collection: 0.838s, learning 0.092s)
             Mean action noise std: 5.84
          Mean value_function loss: 40.8500
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 24.9818
                       Mean reward: 860.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7504
     Episode_Reward/lifting_object: 170.4534
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0945
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 0.93s
                      Time elapsed: 00:21:58
                               ETA: 00:11:58

################################################################################
                     [1m Learning iteration 1295/2000 [0m                     

                       Computation: 108705 steps/s (collection: 0.800s, learning 0.104s)
             Mean action noise std: 5.85
          Mean value_function loss: 33.3926
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 24.9893
                       Mean reward: 877.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7627
     Episode_Reward/lifting_object: 172.3437
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0946
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 0.90s
                      Time elapsed: 00:21:58
                               ETA: 00:11:57

################################################################################
                     [1m Learning iteration 1296/2000 [0m                     

                       Computation: 102843 steps/s (collection: 0.853s, learning 0.103s)
             Mean action noise std: 5.85
          Mean value_function loss: 36.8418
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 24.9991
                       Mean reward: 854.13
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7546
     Episode_Reward/lifting_object: 169.3203
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.0945
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 0.96s
                      Time elapsed: 00:21:59
                               ETA: 00:11:56

################################################################################
                     [1m Learning iteration 1297/2000 [0m                     

                       Computation: 106294 steps/s (collection: 0.830s, learning 0.095s)
             Mean action noise std: 5.86
          Mean value_function loss: 30.0236
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 25.0112
                       Mean reward: 870.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7591
     Episode_Reward/lifting_object: 170.6284
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0938
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 0.92s
                      Time elapsed: 00:22:00
                               ETA: 00:11:55

################################################################################
                     [1m Learning iteration 1298/2000 [0m                     

                       Computation: 106579 steps/s (collection: 0.824s, learning 0.098s)
             Mean action noise std: 5.86
          Mean value_function loss: 28.0189
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 25.0178
                       Mean reward: 864.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 171.9641
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0949
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 0.92s
                      Time elapsed: 00:22:01
                               ETA: 00:11:54

################################################################################
                     [1m Learning iteration 1299/2000 [0m                     

                       Computation: 103261 steps/s (collection: 0.848s, learning 0.104s)
             Mean action noise std: 5.87
          Mean value_function loss: 35.1472
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 25.0211
                       Mean reward: 850.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7674
     Episode_Reward/lifting_object: 171.5643
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0952
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 0.95s
                      Time elapsed: 00:22:02
                               ETA: 00:11:53

################################################################################
                     [1m Learning iteration 1300/2000 [0m                     

                       Computation: 105947 steps/s (collection: 0.827s, learning 0.101s)
             Mean action noise std: 5.87
          Mean value_function loss: 37.1043
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 25.0298
                       Mean reward: 864.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 172.2648
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.0952
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 0.93s
                      Time elapsed: 00:22:03
                               ETA: 00:11:52

################################################################################
                     [1m Learning iteration 1301/2000 [0m                     

                       Computation: 106108 steps/s (collection: 0.816s, learning 0.110s)
             Mean action noise std: 5.88
          Mean value_function loss: 34.9962
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 25.0431
                       Mean reward: 850.51
               Mean episode length: 247.56
    Episode_Reward/reaching_object: 0.7566
     Episode_Reward/lifting_object: 169.7608
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0943
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 0.93s
                      Time elapsed: 00:22:04
                               ETA: 00:11:51

################################################################################
                     [1m Learning iteration 1302/2000 [0m                     

                       Computation: 107312 steps/s (collection: 0.803s, learning 0.113s)
             Mean action noise std: 5.89
          Mean value_function loss: 28.5988
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 25.0558
                       Mean reward: 839.85
               Mean episode length: 249.06
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 171.4970
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0945
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 0.92s
                      Time elapsed: 00:22:05
                               ETA: 00:11:50

################################################################################
                     [1m Learning iteration 1303/2000 [0m                     

                       Computation: 104765 steps/s (collection: 0.827s, learning 0.112s)
             Mean action noise std: 5.90
          Mean value_function loss: 31.2667
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 25.0690
                       Mean reward: 857.12
               Mean episode length: 248.83
    Episode_Reward/reaching_object: 0.7686
     Episode_Reward/lifting_object: 171.2160
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0950
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 0.94s
                      Time elapsed: 00:22:06
                               ETA: 00:11:48

################################################################################
                     [1m Learning iteration 1304/2000 [0m                     

                       Computation: 109525 steps/s (collection: 0.804s, learning 0.094s)
             Mean action noise std: 5.91
          Mean value_function loss: 44.6281
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 25.0791
                       Mean reward: 879.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 171.4458
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.0946
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 0.90s
                      Time elapsed: 00:22:07
                               ETA: 00:11:47

################################################################################
                     [1m Learning iteration 1305/2000 [0m                     

                       Computation: 107462 steps/s (collection: 0.815s, learning 0.100s)
             Mean action noise std: 5.92
          Mean value_function loss: 39.7655
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 25.0885
                       Mean reward: 870.23
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7664
     Episode_Reward/lifting_object: 171.4378
      Episode_Reward/object_height: 0.0648
        Episode_Reward/action_rate: -0.0950
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 0.91s
                      Time elapsed: 00:22:08
                               ETA: 00:11:46

################################################################################
                     [1m Learning iteration 1306/2000 [0m                     

                       Computation: 106213 steps/s (collection: 0.825s, learning 0.101s)
             Mean action noise std: 5.92
          Mean value_function loss: 30.5623
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 25.0974
                       Mean reward: 869.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 171.2204
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.0948
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 0.93s
                      Time elapsed: 00:22:09
                               ETA: 00:11:45

################################################################################
                     [1m Learning iteration 1307/2000 [0m                     

                       Computation: 106743 steps/s (collection: 0.820s, learning 0.101s)
             Mean action noise std: 5.93
          Mean value_function loss: 30.4030
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 25.1101
                       Mean reward: 865.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 171.7742
      Episode_Reward/object_height: 0.0655
        Episode_Reward/action_rate: -0.0949
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 0.92s
                      Time elapsed: 00:22:10
                               ETA: 00:11:44

################################################################################
                     [1m Learning iteration 1308/2000 [0m                     

                       Computation: 106983 steps/s (collection: 0.823s, learning 0.096s)
             Mean action noise std: 5.94
          Mean value_function loss: 28.2941
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 25.1218
                       Mean reward: 855.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 170.2179
      Episode_Reward/object_height: 0.0651
        Episode_Reward/action_rate: -0.0948
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 0.92s
                      Time elapsed: 00:22:10
                               ETA: 00:11:43

################################################################################
                     [1m Learning iteration 1309/2000 [0m                     

                       Computation: 110032 steps/s (collection: 0.802s, learning 0.092s)
             Mean action noise std: 5.95
          Mean value_function loss: 35.0468
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 25.1319
                       Mean reward: 833.52
               Mean episode length: 246.78
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 169.8695
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.0947
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 0.89s
                      Time elapsed: 00:22:11
                               ETA: 00:11:42

################################################################################
                     [1m Learning iteration 1310/2000 [0m                     

                       Computation: 105291 steps/s (collection: 0.836s, learning 0.098s)
             Mean action noise std: 5.96
          Mean value_function loss: 29.3697
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 25.1469
                       Mean reward: 873.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7746
     Episode_Reward/lifting_object: 172.8594
      Episode_Reward/object_height: 0.0656
        Episode_Reward/action_rate: -0.0960
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 0.93s
                      Time elapsed: 00:22:12
                               ETA: 00:11:41

################################################################################
                     [1m Learning iteration 1311/2000 [0m                     

                       Computation: 105279 steps/s (collection: 0.835s, learning 0.099s)
             Mean action noise std: 5.97
          Mean value_function loss: 30.2444
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 25.1558
                       Mean reward: 858.42
               Mean episode length: 248.45
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 171.3385
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.0951
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 0.93s
                      Time elapsed: 00:22:13
                               ETA: 00:11:40

################################################################################
                     [1m Learning iteration 1312/2000 [0m                     

                       Computation: 103776 steps/s (collection: 0.836s, learning 0.111s)
             Mean action noise std: 5.97
          Mean value_function loss: 41.6584
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 25.1664
                       Mean reward: 870.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7738
     Episode_Reward/lifting_object: 172.5853
      Episode_Reward/object_height: 0.0656
        Episode_Reward/action_rate: -0.0952
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 0.95s
                      Time elapsed: 00:22:14
                               ETA: 00:11:39

################################################################################
                     [1m Learning iteration 1313/2000 [0m                     

                       Computation: 102676 steps/s (collection: 0.834s, learning 0.124s)
             Mean action noise std: 5.98
          Mean value_function loss: 30.8137
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 25.1774
                       Mean reward: 843.28
               Mean episode length: 245.00
    Episode_Reward/reaching_object: 0.7610
     Episode_Reward/lifting_object: 169.6971
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0947
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 0.96s
                      Time elapsed: 00:22:15
                               ETA: 00:11:38

################################################################################
                     [1m Learning iteration 1314/2000 [0m                     

                       Computation: 107741 steps/s (collection: 0.785s, learning 0.127s)
             Mean action noise std: 5.99
          Mean value_function loss: 25.7595
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 25.1884
                       Mean reward: 847.74
               Mean episode length: 248.62
    Episode_Reward/reaching_object: 0.7602
     Episode_Reward/lifting_object: 170.8204
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0958
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 0.91s
                      Time elapsed: 00:22:16
                               ETA: 00:11:37

################################################################################
                     [1m Learning iteration 1315/2000 [0m                     

                       Computation: 107575 steps/s (collection: 0.794s, learning 0.120s)
             Mean action noise std: 6.00
          Mean value_function loss: 30.0873
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 25.2057
                       Mean reward: 876.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7727
     Episode_Reward/lifting_object: 173.5661
      Episode_Reward/object_height: 0.0661
        Episode_Reward/action_rate: -0.0965
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 0.91s
                      Time elapsed: 00:22:17
                               ETA: 00:11:36

################################################################################
                     [1m Learning iteration 1316/2000 [0m                     

                       Computation: 108100 steps/s (collection: 0.791s, learning 0.118s)
             Mean action noise std: 6.02
          Mean value_function loss: 31.0747
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 25.2230
                       Mean reward: 868.36
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.7686
     Episode_Reward/lifting_object: 172.2766
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0964
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 0.91s
                      Time elapsed: 00:22:18
                               ETA: 00:11:35

################################################################################
                     [1m Learning iteration 1317/2000 [0m                     

                       Computation: 106077 steps/s (collection: 0.826s, learning 0.101s)
             Mean action noise std: 6.03
          Mean value_function loss: 37.5194
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 25.2418
                       Mean reward: 867.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 172.4479
      Episode_Reward/object_height: 0.0657
        Episode_Reward/action_rate: -0.0968
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 0.93s
                      Time elapsed: 00:22:19
                               ETA: 00:11:34

################################################################################
                     [1m Learning iteration 1318/2000 [0m                     

                       Computation: 108135 steps/s (collection: 0.811s, learning 0.099s)
             Mean action noise std: 6.03
          Mean value_function loss: 35.2698
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 25.2522
                       Mean reward: 857.89
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 170.2213
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0973
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 0.91s
                      Time elapsed: 00:22:20
                               ETA: 00:11:32

################################################################################
                     [1m Learning iteration 1319/2000 [0m                     

                       Computation: 103060 steps/s (collection: 0.854s, learning 0.100s)
             Mean action noise std: 6.03
          Mean value_function loss: 43.9948
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 25.2562
                       Mean reward: 869.23
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.7708
     Episode_Reward/lifting_object: 172.5623
      Episode_Reward/object_height: 0.0653
        Episode_Reward/action_rate: -0.0978
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 0.95s
                      Time elapsed: 00:22:21
                               ETA: 00:11:31

################################################################################
                     [1m Learning iteration 1320/2000 [0m                     

                       Computation: 102286 steps/s (collection: 0.858s, learning 0.103s)
             Mean action noise std: 6.05
          Mean value_function loss: 36.4211
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 25.2661
                       Mean reward: 863.18
               Mean episode length: 247.02
    Episode_Reward/reaching_object: 0.7621
     Episode_Reward/lifting_object: 170.9057
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.0967
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 0.96s
                      Time elapsed: 00:22:22
                               ETA: 00:11:30

################################################################################
                     [1m Learning iteration 1321/2000 [0m                     

                       Computation: 102406 steps/s (collection: 0.860s, learning 0.100s)
             Mean action noise std: 6.05
          Mean value_function loss: 42.6917
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 25.2813
                       Mean reward: 861.35
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7628
     Episode_Reward/lifting_object: 171.3935
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.0977
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 0.96s
                      Time elapsed: 00:22:23
                               ETA: 00:11:29

################################################################################
                     [1m Learning iteration 1322/2000 [0m                     

                       Computation: 105389 steps/s (collection: 0.838s, learning 0.095s)
             Mean action noise std: 6.07
          Mean value_function loss: 36.7095
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 25.2951
                       Mean reward: 859.69
               Mean episode length: 247.44
    Episode_Reward/reaching_object: 0.7590
     Episode_Reward/lifting_object: 169.7119
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0973
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 0.93s
                      Time elapsed: 00:22:24
                               ETA: 00:11:28

################################################################################
                     [1m Learning iteration 1323/2000 [0m                     

                       Computation: 105142 steps/s (collection: 0.839s, learning 0.096s)
             Mean action noise std: 6.08
          Mean value_function loss: 49.0716
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 25.3150
                       Mean reward: 847.83
               Mean episode length: 246.36
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 172.0021
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0981
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 0.93s
                      Time elapsed: 00:22:24
                               ETA: 00:11:27

################################################################################
                     [1m Learning iteration 1324/2000 [0m                     

                       Computation: 100795 steps/s (collection: 0.855s, learning 0.121s)
             Mean action noise std: 6.08
          Mean value_function loss: 44.3342
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 25.3249
                       Mean reward: 859.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 171.0170
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0988
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 0.98s
                      Time elapsed: 00:22:25
                               ETA: 00:11:26

################################################################################
                     [1m Learning iteration 1325/2000 [0m                     

                       Computation: 103287 steps/s (collection: 0.851s, learning 0.101s)
             Mean action noise std: 6.09
          Mean value_function loss: 38.1454
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 25.3350
                       Mean reward: 866.75
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.7541
     Episode_Reward/lifting_object: 169.2528
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0979
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 0.95s
                      Time elapsed: 00:22:26
                               ETA: 00:11:25

################################################################################
                     [1m Learning iteration 1326/2000 [0m                     

                       Computation: 103824 steps/s (collection: 0.826s, learning 0.121s)
             Mean action noise std: 6.10
          Mean value_function loss: 50.8225
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 25.3474
                       Mean reward: 870.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7587
     Episode_Reward/lifting_object: 170.6856
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0996
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 0.95s
                      Time elapsed: 00:22:27
                               ETA: 00:11:24

################################################################################
                     [1m Learning iteration 1327/2000 [0m                     

                       Computation: 102679 steps/s (collection: 0.845s, learning 0.113s)
             Mean action noise std: 6.11
          Mean value_function loss: 43.5428
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 25.3593
                       Mean reward: 848.84
               Mean episode length: 246.74
    Episode_Reward/reaching_object: 0.7543
     Episode_Reward/lifting_object: 170.2769
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0989
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 0.96s
                      Time elapsed: 00:22:28
                               ETA: 00:11:23

################################################################################
                     [1m Learning iteration 1328/2000 [0m                     

                       Computation: 102212 steps/s (collection: 0.853s, learning 0.109s)
             Mean action noise std: 6.11
          Mean value_function loss: 46.1563
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 25.3697
                       Mean reward: 857.66
               Mean episode length: 249.56
    Episode_Reward/reaching_object: 0.7542
     Episode_Reward/lifting_object: 170.0147
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0994
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 0.96s
                      Time elapsed: 00:22:29
                               ETA: 00:11:22

################################################################################
                     [1m Learning iteration 1329/2000 [0m                     

                       Computation: 106417 steps/s (collection: 0.829s, learning 0.095s)
             Mean action noise std: 6.12
          Mean value_function loss: 38.3886
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 25.3767
                       Mean reward: 858.70
               Mean episode length: 247.56
    Episode_Reward/reaching_object: 0.7561
     Episode_Reward/lifting_object: 171.2814
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0989
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 0.92s
                      Time elapsed: 00:22:30
                               ETA: 00:11:21

################################################################################
                     [1m Learning iteration 1330/2000 [0m                     

                       Computation: 105037 steps/s (collection: 0.826s, learning 0.110s)
             Mean action noise std: 6.13
          Mean value_function loss: 35.9160
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 25.3858
                       Mean reward: 839.12
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7536
     Episode_Reward/lifting_object: 170.1746
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.0999
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 0.94s
                      Time elapsed: 00:22:31
                               ETA: 00:11:20

################################################################################
                     [1m Learning iteration 1331/2000 [0m                     

                       Computation: 100954 steps/s (collection: 0.870s, learning 0.103s)
             Mean action noise std: 6.14
          Mean value_function loss: 57.5425
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 25.3990
                       Mean reward: 852.91
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.7500
     Episode_Reward/lifting_object: 170.0306
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.1002
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 0.97s
                      Time elapsed: 00:22:32
                               ETA: 00:11:19

################################################################################
                     [1m Learning iteration 1332/2000 [0m                     

                       Computation: 107761 steps/s (collection: 0.816s, learning 0.097s)
             Mean action noise std: 6.15
          Mean value_function loss: 37.1293
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 25.4058
                       Mean reward: 826.94
               Mean episode length: 249.35
    Episode_Reward/reaching_object: 0.7387
     Episode_Reward/lifting_object: 166.3934
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.1003
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 0.91s
                      Time elapsed: 00:22:33
                               ETA: 00:11:18

################################################################################
                     [1m Learning iteration 1333/2000 [0m                     

                       Computation: 43116 steps/s (collection: 2.182s, learning 0.098s)
             Mean action noise std: 6.15
          Mean value_function loss: 51.7318
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 25.4126
                       Mean reward: 854.16
               Mean episode length: 247.28
    Episode_Reward/reaching_object: 0.7490
     Episode_Reward/lifting_object: 170.2590
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.1010
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 2.28s
                      Time elapsed: 00:22:35
                               ETA: 00:11:17

################################################################################
                     [1m Learning iteration 1334/2000 [0m                     

                       Computation: 30040 steps/s (collection: 3.141s, learning 0.132s)
             Mean action noise std: 6.15
          Mean value_function loss: 38.7636
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 25.4168
                       Mean reward: 834.16
               Mean episode length: 247.74
    Episode_Reward/reaching_object: 0.7430
     Episode_Reward/lifting_object: 168.2093
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.1014
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 3.27s
                      Time elapsed: 00:22:39
                               ETA: 00:11:18

################################################################################
                     [1m Learning iteration 1335/2000 [0m                     

                       Computation: 32372 steps/s (collection: 2.925s, learning 0.112s)
             Mean action noise std: 6.16
          Mean value_function loss: 31.1294
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 25.4213
                       Mean reward: 871.80
               Mean episode length: 248.64
    Episode_Reward/reaching_object: 0.7502
     Episode_Reward/lifting_object: 168.6054
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.1015
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 3.04s
                      Time elapsed: 00:22:42
                               ETA: 00:11:17

################################################################################
                     [1m Learning iteration 1336/2000 [0m                     

                       Computation: 29937 steps/s (collection: 3.165s, learning 0.119s)
             Mean action noise std: 6.16
          Mean value_function loss: 33.4107
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 25.4270
                       Mean reward: 862.60
               Mean episode length: 249.97
    Episode_Reward/reaching_object: 0.7532
     Episode_Reward/lifting_object: 170.6221
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.1013
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 3.28s
                      Time elapsed: 00:22:45
                               ETA: 00:11:18

################################################################################
                     [1m Learning iteration 1337/2000 [0m                     

                       Computation: 31946 steps/s (collection: 2.949s, learning 0.128s)
             Mean action noise std: 6.17
          Mean value_function loss: 39.2906
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 25.4331
                       Mean reward: 852.49
               Mean episode length: 248.82
    Episode_Reward/reaching_object: 0.7620
     Episode_Reward/lifting_object: 171.1804
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.1025
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 3.08s
                      Time elapsed: 00:22:48
                               ETA: 00:11:18

################################################################################
                     [1m Learning iteration 1338/2000 [0m                     

                       Computation: 31158 steps/s (collection: 3.034s, learning 0.121s)
             Mean action noise std: 6.18
          Mean value_function loss: 38.4636
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 25.4421
                       Mean reward: 869.21
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 0.7609
     Episode_Reward/lifting_object: 172.2845
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.1022
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 3.15s
                      Time elapsed: 00:22:51
                               ETA: 00:11:18

################################################################################
                     [1m Learning iteration 1339/2000 [0m                     

                       Computation: 32552 steps/s (collection: 2.900s, learning 0.120s)
             Mean action noise std: 6.18
          Mean value_function loss: 35.0335
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 25.4548
                       Mean reward: 852.81
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.7535
     Episode_Reward/lifting_object: 169.1696
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.1026
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 3.02s
                      Time elapsed: 00:22:54
                               ETA: 00:11:18

################################################################################
                     [1m Learning iteration 1340/2000 [0m                     

                       Computation: 30803 steps/s (collection: 3.057s, learning 0.134s)
             Mean action noise std: 6.20
          Mean value_function loss: 29.7286
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 25.4707
                       Mean reward: 860.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 170.6598
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.1030
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 3.19s
                      Time elapsed: 00:22:57
                               ETA: 00:11:18

################################################################################
                     [1m Learning iteration 1341/2000 [0m                     

                       Computation: 22937 steps/s (collection: 4.152s, learning 0.134s)
             Mean action noise std: 6.20
          Mean value_function loss: 31.3167
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 25.4833
                       Mean reward: 857.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 171.8943
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.1041
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 4.29s
                      Time elapsed: 00:23:02
                               ETA: 00:11:18

################################################################################
                     [1m Learning iteration 1342/2000 [0m                     

                       Computation: 99864 steps/s (collection: 0.881s, learning 0.104s)
             Mean action noise std: 6.21
          Mean value_function loss: 32.4981
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 25.4911
                       Mean reward: 861.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7536
     Episode_Reward/lifting_object: 171.2283
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.1033
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 0.98s
                      Time elapsed: 00:23:03
                               ETA: 00:11:17

################################################################################
                     [1m Learning iteration 1343/2000 [0m                     

                       Computation: 102822 steps/s (collection: 0.828s, learning 0.129s)
             Mean action noise std: 6.21
          Mean value_function loss: 35.2093
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 25.4991
                       Mean reward: 874.38
               Mean episode length: 249.30
    Episode_Reward/reaching_object: 0.7630
     Episode_Reward/lifting_object: 171.1802
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.1041
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 0.96s
                      Time elapsed: 00:23:04
                               ETA: 00:11:16

################################################################################
                     [1m Learning iteration 1344/2000 [0m                     

                       Computation: 93460 steps/s (collection: 0.929s, learning 0.123s)
             Mean action noise std: 6.22
          Mean value_function loss: 35.7377
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 25.5055
                       Mean reward: 865.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 172.4155
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.1040
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 1.05s
                      Time elapsed: 00:23:05
                               ETA: 00:11:15

################################################################################
                     [1m Learning iteration 1345/2000 [0m                     

                       Computation: 103544 steps/s (collection: 0.839s, learning 0.110s)
             Mean action noise std: 6.23
          Mean value_function loss: 36.9446
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 25.5126
                       Mean reward: 863.48
               Mean episode length: 249.94
    Episode_Reward/reaching_object: 0.7694
     Episode_Reward/lifting_object: 172.7379
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.1045
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 0.95s
                      Time elapsed: 00:23:06
                               ETA: 00:11:14

################################################################################
                     [1m Learning iteration 1346/2000 [0m                     

                       Computation: 106750 steps/s (collection: 0.819s, learning 0.102s)
             Mean action noise std: 6.24
          Mean value_function loss: 34.0162
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 25.5262
                       Mean reward: 862.72
               Mean episode length: 247.21
    Episode_Reward/reaching_object: 0.7636
     Episode_Reward/lifting_object: 171.4845
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.1042
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 0.92s
                      Time elapsed: 00:23:06
                               ETA: 00:11:13

################################################################################
                     [1m Learning iteration 1347/2000 [0m                     

                       Computation: 109959 steps/s (collection: 0.796s, learning 0.098s)
             Mean action noise std: 6.25
          Mean value_function loss: 36.3261
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 25.5397
                       Mean reward: 847.57
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7591
     Episode_Reward/lifting_object: 171.2671
      Episode_Reward/object_height: 0.0623
        Episode_Reward/action_rate: -0.1048
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 0.89s
                      Time elapsed: 00:23:07
                               ETA: 00:11:12

################################################################################
                     [1m Learning iteration 1348/2000 [0m                     

                       Computation: 109315 steps/s (collection: 0.798s, learning 0.101s)
             Mean action noise std: 6.26
          Mean value_function loss: 33.5312
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 25.5542
                       Mean reward: 868.30
               Mean episode length: 249.20
    Episode_Reward/reaching_object: 0.7666
     Episode_Reward/lifting_object: 171.9654
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.1043
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 0.90s
                      Time elapsed: 00:23:08
                               ETA: 00:11:11

################################################################################
                     [1m Learning iteration 1349/2000 [0m                     

                       Computation: 105829 steps/s (collection: 0.821s, learning 0.108s)
             Mean action noise std: 6.27
          Mean value_function loss: 27.5961
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 25.5680
                       Mean reward: 852.84
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.7536
     Episode_Reward/lifting_object: 169.3087
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.1038
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 0.93s
                      Time elapsed: 00:23:09
                               ETA: 00:11:10

################################################################################
                     [1m Learning iteration 1350/2000 [0m                     

                       Computation: 110058 steps/s (collection: 0.796s, learning 0.098s)
             Mean action noise std: 6.28
          Mean value_function loss: 30.0362
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 25.5839
                       Mean reward: 869.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 172.0713
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.1045
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 0.89s
                      Time elapsed: 00:23:10
                               ETA: 00:11:09

################################################################################
                     [1m Learning iteration 1351/2000 [0m                     

                       Computation: 109435 steps/s (collection: 0.804s, learning 0.095s)
             Mean action noise std: 6.29
          Mean value_function loss: 28.9785
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 25.5985
                       Mean reward: 861.05
               Mean episode length: 248.67
    Episode_Reward/reaching_object: 0.7602
     Episode_Reward/lifting_object: 170.8121
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.1044
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 0.90s
                      Time elapsed: 00:23:11
                               ETA: 00:11:07

################################################################################
                     [1m Learning iteration 1352/2000 [0m                     

                       Computation: 108580 steps/s (collection: 0.809s, learning 0.096s)
             Mean action noise std: 6.30
          Mean value_function loss: 39.1274
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 25.6104
                       Mean reward: 856.60
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 172.0456
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.1053
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 0.91s
                      Time elapsed: 00:23:12
                               ETA: 00:11:06

################################################################################
                     [1m Learning iteration 1353/2000 [0m                     

                       Computation: 112429 steps/s (collection: 0.781s, learning 0.094s)
             Mean action noise std: 6.31
          Mean value_function loss: 38.2552
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 25.6254
                       Mean reward: 829.73
               Mean episode length: 244.08
    Episode_Reward/reaching_object: 0.7534
     Episode_Reward/lifting_object: 169.7205
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.1046
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 0.87s
                      Time elapsed: 00:23:13
                               ETA: 00:11:05

################################################################################
                     [1m Learning iteration 1354/2000 [0m                     

                       Computation: 110719 steps/s (collection: 0.795s, learning 0.093s)
             Mean action noise std: 6.32
          Mean value_function loss: 38.6810
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 25.6369
                       Mean reward: 864.83
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.7630
     Episode_Reward/lifting_object: 171.4822
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.1055
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 0.89s
                      Time elapsed: 00:23:14
                               ETA: 00:11:04

################################################################################
                     [1m Learning iteration 1355/2000 [0m                     

                       Computation: 109195 steps/s (collection: 0.792s, learning 0.109s)
             Mean action noise std: 6.33
          Mean value_function loss: 33.4054
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 25.6489
                       Mean reward: 862.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7684
     Episode_Reward/lifting_object: 172.2324
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.1065
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 0.90s
                      Time elapsed: 00:23:15
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 1356/2000 [0m                     

                       Computation: 107687 steps/s (collection: 0.811s, learning 0.101s)
             Mean action noise std: 6.34
          Mean value_function loss: 41.3008
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 25.6650
                       Mean reward: 854.55
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7490
     Episode_Reward/lifting_object: 169.4425
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.1057
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 0.91s
                      Time elapsed: 00:23:15
                               ETA: 00:11:02

################################################################################
                     [1m Learning iteration 1357/2000 [0m                     

                       Computation: 111068 steps/s (collection: 0.782s, learning 0.103s)
             Mean action noise std: 6.34
          Mean value_function loss: 43.5527
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 25.6688
                       Mean reward: 873.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 172.6086
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.1069
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 0.89s
                      Time elapsed: 00:23:16
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 1358/2000 [0m                     

                       Computation: 106829 steps/s (collection: 0.819s, learning 0.101s)
             Mean action noise std: 6.34
          Mean value_function loss: 31.5403
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 25.6711
                       Mean reward: 860.86
               Mean episode length: 248.98
    Episode_Reward/reaching_object: 0.7578
     Episode_Reward/lifting_object: 170.4411
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.1064
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 0.92s
                      Time elapsed: 00:23:17
                               ETA: 00:11:00

################################################################################
                     [1m Learning iteration 1359/2000 [0m                     

                       Computation: 103937 steps/s (collection: 0.845s, learning 0.101s)
             Mean action noise std: 6.35
          Mean value_function loss: 46.8606
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 25.6777
                       Mean reward: 857.97
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 172.6476
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.1076
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 0.95s
                      Time elapsed: 00:23:18
                               ETA: 00:10:59

################################################################################
                     [1m Learning iteration 1360/2000 [0m                     

                       Computation: 109790 steps/s (collection: 0.786s, learning 0.110s)
             Mean action noise std: 6.36
          Mean value_function loss: 40.7567
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 25.6930
                       Mean reward: 852.67
               Mean episode length: 249.45
    Episode_Reward/reaching_object: 0.7495
     Episode_Reward/lifting_object: 170.3358
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.1078
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 0.90s
                      Time elapsed: 00:23:19
                               ETA: 00:10:58

################################################################################
                     [1m Learning iteration 1361/2000 [0m                     

                       Computation: 100584 steps/s (collection: 0.855s, learning 0.122s)
             Mean action noise std: 6.38
          Mean value_function loss: 46.4990
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 25.7107
                       Mean reward: 846.69
               Mean episode length: 246.59
    Episode_Reward/reaching_object: 0.7526
     Episode_Reward/lifting_object: 169.5002
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.1083
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 0.98s
                      Time elapsed: 00:23:20
                               ETA: 00:10:57

################################################################################
                     [1m Learning iteration 1362/2000 [0m                     

                       Computation: 101634 steps/s (collection: 0.845s, learning 0.122s)
             Mean action noise std: 6.38
          Mean value_function loss: 38.9399
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 25.7263
                       Mean reward: 845.30
               Mean episode length: 248.86
    Episode_Reward/reaching_object: 0.7628
     Episode_Reward/lifting_object: 171.4421
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.1094
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 0.97s
                      Time elapsed: 00:23:21
                               ETA: 00:10:56

################################################################################
                     [1m Learning iteration 1363/2000 [0m                     

                       Computation: 107859 steps/s (collection: 0.811s, learning 0.101s)
             Mean action noise std: 6.39
          Mean value_function loss: 42.2017
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 25.7336
                       Mean reward: 850.26
               Mean episode length: 247.46
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 172.0857
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.1099
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 0.91s
                      Time elapsed: 00:23:22
                               ETA: 00:10:54

################################################################################
                     [1m Learning iteration 1364/2000 [0m                     

                       Computation: 110780 steps/s (collection: 0.795s, learning 0.092s)
             Mean action noise std: 6.40
          Mean value_function loss: 32.5950
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 25.7404
                       Mean reward: 857.99
               Mean episode length: 247.42
    Episode_Reward/reaching_object: 0.7620
     Episode_Reward/lifting_object: 171.0367
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.1093
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 0.89s
                      Time elapsed: 00:23:23
                               ETA: 00:10:53

################################################################################
                     [1m Learning iteration 1365/2000 [0m                     

                       Computation: 113141 steps/s (collection: 0.773s, learning 0.096s)
             Mean action noise std: 6.40
          Mean value_function loss: 34.4708
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 25.7486
                       Mean reward: 851.57
               Mean episode length: 248.63
    Episode_Reward/reaching_object: 0.7548
     Episode_Reward/lifting_object: 170.2484
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.1096
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 0.87s
                      Time elapsed: 00:23:24
                               ETA: 00:10:52

################################################################################
                     [1m Learning iteration 1366/2000 [0m                     

                       Computation: 111109 steps/s (collection: 0.786s, learning 0.099s)
             Mean action noise std: 6.41
          Mean value_function loss: 25.8279
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 25.7587
                       Mean reward: 870.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7674
     Episode_Reward/lifting_object: 172.3586
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.1108
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 0.88s
                      Time elapsed: 00:23:25
                               ETA: 00:10:51

################################################################################
                     [1m Learning iteration 1367/2000 [0m                     

                       Computation: 113608 steps/s (collection: 0.766s, learning 0.100s)
             Mean action noise std: 6.42
          Mean value_function loss: 32.4750
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 25.7711
                       Mean reward: 864.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7612
     Episode_Reward/lifting_object: 171.7359
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.1102
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 0.87s
                      Time elapsed: 00:23:25
                               ETA: 00:10:50

################################################################################
                     [1m Learning iteration 1368/2000 [0m                     

                       Computation: 107685 steps/s (collection: 0.803s, learning 0.110s)
             Mean action noise std: 6.43
          Mean value_function loss: 30.7670
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 25.7830
                       Mean reward: 857.87
               Mean episode length: 249.97
    Episode_Reward/reaching_object: 0.7672
     Episode_Reward/lifting_object: 172.0142
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.1109
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 0.91s
                      Time elapsed: 00:23:26
                               ETA: 00:10:49

################################################################################
                     [1m Learning iteration 1369/2000 [0m                     

                       Computation: 112678 steps/s (collection: 0.784s, learning 0.089s)
             Mean action noise std: 6.43
          Mean value_function loss: 39.3132
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 25.7872
                       Mean reward: 862.95
               Mean episode length: 246.73
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 170.4949
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.1095
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 0.87s
                      Time elapsed: 00:23:27
                               ETA: 00:10:48

################################################################################
                     [1m Learning iteration 1370/2000 [0m                     

                       Computation: 106410 steps/s (collection: 0.814s, learning 0.110s)
             Mean action noise std: 6.43
          Mean value_function loss: 36.3853
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 25.7892
                       Mean reward: 868.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7470
     Episode_Reward/lifting_object: 169.4991
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.1098
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 0.92s
                      Time elapsed: 00:23:28
                               ETA: 00:10:47

################################################################################
                     [1m Learning iteration 1371/2000 [0m                     

                       Computation: 108478 steps/s (collection: 0.807s, learning 0.100s)
             Mean action noise std: 6.45
          Mean value_function loss: 41.2881
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 25.8005
                       Mean reward: 849.85
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.7640
     Episode_Reward/lifting_object: 171.3082
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.1101
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 0.91s
                      Time elapsed: 00:23:29
                               ETA: 00:10:46

################################################################################
                     [1m Learning iteration 1372/2000 [0m                     

                       Computation: 112613 steps/s (collection: 0.777s, learning 0.096s)
             Mean action noise std: 6.46
          Mean value_function loss: 41.7621
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 25.8197
                       Mean reward: 868.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7666
     Episode_Reward/lifting_object: 172.7679
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.1095
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 0.87s
                      Time elapsed: 00:23:30
                               ETA: 00:10:45

################################################################################
                     [1m Learning iteration 1373/2000 [0m                     

                       Computation: 106985 steps/s (collection: 0.822s, learning 0.097s)
             Mean action noise std: 6.46
          Mean value_function loss: 41.2281
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 25.8301
                       Mean reward: 855.76
               Mean episode length: 248.45
    Episode_Reward/reaching_object: 0.7493
     Episode_Reward/lifting_object: 169.3017
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.1088
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 0.92s
                      Time elapsed: 00:23:31
                               ETA: 00:10:44

################################################################################
                     [1m Learning iteration 1374/2000 [0m                     

                       Computation: 112493 steps/s (collection: 0.784s, learning 0.090s)
             Mean action noise std: 6.47
          Mean value_function loss: 40.4548
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 25.8359
                       Mean reward: 821.10
               Mean episode length: 248.17
    Episode_Reward/reaching_object: 0.7546
     Episode_Reward/lifting_object: 169.9272
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.1098
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 0.87s
                      Time elapsed: 00:23:32
                               ETA: 00:10:42

################################################################################
                     [1m Learning iteration 1375/2000 [0m                     

                       Computation: 111541 steps/s (collection: 0.793s, learning 0.088s)
             Mean action noise std: 6.48
          Mean value_function loss: 45.6317
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 25.8457
                       Mean reward: 853.78
               Mean episode length: 246.16
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 171.3782
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.1093
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 0.88s
                      Time elapsed: 00:23:33
                               ETA: 00:10:41

################################################################################
                     [1m Learning iteration 1376/2000 [0m                     

                       Computation: 115082 steps/s (collection: 0.764s, learning 0.090s)
             Mean action noise std: 6.49
          Mean value_function loss: 41.9132
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 25.8600
                       Mean reward: 860.37
               Mean episode length: 248.56
    Episode_Reward/reaching_object: 0.7546
     Episode_Reward/lifting_object: 170.4667
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.1090
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 0.85s
                      Time elapsed: 00:23:33
                               ETA: 00:10:40

################################################################################
                     [1m Learning iteration 1377/2000 [0m                     

                       Computation: 113535 steps/s (collection: 0.773s, learning 0.093s)
             Mean action noise std: 6.50
          Mean value_function loss: 41.5768
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 25.8758
                       Mean reward: 848.42
               Mean episode length: 249.85
    Episode_Reward/reaching_object: 0.7556
     Episode_Reward/lifting_object: 170.1504
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.1108
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 0.87s
                      Time elapsed: 00:23:34
                               ETA: 00:10:39

################################################################################
                     [1m Learning iteration 1378/2000 [0m                     

                       Computation: 111816 steps/s (collection: 0.768s, learning 0.111s)
             Mean action noise std: 6.51
          Mean value_function loss: 49.7987
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 25.8859
                       Mean reward: 861.88
               Mean episode length: 248.63
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 171.1441
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.1106
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 0.88s
                      Time elapsed: 00:23:35
                               ETA: 00:10:38

################################################################################
                     [1m Learning iteration 1379/2000 [0m                     

                       Computation: 113416 steps/s (collection: 0.765s, learning 0.102s)
             Mean action noise std: 6.52
          Mean value_function loss: 47.8358
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 25.8949
                       Mean reward: 854.97
               Mean episode length: 246.75
    Episode_Reward/reaching_object: 0.7575
     Episode_Reward/lifting_object: 171.1631
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.1104
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 0.87s
                      Time elapsed: 00:23:36
                               ETA: 00:10:37

################################################################################
                     [1m Learning iteration 1380/2000 [0m                     

                       Computation: 117649 steps/s (collection: 0.745s, learning 0.091s)
             Mean action noise std: 6.52
          Mean value_function loss: 45.8264
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 25.9065
                       Mean reward: 872.69
               Mean episode length: 249.98
    Episode_Reward/reaching_object: 0.7509
     Episode_Reward/lifting_object: 170.2581
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.1107
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 0.84s
                      Time elapsed: 00:23:37
                               ETA: 00:10:36

################################################################################
                     [1m Learning iteration 1381/2000 [0m                     

                       Computation: 113054 steps/s (collection: 0.770s, learning 0.100s)
             Mean action noise std: 6.53
          Mean value_function loss: 38.9457
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 25.9121
                       Mean reward: 872.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7611
     Episode_Reward/lifting_object: 173.0945
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.1107
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 0.87s
                      Time elapsed: 00:23:38
                               ETA: 00:10:35

################################################################################
                     [1m Learning iteration 1382/2000 [0m                     

                       Computation: 111534 steps/s (collection: 0.767s, learning 0.114s)
             Mean action noise std: 6.54
          Mean value_function loss: 42.6536
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 25.9231
                       Mean reward: 845.03
               Mean episode length: 249.56
    Episode_Reward/reaching_object: 0.7447
     Episode_Reward/lifting_object: 168.5044
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.1101
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 0.88s
                      Time elapsed: 00:23:39
                               ETA: 00:10:34

################################################################################
                     [1m Learning iteration 1383/2000 [0m                     

                       Computation: 109342 steps/s (collection: 0.801s, learning 0.098s)
             Mean action noise std: 6.54
          Mean value_function loss: 43.9379
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 25.9348
                       Mean reward: 857.47
               Mean episode length: 249.35
    Episode_Reward/reaching_object: 0.7490
     Episode_Reward/lifting_object: 169.2755
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.1099
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 0.90s
                      Time elapsed: 00:23:40
                               ETA: 00:10:33

################################################################################
                     [1m Learning iteration 1384/2000 [0m                     

                       Computation: 112551 steps/s (collection: 0.781s, learning 0.093s)
             Mean action noise std: 6.55
          Mean value_function loss: 40.6113
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 25.9399
                       Mean reward: 849.48
               Mean episode length: 246.82
    Episode_Reward/reaching_object: 0.7432
     Episode_Reward/lifting_object: 169.2375
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.1097
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 0.87s
                      Time elapsed: 00:23:40
                               ETA: 00:10:31

################################################################################
                     [1m Learning iteration 1385/2000 [0m                     

                       Computation: 111160 steps/s (collection: 0.791s, learning 0.094s)
             Mean action noise std: 6.57
          Mean value_function loss: 40.9387
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 25.9528
                       Mean reward: 848.78
               Mean episode length: 247.24
    Episode_Reward/reaching_object: 0.7436
     Episode_Reward/lifting_object: 168.9624
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.1102
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 0.88s
                      Time elapsed: 00:23:41
                               ETA: 00:10:30

################################################################################
                     [1m Learning iteration 1386/2000 [0m                     

                       Computation: 114753 steps/s (collection: 0.769s, learning 0.088s)
             Mean action noise std: 6.57
          Mean value_function loss: 54.8280
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 25.9662
                       Mean reward: 853.07
               Mean episode length: 249.06
    Episode_Reward/reaching_object: 0.7505
     Episode_Reward/lifting_object: 171.1879
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.1109
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 0.86s
                      Time elapsed: 00:23:42
                               ETA: 00:10:29

################################################################################
                     [1m Learning iteration 1387/2000 [0m                     

                       Computation: 115454 steps/s (collection: 0.740s, learning 0.112s)
             Mean action noise std: 6.58
          Mean value_function loss: 39.0698
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 25.9746
                       Mean reward: 856.28
               Mean episode length: 249.37
    Episode_Reward/reaching_object: 0.7465
     Episode_Reward/lifting_object: 170.0663
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.1110
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 0.85s
                      Time elapsed: 00:23:43
                               ETA: 00:10:28

################################################################################
                     [1m Learning iteration 1388/2000 [0m                     

                       Computation: 113920 steps/s (collection: 0.767s, learning 0.096s)
             Mean action noise std: 6.59
          Mean value_function loss: 36.8588
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 25.9919
                       Mean reward: 853.45
               Mean episode length: 246.36
    Episode_Reward/reaching_object: 0.7480
     Episode_Reward/lifting_object: 169.5652
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.1101
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 0.86s
                      Time elapsed: 00:23:44
                               ETA: 00:10:27

################################################################################
                     [1m Learning iteration 1389/2000 [0m                     

                       Computation: 114928 steps/s (collection: 0.759s, learning 0.096s)
             Mean action noise std: 6.60
          Mean value_function loss: 36.1715
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 26.0043
                       Mean reward: 870.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 172.2412
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.1108
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 0.86s
                      Time elapsed: 00:23:45
                               ETA: 00:10:26

################################################################################
                     [1m Learning iteration 1390/2000 [0m                     

                       Computation: 108323 steps/s (collection: 0.804s, learning 0.103s)
             Mean action noise std: 6.61
          Mean value_function loss: 36.6737
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 26.0125
                       Mean reward: 876.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7634
     Episode_Reward/lifting_object: 172.7814
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.1111
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 0.91s
                      Time elapsed: 00:23:46
                               ETA: 00:10:25

################################################################################
                     [1m Learning iteration 1391/2000 [0m                     

                       Computation: 106422 steps/s (collection: 0.829s, learning 0.095s)
             Mean action noise std: 6.62
          Mean value_function loss: 37.5070
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 26.0223
                       Mean reward: 865.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7470
     Episode_Reward/lifting_object: 169.3107
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.1107
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 0.92s
                      Time elapsed: 00:23:47
                               ETA: 00:10:24

################################################################################
                     [1m Learning iteration 1392/2000 [0m                     

                       Computation: 108551 steps/s (collection: 0.812s, learning 0.093s)
             Mean action noise std: 6.62
          Mean value_function loss: 42.8538
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 26.0255
                       Mean reward: 851.51
               Mean episode length: 247.51
    Episode_Reward/reaching_object: 0.7503
     Episode_Reward/lifting_object: 168.3310
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.1107
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 0.91s
                      Time elapsed: 00:23:48
                               ETA: 00:10:23

################################################################################
                     [1m Learning iteration 1393/2000 [0m                     

                       Computation: 104576 steps/s (collection: 0.844s, learning 0.096s)
             Mean action noise std: 6.62
          Mean value_function loss: 41.0447
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 26.0281
                       Mean reward: 856.21
               Mean episode length: 249.66
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 171.4243
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.1114
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 0.94s
                      Time elapsed: 00:23:48
                               ETA: 00:10:22

################################################################################
                     [1m Learning iteration 1394/2000 [0m                     

                       Computation: 109383 steps/s (collection: 0.799s, learning 0.100s)
             Mean action noise std: 6.62
          Mean value_function loss: 35.5917
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 26.0327
                       Mean reward: 871.38
               Mean episode length: 249.26
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 172.0264
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.1113
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 0.90s
                      Time elapsed: 00:23:49
                               ETA: 00:10:21

################################################################################
                     [1m Learning iteration 1395/2000 [0m                     

                       Computation: 107860 steps/s (collection: 0.801s, learning 0.110s)
             Mean action noise std: 6.63
          Mean value_function loss: 36.1915
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 26.0390
                       Mean reward: 860.40
               Mean episode length: 248.91
    Episode_Reward/reaching_object: 0.7672
     Episode_Reward/lifting_object: 172.4514
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.1113
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 0.91s
                      Time elapsed: 00:23:50
                               ETA: 00:10:20

################################################################################
                     [1m Learning iteration 1396/2000 [0m                     

                       Computation: 114120 steps/s (collection: 0.761s, learning 0.101s)
             Mean action noise std: 6.64
          Mean value_function loss: 47.5923
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 26.0469
                       Mean reward: 843.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 171.4807
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.1116
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 0.86s
                      Time elapsed: 00:23:51
                               ETA: 00:10:18

################################################################################
                     [1m Learning iteration 1397/2000 [0m                     

                       Computation: 115133 steps/s (collection: 0.756s, learning 0.098s)
             Mean action noise std: 6.64
          Mean value_function loss: 36.8752
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 26.0544
                       Mean reward: 845.00
               Mean episode length: 247.83
    Episode_Reward/reaching_object: 0.7567
     Episode_Reward/lifting_object: 169.6083
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.1115
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 0.85s
                      Time elapsed: 00:23:52
                               ETA: 00:10:17

################################################################################
                     [1m Learning iteration 1398/2000 [0m                     

                       Computation: 110740 steps/s (collection: 0.781s, learning 0.107s)
             Mean action noise std: 6.65
          Mean value_function loss: 45.9862
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 26.0652
                       Mean reward: 843.59
               Mean episode length: 246.00
    Episode_Reward/reaching_object: 0.7525
     Episode_Reward/lifting_object: 169.3917
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.1122
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 0.89s
                      Time elapsed: 00:23:53
                               ETA: 00:10:16

################################################################################
                     [1m Learning iteration 1399/2000 [0m                     

                       Computation: 113539 steps/s (collection: 0.764s, learning 0.102s)
             Mean action noise std: 6.66
          Mean value_function loss: 44.6321
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 26.0737
                       Mean reward: 859.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 171.6359
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.1137
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 0.87s
                      Time elapsed: 00:23:54
                               ETA: 00:10:15

################################################################################
                     [1m Learning iteration 1400/2000 [0m                     

                       Computation: 111204 steps/s (collection: 0.784s, learning 0.100s)
             Mean action noise std: 6.67
          Mean value_function loss: 40.3017
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 26.0817
                       Mean reward: 862.22
               Mean episode length: 249.86
    Episode_Reward/reaching_object: 0.7663
     Episode_Reward/lifting_object: 171.8508
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.1132
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 0.88s
                      Time elapsed: 00:23:55
                               ETA: 00:10:14

################################################################################
                     [1m Learning iteration 1401/2000 [0m                     

                       Computation: 113319 steps/s (collection: 0.762s, learning 0.106s)
             Mean action noise std: 6.67
          Mean value_function loss: 51.3090
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 26.0884
                       Mean reward: 861.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 171.5440
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.1131
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 0.87s
                      Time elapsed: 00:23:55
                               ETA: 00:10:13

################################################################################
                     [1m Learning iteration 1402/2000 [0m                     

                       Computation: 114174 steps/s (collection: 0.757s, learning 0.104s)
             Mean action noise std: 6.68
          Mean value_function loss: 40.1708
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 26.0968
                       Mean reward: 849.56
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.7543
     Episode_Reward/lifting_object: 169.9480
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.1134
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 0.86s
                      Time elapsed: 00:23:56
                               ETA: 00:10:12

################################################################################
                     [1m Learning iteration 1403/2000 [0m                     

                       Computation: 114882 steps/s (collection: 0.762s, learning 0.094s)
             Mean action noise std: 6.70
          Mean value_function loss: 40.5680
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 26.1110
                       Mean reward: 859.00
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 170.3671
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.1132
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 0.86s
                      Time elapsed: 00:23:57
                               ETA: 00:10:11

################################################################################
                     [1m Learning iteration 1404/2000 [0m                     

                       Computation: 115224 steps/s (collection: 0.763s, learning 0.090s)
             Mean action noise std: 6.70
          Mean value_function loss: 35.2492
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 26.1235
                       Mean reward: 842.40
               Mean episode length: 246.02
    Episode_Reward/reaching_object: 0.7514
     Episode_Reward/lifting_object: 169.8449
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.1137
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 0.85s
                      Time elapsed: 00:23:58
                               ETA: 00:10:10

################################################################################
                     [1m Learning iteration 1405/2000 [0m                     

                       Computation: 113980 steps/s (collection: 0.769s, learning 0.093s)
             Mean action noise std: 6.72
          Mean value_function loss: 39.0651
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 26.1371
                       Mean reward: 844.71
               Mean episode length: 249.67
    Episode_Reward/reaching_object: 0.7541
     Episode_Reward/lifting_object: 170.1681
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.1142
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 0.86s
                      Time elapsed: 00:23:59
                               ETA: 00:10:09

################################################################################
                     [1m Learning iteration 1406/2000 [0m                     

                       Computation: 113095 steps/s (collection: 0.780s, learning 0.089s)
             Mean action noise std: 6.73
          Mean value_function loss: 36.5427
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 26.1538
                       Mean reward: 854.37
               Mean episode length: 247.50
    Episode_Reward/reaching_object: 0.7558
     Episode_Reward/lifting_object: 171.3874
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.1133
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 0.87s
                      Time elapsed: 00:24:00
                               ETA: 00:10:08

################################################################################
                     [1m Learning iteration 1407/2000 [0m                     

                       Computation: 106868 steps/s (collection: 0.832s, learning 0.088s)
             Mean action noise std: 6.74
          Mean value_function loss: 55.3661
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 26.1680
                       Mean reward: 849.36
               Mean episode length: 249.89
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 170.8147
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.1143
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 0.92s
                      Time elapsed: 00:24:01
                               ETA: 00:10:06

################################################################################
                     [1m Learning iteration 1408/2000 [0m                     

                       Computation: 108762 steps/s (collection: 0.805s, learning 0.099s)
             Mean action noise std: 6.74
          Mean value_function loss: 53.8827
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 26.1820
                       Mean reward: 872.37
               Mean episode length: 249.33
    Episode_Reward/reaching_object: 0.7519
     Episode_Reward/lifting_object: 171.3362
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.1147
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 0.90s
                      Time elapsed: 00:24:02
                               ETA: 00:10:05

################################################################################
                     [1m Learning iteration 1409/2000 [0m                     

                       Computation: 111861 steps/s (collection: 0.795s, learning 0.084s)
             Mean action noise std: 6.75
          Mean value_function loss: 61.9600
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 26.1912
                       Mean reward: 814.74
               Mean episode length: 242.77
    Episode_Reward/reaching_object: 0.7445
     Episode_Reward/lifting_object: 168.4486
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.1133
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 0.88s
                      Time elapsed: 00:24:02
                               ETA: 00:10:04

################################################################################
                     [1m Learning iteration 1410/2000 [0m                     

                       Computation: 110113 steps/s (collection: 0.778s, learning 0.114s)
             Mean action noise std: 6.76
          Mean value_function loss: 50.1856
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 26.2013
                       Mean reward: 853.90
               Mean episode length: 246.55
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 170.9907
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.1143
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 0.89s
                      Time elapsed: 00:24:03
                               ETA: 00:10:03

################################################################################
                     [1m Learning iteration 1411/2000 [0m                     

                       Computation: 108093 steps/s (collection: 0.804s, learning 0.105s)
             Mean action noise std: 6.76
          Mean value_function loss: 48.3803
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 26.2027
                       Mean reward: 851.39
               Mean episode length: 249.62
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 171.3303
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.1151
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 0.91s
                      Time elapsed: 00:24:04
                               ETA: 00:10:02

################################################################################
                     [1m Learning iteration 1412/2000 [0m                     

                       Computation: 111756 steps/s (collection: 0.776s, learning 0.104s)
             Mean action noise std: 6.77
          Mean value_function loss: 62.6071
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 26.2111
                       Mean reward: 857.53
               Mean episode length: 249.18
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 171.7151
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.1152
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 0.88s
                      Time elapsed: 00:24:05
                               ETA: 00:10:01

################################################################################
                     [1m Learning iteration 1413/2000 [0m                     

                       Computation: 111728 steps/s (collection: 0.776s, learning 0.104s)
             Mean action noise std: 6.78
          Mean value_function loss: 50.4175
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 26.2225
                       Mean reward: 841.02
               Mean episode length: 249.94
    Episode_Reward/reaching_object: 0.7500
     Episode_Reward/lifting_object: 168.9109
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.1162
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 0.88s
                      Time elapsed: 00:24:06
                               ETA: 00:10:00

################################################################################
                     [1m Learning iteration 1414/2000 [0m                     

                       Computation: 107093 steps/s (collection: 0.817s, learning 0.101s)
             Mean action noise std: 6.80
          Mean value_function loss: 60.4648
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 26.2392
                       Mean reward: 855.69
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.7612
     Episode_Reward/lifting_object: 171.3278
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.1164
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 0.92s
                      Time elapsed: 00:24:07
                               ETA: 00:09:59

################################################################################
                     [1m Learning iteration 1415/2000 [0m                     

                       Computation: 110943 steps/s (collection: 0.781s, learning 0.106s)
             Mean action noise std: 6.80
          Mean value_function loss: 41.8507
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 26.2541
                       Mean reward: 846.87
               Mean episode length: 248.62
    Episode_Reward/reaching_object: 0.7403
     Episode_Reward/lifting_object: 166.8301
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.1159
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 0.89s
                      Time elapsed: 00:24:08
                               ETA: 00:09:58

################################################################################
                     [1m Learning iteration 1416/2000 [0m                     

                       Computation: 107667 steps/s (collection: 0.809s, learning 0.105s)
             Mean action noise std: 6.81
          Mean value_function loss: 55.7068
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 26.2617
                       Mean reward: 838.18
               Mean episode length: 249.82
    Episode_Reward/reaching_object: 0.7427
     Episode_Reward/lifting_object: 166.7197
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.1169
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 0.91s
                      Time elapsed: 00:24:09
                               ETA: 00:09:57

################################################################################
                     [1m Learning iteration 1417/2000 [0m                     

                       Computation: 104770 steps/s (collection: 0.814s, learning 0.124s)
             Mean action noise std: 6.82
          Mean value_function loss: 49.6872
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 26.2728
                       Mean reward: 851.35
               Mean episode length: 249.82
    Episode_Reward/reaching_object: 0.7277
     Episode_Reward/lifting_object: 164.4379
      Episode_Reward/object_height: 0.0615
        Episode_Reward/action_rate: -0.1156
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 0.94s
                      Time elapsed: 00:24:10
                               ETA: 00:09:56

################################################################################
                     [1m Learning iteration 1418/2000 [0m                     

                       Computation: 111271 steps/s (collection: 0.788s, learning 0.096s)
             Mean action noise std: 6.83
          Mean value_function loss: 55.3301
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 26.2823
                       Mean reward: 843.99
               Mean episode length: 249.25
    Episode_Reward/reaching_object: 0.7449
     Episode_Reward/lifting_object: 168.0568
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.1178
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 0.88s
                      Time elapsed: 00:24:11
                               ETA: 00:09:55

################################################################################
                     [1m Learning iteration 1419/2000 [0m                     

                       Computation: 107803 steps/s (collection: 0.822s, learning 0.090s)
             Mean action noise std: 6.84
          Mean value_function loss: 57.0918
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 26.2967
                       Mean reward: 840.61
               Mean episode length: 248.88
    Episode_Reward/reaching_object: 0.7518
     Episode_Reward/lifting_object: 169.6239
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.1175
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 0.91s
                      Time elapsed: 00:24:11
                               ETA: 00:09:54

################################################################################
                     [1m Learning iteration 1420/2000 [0m                     

                       Computation: 111655 steps/s (collection: 0.791s, learning 0.090s)
             Mean action noise std: 6.85
          Mean value_function loss: 54.4843
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 26.3099
                       Mean reward: 864.25
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 170.9494
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.1181
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 0.88s
                      Time elapsed: 00:24:12
                               ETA: 00:09:53

################################################################################
                     [1m Learning iteration 1421/2000 [0m                     

                       Computation: 109318 steps/s (collection: 0.793s, learning 0.107s)
             Mean action noise std: 6.85
          Mean value_function loss: 42.2392
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 26.3207
                       Mean reward: 863.38
               Mean episode length: 249.95
    Episode_Reward/reaching_object: 0.7508
     Episode_Reward/lifting_object: 169.6630
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.1182
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 0.90s
                      Time elapsed: 00:24:13
                               ETA: 00:09:51

################################################################################
                     [1m Learning iteration 1422/2000 [0m                     

                       Computation: 112428 steps/s (collection: 0.772s, learning 0.102s)
             Mean action noise std: 6.87
          Mean value_function loss: 44.9055
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 26.3292
                       Mean reward: 835.30
               Mean episode length: 247.09
    Episode_Reward/reaching_object: 0.7468
     Episode_Reward/lifting_object: 168.7283
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.1181
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 0.87s
                      Time elapsed: 00:24:14
                               ETA: 00:09:50

################################################################################
                     [1m Learning iteration 1423/2000 [0m                     

                       Computation: 109727 steps/s (collection: 0.802s, learning 0.094s)
             Mean action noise std: 6.88
          Mean value_function loss: 37.9917
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 26.3444
                       Mean reward: 834.66
               Mean episode length: 247.16
    Episode_Reward/reaching_object: 0.7389
     Episode_Reward/lifting_object: 167.3446
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.1181
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 0.90s
                      Time elapsed: 00:24:15
                               ETA: 00:09:49

################################################################################
                     [1m Learning iteration 1424/2000 [0m                     

                       Computation: 111686 steps/s (collection: 0.775s, learning 0.106s)
             Mean action noise std: 6.89
          Mean value_function loss: 50.7092
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 26.3532
                       Mean reward: 848.47
               Mean episode length: 248.58
    Episode_Reward/reaching_object: 0.7518
     Episode_Reward/lifting_object: 171.1909
      Episode_Reward/object_height: 0.0648
        Episode_Reward/action_rate: -0.1193
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 0.88s
                      Time elapsed: 00:24:16
                               ETA: 00:09:48

################################################################################
                     [1m Learning iteration 1425/2000 [0m                     

                       Computation: 108938 steps/s (collection: 0.788s, learning 0.115s)
             Mean action noise std: 6.89
          Mean value_function loss: 35.0615
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 26.3620
                       Mean reward: 852.59
               Mean episode length: 248.52
    Episode_Reward/reaching_object: 0.7440
     Episode_Reward/lifting_object: 168.4068
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.1189
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 0.90s
                      Time elapsed: 00:24:17
                               ETA: 00:09:47

################################################################################
                     [1m Learning iteration 1426/2000 [0m                     

                       Computation: 110313 steps/s (collection: 0.776s, learning 0.115s)
             Mean action noise std: 6.90
          Mean value_function loss: 42.6500
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 26.3739
                       Mean reward: 871.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7536
     Episode_Reward/lifting_object: 171.3296
      Episode_Reward/object_height: 0.0648
        Episode_Reward/action_rate: -0.1192
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 0.89s
                      Time elapsed: 00:24:18
                               ETA: 00:09:46

################################################################################
                     [1m Learning iteration 1427/2000 [0m                     

                       Computation: 108520 steps/s (collection: 0.811s, learning 0.095s)
             Mean action noise std: 6.91
          Mean value_function loss: 48.7593
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 26.3873
                       Mean reward: 845.70
               Mean episode length: 247.42
    Episode_Reward/reaching_object: 0.7482
     Episode_Reward/lifting_object: 169.3514
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.1194
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 0.91s
                      Time elapsed: 00:24:19
                               ETA: 00:09:45

################################################################################
                     [1m Learning iteration 1428/2000 [0m                     

                       Computation: 112014 steps/s (collection: 0.793s, learning 0.085s)
             Mean action noise std: 6.92
          Mean value_function loss: 46.3086
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 26.3959
                       Mean reward: 846.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7516
     Episode_Reward/lifting_object: 170.3355
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.1200
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 0.88s
                      Time elapsed: 00:24:20
                               ETA: 00:09:44

################################################################################
                     [1m Learning iteration 1429/2000 [0m                     

                       Computation: 106788 steps/s (collection: 0.828s, learning 0.093s)
             Mean action noise std: 6.93
          Mean value_function loss: 61.9845
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 26.4093
                       Mean reward: 843.76
               Mean episode length: 249.42
    Episode_Reward/reaching_object: 0.7516
     Episode_Reward/lifting_object: 169.1693
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.1190
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 0.92s
                      Time elapsed: 00:24:20
                               ETA: 00:09:43

################################################################################
                     [1m Learning iteration 1430/2000 [0m                     

                       Computation: 109029 steps/s (collection: 0.815s, learning 0.087s)
             Mean action noise std: 6.94
          Mean value_function loss: 41.1987
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 26.4158
                       Mean reward: 838.70
               Mean episode length: 244.95
    Episode_Reward/reaching_object: 0.7480
     Episode_Reward/lifting_object: 169.8955
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.1191
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 0.90s
                      Time elapsed: 00:24:21
                               ETA: 00:09:42

################################################################################
                     [1m Learning iteration 1431/2000 [0m                     

                       Computation: 106812 steps/s (collection: 0.796s, learning 0.124s)
             Mean action noise std: 6.95
          Mean value_function loss: 40.1152
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 26.4284
                       Mean reward: 844.94
               Mean episode length: 246.96
    Episode_Reward/reaching_object: 0.7524
     Episode_Reward/lifting_object: 169.5065
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.1192
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 0.92s
                      Time elapsed: 00:24:22
                               ETA: 00:09:41

################################################################################
                     [1m Learning iteration 1432/2000 [0m                     

                       Computation: 110060 steps/s (collection: 0.800s, learning 0.093s)
             Mean action noise std: 6.97
          Mean value_function loss: 41.2529
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 26.4491
                       Mean reward: 861.05
               Mean episode length: 249.01
    Episode_Reward/reaching_object: 0.7612
     Episode_Reward/lifting_object: 171.5625
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.1189
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 0.89s
                      Time elapsed: 00:24:23
                               ETA: 00:09:40

################################################################################
                     [1m Learning iteration 1433/2000 [0m                     

                       Computation: 111609 steps/s (collection: 0.756s, learning 0.125s)
             Mean action noise std: 6.98
          Mean value_function loss: 45.2711
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 26.4608
                       Mean reward: 862.21
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7586
     Episode_Reward/lifting_object: 171.0513
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.1198
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 0.88s
                      Time elapsed: 00:24:24
                               ETA: 00:09:39

################################################################################
                     [1m Learning iteration 1434/2000 [0m                     

                       Computation: 109941 steps/s (collection: 0.787s, learning 0.108s)
             Mean action noise std: 6.99
          Mean value_function loss: 44.4913
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 26.4734
                       Mean reward: 872.74
               Mean episode length: 249.55
    Episode_Reward/reaching_object: 0.7584
     Episode_Reward/lifting_object: 171.6086
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.1202
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 0.89s
                      Time elapsed: 00:24:25
                               ETA: 00:09:37

################################################################################
                     [1m Learning iteration 1435/2000 [0m                     

                       Computation: 110851 steps/s (collection: 0.795s, learning 0.092s)
             Mean action noise std: 6.99
          Mean value_function loss: 49.7259
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 26.4828
                       Mean reward: 847.45
               Mean episode length: 248.81
    Episode_Reward/reaching_object: 0.7609
     Episode_Reward/lifting_object: 171.4369
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.1202
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 0.89s
                      Time elapsed: 00:24:26
                               ETA: 00:09:36

################################################################################
                     [1m Learning iteration 1436/2000 [0m                     

                       Computation: 111425 steps/s (collection: 0.794s, learning 0.089s)
             Mean action noise std: 7.00
          Mean value_function loss: 42.6766
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 26.4935
                       Mean reward: 830.26
               Mean episode length: 248.17
    Episode_Reward/reaching_object: 0.7535
     Episode_Reward/lifting_object: 170.2255
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.1203
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 0.88s
                      Time elapsed: 00:24:27
                               ETA: 00:09:35

################################################################################
                     [1m Learning iteration 1437/2000 [0m                     

                       Computation: 111717 steps/s (collection: 0.786s, learning 0.094s)
             Mean action noise std: 7.01
          Mean value_function loss: 53.8197
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 26.5080
                       Mean reward: 835.65
               Mean episode length: 242.19
    Episode_Reward/reaching_object: 0.7571
     Episode_Reward/lifting_object: 171.7902
      Episode_Reward/object_height: 0.0651
        Episode_Reward/action_rate: -0.1195
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 0.88s
                      Time elapsed: 00:24:28
                               ETA: 00:09:34

################################################################################
                     [1m Learning iteration 1438/2000 [0m                     

                       Computation: 110319 steps/s (collection: 0.783s, learning 0.108s)
             Mean action noise std: 7.02
          Mean value_function loss: 36.9750
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 26.5177
                       Mean reward: 848.32
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.7429
     Episode_Reward/lifting_object: 166.9683
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.1207
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 0.89s
                      Time elapsed: 00:24:28
                               ETA: 00:09:33

################################################################################
                     [1m Learning iteration 1439/2000 [0m                     

                       Computation: 110886 steps/s (collection: 0.785s, learning 0.101s)
             Mean action noise std: 7.03
          Mean value_function loss: 41.2475
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 26.5270
                       Mean reward: 857.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 170.1888
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.1211
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 0.89s
                      Time elapsed: 00:24:29
                               ETA: 00:09:32

################################################################################
                     [1m Learning iteration 1440/2000 [0m                     

                       Computation: 111356 steps/s (collection: 0.781s, learning 0.102s)
             Mean action noise std: 7.04
          Mean value_function loss: 42.7285
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 26.5377
                       Mean reward: 844.76
               Mean episode length: 248.81
    Episode_Reward/reaching_object: 0.7584
     Episode_Reward/lifting_object: 170.3225
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.1213
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 0.88s
                      Time elapsed: 00:24:30
                               ETA: 00:09:31

################################################################################
                     [1m Learning iteration 1441/2000 [0m                     

                       Computation: 107988 steps/s (collection: 0.815s, learning 0.095s)
             Mean action noise std: 7.05
          Mean value_function loss: 49.4306
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 26.5479
                       Mean reward: 853.83
               Mean episode length: 249.49
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 170.0225
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.1215
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 0.91s
                      Time elapsed: 00:24:31
                               ETA: 00:09:30

################################################################################
                     [1m Learning iteration 1442/2000 [0m                     

                       Computation: 107313 steps/s (collection: 0.809s, learning 0.107s)
             Mean action noise std: 7.06
          Mean value_function loss: 43.4501
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 26.5587
                       Mean reward: 861.85
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.7543
     Episode_Reward/lifting_object: 170.7504
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.1224
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 0.92s
                      Time elapsed: 00:24:32
                               ETA: 00:09:29

################################################################################
                     [1m Learning iteration 1443/2000 [0m                     

                       Computation: 117148 steps/s (collection: 0.741s, learning 0.098s)
             Mean action noise std: 7.07
          Mean value_function loss: 35.9753
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 26.5691
                       Mean reward: 864.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7526
     Episode_Reward/lifting_object: 169.7792
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.1232
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 0.84s
                      Time elapsed: 00:24:33
                               ETA: 00:09:28

################################################################################
                     [1m Learning iteration 1444/2000 [0m                     

                       Computation: 107738 steps/s (collection: 0.811s, learning 0.101s)
             Mean action noise std: 7.08
          Mean value_function loss: 44.1604
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 26.5826
                       Mean reward: 858.61
               Mean episode length: 247.57
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 169.5695
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.1243
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 0.91s
                      Time elapsed: 00:24:34
                               ETA: 00:09:27

################################################################################
                     [1m Learning iteration 1445/2000 [0m                     

                       Computation: 115536 steps/s (collection: 0.765s, learning 0.086s)
             Mean action noise std: 7.08
          Mean value_function loss: 43.0631
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 26.5899
                       Mean reward: 841.79
               Mean episode length: 249.05
    Episode_Reward/reaching_object: 0.7596
     Episode_Reward/lifting_object: 169.9471
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.1251
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 0.85s
                      Time elapsed: 00:24:35
                               ETA: 00:09:26

################################################################################
                     [1m Learning iteration 1446/2000 [0m                     

                       Computation: 113418 steps/s (collection: 0.779s, learning 0.088s)
             Mean action noise std: 7.09
          Mean value_function loss: 36.1257
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 26.5932
                       Mean reward: 858.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7532
     Episode_Reward/lifting_object: 169.2681
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.1252
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 0.87s
                      Time elapsed: 00:24:36
                               ETA: 00:09:25

################################################################################
                     [1m Learning iteration 1447/2000 [0m                     

                       Computation: 110780 steps/s (collection: 0.792s, learning 0.095s)
             Mean action noise std: 7.09
          Mean value_function loss: 46.2270
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 26.6002
                       Mean reward: 858.86
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.7624
     Episode_Reward/lifting_object: 170.8611
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.1261
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 0.89s
                      Time elapsed: 00:24:36
                               ETA: 00:09:24

################################################################################
                     [1m Learning iteration 1448/2000 [0m                     

                       Computation: 111811 steps/s (collection: 0.770s, learning 0.110s)
             Mean action noise std: 7.11
          Mean value_function loss: 48.8148
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 26.6119
                       Mean reward: 859.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7556
     Episode_Reward/lifting_object: 169.8672
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.1270
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 0.88s
                      Time elapsed: 00:24:37
                               ETA: 00:09:22

################################################################################
                     [1m Learning iteration 1449/2000 [0m                     

                       Computation: 109556 steps/s (collection: 0.791s, learning 0.106s)
             Mean action noise std: 7.11
          Mean value_function loss: 53.0087
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 26.6248
                       Mean reward: 858.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7591
     Episode_Reward/lifting_object: 169.8375
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.1261
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 0.90s
                      Time elapsed: 00:24:38
                               ETA: 00:09:21

################################################################################
                     [1m Learning iteration 1450/2000 [0m                     

                       Computation: 111789 steps/s (collection: 0.778s, learning 0.102s)
             Mean action noise std: 7.12
          Mean value_function loss: 40.0239
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 26.6317
                       Mean reward: 842.66
               Mean episode length: 248.59
    Episode_Reward/reaching_object: 0.7526
     Episode_Reward/lifting_object: 169.5779
      Episode_Reward/object_height: 0.0623
        Episode_Reward/action_rate: -0.1279
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 0.88s
                      Time elapsed: 00:24:39
                               ETA: 00:09:20

################################################################################
                     [1m Learning iteration 1451/2000 [0m                     

                       Computation: 113556 steps/s (collection: 0.772s, learning 0.094s)
             Mean action noise std: 7.13
          Mean value_function loss: 56.2561
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 26.6364
                       Mean reward: 839.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7539
     Episode_Reward/lifting_object: 169.9394
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.1282
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 0.87s
                      Time elapsed: 00:24:40
                               ETA: 00:09:19

################################################################################
                     [1m Learning iteration 1452/2000 [0m                     

                       Computation: 113998 steps/s (collection: 0.775s, learning 0.088s)
             Mean action noise std: 7.14
          Mean value_function loss: 50.5251
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 26.6435
                       Mean reward: 854.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7485
     Episode_Reward/lifting_object: 168.0846
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.1285
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 0.86s
                      Time elapsed: 00:24:41
                               ETA: 00:09:18

################################################################################
                     [1m Learning iteration 1453/2000 [0m                     

                       Computation: 110950 steps/s (collection: 0.789s, learning 0.097s)
             Mean action noise std: 7.15
          Mean value_function loss: 55.9878
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 26.6566
                       Mean reward: 829.61
               Mean episode length: 244.78
    Episode_Reward/reaching_object: 0.7485
     Episode_Reward/lifting_object: 168.7174
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.1299
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 0.89s
                      Time elapsed: 00:24:42
                               ETA: 00:09:17

################################################################################
                     [1m Learning iteration 1454/2000 [0m                     

                       Computation: 105839 steps/s (collection: 0.835s, learning 0.094s)
             Mean action noise std: 7.15
          Mean value_function loss: 64.0115
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 26.6641
                       Mean reward: 857.83
               Mean episode length: 249.92
    Episode_Reward/reaching_object: 0.7544
     Episode_Reward/lifting_object: 170.8238
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.1311
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 0.93s
                      Time elapsed: 00:24:43
                               ETA: 00:09:16

################################################################################
                     [1m Learning iteration 1455/2000 [0m                     

                       Computation: 113813 steps/s (collection: 0.767s, learning 0.097s)
             Mean action noise std: 7.15
          Mean value_function loss: 43.6052
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 26.6668
                       Mean reward: 859.53
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 171.4496
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.1311
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 0.86s
                      Time elapsed: 00:24:43
                               ETA: 00:09:15

################################################################################
                     [1m Learning iteration 1456/2000 [0m                     

                       Computation: 105660 steps/s (collection: 0.834s, learning 0.096s)
             Mean action noise std: 7.16
          Mean value_function loss: 45.6490
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 26.6762
                       Mean reward: 843.09
               Mean episode length: 248.58
    Episode_Reward/reaching_object: 0.7614
     Episode_Reward/lifting_object: 170.6699
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.1319
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 0.93s
                      Time elapsed: 00:24:44
                               ETA: 00:09:14

################################################################################
                     [1m Learning iteration 1457/2000 [0m                     

                       Computation: 110283 steps/s (collection: 0.794s, learning 0.097s)
             Mean action noise std: 7.18
          Mean value_function loss: 51.0700
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 26.6920
                       Mean reward: 867.30
               Mean episode length: 249.12
    Episode_Reward/reaching_object: 0.7608
     Episode_Reward/lifting_object: 171.9599
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.1326
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 0.89s
                      Time elapsed: 00:24:45
                               ETA: 00:09:13

################################################################################
                     [1m Learning iteration 1458/2000 [0m                     

                       Computation: 109304 steps/s (collection: 0.799s, learning 0.100s)
             Mean action noise std: 7.18
          Mean value_function loss: 47.2223
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 26.7062
                       Mean reward: 805.97
               Mean episode length: 247.92
    Episode_Reward/reaching_object: 0.7390
     Episode_Reward/lifting_object: 167.2285
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.1327
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 0.90s
                      Time elapsed: 00:24:46
                               ETA: 00:09:12

################################################################################
                     [1m Learning iteration 1459/2000 [0m                     

                       Computation: 110723 steps/s (collection: 0.786s, learning 0.102s)
             Mean action noise std: 7.19
          Mean value_function loss: 55.5570
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 26.7143
                       Mean reward: 863.89
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.7433
     Episode_Reward/lifting_object: 167.8229
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.1318
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 0.89s
                      Time elapsed: 00:24:47
                               ETA: 00:09:11

################################################################################
                     [1m Learning iteration 1460/2000 [0m                     

                       Computation: 114276 steps/s (collection: 0.767s, learning 0.093s)
             Mean action noise std: 7.20
          Mean value_function loss: 61.9078
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 26.7253
                       Mean reward: 859.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7487
     Episode_Reward/lifting_object: 169.2290
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.1323
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 0.86s
                      Time elapsed: 00:24:48
                               ETA: 00:09:10

################################################################################
                     [1m Learning iteration 1461/2000 [0m                     

                       Computation: 110852 steps/s (collection: 0.790s, learning 0.097s)
             Mean action noise std: 7.21
          Mean value_function loss: 45.3408
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 26.7394
                       Mean reward: 837.81
               Mean episode length: 249.70
    Episode_Reward/reaching_object: 0.7496
     Episode_Reward/lifting_object: 169.7989
      Episode_Reward/object_height: 0.0623
        Episode_Reward/action_rate: -0.1326
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 0.89s
                      Time elapsed: 00:24:49
                               ETA: 00:09:09

################################################################################
                     [1m Learning iteration 1462/2000 [0m                     

                       Computation: 108801 steps/s (collection: 0.815s, learning 0.089s)
             Mean action noise std: 7.22
          Mean value_function loss: 46.1623
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 26.7470
                       Mean reward: 836.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7370
     Episode_Reward/lifting_object: 167.1219
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.1326
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 0.90s
                      Time elapsed: 00:24:50
                               ETA: 00:09:08

################################################################################
                     [1m Learning iteration 1463/2000 [0m                     

                       Computation: 112313 steps/s (collection: 0.784s, learning 0.092s)
             Mean action noise std: 7.23
          Mean value_function loss: 36.2951
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 26.7629
                       Mean reward: 845.31
               Mean episode length: 249.18
    Episode_Reward/reaching_object: 0.7439
     Episode_Reward/lifting_object: 168.4245
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.1320
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 0.88s
                      Time elapsed: 00:24:51
                               ETA: 00:09:06

################################################################################
                     [1m Learning iteration 1464/2000 [0m                     

                       Computation: 110234 steps/s (collection: 0.799s, learning 0.093s)
             Mean action noise std: 7.24
          Mean value_function loss: 36.0525
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 26.7729
                       Mean reward: 843.25
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.7497
     Episode_Reward/lifting_object: 169.6064
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.1321
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 0.89s
                      Time elapsed: 00:24:51
                               ETA: 00:09:05

################################################################################
                     [1m Learning iteration 1465/2000 [0m                     

                       Computation: 112374 steps/s (collection: 0.782s, learning 0.093s)
             Mean action noise std: 7.24
          Mean value_function loss: 43.9098
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 26.7776
                       Mean reward: 855.29
               Mean episode length: 247.33
    Episode_Reward/reaching_object: 0.7465
     Episode_Reward/lifting_object: 169.6191
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.1332
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 0.87s
                      Time elapsed: 00:24:52
                               ETA: 00:09:04

################################################################################
                     [1m Learning iteration 1466/2000 [0m                     

                       Computation: 109357 steps/s (collection: 0.792s, learning 0.107s)
             Mean action noise std: 7.25
          Mean value_function loss: 48.2734
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 26.7884
                       Mean reward: 844.23
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7469
     Episode_Reward/lifting_object: 167.8802
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.1329
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 0.90s
                      Time elapsed: 00:24:53
                               ETA: 00:09:03

################################################################################
                     [1m Learning iteration 1467/2000 [0m                     

                       Computation: 104486 steps/s (collection: 0.817s, learning 0.124s)
             Mean action noise std: 7.26
          Mean value_function loss: 51.7746
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 26.7972
                       Mean reward: 856.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7528
     Episode_Reward/lifting_object: 170.0977
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.1330
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 0.94s
                      Time elapsed: 00:24:54
                               ETA: 00:09:02

################################################################################
                     [1m Learning iteration 1468/2000 [0m                     

                       Computation: 111344 steps/s (collection: 0.780s, learning 0.103s)
             Mean action noise std: 7.27
          Mean value_function loss: 35.7568
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 26.8045
                       Mean reward: 866.53
               Mean episode length: 249.62
    Episode_Reward/reaching_object: 0.7535
     Episode_Reward/lifting_object: 170.8241
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.1342
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 0.88s
                      Time elapsed: 00:24:55
                               ETA: 00:09:01

################################################################################
                     [1m Learning iteration 1469/2000 [0m                     

                       Computation: 108242 steps/s (collection: 0.785s, learning 0.124s)
             Mean action noise std: 7.28
          Mean value_function loss: 47.5186
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 26.8182
                       Mean reward: 861.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7507
     Episode_Reward/lifting_object: 169.8009
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.1354
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 0.91s
                      Time elapsed: 00:24:56
                               ETA: 00:09:00

################################################################################
                     [1m Learning iteration 1470/2000 [0m                     

                       Computation: 109419 steps/s (collection: 0.789s, learning 0.109s)
             Mean action noise std: 7.29
          Mean value_function loss: 44.0707
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 26.8287
                       Mean reward: 851.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 172.4822
      Episode_Reward/object_height: 0.0653
        Episode_Reward/action_rate: -0.1356
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 0.90s
                      Time elapsed: 00:24:57
                               ETA: 00:08:59

################################################################################
                     [1m Learning iteration 1471/2000 [0m                     

                       Computation: 106807 steps/s (collection: 0.817s, learning 0.103s)
             Mean action noise std: 7.30
          Mean value_function loss: 38.6015
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 26.8372
                       Mean reward: 852.62
               Mean episode length: 246.51
    Episode_Reward/reaching_object: 0.7508
     Episode_Reward/lifting_object: 170.7086
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.1351
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 0.92s
                      Time elapsed: 00:24:58
                               ETA: 00:08:58

################################################################################
                     [1m Learning iteration 1472/2000 [0m                     

                       Computation: 115784 steps/s (collection: 0.755s, learning 0.094s)
             Mean action noise std: 7.30
          Mean value_function loss: 40.2842
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 26.8441
                       Mean reward: 859.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7620
     Episode_Reward/lifting_object: 172.0414
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.1362
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 0.85s
                      Time elapsed: 00:24:59
                               ETA: 00:08:57

################################################################################
                     [1m Learning iteration 1473/2000 [0m                     

                       Computation: 111665 steps/s (collection: 0.781s, learning 0.100s)
             Mean action noise std: 7.31
          Mean value_function loss: 39.0679
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 26.8513
                       Mean reward: 866.75
               Mean episode length: 249.41
    Episode_Reward/reaching_object: 0.7608
     Episode_Reward/lifting_object: 171.5315
      Episode_Reward/object_height: 0.0647
        Episode_Reward/action_rate: -0.1372
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 0.88s
                      Time elapsed: 00:25:00
                               ETA: 00:08:56

################################################################################
                     [1m Learning iteration 1474/2000 [0m                     

                       Computation: 113524 steps/s (collection: 0.776s, learning 0.090s)
             Mean action noise std: 7.31
          Mean value_function loss: 38.7779
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 26.8576
                       Mean reward: 854.01
               Mean episode length: 248.33
    Episode_Reward/reaching_object: 0.7596
     Episode_Reward/lifting_object: 171.4079
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.1381
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 0.87s
                      Time elapsed: 00:25:00
                               ETA: 00:08:55

################################################################################
                     [1m Learning iteration 1475/2000 [0m                     

                       Computation: 112911 steps/s (collection: 0.778s, learning 0.093s)
             Mean action noise std: 7.32
          Mean value_function loss: 37.3737
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 26.8625
                       Mean reward: 863.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 170.5793
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.1383
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 0.87s
                      Time elapsed: 00:25:01
                               ETA: 00:08:54

################################################################################
                     [1m Learning iteration 1476/2000 [0m                     

                       Computation: 111415 steps/s (collection: 0.789s, learning 0.094s)
             Mean action noise std: 7.33
          Mean value_function loss: 49.4234
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 26.8743
                       Mean reward: 853.01
               Mean episode length: 248.43
    Episode_Reward/reaching_object: 0.7556
     Episode_Reward/lifting_object: 169.4200
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.1387
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 0.88s
                      Time elapsed: 00:25:02
                               ETA: 00:08:53

################################################################################
                     [1m Learning iteration 1477/2000 [0m                     

                       Computation: 116926 steps/s (collection: 0.749s, learning 0.092s)
             Mean action noise std: 7.34
          Mean value_function loss: 48.4037
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 26.8862
                       Mean reward: 867.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7591
     Episode_Reward/lifting_object: 170.4664
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.1392
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 0.84s
                      Time elapsed: 00:25:03
                               ETA: 00:08:52

################################################################################
                     [1m Learning iteration 1478/2000 [0m                     

                       Computation: 113741 steps/s (collection: 0.770s, learning 0.095s)
             Mean action noise std: 7.35
          Mean value_function loss: 33.7259
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 26.8918
                       Mean reward: 854.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 171.1272
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.1402
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 0.86s
                      Time elapsed: 00:25:04
                               ETA: 00:08:50

################################################################################
                     [1m Learning iteration 1479/2000 [0m                     

                       Computation: 112333 steps/s (collection: 0.785s, learning 0.090s)
             Mean action noise std: 7.35
          Mean value_function loss: 52.8220
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 26.8961
                       Mean reward: 826.40
               Mean episode length: 247.93
    Episode_Reward/reaching_object: 0.7433
     Episode_Reward/lifting_object: 168.4902
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.1392
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 0.88s
                      Time elapsed: 00:25:05
                               ETA: 00:08:49

################################################################################
                     [1m Learning iteration 1480/2000 [0m                     

                       Computation: 115762 steps/s (collection: 0.752s, learning 0.097s)
             Mean action noise std: 7.36
          Mean value_function loss: 46.2045
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 26.9031
                       Mean reward: 863.13
               Mean episode length: 249.78
    Episode_Reward/reaching_object: 0.7475
     Episode_Reward/lifting_object: 169.8517
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.1400
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 0.85s
                      Time elapsed: 00:25:06
                               ETA: 00:08:48

################################################################################
                     [1m Learning iteration 1481/2000 [0m                     

                       Computation: 113478 steps/s (collection: 0.775s, learning 0.092s)
             Mean action noise std: 7.37
          Mean value_function loss: 35.7953
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 26.9110
                       Mean reward: 848.10
               Mean episode length: 249.68
    Episode_Reward/reaching_object: 0.7457
     Episode_Reward/lifting_object: 168.8146
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.1399
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 0.87s
                      Time elapsed: 00:25:06
                               ETA: 00:08:47

################################################################################
                     [1m Learning iteration 1482/2000 [0m                     

                       Computation: 115677 steps/s (collection: 0.765s, learning 0.085s)
             Mean action noise std: 7.37
          Mean value_function loss: 40.2589
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 26.9189
                       Mean reward: 845.80
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7550
     Episode_Reward/lifting_object: 169.4126
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.1398
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 0.85s
                      Time elapsed: 00:25:07
                               ETA: 00:08:46

################################################################################
                     [1m Learning iteration 1483/2000 [0m                     

                       Computation: 113613 steps/s (collection: 0.772s, learning 0.094s)
             Mean action noise std: 7.37
          Mean value_function loss: 35.1327
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 26.9206
                       Mean reward: 864.74
               Mean episode length: 249.97
    Episode_Reward/reaching_object: 0.7641
     Episode_Reward/lifting_object: 171.9274
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.1394
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 0.87s
                      Time elapsed: 00:25:08
                               ETA: 00:08:45

################################################################################
                     [1m Learning iteration 1484/2000 [0m                     

                       Computation: 112832 steps/s (collection: 0.775s, learning 0.097s)
             Mean action noise std: 7.38
          Mean value_function loss: 39.2265
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 26.9251
                       Mean reward: 857.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7538
     Episode_Reward/lifting_object: 170.2897
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.1393
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 0.87s
                      Time elapsed: 00:25:09
                               ETA: 00:08:44

################################################################################
                     [1m Learning iteration 1485/2000 [0m                     

                       Computation: 114014 steps/s (collection: 0.772s, learning 0.090s)
             Mean action noise std: 7.38
          Mean value_function loss: 37.8819
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 26.9321
                       Mean reward: 838.74
               Mean episode length: 249.93
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 170.8551
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.1397
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 0.86s
                      Time elapsed: 00:25:10
                               ETA: 00:08:43

################################################################################
                     [1m Learning iteration 1486/2000 [0m                     

                       Computation: 113842 steps/s (collection: 0.766s, learning 0.097s)
             Mean action noise std: 7.39
          Mean value_function loss: 32.7145
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 26.9391
                       Mean reward: 839.37
               Mean episode length: 248.68
    Episode_Reward/reaching_object: 0.7611
     Episode_Reward/lifting_object: 170.8738
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.1394
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 0.86s
                      Time elapsed: 00:25:11
                               ETA: 00:08:42

################################################################################
                     [1m Learning iteration 1487/2000 [0m                     

                       Computation: 115797 steps/s (collection: 0.759s, learning 0.090s)
             Mean action noise std: 7.40
          Mean value_function loss: 34.2429
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 26.9456
                       Mean reward: 859.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7515
     Episode_Reward/lifting_object: 170.1304
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.1392
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 0.85s
                      Time elapsed: 00:25:12
                               ETA: 00:08:41

################################################################################
                     [1m Learning iteration 1488/2000 [0m                     

                       Computation: 111475 steps/s (collection: 0.791s, learning 0.091s)
             Mean action noise std: 7.41
          Mean value_function loss: 33.4381
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 26.9547
                       Mean reward: 865.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7454
     Episode_Reward/lifting_object: 169.3471
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.1390
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 0.88s
                      Time elapsed: 00:25:13
                               ETA: 00:08:40

################################################################################
                     [1m Learning iteration 1489/2000 [0m                     

                       Computation: 110177 steps/s (collection: 0.799s, learning 0.094s)
             Mean action noise std: 7.42
          Mean value_function loss: 45.8442
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 26.9694
                       Mean reward: 849.70
               Mean episode length: 249.20
    Episode_Reward/reaching_object: 0.7483
     Episode_Reward/lifting_object: 168.5537
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.1378
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 0.89s
                      Time elapsed: 00:25:13
                               ETA: 00:08:39

################################################################################
                     [1m Learning iteration 1490/2000 [0m                     

                       Computation: 111118 steps/s (collection: 0.797s, learning 0.088s)
             Mean action noise std: 7.42
          Mean value_function loss: 36.9835
               Mean surrogate loss: 0.0136
                 Mean entropy loss: 26.9800
                       Mean reward: 845.40
               Mean episode length: 245.32
    Episode_Reward/reaching_object: 0.7558
     Episode_Reward/lifting_object: 169.9012
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.1380
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 0.88s
                      Time elapsed: 00:25:14
                               ETA: 00:08:38

################################################################################
                     [1m Learning iteration 1491/2000 [0m                     

                       Computation: 113523 steps/s (collection: 0.768s, learning 0.098s)
             Mean action noise std: 7.42
          Mean value_function loss: 46.6065
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 26.9808
                       Mean reward: 869.99
               Mean episode length: 249.96
    Episode_Reward/reaching_object: 0.7659
     Episode_Reward/lifting_object: 172.4128
      Episode_Reward/object_height: 0.0653
        Episode_Reward/action_rate: -0.1387
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 0.87s
                      Time elapsed: 00:25:15
                               ETA: 00:08:37

################################################################################
                     [1m Learning iteration 1492/2000 [0m                     

                       Computation: 112690 steps/s (collection: 0.774s, learning 0.098s)
             Mean action noise std: 7.43
          Mean value_function loss: 38.9892
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 26.9827
                       Mean reward: 846.45
               Mean episode length: 247.84
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 170.4425
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.1381
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 0.87s
                      Time elapsed: 00:25:16
                               ETA: 00:08:36

################################################################################
                     [1m Learning iteration 1493/2000 [0m                     

                       Computation: 112329 steps/s (collection: 0.774s, learning 0.102s)
             Mean action noise std: 7.44
          Mean value_function loss: 45.3488
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 26.9930
                       Mean reward: 830.41
               Mean episode length: 240.21
    Episode_Reward/reaching_object: 0.7543
     Episode_Reward/lifting_object: 170.3736
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.1370
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 0.88s
                      Time elapsed: 00:25:17
                               ETA: 00:08:34

################################################################################
                     [1m Learning iteration 1494/2000 [0m                     

                       Computation: 108086 steps/s (collection: 0.813s, learning 0.097s)
             Mean action noise std: 7.45
          Mean value_function loss: 49.9151
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 27.0093
                       Mean reward: 856.07
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.7538
     Episode_Reward/lifting_object: 170.4889
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.1378
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 0.91s
                      Time elapsed: 00:25:18
                               ETA: 00:08:33

################################################################################
                     [1m Learning iteration 1495/2000 [0m                     

                       Computation: 112515 steps/s (collection: 0.772s, learning 0.102s)
             Mean action noise std: 7.46
          Mean value_function loss: 45.3422
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 27.0172
                       Mean reward: 841.38
               Mean episode length: 247.86
    Episode_Reward/reaching_object: 0.7509
     Episode_Reward/lifting_object: 169.1525
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.1382
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 0.87s
                      Time elapsed: 00:25:19
                               ETA: 00:08:32

################################################################################
                     [1m Learning iteration 1496/2000 [0m                     

                       Computation: 111194 steps/s (collection: 0.781s, learning 0.103s)
             Mean action noise std: 7.47
          Mean value_function loss: 41.3165
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 27.0286
                       Mean reward: 842.74
               Mean episode length: 248.31
    Episode_Reward/reaching_object: 0.7537
     Episode_Reward/lifting_object: 170.8262
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.1389
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 0.88s
                      Time elapsed: 00:25:20
                               ETA: 00:08:31

################################################################################
                     [1m Learning iteration 1497/2000 [0m                     

                       Computation: 106529 steps/s (collection: 0.820s, learning 0.103s)
             Mean action noise std: 7.48
          Mean value_function loss: 45.9382
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 27.0432
                       Mean reward: 846.15
               Mean episode length: 248.28
    Episode_Reward/reaching_object: 0.7442
     Episode_Reward/lifting_object: 168.8648
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.1393
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 0.92s
                      Time elapsed: 00:25:20
                               ETA: 00:08:30

################################################################################
                     [1m Learning iteration 1498/2000 [0m                     

                       Computation: 108029 steps/s (collection: 0.811s, learning 0.099s)
             Mean action noise std: 7.48
          Mean value_function loss: 38.9752
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 27.0493
                       Mean reward: 862.09
               Mean episode length: 247.72
    Episode_Reward/reaching_object: 0.7480
     Episode_Reward/lifting_object: 170.1361
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.1398
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 0.91s
                      Time elapsed: 00:25:21
                               ETA: 00:08:29

################################################################################
                     [1m Learning iteration 1499/2000 [0m                     

                       Computation: 111800 steps/s (collection: 0.785s, learning 0.094s)
             Mean action noise std: 7.49
          Mean value_function loss: 38.6261
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 27.0525
                       Mean reward: 834.12
               Mean episode length: 244.73
    Episode_Reward/reaching_object: 0.7519
     Episode_Reward/lifting_object: 169.7736
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.1394
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 0.88s
                      Time elapsed: 00:25:22
                               ETA: 00:08:28

################################################################################
                     [1m Learning iteration 1500/2000 [0m                     

                       Computation: 111231 steps/s (collection: 0.773s, learning 0.111s)
             Mean action noise std: 7.50
          Mean value_function loss: 31.2656
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 27.0609
                       Mean reward: 857.13
               Mean episode length: 249.12
    Episode_Reward/reaching_object: 0.7600
     Episode_Reward/lifting_object: 172.1136
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.1419
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 0.88s
                      Time elapsed: 00:25:23
                               ETA: 00:08:27

################################################################################
                     [1m Learning iteration 1501/2000 [0m                     

                       Computation: 112210 steps/s (collection: 0.776s, learning 0.100s)
             Mean action noise std: 7.50
          Mean value_function loss: 38.0405
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 27.0698
                       Mean reward: 869.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7560
     Episode_Reward/lifting_object: 171.1281
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.1426
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 0.88s
                      Time elapsed: 00:25:24
                               ETA: 00:08:26

################################################################################
                     [1m Learning iteration 1502/2000 [0m                     

                       Computation: 111605 steps/s (collection: 0.774s, learning 0.107s)
             Mean action noise std: 7.51
          Mean value_function loss: 35.2677
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 27.0776
                       Mean reward: 849.27
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 171.2637
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.1427
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 0.88s
                      Time elapsed: 00:25:25
                               ETA: 00:08:25

################################################################################
                     [1m Learning iteration 1503/2000 [0m                     

                       Computation: 111797 steps/s (collection: 0.779s, learning 0.101s)
             Mean action noise std: 7.52
          Mean value_function loss: 42.2383
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 27.0872
                       Mean reward: 851.37
               Mean episode length: 249.43
    Episode_Reward/reaching_object: 0.7577
     Episode_Reward/lifting_object: 169.2744
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.1433
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 0.88s
                      Time elapsed: 00:25:26
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 1504/2000 [0m                     

                       Computation: 110331 steps/s (collection: 0.789s, learning 0.102s)
             Mean action noise std: 7.53
          Mean value_function loss: 46.4415
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 27.0967
                       Mean reward: 863.10
               Mean episode length: 249.49
    Episode_Reward/reaching_object: 0.7555
     Episode_Reward/lifting_object: 169.8354
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.1441
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 0.89s
                      Time elapsed: 00:25:27
                               ETA: 00:08:23

################################################################################
                     [1m Learning iteration 1505/2000 [0m                     

                       Computation: 111356 steps/s (collection: 0.780s, learning 0.103s)
             Mean action noise std: 7.54
          Mean value_function loss: 56.2568
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 27.1022
                       Mean reward: 858.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7558
     Episode_Reward/lifting_object: 170.1430
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.1434
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 0.88s
                      Time elapsed: 00:25:28
                               ETA: 00:08:22

################################################################################
                     [1m Learning iteration 1506/2000 [0m                     

                       Computation: 115216 steps/s (collection: 0.761s, learning 0.093s)
             Mean action noise std: 7.54
          Mean value_function loss: 36.7260
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.1088
                       Mean reward: 841.44
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.7564
     Episode_Reward/lifting_object: 169.4892
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.1437
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 0.85s
                      Time elapsed: 00:25:28
                               ETA: 00:08:21

################################################################################
                     [1m Learning iteration 1507/2000 [0m                     

                       Computation: 110242 steps/s (collection: 0.797s, learning 0.095s)
             Mean action noise std: 7.55
          Mean value_function loss: 36.2512
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 27.1150
                       Mean reward: 870.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7503
     Episode_Reward/lifting_object: 168.9639
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.1443
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 0.89s
                      Time elapsed: 00:25:29
                               ETA: 00:08:20

################################################################################
                     [1m Learning iteration 1508/2000 [0m                     

                       Computation: 102801 steps/s (collection: 0.844s, learning 0.112s)
             Mean action noise std: 7.56
          Mean value_function loss: 39.0447
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.1249
                       Mean reward: 850.86
               Mean episode length: 246.52
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 172.2220
      Episode_Reward/object_height: 0.0664
        Episode_Reward/action_rate: -0.1448
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 0.96s
                      Time elapsed: 00:25:30
                               ETA: 00:08:19

################################################################################
                     [1m Learning iteration 1509/2000 [0m                     

                       Computation: 108771 steps/s (collection: 0.798s, learning 0.106s)
             Mean action noise std: 7.56
          Mean value_function loss: 36.9538
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 27.1333
                       Mean reward: 846.23
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.7525
     Episode_Reward/lifting_object: 170.4492
      Episode_Reward/object_height: 0.0658
        Episode_Reward/action_rate: -0.1456
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 0.90s
                      Time elapsed: 00:25:31
                               ETA: 00:08:18

################################################################################
                     [1m Learning iteration 1510/2000 [0m                     

                       Computation: 106408 steps/s (collection: 0.815s, learning 0.109s)
             Mean action noise std: 7.57
          Mean value_function loss: 38.3940
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.1409
                       Mean reward: 859.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7554
     Episode_Reward/lifting_object: 170.8379
      Episode_Reward/object_height: 0.0662
        Episode_Reward/action_rate: -0.1470
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 0.92s
                      Time elapsed: 00:25:32
                               ETA: 00:08:17

################################################################################
                     [1m Learning iteration 1511/2000 [0m                     

                       Computation: 106556 steps/s (collection: 0.826s, learning 0.096s)
             Mean action noise std: 7.58
          Mean value_function loss: 38.9761
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 27.1537
                       Mean reward: 852.34
               Mean episode length: 247.15
    Episode_Reward/reaching_object: 0.7547
     Episode_Reward/lifting_object: 170.2453
      Episode_Reward/object_height: 0.0660
        Episode_Reward/action_rate: -0.1466
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 0.92s
                      Time elapsed: 00:25:33
                               ETA: 00:08:15

################################################################################
                     [1m Learning iteration 1512/2000 [0m                     

                       Computation: 110381 steps/s (collection: 0.795s, learning 0.096s)
             Mean action noise std: 7.59
          Mean value_function loss: 36.5242
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 27.1625
                       Mean reward: 859.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7544
     Episode_Reward/lifting_object: 170.6609
      Episode_Reward/object_height: 0.0664
        Episode_Reward/action_rate: -0.1482
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 0.89s
                      Time elapsed: 00:25:34
                               ETA: 00:08:14

################################################################################
                     [1m Learning iteration 1513/2000 [0m                     

                       Computation: 106179 steps/s (collection: 0.832s, learning 0.094s)
             Mean action noise std: 7.60
          Mean value_function loss: 37.6644
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 27.1707
                       Mean reward: 863.08
               Mean episode length: 248.60
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 170.5467
      Episode_Reward/object_height: 0.0662
        Episode_Reward/action_rate: -0.1485
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 0.93s
                      Time elapsed: 00:25:35
                               ETA: 00:08:13

################################################################################
                     [1m Learning iteration 1514/2000 [0m                     

                       Computation: 110774 steps/s (collection: 0.791s, learning 0.097s)
             Mean action noise std: 7.61
          Mean value_function loss: 42.9919
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.1838
                       Mean reward: 846.72
               Mean episode length: 247.37
    Episode_Reward/reaching_object: 0.7557
     Episode_Reward/lifting_object: 168.7363
      Episode_Reward/object_height: 0.0655
        Episode_Reward/action_rate: -0.1480
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 0.89s
                      Time elapsed: 00:25:36
                               ETA: 00:08:12

################################################################################
                     [1m Learning iteration 1515/2000 [0m                     

                       Computation: 101131 steps/s (collection: 0.868s, learning 0.104s)
             Mean action noise std: 7.62
          Mean value_function loss: 43.9027
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 27.1973
                       Mean reward: 863.20
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.7615
     Episode_Reward/lifting_object: 171.3956
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.1481
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 0.97s
                      Time elapsed: 00:25:37
                               ETA: 00:08:11

################################################################################
                     [1m Learning iteration 1516/2000 [0m                     

                       Computation: 110170 steps/s (collection: 0.796s, learning 0.096s)
             Mean action noise std: 7.63
          Mean value_function loss: 52.8613
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 27.2052
                       Mean reward: 864.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7666
     Episode_Reward/lifting_object: 172.7042
      Episode_Reward/object_height: 0.0664
        Episode_Reward/action_rate: -0.1496
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 0.89s
                      Time elapsed: 00:25:38
                               ETA: 00:08:10

################################################################################
                     [1m Learning iteration 1517/2000 [0m                     

                       Computation: 109069 steps/s (collection: 0.796s, learning 0.106s)
             Mean action noise std: 7.63
          Mean value_function loss: 46.2622
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 27.2089
                       Mean reward: 862.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7445
     Episode_Reward/lifting_object: 168.2198
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.1489
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 0.90s
                      Time elapsed: 00:25:38
                               ETA: 00:08:09

################################################################################
                     [1m Learning iteration 1518/2000 [0m                     

                       Computation: 110929 steps/s (collection: 0.789s, learning 0.097s)
             Mean action noise std: 7.64
          Mean value_function loss: 37.7782
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 27.2174
                       Mean reward: 876.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7621
     Episode_Reward/lifting_object: 170.9818
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.1503
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 0.89s
                      Time elapsed: 00:25:39
                               ETA: 00:08:08

################################################################################
                     [1m Learning iteration 1519/2000 [0m                     

                       Computation: 112339 steps/s (collection: 0.786s, learning 0.089s)
             Mean action noise std: 7.65
          Mean value_function loss: 47.7274
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 27.2269
                       Mean reward: 858.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 170.5024
      Episode_Reward/object_height: 0.0647
        Episode_Reward/action_rate: -0.1497
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 0.88s
                      Time elapsed: 00:25:40
                               ETA: 00:08:07

################################################################################
                     [1m Learning iteration 1520/2000 [0m                     

                       Computation: 110849 steps/s (collection: 0.775s, learning 0.112s)
             Mean action noise std: 7.66
          Mean value_function loss: 48.1651
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 27.2372
                       Mean reward: 837.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7525
     Episode_Reward/lifting_object: 169.6471
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.1503
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 0.89s
                      Time elapsed: 00:25:41
                               ETA: 00:08:06

################################################################################
                     [1m Learning iteration 1521/2000 [0m                     

                       Computation: 106238 steps/s (collection: 0.833s, learning 0.092s)
             Mean action noise std: 7.66
          Mean value_function loss: 47.9843
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 27.2434
                       Mean reward: 851.60
               Mean episode length: 248.73
    Episode_Reward/reaching_object: 0.7523
     Episode_Reward/lifting_object: 169.1183
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.1509
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 0.93s
                      Time elapsed: 00:25:42
                               ETA: 00:08:05

################################################################################
                     [1m Learning iteration 1522/2000 [0m                     

                       Computation: 106425 steps/s (collection: 0.808s, learning 0.116s)
             Mean action noise std: 7.67
          Mean value_function loss: 48.1599
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 27.2526
                       Mean reward: 845.92
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.7572
     Episode_Reward/lifting_object: 169.5583
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.1507
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 0.92s
                      Time elapsed: 00:25:43
                               ETA: 00:08:04

################################################################################
                     [1m Learning iteration 1523/2000 [0m                     

                       Computation: 104811 steps/s (collection: 0.827s, learning 0.111s)
             Mean action noise std: 7.68
          Mean value_function loss: 48.0429
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 27.2616
                       Mean reward: 847.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7532
     Episode_Reward/lifting_object: 169.3081
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.1515
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 0.94s
                      Time elapsed: 00:25:44
                               ETA: 00:08:03

################################################################################
                     [1m Learning iteration 1524/2000 [0m                     

                       Computation: 105340 steps/s (collection: 0.831s, learning 0.102s)
             Mean action noise std: 7.69
          Mean value_function loss: 47.7204
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 27.2691
                       Mean reward: 855.35
               Mean episode length: 248.88
    Episode_Reward/reaching_object: 0.7562
     Episode_Reward/lifting_object: 169.2200
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.1518
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 0.93s
                      Time elapsed: 00:25:45
                               ETA: 00:08:02

################################################################################
                     [1m Learning iteration 1525/2000 [0m                     

                       Computation: 111074 steps/s (collection: 0.783s, learning 0.102s)
             Mean action noise std: 7.69
          Mean value_function loss: 45.0218
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 27.2765
                       Mean reward: 856.71
               Mean episode length: 249.69
    Episode_Reward/reaching_object: 0.7517
     Episode_Reward/lifting_object: 169.8788
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.1529
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.1667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 0.89s
                      Time elapsed: 00:25:46
                               ETA: 00:08:01

################################################################################
                     [1m Learning iteration 1526/2000 [0m                     

                       Computation: 110991 steps/s (collection: 0.789s, learning 0.097s)
             Mean action noise std: 7.70
          Mean value_function loss: 43.4912
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.2821
                       Mean reward: 855.02
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 171.0206
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.1532
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 0.89s
                      Time elapsed: 00:25:47
                               ETA: 00:08:00

################################################################################
                     [1m Learning iteration 1527/2000 [0m                     

                       Computation: 111525 steps/s (collection: 0.779s, learning 0.103s)
             Mean action noise std: 7.70
          Mean value_function loss: 41.9088
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 27.2855
                       Mean reward: 873.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 172.0448
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.1547
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 0.88s
                      Time elapsed: 00:25:48
                               ETA: 00:07:59

################################################################################
                     [1m Learning iteration 1528/2000 [0m                     

                       Computation: 112542 steps/s (collection: 0.782s, learning 0.092s)
             Mean action noise std: 7.71
          Mean value_function loss: 40.5882
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 27.2923
                       Mean reward: 846.89
               Mean episode length: 249.06
    Episode_Reward/reaching_object: 0.7494
     Episode_Reward/lifting_object: 168.8323
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.1542
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 0.87s
                      Time elapsed: 00:25:48
                               ETA: 00:07:58

################################################################################
                     [1m Learning iteration 1529/2000 [0m                     

                       Computation: 113958 steps/s (collection: 0.773s, learning 0.090s)
             Mean action noise std: 7.72
          Mean value_function loss: 40.4399
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 27.2937
                       Mean reward: 843.97
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.7587
     Episode_Reward/lifting_object: 169.9873
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.1552
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 0.86s
                      Time elapsed: 00:25:49
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1530/2000 [0m                     

                       Computation: 113770 steps/s (collection: 0.773s, learning 0.091s)
             Mean action noise std: 7.72
          Mean value_function loss: 46.5684
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 27.2984
                       Mean reward: 838.03
               Mean episode length: 247.95
    Episode_Reward/reaching_object: 0.7505
     Episode_Reward/lifting_object: 169.3477
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.1565
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 0.86s
                      Time elapsed: 00:25:50
                               ETA: 00:07:56

################################################################################
                     [1m Learning iteration 1531/2000 [0m                     

                       Computation: 114109 steps/s (collection: 0.765s, learning 0.096s)
             Mean action noise std: 7.73
          Mean value_function loss: 35.1277
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 27.3055
                       Mean reward: 856.38
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.7540
     Episode_Reward/lifting_object: 169.5162
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.1557
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 0.86s
                      Time elapsed: 00:25:51
                               ETA: 00:07:54

################################################################################
                     [1m Learning iteration 1532/2000 [0m                     

                       Computation: 109710 steps/s (collection: 0.803s, learning 0.093s)
             Mean action noise std: 7.74
          Mean value_function loss: 50.1884
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 27.3144
                       Mean reward: 846.31
               Mean episode length: 249.29
    Episode_Reward/reaching_object: 0.7546
     Episode_Reward/lifting_object: 170.2140
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.1572
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 0.90s
                      Time elapsed: 00:25:52
                               ETA: 00:07:53

################################################################################
                     [1m Learning iteration 1533/2000 [0m                     

                       Computation: 105997 steps/s (collection: 0.833s, learning 0.094s)
             Mean action noise std: 7.74
          Mean value_function loss: 35.3615
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 27.3212
                       Mean reward: 871.94
               Mean episode length: 249.99
    Episode_Reward/reaching_object: 0.7562
     Episode_Reward/lifting_object: 170.1027
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.1572
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 0.93s
                      Time elapsed: 00:25:53
                               ETA: 00:07:52

################################################################################
                     [1m Learning iteration 1534/2000 [0m                     

                       Computation: 108296 steps/s (collection: 0.806s, learning 0.102s)
             Mean action noise std: 7.75
          Mean value_function loss: 41.9441
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 27.3247
                       Mean reward: 876.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7556
     Episode_Reward/lifting_object: 169.2918
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.1576
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 0.91s
                      Time elapsed: 00:25:54
                               ETA: 00:07:51

################################################################################
                     [1m Learning iteration 1535/2000 [0m                     

                       Computation: 106792 steps/s (collection: 0.808s, learning 0.112s)
             Mean action noise std: 7.76
          Mean value_function loss: 34.5730
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 27.3309
                       Mean reward: 828.40
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.7482
     Episode_Reward/lifting_object: 167.8812
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.1560
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 0.92s
                      Time elapsed: 00:25:55
                               ETA: 00:07:50

################################################################################
                     [1m Learning iteration 1536/2000 [0m                     

                       Computation: 104738 steps/s (collection: 0.825s, learning 0.114s)
             Mean action noise std: 7.76
          Mean value_function loss: 38.9085
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 27.3403
                       Mean reward: 852.70
               Mean episode length: 249.03
    Episode_Reward/reaching_object: 0.7530
     Episode_Reward/lifting_object: 169.3378
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.1560
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 0.94s
                      Time elapsed: 00:25:56
                               ETA: 00:07:49

################################################################################
                     [1m Learning iteration 1537/2000 [0m                     

                       Computation: 106554 steps/s (collection: 0.814s, learning 0.108s)
             Mean action noise std: 7.77
          Mean value_function loss: 40.0733
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 27.3478
                       Mean reward: 871.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 172.3398
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.1551
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 0.92s
                      Time elapsed: 00:25:56
                               ETA: 00:07:48

################################################################################
                     [1m Learning iteration 1538/2000 [0m                     

                       Computation: 105130 steps/s (collection: 0.828s, learning 0.107s)
             Mean action noise std: 7.78
          Mean value_function loss: 32.8662
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 27.3572
                       Mean reward: 873.10
               Mean episode length: 248.61
    Episode_Reward/reaching_object: 0.7487
     Episode_Reward/lifting_object: 168.6345
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.1562
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 0.94s
                      Time elapsed: 00:25:57
                               ETA: 00:07:47

################################################################################
                     [1m Learning iteration 1539/2000 [0m                     

                       Computation: 103483 steps/s (collection: 0.834s, learning 0.116s)
             Mean action noise std: 7.78
          Mean value_function loss: 45.2524
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 27.3626
                       Mean reward: 852.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7363
     Episode_Reward/lifting_object: 166.2830
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.1561
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 0.95s
                      Time elapsed: 00:25:58
                               ETA: 00:07:46

################################################################################
                     [1m Learning iteration 1540/2000 [0m                     

                       Computation: 102098 steps/s (collection: 0.846s, learning 0.117s)
             Mean action noise std: 7.79
          Mean value_function loss: 36.7787
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 27.3696
                       Mean reward: 853.16
               Mean episode length: 246.28
    Episode_Reward/reaching_object: 0.7583
     Episode_Reward/lifting_object: 170.4708
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.1558
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 0.96s
                      Time elapsed: 00:25:59
                               ETA: 00:07:45

################################################################################
                     [1m Learning iteration 1541/2000 [0m                     

                       Computation: 102200 steps/s (collection: 0.850s, learning 0.112s)
             Mean action noise std: 7.80
          Mean value_function loss: 39.5968
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 27.3789
                       Mean reward: 835.34
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.7572
     Episode_Reward/lifting_object: 170.0186
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.1563
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 0.96s
                      Time elapsed: 00:26:00
                               ETA: 00:07:44

################################################################################
                     [1m Learning iteration 1542/2000 [0m                     

                       Computation: 106711 steps/s (collection: 0.813s, learning 0.108s)
             Mean action noise std: 7.80
          Mean value_function loss: 35.4554
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 27.3852
                       Mean reward: 874.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7772
     Episode_Reward/lifting_object: 174.4398
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.1571
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 0.92s
                      Time elapsed: 00:26:01
                               ETA: 00:07:43

################################################################################
                     [1m Learning iteration 1543/2000 [0m                     

                       Computation: 111014 steps/s (collection: 0.784s, learning 0.102s)
             Mean action noise std: 7.80
          Mean value_function loss: 35.2642
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 27.3876
                       Mean reward: 860.60
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 171.9355
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.1563
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 0.89s
                      Time elapsed: 00:26:02
                               ETA: 00:07:42

################################################################################
                     [1m Learning iteration 1544/2000 [0m                     

                       Computation: 107808 steps/s (collection: 0.812s, learning 0.100s)
             Mean action noise std: 7.81
          Mean value_function loss: 34.0335
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 27.3925
                       Mean reward: 853.39
               Mean episode length: 248.33
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 172.2020
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.1564
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 0.91s
                      Time elapsed: 00:26:03
                               ETA: 00:07:41

################################################################################
                     [1m Learning iteration 1545/2000 [0m                     

                       Computation: 110354 steps/s (collection: 0.793s, learning 0.098s)
             Mean action noise std: 7.82
          Mean value_function loss: 41.0307
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 27.4000
                       Mean reward: 866.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 171.6120
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.1562
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 0.89s
                      Time elapsed: 00:26:04
                               ETA: 00:07:40

################################################################################
                     [1m Learning iteration 1546/2000 [0m                     

                       Computation: 113651 steps/s (collection: 0.773s, learning 0.092s)
             Mean action noise std: 7.83
          Mean value_function loss: 25.1842
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 27.4082
                       Mean reward: 868.01
               Mean episode length: 249.06
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 171.4444
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.1561
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 0.86s
                      Time elapsed: 00:26:05
                               ETA: 00:07:39

################################################################################
                     [1m Learning iteration 1547/2000 [0m                     

                       Computation: 109224 steps/s (collection: 0.806s, learning 0.094s)
             Mean action noise std: 7.83
          Mean value_function loss: 40.6339
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 27.4172
                       Mean reward: 850.65
               Mean episode length: 247.33
    Episode_Reward/reaching_object: 0.7554
     Episode_Reward/lifting_object: 170.9181
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.1569
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 0.90s
                      Time elapsed: 00:26:06
                               ETA: 00:07:38

################################################################################
                     [1m Learning iteration 1548/2000 [0m                     

                       Computation: 107000 steps/s (collection: 0.829s, learning 0.090s)
             Mean action noise std: 7.84
          Mean value_function loss: 36.7390
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.4232
                       Mean reward: 842.20
               Mean episode length: 248.52
    Episode_Reward/reaching_object: 0.7504
     Episode_Reward/lifting_object: 170.4778
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.1583
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 0.92s
                      Time elapsed: 00:26:07
                               ETA: 00:07:37

################################################################################
                     [1m Learning iteration 1549/2000 [0m                     

                       Computation: 106463 steps/s (collection: 0.822s, learning 0.102s)
             Mean action noise std: 7.85
          Mean value_function loss: 43.5048
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 27.4296
                       Mean reward: 862.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7549
     Episode_Reward/lifting_object: 169.9244
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.1571
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 0.92s
                      Time elapsed: 00:26:08
                               ETA: 00:07:36

################################################################################
                     [1m Learning iteration 1550/2000 [0m                     

                       Computation: 104306 steps/s (collection: 0.839s, learning 0.104s)
             Mean action noise std: 7.86
          Mean value_function loss: 39.8664
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 27.4410
                       Mean reward: 848.34
               Mean episode length: 247.88
    Episode_Reward/reaching_object: 0.7534
     Episode_Reward/lifting_object: 170.8098
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.1575
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 0.94s
                      Time elapsed: 00:26:08
                               ETA: 00:07:35

################################################################################
                     [1m Learning iteration 1551/2000 [0m                     

                       Computation: 105098 steps/s (collection: 0.837s, learning 0.098s)
             Mean action noise std: 7.87
          Mean value_function loss: 40.2798
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.4510
                       Mean reward: 853.15
               Mean episode length: 246.65
    Episode_Reward/reaching_object: 0.7497
     Episode_Reward/lifting_object: 169.3572
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.1583
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 0.94s
                      Time elapsed: 00:26:09
                               ETA: 00:07:34

################################################################################
                     [1m Learning iteration 1552/2000 [0m                     

                       Computation: 108268 steps/s (collection: 0.814s, learning 0.093s)
             Mean action noise std: 7.88
          Mean value_function loss: 38.0070
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 27.4634
                       Mean reward: 851.88
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.7609
     Episode_Reward/lifting_object: 171.3731
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.1579
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 0.91s
                      Time elapsed: 00:26:10
                               ETA: 00:07:33

################################################################################
                     [1m Learning iteration 1553/2000 [0m                     

                       Computation: 106481 steps/s (collection: 0.816s, learning 0.107s)
             Mean action noise std: 7.89
          Mean value_function loss: 22.7682
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 27.4703
                       Mean reward: 881.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7702
     Episode_Reward/lifting_object: 172.7680
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.1600
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 0.92s
                      Time elapsed: 00:26:11
                               ETA: 00:07:32

################################################################################
                     [1m Learning iteration 1554/2000 [0m                     

                       Computation: 100722 steps/s (collection: 0.852s, learning 0.124s)
             Mean action noise std: 7.90
          Mean value_function loss: 34.0525
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 27.4801
                       Mean reward: 868.74
               Mean episode length: 248.89
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 171.8330
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.1595
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 0.98s
                      Time elapsed: 00:26:12
                               ETA: 00:07:31

################################################################################
                     [1m Learning iteration 1555/2000 [0m                     

                       Computation: 105187 steps/s (collection: 0.798s, learning 0.136s)
             Mean action noise std: 7.91
          Mean value_function loss: 30.1976
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 27.4884
                       Mean reward: 863.96
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7596
     Episode_Reward/lifting_object: 171.4608
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.1594
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 0.93s
                      Time elapsed: 00:26:13
                               ETA: 00:07:30

################################################################################
                     [1m Learning iteration 1556/2000 [0m                     

                       Computation: 108097 steps/s (collection: 0.792s, learning 0.117s)
             Mean action noise std: 7.91
          Mean value_function loss: 31.1357
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 27.4966
                       Mean reward: 866.69
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.7600
     Episode_Reward/lifting_object: 171.5685
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.1606
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 0.91s
                      Time elapsed: 00:26:14
                               ETA: 00:07:29

################################################################################
                     [1m Learning iteration 1557/2000 [0m                     

                       Computation: 107503 steps/s (collection: 0.817s, learning 0.098s)
             Mean action noise std: 7.92
          Mean value_function loss: 44.7266
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 27.5010
                       Mean reward: 856.16
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 171.8199
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.1606
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 0.91s
                      Time elapsed: 00:26:15
                               ETA: 00:07:27

################################################################################
                     [1m Learning iteration 1558/2000 [0m                     

                       Computation: 108849 steps/s (collection: 0.807s, learning 0.096s)
             Mean action noise std: 7.93
          Mean value_function loss: 34.4776
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 27.5121
                       Mean reward: 832.80
               Mean episode length: 244.54
    Episode_Reward/reaching_object: 0.7468
     Episode_Reward/lifting_object: 168.7906
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.1587
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 0.90s
                      Time elapsed: 00:26:16
                               ETA: 00:07:26

################################################################################
                     [1m Learning iteration 1559/2000 [0m                     

                       Computation: 109493 steps/s (collection: 0.807s, learning 0.090s)
             Mean action noise std: 7.93
          Mean value_function loss: 40.7767
               Mean surrogate loss: 0.0197
                 Mean entropy loss: 27.5197
                       Mean reward: 869.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7621
     Episode_Reward/lifting_object: 171.6249
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.1596
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 0.90s
                      Time elapsed: 00:26:17
                               ETA: 00:07:25

################################################################################
                     [1m Learning iteration 1560/2000 [0m                     

                       Computation: 112996 steps/s (collection: 0.779s, learning 0.091s)
             Mean action noise std: 7.94
          Mean value_function loss: 37.8557
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 27.5205
                       Mean reward: 848.65
               Mean episode length: 248.88
    Episode_Reward/reaching_object: 0.7523
     Episode_Reward/lifting_object: 170.6747
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.1605
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 0.87s
                      Time elapsed: 00:26:18
                               ETA: 00:07:24

################################################################################
                     [1m Learning iteration 1561/2000 [0m                     

                       Computation: 110303 steps/s (collection: 0.799s, learning 0.093s)
             Mean action noise std: 7.94
          Mean value_function loss: 50.3053
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 27.5246
                       Mean reward: 858.69
               Mean episode length: 247.78
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 171.8439
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.1609
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 0.89s
                      Time elapsed: 00:26:19
                               ETA: 00:07:23

################################################################################
                     [1m Learning iteration 1562/2000 [0m                     

                       Computation: 107079 steps/s (collection: 0.817s, learning 0.101s)
             Mean action noise std: 7.96
          Mean value_function loss: 33.6680
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 27.5359
                       Mean reward: 873.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 172.3830
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.1615
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 0.92s
                      Time elapsed: 00:26:19
                               ETA: 00:07:22

################################################################################
                     [1m Learning iteration 1563/2000 [0m                     

                       Computation: 106124 steps/s (collection: 0.808s, learning 0.119s)
             Mean action noise std: 7.97
          Mean value_function loss: 39.9855
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 27.5485
                       Mean reward: 870.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7678
     Episode_Reward/lifting_object: 172.3545
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.1610
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 0.93s
                      Time elapsed: 00:26:20
                               ETA: 00:07:21

################################################################################
                     [1m Learning iteration 1564/2000 [0m                     

                       Computation: 108771 steps/s (collection: 0.807s, learning 0.097s)
             Mean action noise std: 7.98
          Mean value_function loss: 52.1075
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 27.5596
                       Mean reward: 870.24
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 173.5826
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.1613
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 0.90s
                      Time elapsed: 00:26:21
                               ETA: 00:07:20

################################################################################
                     [1m Learning iteration 1565/2000 [0m                     

                       Computation: 107463 steps/s (collection: 0.815s, learning 0.100s)
             Mean action noise std: 7.98
          Mean value_function loss: 60.0451
               Mean surrogate loss: 0.0147
                 Mean entropy loss: 27.5648
                       Mean reward: 844.96
               Mean episode length: 247.42
    Episode_Reward/reaching_object: 0.7553
     Episode_Reward/lifting_object: 170.3465
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.1609
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 0.91s
                      Time elapsed: 00:26:22
                               ETA: 00:07:19

################################################################################
                     [1m Learning iteration 1566/2000 [0m                     

                       Computation: 103486 steps/s (collection: 0.825s, learning 0.125s)
             Mean action noise std: 7.98
          Mean value_function loss: 50.8315
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 27.5654
                       Mean reward: 867.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 170.5138
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.1607
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 0.95s
                      Time elapsed: 00:26:23
                               ETA: 00:07:18

################################################################################
                     [1m Learning iteration 1567/2000 [0m                     

                       Computation: 113316 steps/s (collection: 0.776s, learning 0.092s)
             Mean action noise std: 7.98
          Mean value_function loss: 58.4596
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 27.5694
                       Mean reward: 872.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 170.8529
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.1617
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 0.87s
                      Time elapsed: 00:26:24
                               ETA: 00:07:17

################################################################################
                     [1m Learning iteration 1568/2000 [0m                     

                       Computation: 111796 steps/s (collection: 0.785s, learning 0.094s)
             Mean action noise std: 7.99
          Mean value_function loss: 63.2465
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 27.5754
                       Mean reward: 853.59
               Mean episode length: 246.92
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 171.8523
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.1620
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 0.88s
                      Time elapsed: 00:26:25
                               ETA: 00:07:16

################################################################################
                     [1m Learning iteration 1569/2000 [0m                     

                       Computation: 108565 steps/s (collection: 0.801s, learning 0.105s)
             Mean action noise std: 8.00
          Mean value_function loss: 66.0675
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 27.5843
                       Mean reward: 826.79
               Mean episode length: 249.13
    Episode_Reward/reaching_object: 0.7416
     Episode_Reward/lifting_object: 168.2001
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.1635
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 0.91s
                      Time elapsed: 00:26:26
                               ETA: 00:07:15

################################################################################
                     [1m Learning iteration 1570/2000 [0m                     

                       Computation: 110559 steps/s (collection: 0.788s, learning 0.102s)
             Mean action noise std: 8.00
          Mean value_function loss: 65.5311
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 27.5908
                       Mean reward: 854.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7506
     Episode_Reward/lifting_object: 170.0085
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.1648
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 0.89s
                      Time elapsed: 00:26:27
                               ETA: 00:07:14

################################################################################
                     [1m Learning iteration 1571/2000 [0m                     

                       Computation: 109356 steps/s (collection: 0.803s, learning 0.095s)
             Mean action noise std: 8.01
          Mean value_function loss: 60.2894
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 27.5980
                       Mean reward: 852.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7376
     Episode_Reward/lifting_object: 167.6678
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.1639
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 0.90s
                      Time elapsed: 00:26:28
                               ETA: 00:07:13

################################################################################
                     [1m Learning iteration 1572/2000 [0m                     

                       Computation: 106352 steps/s (collection: 0.830s, learning 0.094s)
             Mean action noise std: 8.02
          Mean value_function loss: 58.8512
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 27.6063
                       Mean reward: 844.59
               Mean episode length: 247.77
    Episode_Reward/reaching_object: 0.7404
     Episode_Reward/lifting_object: 168.8794
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.1630
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 0.92s
                      Time elapsed: 00:26:28
                               ETA: 00:07:12

################################################################################
                     [1m Learning iteration 1573/2000 [0m                     

                       Computation: 106399 steps/s (collection: 0.827s, learning 0.097s)
             Mean action noise std: 8.02
          Mean value_function loss: 65.0192
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 27.6119
                       Mean reward: 809.96
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7218
     Episode_Reward/lifting_object: 164.0432
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.1639
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 0.92s
                      Time elapsed: 00:26:29
                               ETA: 00:07:11

################################################################################
                     [1m Learning iteration 1574/2000 [0m                     

                       Computation: 105778 steps/s (collection: 0.811s, learning 0.118s)
             Mean action noise std: 8.03
          Mean value_function loss: 59.0125
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 27.6216
                       Mean reward: 831.67
               Mean episode length: 248.75
    Episode_Reward/reaching_object: 0.7264
     Episode_Reward/lifting_object: 165.5846
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.1639
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 0.93s
                      Time elapsed: 00:26:30
                               ETA: 00:07:10

################################################################################
                     [1m Learning iteration 1575/2000 [0m                     

                       Computation: 103813 steps/s (collection: 0.838s, learning 0.109s)
             Mean action noise std: 8.04
          Mean value_function loss: 69.5103
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 27.6302
                       Mean reward: 854.11
               Mean episode length: 249.57
    Episode_Reward/reaching_object: 0.7388
     Episode_Reward/lifting_object: 168.4673
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.1649
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 0.95s
                      Time elapsed: 00:26:31
                               ETA: 00:07:09

################################################################################
                     [1m Learning iteration 1576/2000 [0m                     

                       Computation: 109394 steps/s (collection: 0.804s, learning 0.095s)
             Mean action noise std: 8.05
          Mean value_function loss: 43.9849
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 27.6395
                       Mean reward: 827.90
               Mean episode length: 247.80
    Episode_Reward/reaching_object: 0.7339
     Episode_Reward/lifting_object: 166.3048
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.1651
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 0.90s
                      Time elapsed: 00:26:32
                               ETA: 00:07:08

################################################################################
                     [1m Learning iteration 1577/2000 [0m                     

                       Computation: 108287 steps/s (collection: 0.798s, learning 0.110s)
             Mean action noise std: 8.06
          Mean value_function loss: 44.3474
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 27.6526
                       Mean reward: 852.12
               Mean episode length: 248.44
    Episode_Reward/reaching_object: 0.7442
     Episode_Reward/lifting_object: 168.7528
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.1658
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 0.91s
                      Time elapsed: 00:26:33
                               ETA: 00:07:07

################################################################################
                     [1m Learning iteration 1578/2000 [0m                     

                       Computation: 104144 steps/s (collection: 0.832s, learning 0.112s)
             Mean action noise std: 8.07
          Mean value_function loss: 47.0670
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 27.6612
                       Mean reward: 820.96
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7424
     Episode_Reward/lifting_object: 168.2372
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.1652
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 0.94s
                      Time elapsed: 00:26:34
                               ETA: 00:07:06

################################################################################
                     [1m Learning iteration 1579/2000 [0m                     

                       Computation: 106918 steps/s (collection: 0.813s, learning 0.106s)
             Mean action noise std: 8.08
          Mean value_function loss: 42.7202
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 27.6691
                       Mean reward: 852.20
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 0.7427
     Episode_Reward/lifting_object: 170.2111
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.1654
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 0.92s
                      Time elapsed: 00:26:35
                               ETA: 00:07:05

################################################################################
                     [1m Learning iteration 1580/2000 [0m                     

                       Computation: 109519 steps/s (collection: 0.804s, learning 0.094s)
             Mean action noise std: 8.08
          Mean value_function loss: 37.9752
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 27.6750
                       Mean reward: 857.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7372
     Episode_Reward/lifting_object: 168.9659
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.1683
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 0.90s
                      Time elapsed: 00:26:36
                               ETA: 00:07:04

################################################################################
                     [1m Learning iteration 1581/2000 [0m                     

                       Computation: 109722 steps/s (collection: 0.802s, learning 0.094s)
             Mean action noise std: 8.10
          Mean value_function loss: 45.2537
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.6832
                       Mean reward: 856.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7327
     Episode_Reward/lifting_object: 167.9060
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.1663
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 155516928
                    Iteration time: 0.90s
                      Time elapsed: 00:26:37
                               ETA: 00:07:03

################################################################################
                     [1m Learning iteration 1582/2000 [0m                     

                       Computation: 109034 steps/s (collection: 0.812s, learning 0.089s)
             Mean action noise std: 8.11
          Mean value_function loss: 42.7857
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 27.6971
                       Mean reward: 843.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7382
     Episode_Reward/lifting_object: 169.3382
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.1665
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 155615232
                    Iteration time: 0.90s
                      Time elapsed: 00:26:38
                               ETA: 00:07:02

################################################################################
                     [1m Learning iteration 1583/2000 [0m                     

                       Computation: 106841 steps/s (collection: 0.808s, learning 0.113s)
             Mean action noise std: 8.12
          Mean value_function loss: 49.8353
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 27.7067
                       Mean reward: 843.33
               Mean episode length: 248.39
    Episode_Reward/reaching_object: 0.7440
     Episode_Reward/lifting_object: 169.4482
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.1672
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 155713536
                    Iteration time: 0.92s
                      Time elapsed: 00:26:39
                               ETA: 00:07:00

################################################################################
                     [1m Learning iteration 1584/2000 [0m                     

                       Computation: 108326 steps/s (collection: 0.799s, learning 0.109s)
             Mean action noise std: 8.12
          Mean value_function loss: 43.9164
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 27.7105
                       Mean reward: 833.26
               Mean episode length: 247.95
    Episode_Reward/reaching_object: 0.7453
     Episode_Reward/lifting_object: 169.4078
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.1675
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 155811840
                    Iteration time: 0.91s
                      Time elapsed: 00:26:39
                               ETA: 00:06:59

################################################################################
                     [1m Learning iteration 1585/2000 [0m                     

                       Computation: 108763 steps/s (collection: 0.806s, learning 0.098s)
             Mean action noise std: 8.13
          Mean value_function loss: 41.4256
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 27.7193
                       Mean reward: 846.28
               Mean episode length: 247.13
    Episode_Reward/reaching_object: 0.7440
     Episode_Reward/lifting_object: 169.0778
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.1673
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 155910144
                    Iteration time: 0.90s
                      Time elapsed: 00:26:40
                               ETA: 00:06:58

################################################################################
                     [1m Learning iteration 1586/2000 [0m                     

                       Computation: 109361 steps/s (collection: 0.804s, learning 0.095s)
             Mean action noise std: 8.14
          Mean value_function loss: 37.0060
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 27.7300
                       Mean reward: 852.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7437
     Episode_Reward/lifting_object: 168.3719
      Episode_Reward/object_height: 0.0623
        Episode_Reward/action_rate: -0.1671
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 156008448
                    Iteration time: 0.90s
                      Time elapsed: 00:26:41
                               ETA: 00:06:57

################################################################################
                     [1m Learning iteration 1587/2000 [0m                     

                       Computation: 107505 steps/s (collection: 0.815s, learning 0.100s)
             Mean action noise std: 8.15
          Mean value_function loss: 47.2300
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 27.7405
                       Mean reward: 843.31
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.7481
     Episode_Reward/lifting_object: 168.4606
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.1669
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 156106752
                    Iteration time: 0.91s
                      Time elapsed: 00:26:42
                               ETA: 00:06:56

################################################################################
                     [1m Learning iteration 1588/2000 [0m                     

                       Computation: 103648 steps/s (collection: 0.819s, learning 0.130s)
             Mean action noise std: 8.16
          Mean value_function loss: 44.8933
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 27.7467
                       Mean reward: 860.47
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.7518
     Episode_Reward/lifting_object: 170.6996
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.1681
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.0833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 156205056
                    Iteration time: 0.95s
                      Time elapsed: 00:26:43
                               ETA: 00:06:55

################################################################################
                     [1m Learning iteration 1589/2000 [0m                     

                       Computation: 103827 steps/s (collection: 0.815s, learning 0.132s)
             Mean action noise std: 8.17
          Mean value_function loss: 46.9620
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 27.7531
                       Mean reward: 831.26
               Mean episode length: 247.54
    Episode_Reward/reaching_object: 0.7493
     Episode_Reward/lifting_object: 170.0818
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.1679
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 156303360
                    Iteration time: 0.95s
                      Time elapsed: 00:26:44
                               ETA: 00:06:54

################################################################################
                     [1m Learning iteration 1590/2000 [0m                     

                       Computation: 107488 steps/s (collection: 0.817s, learning 0.098s)
             Mean action noise std: 8.18
          Mean value_function loss: 44.4053
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.7659
                       Mean reward: 869.12
               Mean episode length: 249.60
    Episode_Reward/reaching_object: 0.7549
     Episode_Reward/lifting_object: 171.0333
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.1679
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 156401664
                    Iteration time: 0.91s
                      Time elapsed: 00:26:45
                               ETA: 00:06:53

################################################################################
                     [1m Learning iteration 1591/2000 [0m                     

                       Computation: 110594 steps/s (collection: 0.794s, learning 0.095s)
             Mean action noise std: 8.20
          Mean value_function loss: 54.1548
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 27.7800
                       Mean reward: 852.33
               Mean episode length: 249.16
    Episode_Reward/reaching_object: 0.7421
     Episode_Reward/lifting_object: 169.4203
      Episode_Reward/object_height: 0.0623
        Episode_Reward/action_rate: -0.1688
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 156499968
                    Iteration time: 0.89s
                      Time elapsed: 00:26:46
                               ETA: 00:06:52

################################################################################
                     [1m Learning iteration 1592/2000 [0m                     

                       Computation: 109616 steps/s (collection: 0.804s, learning 0.093s)
             Mean action noise std: 8.21
          Mean value_function loss: 40.6052
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.7948
                       Mean reward: 811.44
               Mean episode length: 245.39
    Episode_Reward/reaching_object: 0.7399
     Episode_Reward/lifting_object: 167.0523
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.1689
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 156598272
                    Iteration time: 0.90s
                      Time elapsed: 00:26:47
                               ETA: 00:06:51

################################################################################
                     [1m Learning iteration 1593/2000 [0m                     

                       Computation: 106248 steps/s (collection: 0.829s, learning 0.097s)
             Mean action noise std: 8.21
          Mean value_function loss: 43.9861
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 27.8028
                       Mean reward: 851.42
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.7491
     Episode_Reward/lifting_object: 170.8342
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.1702
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 156696576
                    Iteration time: 0.93s
                      Time elapsed: 00:26:48
                               ETA: 00:06:50

################################################################################
                     [1m Learning iteration 1594/2000 [0m                     

                       Computation: 106704 steps/s (collection: 0.814s, learning 0.107s)
             Mean action noise std: 8.23
          Mean value_function loss: 45.9073
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 27.8105
                       Mean reward: 852.14
               Mean episode length: 249.42
    Episode_Reward/reaching_object: 0.7453
     Episode_Reward/lifting_object: 169.8223
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.1713
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 156794880
                    Iteration time: 0.92s
                      Time elapsed: 00:26:49
                               ETA: 00:06:49

################################################################################
                     [1m Learning iteration 1595/2000 [0m                     

                       Computation: 108978 steps/s (collection: 0.810s, learning 0.092s)
             Mean action noise std: 8.23
          Mean value_function loss: 38.5844
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 27.8191
                       Mean reward: 858.67
               Mean episode length: 249.87
    Episode_Reward/reaching_object: 0.7515
     Episode_Reward/lifting_object: 170.0566
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.1709
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 156893184
                    Iteration time: 0.90s
                      Time elapsed: 00:26:50
                               ETA: 00:06:48

################################################################################
                     [1m Learning iteration 1596/2000 [0m                     

                       Computation: 106026 steps/s (collection: 0.837s, learning 0.091s)
             Mean action noise std: 8.24
          Mean value_function loss: 43.0633
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 27.8244
                       Mean reward: 851.17
               Mean episode length: 249.28
    Episode_Reward/reaching_object: 0.7466
     Episode_Reward/lifting_object: 169.5681
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.1716
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 156991488
                    Iteration time: 0.93s
                      Time elapsed: 00:26:50
                               ETA: 00:06:47

################################################################################
                     [1m Learning iteration 1597/2000 [0m                     

                       Computation: 110407 steps/s (collection: 0.801s, learning 0.089s)
             Mean action noise std: 8.24
          Mean value_function loss: 34.1867
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 27.8309
                       Mean reward: 858.57
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.7560
     Episode_Reward/lifting_object: 171.0673
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.1717
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 157089792
                    Iteration time: 0.89s
                      Time elapsed: 00:26:51
                               ETA: 00:06:46

################################################################################
                     [1m Learning iteration 1598/2000 [0m                     

                       Computation: 110770 steps/s (collection: 0.797s, learning 0.090s)
             Mean action noise std: 8.25
          Mean value_function loss: 38.3871
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 27.8375
                       Mean reward: 869.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 172.4366
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.1734
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 19.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 157188096
                    Iteration time: 0.89s
                      Time elapsed: 00:26:52
                               ETA: 00:06:45

################################################################################
                     [1m Learning iteration 1599/2000 [0m                     

                       Computation: 111668 steps/s (collection: 0.781s, learning 0.099s)
             Mean action noise std: 8.26
          Mean value_function loss: 37.3482
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 27.8484
                       Mean reward: 817.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7325
     Episode_Reward/lifting_object: 166.0089
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.1723
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 157286400
                    Iteration time: 0.88s
                      Time elapsed: 00:26:53
                               ETA: 00:06:44

################################################################################
                     [1m Learning iteration 1600/2000 [0m                     

                       Computation: 112984 steps/s (collection: 0.775s, learning 0.095s)
             Mean action noise std: 8.28
          Mean value_function loss: 31.4108
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 27.8583
                       Mean reward: 855.19
               Mean episode length: 248.53
    Episode_Reward/reaching_object: 0.7411
     Episode_Reward/lifting_object: 169.2277
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.1727
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 157384704
                    Iteration time: 0.87s
                      Time elapsed: 00:26:54
                               ETA: 00:06:43

################################################################################
                     [1m Learning iteration 1601/2000 [0m                     

                       Computation: 113678 steps/s (collection: 0.772s, learning 0.093s)
             Mean action noise std: 8.28
          Mean value_function loss: 31.7412
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 27.8683
                       Mean reward: 863.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 171.9046
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.1735
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 157483008
                    Iteration time: 0.86s
                      Time elapsed: 00:26:55
                               ETA: 00:06:42

################################################################################
                     [1m Learning iteration 1602/2000 [0m                     

                       Computation: 108814 steps/s (collection: 0.801s, learning 0.103s)
             Mean action noise std: 8.29
          Mean value_function loss: 37.7387
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 27.8739
                       Mean reward: 866.03
               Mean episode length: 248.47
    Episode_Reward/reaching_object: 0.7507
     Episode_Reward/lifting_object: 171.3364
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.1735
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 157581312
                    Iteration time: 0.90s
                      Time elapsed: 00:26:56
                               ETA: 00:06:41

################################################################################
                     [1m Learning iteration 1603/2000 [0m                     

                       Computation: 108986 steps/s (collection: 0.795s, learning 0.107s)
             Mean action noise std: 8.29
          Mean value_function loss: 23.5371
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 27.8790
                       Mean reward: 860.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 170.8058
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.1747
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 157679616
                    Iteration time: 0.90s
                      Time elapsed: 00:26:57
                               ETA: 00:06:40

################################################################################
                     [1m Learning iteration 1604/2000 [0m                     

                       Computation: 106203 steps/s (collection: 0.823s, learning 0.103s)
             Mean action noise std: 8.30
          Mean value_function loss: 30.4596
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 27.8879
                       Mean reward: 871.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 172.2464
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.1745
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 157777920
                    Iteration time: 0.93s
                      Time elapsed: 00:26:58
                               ETA: 00:06:39

################################################################################
                     [1m Learning iteration 1605/2000 [0m                     

                       Computation: 112174 steps/s (collection: 0.772s, learning 0.104s)
             Mean action noise std: 8.32
          Mean value_function loss: 30.5785
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 27.8975
                       Mean reward: 861.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7610
     Episode_Reward/lifting_object: 172.1100
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.1762
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 157876224
                    Iteration time: 0.88s
                      Time elapsed: 00:26:58
                               ETA: 00:06:38

################################################################################
                     [1m Learning iteration 1606/2000 [0m                     

                       Computation: 102874 steps/s (collection: 0.837s, learning 0.119s)
             Mean action noise std: 8.33
          Mean value_function loss: 30.7914
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 27.9128
                       Mean reward: 843.65
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.7516
     Episode_Reward/lifting_object: 169.3596
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.1753
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 157974528
                    Iteration time: 0.96s
                      Time elapsed: 00:26:59
                               ETA: 00:06:37

################################################################################
                     [1m Learning iteration 1607/2000 [0m                     

                       Computation: 107591 steps/s (collection: 0.806s, learning 0.107s)
             Mean action noise std: 8.34
          Mean value_function loss: 34.2019
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 27.9213
                       Mean reward: 861.72
               Mean episode length: 249.37
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 172.0166
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.1773
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 158072832
                    Iteration time: 0.91s
                      Time elapsed: 00:27:00
                               ETA: 00:06:36

################################################################################
                     [1m Learning iteration 1608/2000 [0m                     

                       Computation: 106572 steps/s (collection: 0.801s, learning 0.121s)
             Mean action noise std: 8.35
          Mean value_function loss: 39.0770
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 27.9304
                       Mean reward: 838.18
               Mean episode length: 241.97
    Episode_Reward/reaching_object: 0.7554
     Episode_Reward/lifting_object: 171.1710
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.1764
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 158171136
                    Iteration time: 0.92s
                      Time elapsed: 00:27:01
                               ETA: 00:06:35

################################################################################
                     [1m Learning iteration 1609/2000 [0m                     

                       Computation: 108307 steps/s (collection: 0.805s, learning 0.103s)
             Mean action noise std: 8.35
          Mean value_function loss: 39.7987
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 27.9413
                       Mean reward: 860.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 171.9080
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.1778
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 158269440
                    Iteration time: 0.91s
                      Time elapsed: 00:27:02
                               ETA: 00:06:34

################################################################################
                     [1m Learning iteration 1610/2000 [0m                     

                       Computation: 111532 steps/s (collection: 0.779s, learning 0.103s)
             Mean action noise std: 8.36
          Mean value_function loss: 27.3703
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 27.9461
                       Mean reward: 877.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 171.8228
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.1783
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 158367744
                    Iteration time: 0.88s
                      Time elapsed: 00:27:03
                               ETA: 00:06:33

################################################################################
                     [1m Learning iteration 1611/2000 [0m                     

                       Computation: 108750 steps/s (collection: 0.791s, learning 0.113s)
             Mean action noise std: 8.36
          Mean value_function loss: 24.9167
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 27.9516
                       Mean reward: 870.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7612
     Episode_Reward/lifting_object: 171.8870
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.1785
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 158466048
                    Iteration time: 0.90s
                      Time elapsed: 00:27:04
                               ETA: 00:06:32

################################################################################
                     [1m Learning iteration 1612/2000 [0m                     

                       Computation: 106722 steps/s (collection: 0.826s, learning 0.095s)
             Mean action noise std: 8.37
          Mean value_function loss: 33.3561
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 27.9565
                       Mean reward: 874.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7689
     Episode_Reward/lifting_object: 174.0171
      Episode_Reward/object_height: 0.0647
        Episode_Reward/action_rate: -0.1790
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158564352
                    Iteration time: 0.92s
                      Time elapsed: 00:27:05
                               ETA: 00:06:30

################################################################################
                     [1m Learning iteration 1613/2000 [0m                     

                       Computation: 110411 steps/s (collection: 0.791s, learning 0.099s)
             Mean action noise std: 8.38
          Mean value_function loss: 32.3478
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 27.9631
                       Mean reward: 859.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 172.4709
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.1790
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158662656
                    Iteration time: 0.89s
                      Time elapsed: 00:27:06
                               ETA: 00:06:29

################################################################################
                     [1m Learning iteration 1614/2000 [0m                     

                       Computation: 107158 steps/s (collection: 0.825s, learning 0.092s)
             Mean action noise std: 8.38
          Mean value_function loss: 35.0120
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 27.9698
                       Mean reward: 860.16
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.7621
     Episode_Reward/lifting_object: 172.2765
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.1779
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 158760960
                    Iteration time: 0.92s
                      Time elapsed: 00:27:07
                               ETA: 00:06:28

################################################################################
                     [1m Learning iteration 1615/2000 [0m                     

                       Computation: 105767 steps/s (collection: 0.834s, learning 0.096s)
             Mean action noise std: 8.39
          Mean value_function loss: 33.1460
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 27.9736
                       Mean reward: 861.33
               Mean episode length: 247.97
    Episode_Reward/reaching_object: 0.7639
     Episode_Reward/lifting_object: 172.0660
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.1773
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 158859264
                    Iteration time: 0.93s
                      Time elapsed: 00:27:08
                               ETA: 00:06:27

################################################################################
                     [1m Learning iteration 1616/2000 [0m                     

                       Computation: 105900 steps/s (collection: 0.832s, learning 0.096s)
             Mean action noise std: 8.39
          Mean value_function loss: 35.0066
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 27.9796
                       Mean reward: 835.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7473
     Episode_Reward/lifting_object: 168.5968
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.1795
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 158957568
                    Iteration time: 0.93s
                      Time elapsed: 00:27:09
                               ETA: 00:06:26

################################################################################
                     [1m Learning iteration 1617/2000 [0m                     

                       Computation: 107900 steps/s (collection: 0.810s, learning 0.101s)
             Mean action noise std: 8.40
          Mean value_function loss: 35.1776
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.9845
                       Mean reward: 865.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 170.9847
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.1787
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 159055872
                    Iteration time: 0.91s
                      Time elapsed: 00:27:09
                               ETA: 00:06:25

################################################################################
                     [1m Learning iteration 1618/2000 [0m                     

                       Computation: 105409 steps/s (collection: 0.824s, learning 0.109s)
             Mean action noise std: 8.41
          Mean value_function loss: 26.1448
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 27.9930
                       Mean reward: 870.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7540
     Episode_Reward/lifting_object: 171.4845
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.1801
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 159154176
                    Iteration time: 0.93s
                      Time elapsed: 00:27:10
                               ETA: 00:06:24

################################################################################
                     [1m Learning iteration 1619/2000 [0m                     

                       Computation: 104826 steps/s (collection: 0.823s, learning 0.115s)
             Mean action noise std: 8.42
          Mean value_function loss: 27.3634
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 28.0068
                       Mean reward: 868.76
               Mean episode length: 249.68
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 172.2270
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.1795
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 159252480
                    Iteration time: 0.94s
                      Time elapsed: 00:27:11
                               ETA: 00:06:23

################################################################################
                     [1m Learning iteration 1620/2000 [0m                     

                       Computation: 103403 steps/s (collection: 0.839s, learning 0.112s)
             Mean action noise std: 8.43
          Mean value_function loss: 40.5378
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 28.0170
                       Mean reward: 855.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7582
     Episode_Reward/lifting_object: 170.2197
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.1797
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 159350784
                    Iteration time: 0.95s
                      Time elapsed: 00:27:12
                               ETA: 00:06:22

################################################################################
                     [1m Learning iteration 1621/2000 [0m                     

                       Computation: 110977 steps/s (collection: 0.785s, learning 0.101s)
             Mean action noise std: 8.43
          Mean value_function loss: 40.7742
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 28.0206
                       Mean reward: 859.49
               Mean episode length: 249.38
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 170.6428
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.1799
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 159449088
                    Iteration time: 0.89s
                      Time elapsed: 00:27:13
                               ETA: 00:06:21

################################################################################
                     [1m Learning iteration 1622/2000 [0m                     

                       Computation: 108745 steps/s (collection: 0.800s, learning 0.104s)
             Mean action noise std: 8.43
          Mean value_function loss: 44.5475
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 28.0247
                       Mean reward: 862.67
               Mean episode length: 248.74
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 172.4567
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.1801
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 159547392
                    Iteration time: 0.90s
                      Time elapsed: 00:27:14
                               ETA: 00:06:20

################################################################################
                     [1m Learning iteration 1623/2000 [0m                     

                       Computation: 108153 steps/s (collection: 0.806s, learning 0.103s)
             Mean action noise std: 8.44
          Mean value_function loss: 39.5055
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 28.0319
                       Mean reward: 869.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 172.4118
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.1806
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 159645696
                    Iteration time: 0.91s
                      Time elapsed: 00:27:15
                               ETA: 00:06:19

################################################################################
                     [1m Learning iteration 1624/2000 [0m                     

                       Computation: 103386 steps/s (collection: 0.839s, learning 0.112s)
             Mean action noise std: 8.45
          Mean value_function loss: 38.1801
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 28.0406
                       Mean reward: 866.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 171.1530
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.1814
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 159744000
                    Iteration time: 0.95s
                      Time elapsed: 00:27:16
                               ETA: 00:06:18

################################################################################
                     [1m Learning iteration 1625/2000 [0m                     

                       Computation: 105144 steps/s (collection: 0.814s, learning 0.121s)
             Mean action noise std: 8.45
          Mean value_function loss: 39.4263
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.0435
                       Mean reward: 860.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7557
     Episode_Reward/lifting_object: 170.7075
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.1798
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 159842304
                    Iteration time: 0.93s
                      Time elapsed: 00:27:17
                               ETA: 00:06:17

################################################################################
                     [1m Learning iteration 1626/2000 [0m                     

                       Computation: 107326 steps/s (collection: 0.824s, learning 0.092s)
             Mean action noise std: 8.46
          Mean value_function loss: 37.4830
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.0482
                       Mean reward: 880.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 173.0987
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.1815
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 159940608
                    Iteration time: 0.92s
                      Time elapsed: 00:27:18
                               ETA: 00:06:16

################################################################################
                     [1m Learning iteration 1627/2000 [0m                     

                       Computation: 100722 steps/s (collection: 0.861s, learning 0.115s)
             Mean action noise std: 8.47
          Mean value_function loss: 35.3411
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.0551
                       Mean reward: 870.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 172.1926
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.1829
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160038912
                    Iteration time: 0.98s
                      Time elapsed: 00:27:19
                               ETA: 00:06:15

################################################################################
                     [1m Learning iteration 1628/2000 [0m                     

                       Computation: 101898 steps/s (collection: 0.864s, learning 0.101s)
             Mean action noise std: 8.47
          Mean value_function loss: 31.9545
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 28.0638
                       Mean reward: 870.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 172.8606
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.1828
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 160137216
                    Iteration time: 0.96s
                      Time elapsed: 00:27:20
                               ETA: 00:06:14

################################################################################
                     [1m Learning iteration 1629/2000 [0m                     

                       Computation: 108183 steps/s (collection: 0.801s, learning 0.108s)
             Mean action noise std: 8.48
          Mean value_function loss: 34.8159
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.0711
                       Mean reward: 865.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 170.9337
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.1836
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 160235520
                    Iteration time: 0.91s
                      Time elapsed: 00:27:21
                               ETA: 00:06:13

################################################################################
                     [1m Learning iteration 1630/2000 [0m                     

                       Computation: 105420 steps/s (collection: 0.838s, learning 0.095s)
             Mean action noise std: 8.49
          Mean value_function loss: 40.9051
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 28.0788
                       Mean reward: 826.24
               Mean episode length: 249.14
    Episode_Reward/reaching_object: 0.7362
     Episode_Reward/lifting_object: 166.7615
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.1841
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 160333824
                    Iteration time: 0.93s
                      Time elapsed: 00:27:22
                               ETA: 00:06:12

################################################################################
                     [1m Learning iteration 1631/2000 [0m                     

                       Computation: 104791 steps/s (collection: 0.839s, learning 0.099s)
             Mean action noise std: 8.49
          Mean value_function loss: 26.4171
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 28.0819
                       Mean reward: 857.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7420
     Episode_Reward/lifting_object: 169.2456
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.1857
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160432128
                    Iteration time: 0.94s
                      Time elapsed: 00:27:23
                               ETA: 00:06:11

################################################################################
                     [1m Learning iteration 1632/2000 [0m                     

                       Computation: 108526 steps/s (collection: 0.798s, learning 0.108s)
             Mean action noise std: 8.50
          Mean value_function loss: 31.0340
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 28.0885
                       Mean reward: 854.13
               Mean episode length: 247.64
    Episode_Reward/reaching_object: 0.7445
     Episode_Reward/lifting_object: 169.0644
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.1847
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 160530432
                    Iteration time: 0.91s
                      Time elapsed: 00:27:23
                               ETA: 00:06:10

################################################################################
                     [1m Learning iteration 1633/2000 [0m                     

                       Computation: 104479 steps/s (collection: 0.842s, learning 0.099s)
             Mean action noise std: 8.51
          Mean value_function loss: 32.0725
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 28.0997
                       Mean reward: 867.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 171.5400
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.1869
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 160628736
                    Iteration time: 0.94s
                      Time elapsed: 00:27:24
                               ETA: 00:06:09

################################################################################
                     [1m Learning iteration 1634/2000 [0m                     

                       Computation: 102718 steps/s (collection: 0.849s, learning 0.108s)
             Mean action noise std: 8.52
          Mean value_function loss: 37.4651
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 28.1112
                       Mean reward: 865.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 171.1161
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.1873
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 160727040
                    Iteration time: 0.96s
                      Time elapsed: 00:27:25
                               ETA: 00:06:08

################################################################################
                     [1m Learning iteration 1635/2000 [0m                     

                       Computation: 106404 steps/s (collection: 0.821s, learning 0.103s)
             Mean action noise std: 8.53
          Mean value_function loss: 29.8657
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.1158
                       Mean reward: 852.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 172.2030
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.1875
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 160825344
                    Iteration time: 0.92s
                      Time elapsed: 00:27:26
                               ETA: 00:06:07

################################################################################
                     [1m Learning iteration 1636/2000 [0m                     

                       Computation: 103632 steps/s (collection: 0.846s, learning 0.102s)
             Mean action noise std: 8.53
          Mean value_function loss: 41.1850
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 28.1213
                       Mean reward: 863.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7594
     Episode_Reward/lifting_object: 172.1439
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.1877
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 160923648
                    Iteration time: 0.95s
                      Time elapsed: 00:27:27
                               ETA: 00:06:06

################################################################################
                     [1m Learning iteration 1637/2000 [0m                     

                       Computation: 103230 steps/s (collection: 0.829s, learning 0.123s)
             Mean action noise std: 8.53
          Mean value_function loss: 37.9771
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 28.1236
                       Mean reward: 852.60
               Mean episode length: 247.03
    Episode_Reward/reaching_object: 0.7564
     Episode_Reward/lifting_object: 170.9252
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.1861
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 161021952
                    Iteration time: 0.95s
                      Time elapsed: 00:27:28
                               ETA: 00:06:05

################################################################################
                     [1m Learning iteration 1638/2000 [0m                     

                       Computation: 103925 steps/s (collection: 0.830s, learning 0.116s)
             Mean action noise std: 8.54
          Mean value_function loss: 40.9878
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 28.1293
                       Mean reward: 867.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7627
     Episode_Reward/lifting_object: 173.8778
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.1878
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 161120256
                    Iteration time: 0.95s
                      Time elapsed: 00:27:29
                               ETA: 00:06:04

################################################################################
                     [1m Learning iteration 1639/2000 [0m                     

                       Computation: 103546 steps/s (collection: 0.842s, learning 0.108s)
             Mean action noise std: 8.55
          Mean value_function loss: 37.3823
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 28.1354
                       Mean reward: 861.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7500
     Episode_Reward/lifting_object: 169.9340
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.1879
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 161218560
                    Iteration time: 0.95s
                      Time elapsed: 00:27:30
                               ETA: 00:06:03

################################################################################
                     [1m Learning iteration 1640/2000 [0m                     

                       Computation: 102688 steps/s (collection: 0.857s, learning 0.100s)
             Mean action noise std: 8.55
          Mean value_function loss: 29.2737
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 28.1447
                       Mean reward: 867.28
               Mean episode length: 249.77
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 171.2526
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.1870
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.6667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 161316864
                    Iteration time: 0.96s
                      Time elapsed: 00:27:31
                               ETA: 00:06:02

################################################################################
                     [1m Learning iteration 1641/2000 [0m                     

                       Computation: 106574 steps/s (collection: 0.804s, learning 0.119s)
             Mean action noise std: 8.56
          Mean value_function loss: 24.4740
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.1479
                       Mean reward: 864.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 171.7309
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.1879
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 161415168
                    Iteration time: 0.92s
                      Time elapsed: 00:27:32
                               ETA: 00:06:01

################################################################################
                     [1m Learning iteration 1642/2000 [0m                     

                       Computation: 101398 steps/s (collection: 0.847s, learning 0.123s)
             Mean action noise std: 8.56
          Mean value_function loss: 30.5038
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 28.1523
                       Mean reward: 861.39
               Mean episode length: 248.89
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 170.7365
      Episode_Reward/object_height: 0.0623
        Episode_Reward/action_rate: -0.1879
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 161513472
                    Iteration time: 0.97s
                      Time elapsed: 00:27:33
                               ETA: 00:06:00

################################################################################
                     [1m Learning iteration 1643/2000 [0m                     

                       Computation: 107862 steps/s (collection: 0.816s, learning 0.096s)
             Mean action noise std: 8.57
          Mean value_function loss: 30.1088
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 28.1560
                       Mean reward: 856.13
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.7583
     Episode_Reward/lifting_object: 172.2952
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.1883
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 161611776
                    Iteration time: 0.91s
                      Time elapsed: 00:27:34
                               ETA: 00:05:59

################################################################################
                     [1m Learning iteration 1644/2000 [0m                     

                       Computation: 104254 steps/s (collection: 0.849s, learning 0.094s)
             Mean action noise std: 8.57
          Mean value_function loss: 26.5210
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.1595
                       Mean reward: 863.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 171.1031
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.1876
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 161710080
                    Iteration time: 0.94s
                      Time elapsed: 00:27:35
                               ETA: 00:05:58

################################################################################
                     [1m Learning iteration 1645/2000 [0m                     

                       Computation: 109322 steps/s (collection: 0.799s, learning 0.100s)
             Mean action noise std: 8.58
          Mean value_function loss: 31.0097
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 28.1671
                       Mean reward: 867.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 172.4971
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.1885
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 161808384
                    Iteration time: 0.90s
                      Time elapsed: 00:27:36
                               ETA: 00:05:57

################################################################################
                     [1m Learning iteration 1646/2000 [0m                     

                       Computation: 100045 steps/s (collection: 0.873s, learning 0.109s)
             Mean action noise std: 8.60
          Mean value_function loss: 37.7183
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.1789
                       Mean reward: 862.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7585
     Episode_Reward/lifting_object: 171.7365
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.1884
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 161906688
                    Iteration time: 0.98s
                      Time elapsed: 00:27:37
                               ETA: 00:05:56

################################################################################
                     [1m Learning iteration 1647/2000 [0m                     

                       Computation: 105423 steps/s (collection: 0.834s, learning 0.098s)
             Mean action noise std: 8.61
          Mean value_function loss: 32.1340
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 28.1900
                       Mean reward: 860.36
               Mean episode length: 249.84
    Episode_Reward/reaching_object: 0.7568
     Episode_Reward/lifting_object: 171.8905
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.1881
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 162004992
                    Iteration time: 0.93s
                      Time elapsed: 00:27:38
                               ETA: 00:05:55

################################################################################
                     [1m Learning iteration 1648/2000 [0m                     

                       Computation: 110529 steps/s (collection: 0.799s, learning 0.091s)
             Mean action noise std: 8.61
          Mean value_function loss: 41.1619
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 28.1992
                       Mean reward: 849.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 171.4198
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.1887
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 162103296
                    Iteration time: 0.89s
                      Time elapsed: 00:27:38
                               ETA: 00:05:54

################################################################################
                     [1m Learning iteration 1649/2000 [0m                     

                       Computation: 106448 steps/s (collection: 0.832s, learning 0.092s)
             Mean action noise std: 8.62
          Mean value_function loss: 35.1608
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 28.2063
                       Mean reward: 868.72
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7554
     Episode_Reward/lifting_object: 170.6906
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.1878
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 162201600
                    Iteration time: 0.92s
                      Time elapsed: 00:27:39
                               ETA: 00:05:53

################################################################################
                     [1m Learning iteration 1650/2000 [0m                     

                       Computation: 109481 steps/s (collection: 0.806s, learning 0.092s)
             Mean action noise std: 8.63
          Mean value_function loss: 45.4264
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 28.2151
                       Mean reward: 878.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7541
     Episode_Reward/lifting_object: 171.0901
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.1881
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.8750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 162299904
                    Iteration time: 0.90s
                      Time elapsed: 00:27:40
                               ETA: 00:05:52

################################################################################
                     [1m Learning iteration 1651/2000 [0m                     

                       Computation: 110654 steps/s (collection: 0.798s, learning 0.090s)
             Mean action noise std: 8.64
          Mean value_function loss: 53.6979
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 28.2261
                       Mean reward: 854.03
               Mean episode length: 246.28
    Episode_Reward/reaching_object: 0.7536
     Episode_Reward/lifting_object: 170.8271
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.1891
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 162398208
                    Iteration time: 0.89s
                      Time elapsed: 00:27:41
                               ETA: 00:05:51

################################################################################
                     [1m Learning iteration 1652/2000 [0m                     

                       Computation: 109089 steps/s (collection: 0.804s, learning 0.098s)
             Mean action noise std: 8.65
          Mean value_function loss: 51.8823
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.2359
                       Mean reward: 840.98
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.7519
     Episode_Reward/lifting_object: 170.8133
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.1893
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 162496512
                    Iteration time: 0.90s
                      Time elapsed: 00:27:42
                               ETA: 00:05:50

################################################################################
                     [1m Learning iteration 1653/2000 [0m                     

                       Computation: 107866 steps/s (collection: 0.802s, learning 0.109s)
             Mean action noise std: 8.66
          Mean value_function loss: 38.8266
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 28.2430
                       Mean reward: 846.82
               Mean episode length: 244.99
    Episode_Reward/reaching_object: 0.7451
     Episode_Reward/lifting_object: 169.3779
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.1868
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 162594816
                    Iteration time: 0.91s
                      Time elapsed: 00:27:43
                               ETA: 00:05:48

################################################################################
                     [1m Learning iteration 1654/2000 [0m                     

                       Computation: 102783 steps/s (collection: 0.843s, learning 0.113s)
             Mean action noise std: 8.66
          Mean value_function loss: 39.0810
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 28.2472
                       Mean reward: 853.67
               Mean episode length: 248.70
    Episode_Reward/reaching_object: 0.7584
     Episode_Reward/lifting_object: 171.5749
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.1887
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 162693120
                    Iteration time: 0.96s
                      Time elapsed: 00:27:44
                               ETA: 00:05:47

################################################################################
                     [1m Learning iteration 1655/2000 [0m                     

                       Computation: 97994 steps/s (collection: 0.866s, learning 0.138s)
             Mean action noise std: 8.67
          Mean value_function loss: 33.6706
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 28.2524
                       Mean reward: 851.74
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 171.6670
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.1901
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 162791424
                    Iteration time: 1.00s
                      Time elapsed: 00:27:45
                               ETA: 00:05:46

################################################################################
                     [1m Learning iteration 1656/2000 [0m                     

                       Computation: 105330 steps/s (collection: 0.819s, learning 0.114s)
             Mean action noise std: 8.68
          Mean value_function loss: 29.4234
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 28.2589
                       Mean reward: 848.18
               Mean episode length: 249.50
    Episode_Reward/reaching_object: 0.7470
     Episode_Reward/lifting_object: 168.2325
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.1900
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 162889728
                    Iteration time: 0.93s
                      Time elapsed: 00:27:46
                               ETA: 00:05:45

################################################################################
                     [1m Learning iteration 1657/2000 [0m                     

                       Computation: 109368 steps/s (collection: 0.801s, learning 0.098s)
             Mean action noise std: 8.69
          Mean value_function loss: 31.4194
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 28.2690
                       Mean reward: 869.29
               Mean episode length: 249.50
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 172.4948
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.1911
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 162988032
                    Iteration time: 0.90s
                      Time elapsed: 00:27:47
                               ETA: 00:05:44

################################################################################
                     [1m Learning iteration 1658/2000 [0m                     

                       Computation: 103761 steps/s (collection: 0.837s, learning 0.111s)
             Mean action noise std: 8.70
          Mean value_function loss: 36.6851
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 28.2804
                       Mean reward: 852.25
               Mean episode length: 249.04
    Episode_Reward/reaching_object: 0.7566
     Episode_Reward/lifting_object: 170.7645
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.1901
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 163086336
                    Iteration time: 0.95s
                      Time elapsed: 00:27:48
                               ETA: 00:05:43

################################################################################
                     [1m Learning iteration 1659/2000 [0m                     

                       Computation: 106233 steps/s (collection: 0.824s, learning 0.101s)
             Mean action noise std: 8.71
          Mean value_function loss: 29.0342
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 28.2931
                       Mean reward: 859.88
               Mean episode length: 249.63
    Episode_Reward/reaching_object: 0.7496
     Episode_Reward/lifting_object: 170.4071
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.1903
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 163184640
                    Iteration time: 0.93s
                      Time elapsed: 00:27:49
                               ETA: 00:05:42

################################################################################
                     [1m Learning iteration 1660/2000 [0m                     

                       Computation: 104098 steps/s (collection: 0.836s, learning 0.108s)
             Mean action noise std: 8.71
          Mean value_function loss: 36.9800
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 28.2969
                       Mean reward: 853.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7480
     Episode_Reward/lifting_object: 169.7520
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.1910
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 163282944
                    Iteration time: 0.94s
                      Time elapsed: 00:27:50
                               ETA: 00:05:41

################################################################################
                     [1m Learning iteration 1661/2000 [0m                     

                       Computation: 109566 steps/s (collection: 0.799s, learning 0.099s)
             Mean action noise std: 8.72
          Mean value_function loss: 34.4970
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 28.3023
                       Mean reward: 856.93
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.7585
     Episode_Reward/lifting_object: 171.0417
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.1921
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 163381248
                    Iteration time: 0.90s
                      Time elapsed: 00:27:50
                               ETA: 00:05:40

################################################################################
                     [1m Learning iteration 1662/2000 [0m                     

                       Computation: 108159 steps/s (collection: 0.807s, learning 0.102s)
             Mean action noise std: 8.73
          Mean value_function loss: 29.9963
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 28.3106
                       Mean reward: 852.41
               Mean episode length: 248.32
    Episode_Reward/reaching_object: 0.7535
     Episode_Reward/lifting_object: 169.8640
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.1906
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 163479552
                    Iteration time: 0.91s
                      Time elapsed: 00:27:51
                               ETA: 00:05:39

################################################################################
                     [1m Learning iteration 1663/2000 [0m                     

                       Computation: 107358 steps/s (collection: 0.819s, learning 0.097s)
             Mean action noise std: 8.74
          Mean value_function loss: 27.6787
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 28.3234
                       Mean reward: 847.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 171.1994
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.1930
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 163577856
                    Iteration time: 0.92s
                      Time elapsed: 00:27:52
                               ETA: 00:05:38

################################################################################
                     [1m Learning iteration 1664/2000 [0m                     

                       Computation: 109914 steps/s (collection: 0.800s, learning 0.095s)
             Mean action noise std: 8.76
          Mean value_function loss: 26.0794
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 28.3346
                       Mean reward: 852.29
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 173.0743
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.1928
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 163676160
                    Iteration time: 0.89s
                      Time elapsed: 00:27:53
                               ETA: 00:05:37

################################################################################
                     [1m Learning iteration 1665/2000 [0m                     

                       Computation: 109928 steps/s (collection: 0.792s, learning 0.102s)
             Mean action noise std: 8.76
          Mean value_function loss: 27.5042
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 28.3435
                       Mean reward: 856.09
               Mean episode length: 247.69
    Episode_Reward/reaching_object: 0.7584
     Episode_Reward/lifting_object: 172.0961
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.1916
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 163774464
                    Iteration time: 0.89s
                      Time elapsed: 00:27:54
                               ETA: 00:05:36

################################################################################
                     [1m Learning iteration 1666/2000 [0m                     

                       Computation: 63859 steps/s (collection: 1.444s, learning 0.095s)
             Mean action noise std: 8.77
          Mean value_function loss: 26.3580
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 28.3501
                       Mean reward: 876.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 173.0962
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.1939
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163872768
                    Iteration time: 1.54s
                      Time elapsed: 00:27:56
                               ETA: 00:05:35

################################################################################
                     [1m Learning iteration 1667/2000 [0m                     

                       Computation: 33377 steps/s (collection: 2.843s, learning 0.103s)
             Mean action noise std: 8.78
          Mean value_function loss: 33.4438
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 28.3623
                       Mean reward: 866.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 171.7627
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.1918
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 163971072
                    Iteration time: 2.95s
                      Time elapsed: 00:27:59
                               ETA: 00:05:35

################################################################################
                     [1m Learning iteration 1668/2000 [0m                     

                       Computation: 31599 steps/s (collection: 2.974s, learning 0.137s)
             Mean action noise std: 8.79
          Mean value_function loss: 40.6634
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 28.3688
                       Mean reward: 859.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7646
     Episode_Reward/lifting_object: 171.9371
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.1931
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 164069376
                    Iteration time: 3.11s
                      Time elapsed: 00:28:02
                               ETA: 00:05:34

################################################################################
                     [1m Learning iteration 1669/2000 [0m                     

                       Computation: 32362 steps/s (collection: 2.911s, learning 0.127s)
             Mean action noise std: 8.80
          Mean value_function loss: 30.9910
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 28.3746
                       Mean reward: 864.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7674
     Episode_Reward/lifting_object: 173.0803
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.1921
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 164167680
                    Iteration time: 3.04s
                      Time elapsed: 00:28:05
                               ETA: 00:05:34

################################################################################
                     [1m Learning iteration 1670/2000 [0m                     

                       Computation: 30746 steps/s (collection: 3.068s, learning 0.129s)
             Mean action noise std: 8.81
          Mean value_function loss: 36.6382
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 28.3885
                       Mean reward: 866.47
               Mean episode length: 249.37
    Episode_Reward/reaching_object: 0.7664
     Episode_Reward/lifting_object: 171.5757
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.1932
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 164265984
                    Iteration time: 3.20s
                      Time elapsed: 00:28:08
                               ETA: 00:05:33

################################################################################
                     [1m Learning iteration 1671/2000 [0m                     

                       Computation: 30579 steps/s (collection: 3.074s, learning 0.141s)
             Mean action noise std: 8.82
          Mean value_function loss: 44.5073
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 28.3961
                       Mean reward: 870.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7621
     Episode_Reward/lifting_object: 171.2023
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.1921
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 19.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 164364288
                    Iteration time: 3.21s
                      Time elapsed: 00:28:11
                               ETA: 00:05:32

################################################################################
                     [1m Learning iteration 1672/2000 [0m                     

                       Computation: 30761 steps/s (collection: 3.065s, learning 0.131s)
             Mean action noise std: 8.83
          Mean value_function loss: 30.0812
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.4031
                       Mean reward: 865.48
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.7674
     Episode_Reward/lifting_object: 172.8987
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.1924
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 164462592
                    Iteration time: 3.20s
                      Time elapsed: 00:28:14
                               ETA: 00:05:32

################################################################################
                     [1m Learning iteration 1673/2000 [0m                     

                       Computation: 30071 steps/s (collection: 3.139s, learning 0.130s)
             Mean action noise std: 8.83
          Mean value_function loss: 40.6119
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 28.4074
                       Mean reward: 872.14
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7651
     Episode_Reward/lifting_object: 172.6203
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.1922
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 164560896
                    Iteration time: 3.27s
                      Time elapsed: 00:28:18
                               ETA: 00:05:31

################################################################################
                     [1m Learning iteration 1674/2000 [0m                     

                       Computation: 30714 steps/s (collection: 3.074s, learning 0.127s)
             Mean action noise std: 8.84
          Mean value_function loss: 31.6165
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 28.4132
                       Mean reward: 861.99
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 171.5850
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.1923
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 164659200
                    Iteration time: 3.20s
                      Time elapsed: 00:28:21
                               ETA: 00:05:31

################################################################################
                     [1m Learning iteration 1675/2000 [0m                     

                       Computation: 32262 steps/s (collection: 2.941s, learning 0.106s)
             Mean action noise std: 8.85
          Mean value_function loss: 37.7228
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 28.4181
                       Mean reward: 856.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7627
     Episode_Reward/lifting_object: 172.6573
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.1924
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 164757504
                    Iteration time: 3.05s
                      Time elapsed: 00:28:24
                               ETA: 00:05:30

################################################################################
                     [1m Learning iteration 1676/2000 [0m                     

                       Computation: 107547 steps/s (collection: 0.818s, learning 0.096s)
             Mean action noise std: 8.85
          Mean value_function loss: 37.3524
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.4243
                       Mean reward: 865.36
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.7502
     Episode_Reward/lifting_object: 170.1370
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.1922
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 164855808
                    Iteration time: 0.91s
                      Time elapsed: 00:28:25
                               ETA: 00:05:29

################################################################################
                     [1m Learning iteration 1677/2000 [0m                     

                       Computation: 112541 steps/s (collection: 0.777s, learning 0.097s)
             Mean action noise std: 8.86
          Mean value_function loss: 34.4637
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 28.4325
                       Mean reward: 853.91
               Mean episode length: 249.02
    Episode_Reward/reaching_object: 0.7559
     Episode_Reward/lifting_object: 170.9860
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.1924
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 164954112
                    Iteration time: 0.87s
                      Time elapsed: 00:28:26
                               ETA: 00:05:28

################################################################################
                     [1m Learning iteration 1678/2000 [0m                     

                       Computation: 108325 steps/s (collection: 0.817s, learning 0.091s)
             Mean action noise std: 8.86
          Mean value_function loss: 30.2511
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 28.4366
                       Mean reward: 849.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7498
     Episode_Reward/lifting_object: 169.2511
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.1919
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 165052416
                    Iteration time: 0.91s
                      Time elapsed: 00:28:27
                               ETA: 00:05:27

################################################################################
                     [1m Learning iteration 1679/2000 [0m                     

                       Computation: 106907 steps/s (collection: 0.821s, learning 0.099s)
             Mean action noise std: 8.87
          Mean value_function loss: 38.5292
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.4403
                       Mean reward: 866.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 172.2912
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.1933
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 165150720
                    Iteration time: 0.92s
                      Time elapsed: 00:28:27
                               ETA: 00:05:26

################################################################################
                     [1m Learning iteration 1680/2000 [0m                     

                       Computation: 111024 steps/s (collection: 0.789s, learning 0.096s)
             Mean action noise std: 8.89
          Mean value_function loss: 33.0249
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.4484
                       Mean reward: 851.50
               Mean episode length: 249.52
    Episode_Reward/reaching_object: 0.7545
     Episode_Reward/lifting_object: 170.9953
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.1927
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 165249024
                    Iteration time: 0.89s
                      Time elapsed: 00:28:28
                               ETA: 00:05:25

################################################################################
                     [1m Learning iteration 1681/2000 [0m                     

                       Computation: 108839 steps/s (collection: 0.803s, learning 0.100s)
             Mean action noise std: 8.89
          Mean value_function loss: 30.8760
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 28.4573
                       Mean reward: 866.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 172.1511
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.1925
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 165347328
                    Iteration time: 0.90s
                      Time elapsed: 00:28:29
                               ETA: 00:05:24

################################################################################
                     [1m Learning iteration 1682/2000 [0m                     

                       Computation: 110473 steps/s (collection: 0.793s, learning 0.097s)
             Mean action noise std: 8.90
          Mean value_function loss: 41.5641
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 28.4639
                       Mean reward: 852.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7551
     Episode_Reward/lifting_object: 170.6636
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.1938
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 165445632
                    Iteration time: 0.89s
                      Time elapsed: 00:28:30
                               ETA: 00:05:23

################################################################################
                     [1m Learning iteration 1683/2000 [0m                     

                       Computation: 111142 steps/s (collection: 0.778s, learning 0.107s)
             Mean action noise std: 8.91
          Mean value_function loss: 40.2952
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 28.4701
                       Mean reward: 834.94
               Mean episode length: 244.73
    Episode_Reward/reaching_object: 0.7469
     Episode_Reward/lifting_object: 169.3384
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.1929
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 165543936
                    Iteration time: 0.88s
                      Time elapsed: 00:28:31
                               ETA: 00:05:22

################################################################################
                     [1m Learning iteration 1684/2000 [0m                     

                       Computation: 109483 steps/s (collection: 0.800s, learning 0.098s)
             Mean action noise std: 8.91
          Mean value_function loss: 35.0842
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 28.4741
                       Mean reward: 857.36
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.7558
     Episode_Reward/lifting_object: 170.9881
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.1922
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 165642240
                    Iteration time: 0.90s
                      Time elapsed: 00:28:32
                               ETA: 00:05:21

################################################################################
                     [1m Learning iteration 1685/2000 [0m                     

                       Computation: 107749 steps/s (collection: 0.804s, learning 0.108s)
             Mean action noise std: 8.92
          Mean value_function loss: 28.0202
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.4815
                       Mean reward: 875.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 171.8082
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.1944
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 165740544
                    Iteration time: 0.91s
                      Time elapsed: 00:28:33
                               ETA: 00:05:20

################################################################################
                     [1m Learning iteration 1686/2000 [0m                     

                       Computation: 101898 steps/s (collection: 0.858s, learning 0.107s)
             Mean action noise std: 8.93
          Mean value_function loss: 27.0868
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 28.4865
                       Mean reward: 852.34
               Mean episode length: 247.91
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 172.4631
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.1961
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 165838848
                    Iteration time: 0.96s
                      Time elapsed: 00:28:34
                               ETA: 00:05:19

################################################################################
                     [1m Learning iteration 1687/2000 [0m                     

                       Computation: 103381 steps/s (collection: 0.837s, learning 0.114s)
             Mean action noise std: 8.93
          Mean value_function loss: 22.8274
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 28.4912
                       Mean reward: 857.45
               Mean episode length: 248.49
    Episode_Reward/reaching_object: 0.7533
     Episode_Reward/lifting_object: 170.5588
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.1961
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 165937152
                    Iteration time: 0.95s
                      Time elapsed: 00:28:35
                               ETA: 00:05:18

################################################################################
                     [1m Learning iteration 1688/2000 [0m                     

                       Computation: 106563 steps/s (collection: 0.811s, learning 0.112s)
             Mean action noise std: 8.94
          Mean value_function loss: 42.3445
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 28.4979
                       Mean reward: 847.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7578
     Episode_Reward/lifting_object: 170.4651
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.1970
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 166035456
                    Iteration time: 0.92s
                      Time elapsed: 00:28:36
                               ETA: 00:05:17

################################################################################
                     [1m Learning iteration 1689/2000 [0m                     

                       Computation: 110413 steps/s (collection: 0.790s, learning 0.101s)
             Mean action noise std: 8.95
          Mean value_function loss: 28.6744
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 28.5075
                       Mean reward: 868.01
               Mean episode length: 249.83
    Episode_Reward/reaching_object: 0.7532
     Episode_Reward/lifting_object: 169.7451
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.1948
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 166133760
                    Iteration time: 0.89s
                      Time elapsed: 00:28:37
                               ETA: 00:05:15

################################################################################
                     [1m Learning iteration 1690/2000 [0m                     

                       Computation: 105669 steps/s (collection: 0.816s, learning 0.114s)
             Mean action noise std: 8.96
          Mean value_function loss: 33.3819
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.5141
                       Mean reward: 859.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7563
     Episode_Reward/lifting_object: 170.2777
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.1953
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 166232064
                    Iteration time: 0.93s
                      Time elapsed: 00:28:37
                               ETA: 00:05:14

################################################################################
                     [1m Learning iteration 1691/2000 [0m                     

                       Computation: 107992 steps/s (collection: 0.809s, learning 0.101s)
             Mean action noise std: 8.97
          Mean value_function loss: 31.8884
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.5197
                       Mean reward: 860.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 171.9104
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.1957
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 166330368
                    Iteration time: 0.91s
                      Time elapsed: 00:28:38
                               ETA: 00:05:13

################################################################################
                     [1m Learning iteration 1692/2000 [0m                     

                       Computation: 110903 steps/s (collection: 0.786s, learning 0.101s)
             Mean action noise std: 8.97
          Mean value_function loss: 23.1692
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 28.5253
                       Mean reward: 870.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 171.7139
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.1974
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166428672
                    Iteration time: 0.89s
                      Time elapsed: 00:28:39
                               ETA: 00:05:12

################################################################################
                     [1m Learning iteration 1693/2000 [0m                     

                       Computation: 109731 steps/s (collection: 0.776s, learning 0.120s)
             Mean action noise std: 8.98
          Mean value_function loss: 30.4021
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 28.5297
                       Mean reward: 856.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7669
     Episode_Reward/lifting_object: 172.3829
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.1965
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 166526976
                    Iteration time: 0.90s
                      Time elapsed: 00:28:40
                               ETA: 00:05:11

################################################################################
                     [1m Learning iteration 1694/2000 [0m                     

                       Computation: 112267 steps/s (collection: 0.776s, learning 0.100s)
             Mean action noise std: 8.99
          Mean value_function loss: 42.7315
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 28.5366
                       Mean reward: 831.74
               Mean episode length: 245.58
    Episode_Reward/reaching_object: 0.7532
     Episode_Reward/lifting_object: 169.5333
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.1959
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 166625280
                    Iteration time: 0.88s
                      Time elapsed: 00:28:41
                               ETA: 00:05:10

################################################################################
                     [1m Learning iteration 1695/2000 [0m                     

                       Computation: 111423 steps/s (collection: 0.784s, learning 0.099s)
             Mean action noise std: 8.99
          Mean value_function loss: 32.7066
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 28.5395
                       Mean reward: 875.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 172.4119
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.1968
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 166723584
                    Iteration time: 0.88s
                      Time elapsed: 00:28:42
                               ETA: 00:05:09

################################################################################
                     [1m Learning iteration 1696/2000 [0m                     

                       Computation: 107423 steps/s (collection: 0.817s, learning 0.098s)
             Mean action noise std: 8.99
          Mean value_function loss: 33.2353
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 28.5430
                       Mean reward: 864.65
               Mean episode length: 248.65
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 173.3832
      Episode_Reward/object_height: 0.0623
        Episode_Reward/action_rate: -0.1967
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 166821888
                    Iteration time: 0.92s
                      Time elapsed: 00:28:43
                               ETA: 00:05:08

################################################################################
                     [1m Learning iteration 1697/2000 [0m                     

                       Computation: 112326 steps/s (collection: 0.773s, learning 0.102s)
             Mean action noise std: 9.00
          Mean value_function loss: 24.4974
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.5497
                       Mean reward: 855.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 172.3835
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.1970
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 166920192
                    Iteration time: 0.88s
                      Time elapsed: 00:28:44
                               ETA: 00:05:07

################################################################################
                     [1m Learning iteration 1698/2000 [0m                     

                       Computation: 108470 steps/s (collection: 0.810s, learning 0.097s)
             Mean action noise std: 9.02
          Mean value_function loss: 32.7187
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.5595
                       Mean reward: 862.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7572
     Episode_Reward/lifting_object: 170.3418
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.1974
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 167018496
                    Iteration time: 0.91s
                      Time elapsed: 00:28:45
                               ETA: 00:05:06

################################################################################
                     [1m Learning iteration 1699/2000 [0m                     

                       Computation: 111680 steps/s (collection: 0.787s, learning 0.094s)
             Mean action noise std: 9.02
          Mean value_function loss: 29.9149
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 28.5729
                       Mean reward: 862.17
               Mean episode length: 248.69
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 171.3859
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.1959
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 167116800
                    Iteration time: 0.88s
                      Time elapsed: 00:28:46
                               ETA: 00:05:05

################################################################################
                     [1m Learning iteration 1700/2000 [0m                     

                       Computation: 111871 steps/s (collection: 0.790s, learning 0.089s)
             Mean action noise std: 9.03
          Mean value_function loss: 33.2422
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 28.5795
                       Mean reward: 858.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 171.0292
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.1958
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 167215104
                    Iteration time: 0.88s
                      Time elapsed: 00:28:46
                               ETA: 00:05:04

################################################################################
                     [1m Learning iteration 1701/2000 [0m                     

                       Computation: 110987 steps/s (collection: 0.780s, learning 0.106s)
             Mean action noise std: 9.04
          Mean value_function loss: 39.1850
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 28.5869
                       Mean reward: 869.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 172.7011
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.1955
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 167313408
                    Iteration time: 0.89s
                      Time elapsed: 00:28:47
                               ETA: 00:05:03

################################################################################
                     [1m Learning iteration 1702/2000 [0m                     

                       Computation: 109849 steps/s (collection: 0.789s, learning 0.106s)
             Mean action noise std: 9.04
          Mean value_function loss: 25.5136
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.5908
                       Mean reward: 851.01
               Mean episode length: 248.74
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 171.8768
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.1964
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 167411712
                    Iteration time: 0.89s
                      Time elapsed: 00:28:48
                               ETA: 00:05:02

################################################################################
                     [1m Learning iteration 1703/2000 [0m                     

                       Computation: 111287 steps/s (collection: 0.784s, learning 0.099s)
             Mean action noise std: 9.05
          Mean value_function loss: 31.4896
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 28.5979
                       Mean reward: 868.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 171.7825
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.1965
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 167510016
                    Iteration time: 0.88s
                      Time elapsed: 00:28:49
                               ETA: 00:05:01

################################################################################
                     [1m Learning iteration 1704/2000 [0m                     

                       Computation: 97787 steps/s (collection: 0.874s, learning 0.132s)
             Mean action noise std: 9.07
          Mean value_function loss: 23.8791
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 28.6098
                       Mean reward: 860.37
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 171.8484
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.1960
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 167608320
                    Iteration time: 1.01s
                      Time elapsed: 00:28:50
                               ETA: 00:05:00

################################################################################
                     [1m Learning iteration 1705/2000 [0m                     

                       Computation: 87179 steps/s (collection: 0.979s, learning 0.149s)
             Mean action noise std: 9.08
          Mean value_function loss: 36.5845
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 28.6260
                       Mean reward: 867.57
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.7582
     Episode_Reward/lifting_object: 171.8280
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.1952
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 167706624
                    Iteration time: 1.13s
                      Time elapsed: 00:28:51
                               ETA: 00:04:59

################################################################################
                     [1m Learning iteration 1706/2000 [0m                     

                       Computation: 107493 steps/s (collection: 0.815s, learning 0.100s)
             Mean action noise std: 9.09
          Mean value_function loss: 23.9881
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 28.6368
                       Mean reward: 852.97
               Mean episode length: 247.67
    Episode_Reward/reaching_object: 0.7493
     Episode_Reward/lifting_object: 170.7319
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.1970
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 167804928
                    Iteration time: 0.91s
                      Time elapsed: 00:28:52
                               ETA: 00:04:58

################################################################################
                     [1m Learning iteration 1707/2000 [0m                     

                       Computation: 110469 steps/s (collection: 0.785s, learning 0.105s)
             Mean action noise std: 9.10
          Mean value_function loss: 28.1378
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.6454
                       Mean reward: 862.89
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 173.9863
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.1972
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 167903232
                    Iteration time: 0.89s
                      Time elapsed: 00:28:53
                               ETA: 00:04:57

################################################################################
                     [1m Learning iteration 1708/2000 [0m                     

                       Computation: 110006 steps/s (collection: 0.797s, learning 0.097s)
             Mean action noise std: 9.11
          Mean value_function loss: 36.0113
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 28.6563
                       Mean reward: 875.89
               Mean episode length: 249.27
    Episode_Reward/reaching_object: 0.7585
     Episode_Reward/lifting_object: 173.1974
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.1989
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 168001536
                    Iteration time: 0.89s
                      Time elapsed: 00:28:54
                               ETA: 00:04:56

################################################################################
                     [1m Learning iteration 1709/2000 [0m                     

                       Computation: 113318 steps/s (collection: 0.771s, learning 0.096s)
             Mean action noise std: 9.12
          Mean value_function loss: 25.5521
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 28.6668
                       Mean reward: 859.49
               Mean episode length: 249.03
    Episode_Reward/reaching_object: 0.7628
     Episode_Reward/lifting_object: 172.5840
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.1986
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 168099840
                    Iteration time: 0.87s
                      Time elapsed: 00:28:55
                               ETA: 00:04:55

################################################################################
                     [1m Learning iteration 1710/2000 [0m                     

                       Computation: 111499 steps/s (collection: 0.792s, learning 0.090s)
             Mean action noise std: 9.13
          Mean value_function loss: 36.4776
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 28.6729
                       Mean reward: 872.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7562
     Episode_Reward/lifting_object: 172.6995
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.1990
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 168198144
                    Iteration time: 0.88s
                      Time elapsed: 00:28:56
                               ETA: 00:04:54

################################################################################
                     [1m Learning iteration 1711/2000 [0m                     

                       Computation: 112352 steps/s (collection: 0.784s, learning 0.091s)
             Mean action noise std: 9.14
          Mean value_function loss: 32.6593
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 28.6800
                       Mean reward: 864.75
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7544
     Episode_Reward/lifting_object: 171.3064
      Episode_Reward/object_height: 0.0615
        Episode_Reward/action_rate: -0.2001
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 168296448
                    Iteration time: 0.87s
                      Time elapsed: 00:28:57
                               ETA: 00:04:53

################################################################################
                     [1m Learning iteration 1712/2000 [0m                     

                       Computation: 113994 steps/s (collection: 0.773s, learning 0.090s)
             Mean action noise std: 9.14
          Mean value_function loss: 29.5483
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 28.6888
                       Mean reward: 848.18
               Mean episode length: 248.86
    Episode_Reward/reaching_object: 0.7512
     Episode_Reward/lifting_object: 171.0948
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.1988
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 168394752
                    Iteration time: 0.86s
                      Time elapsed: 00:28:57
                               ETA: 00:04:52

################################################################################
                     [1m Learning iteration 1713/2000 [0m                     

                       Computation: 111651 steps/s (collection: 0.783s, learning 0.097s)
             Mean action noise std: 9.15
          Mean value_function loss: 32.8935
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.6969
                       Mean reward: 866.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7544
     Episode_Reward/lifting_object: 171.9477
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.1993
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.4167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 168493056
                    Iteration time: 0.88s
                      Time elapsed: 00:28:58
                               ETA: 00:04:51

################################################################################
                     [1m Learning iteration 1714/2000 [0m                     

                       Computation: 113393 steps/s (collection: 0.770s, learning 0.097s)
             Mean action noise std: 9.16
          Mean value_function loss: 29.8683
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 28.7072
                       Mean reward: 844.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7484
     Episode_Reward/lifting_object: 169.8441
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.1993
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 168591360
                    Iteration time: 0.87s
                      Time elapsed: 00:28:59
                               ETA: 00:04:50

################################################################################
                     [1m Learning iteration 1715/2000 [0m                     

                       Computation: 115271 steps/s (collection: 0.758s, learning 0.095s)
             Mean action noise std: 9.17
          Mean value_function loss: 24.7606
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.7139
                       Mean reward: 862.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7590
     Episode_Reward/lifting_object: 171.2636
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.2015
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 168689664
                    Iteration time: 0.85s
                      Time elapsed: 00:29:00
                               ETA: 00:04:49

################################################################################
                     [1m Learning iteration 1716/2000 [0m                     

                       Computation: 115124 steps/s (collection: 0.761s, learning 0.093s)
             Mean action noise std: 9.18
          Mean value_function loss: 28.2311
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 28.7228
                       Mean reward: 872.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 172.9532
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.2024
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168787968
                    Iteration time: 0.85s
                      Time elapsed: 00:29:01
                               ETA: 00:04:48

################################################################################
                     [1m Learning iteration 1717/2000 [0m                     

                       Computation: 112251 steps/s (collection: 0.787s, learning 0.089s)
             Mean action noise std: 9.19
          Mean value_function loss: 26.7193
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.7317
                       Mean reward: 864.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7549
     Episode_Reward/lifting_object: 172.1535
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.2011
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 168886272
                    Iteration time: 0.88s
                      Time elapsed: 00:29:02
                               ETA: 00:04:46

################################################################################
                     [1m Learning iteration 1718/2000 [0m                     

                       Computation: 115125 steps/s (collection: 0.757s, learning 0.097s)
             Mean action noise std: 9.20
          Mean value_function loss: 35.5937
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 28.7379
                       Mean reward: 856.46
               Mean episode length: 246.83
    Episode_Reward/reaching_object: 0.7620
     Episode_Reward/lifting_object: 172.3833
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.2014
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 168984576
                    Iteration time: 0.85s
                      Time elapsed: 00:29:03
                               ETA: 00:04:45

################################################################################
                     [1m Learning iteration 1719/2000 [0m                     

                       Computation: 113100 steps/s (collection: 0.780s, learning 0.090s)
             Mean action noise std: 9.21
          Mean value_function loss: 39.0687
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 28.7466
                       Mean reward: 869.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 172.4500
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.2014
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 169082880
                    Iteration time: 0.87s
                      Time elapsed: 00:29:03
                               ETA: 00:04:44

################################################################################
                     [1m Learning iteration 1720/2000 [0m                     

                       Computation: 115480 steps/s (collection: 0.760s, learning 0.091s)
             Mean action noise std: 9.22
          Mean value_function loss: 29.6607
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 28.7582
                       Mean reward: 855.88
               Mean episode length: 245.81
    Episode_Reward/reaching_object: 0.7538
     Episode_Reward/lifting_object: 171.1641
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.2016
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 169181184
                    Iteration time: 0.85s
                      Time elapsed: 00:29:04
                               ETA: 00:04:43

################################################################################
                     [1m Learning iteration 1721/2000 [0m                     

                       Computation: 113793 steps/s (collection: 0.760s, learning 0.104s)
             Mean action noise std: 9.23
          Mean value_function loss: 35.1223
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 28.7637
                       Mean reward: 848.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 171.9596
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.2035
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 169279488
                    Iteration time: 0.86s
                      Time elapsed: 00:29:05
                               ETA: 00:04:42

################################################################################
                     [1m Learning iteration 1722/2000 [0m                     

                       Computation: 111601 steps/s (collection: 0.762s, learning 0.119s)
             Mean action noise std: 9.24
          Mean value_function loss: 31.1864
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.7695
                       Mean reward: 858.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7541
     Episode_Reward/lifting_object: 171.0094
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.2035
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 169377792
                    Iteration time: 0.88s
                      Time elapsed: 00:29:06
                               ETA: 00:04:41

################################################################################
                     [1m Learning iteration 1723/2000 [0m                     

                       Computation: 108367 steps/s (collection: 0.798s, learning 0.109s)
             Mean action noise std: 9.24
          Mean value_function loss: 33.9161
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 28.7779
                       Mean reward: 859.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 172.4076
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.2044
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 19.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 169476096
                    Iteration time: 0.91s
                      Time elapsed: 00:29:07
                               ETA: 00:04:40

################################################################################
                     [1m Learning iteration 1724/2000 [0m                     

                       Computation: 115800 steps/s (collection: 0.752s, learning 0.097s)
             Mean action noise std: 9.25
          Mean value_function loss: 36.4359
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 28.7830
                       Mean reward: 854.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7518
     Episode_Reward/lifting_object: 171.1971
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.2052
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 169574400
                    Iteration time: 0.85s
                      Time elapsed: 00:29:08
                               ETA: 00:04:39

################################################################################
                     [1m Learning iteration 1725/2000 [0m                     

                       Computation: 111331 steps/s (collection: 0.767s, learning 0.116s)
             Mean action noise std: 9.25
          Mean value_function loss: 38.5948
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 28.7867
                       Mean reward: 866.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 172.8952
      Episode_Reward/object_height: 0.0623
        Episode_Reward/action_rate: -0.2052
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 169672704
                    Iteration time: 0.88s
                      Time elapsed: 00:29:09
                               ETA: 00:04:38

################################################################################
                     [1m Learning iteration 1726/2000 [0m                     

                       Computation: 111656 steps/s (collection: 0.767s, learning 0.113s)
             Mean action noise std: 9.25
          Mean value_function loss: 32.6162
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.7875
                       Mean reward: 851.65
               Mean episode length: 249.63
    Episode_Reward/reaching_object: 0.7652
     Episode_Reward/lifting_object: 173.0864
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.2051
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 169771008
                    Iteration time: 0.88s
                      Time elapsed: 00:29:10
                               ETA: 00:04:37

################################################################################
                     [1m Learning iteration 1727/2000 [0m                     

                       Computation: 110516 steps/s (collection: 0.775s, learning 0.115s)
             Mean action noise std: 9.26
          Mean value_function loss: 29.9595
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.7933
                       Mean reward: 840.75
               Mean episode length: 249.66
    Episode_Reward/reaching_object: 0.7640
     Episode_Reward/lifting_object: 172.2581
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.2065
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 169869312
                    Iteration time: 0.89s
                      Time elapsed: 00:29:10
                               ETA: 00:04:36

################################################################################
                     [1m Learning iteration 1728/2000 [0m                     

                       Computation: 112159 steps/s (collection: 0.782s, learning 0.094s)
             Mean action noise std: 9.27
          Mean value_function loss: 34.2196
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 28.8023
                       Mean reward: 842.98
               Mean episode length: 249.33
    Episode_Reward/reaching_object: 0.7572
     Episode_Reward/lifting_object: 170.7336
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.2059
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 169967616
                    Iteration time: 0.88s
                      Time elapsed: 00:29:11
                               ETA: 00:04:35

################################################################################
                     [1m Learning iteration 1729/2000 [0m                     

                       Computation: 116433 steps/s (collection: 0.751s, learning 0.093s)
             Mean action noise std: 9.27
          Mean value_function loss: 24.4411
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 28.8076
                       Mean reward: 863.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 171.4347
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.2076
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 170065920
                    Iteration time: 0.84s
                      Time elapsed: 00:29:12
                               ETA: 00:04:34

################################################################################
                     [1m Learning iteration 1730/2000 [0m                     

                       Computation: 115546 steps/s (collection: 0.760s, learning 0.091s)
             Mean action noise std: 9.28
          Mean value_function loss: 26.2578
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.8130
                       Mean reward: 860.80
               Mean episode length: 248.67
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 173.2557
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.2070
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 170164224
                    Iteration time: 0.85s
                      Time elapsed: 00:29:13
                               ETA: 00:04:33

################################################################################
                     [1m Learning iteration 1731/2000 [0m                     

                       Computation: 119553 steps/s (collection: 0.723s, learning 0.099s)
             Mean action noise std: 9.29
          Mean value_function loss: 29.9593
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 28.8221
                       Mean reward: 854.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7636
     Episode_Reward/lifting_object: 171.6609
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.2082
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 170262528
                    Iteration time: 0.82s
                      Time elapsed: 00:29:14
                               ETA: 00:04:32

################################################################################
                     [1m Learning iteration 1732/2000 [0m                     

                       Computation: 116146 steps/s (collection: 0.752s, learning 0.094s)
             Mean action noise std: 9.30
          Mean value_function loss: 27.1563
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.8279
                       Mean reward: 878.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7753
     Episode_Reward/lifting_object: 174.5365
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.2086
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 170360832
                    Iteration time: 0.85s
                      Time elapsed: 00:29:15
                               ETA: 00:04:31

################################################################################
                     [1m Learning iteration 1733/2000 [0m                     

                       Computation: 116698 steps/s (collection: 0.749s, learning 0.093s)
             Mean action noise std: 9.32
          Mean value_function loss: 41.8287
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 28.8395
                       Mean reward: 871.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7683
     Episode_Reward/lifting_object: 173.3488
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.2092
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 170459136
                    Iteration time: 0.84s
                      Time elapsed: 00:29:16
                               ETA: 00:04:30

################################################################################
                     [1m Learning iteration 1734/2000 [0m                     

                       Computation: 113535 steps/s (collection: 0.768s, learning 0.098s)
             Mean action noise std: 9.33
          Mean value_function loss: 29.2416
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.8509
                       Mean reward: 863.64
               Mean episode length: 249.99
    Episode_Reward/reaching_object: 0.7699
     Episode_Reward/lifting_object: 173.0537
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.2098
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 170557440
                    Iteration time: 0.87s
                      Time elapsed: 00:29:16
                               ETA: 00:04:29

################################################################################
                     [1m Learning iteration 1735/2000 [0m                     

                       Computation: 117227 steps/s (collection: 0.747s, learning 0.092s)
             Mean action noise std: 9.34
          Mean value_function loss: 39.0002
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 28.8597
                       Mean reward: 854.43
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.7612
     Episode_Reward/lifting_object: 171.3608
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.2101
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 170655744
                    Iteration time: 0.84s
                      Time elapsed: 00:29:17
                               ETA: 00:04:28

################################################################################
                     [1m Learning iteration 1736/2000 [0m                     

                       Computation: 110786 steps/s (collection: 0.769s, learning 0.118s)
             Mean action noise std: 9.34
          Mean value_function loss: 28.0776
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 28.8689
                       Mean reward: 877.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7692
     Episode_Reward/lifting_object: 173.7457
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.2106
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 170754048
                    Iteration time: 0.89s
                      Time elapsed: 00:29:18
                               ETA: 00:04:27

################################################################################
                     [1m Learning iteration 1737/2000 [0m                     

                       Computation: 115549 steps/s (collection: 0.729s, learning 0.122s)
             Mean action noise std: 9.36
          Mean value_function loss: 34.8208
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 28.8766
                       Mean reward: 856.61
               Mean episode length: 248.80
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 171.4297
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.2109
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 170852352
                    Iteration time: 0.85s
                      Time elapsed: 00:29:19
                               ETA: 00:04:26

################################################################################
                     [1m Learning iteration 1738/2000 [0m                     

                       Computation: 112776 steps/s (collection: 0.784s, learning 0.088s)
             Mean action noise std: 9.36
          Mean value_function loss: 33.1488
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 28.8884
                       Mean reward: 868.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 171.9137
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.2119
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 170950656
                    Iteration time: 0.87s
                      Time elapsed: 00:29:20
                               ETA: 00:04:25

################################################################################
                     [1m Learning iteration 1739/2000 [0m                     

                       Computation: 119823 steps/s (collection: 0.732s, learning 0.089s)
             Mean action noise std: 9.37
          Mean value_function loss: 33.4820
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 28.8927
                       Mean reward: 870.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 172.1847
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.2120
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 171048960
                    Iteration time: 0.82s
                      Time elapsed: 00:29:21
                               ETA: 00:04:24

################################################################################
                     [1m Learning iteration 1740/2000 [0m                     

                       Computation: 113986 steps/s (collection: 0.768s, learning 0.095s)
             Mean action noise std: 9.38
          Mean value_function loss: 25.3618
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 28.9014
                       Mean reward: 871.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7615
     Episode_Reward/lifting_object: 172.1009
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.2137
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 171147264
                    Iteration time: 0.86s
                      Time elapsed: 00:29:22
                               ETA: 00:04:23

################################################################################
                     [1m Learning iteration 1741/2000 [0m                     

                       Computation: 117954 steps/s (collection: 0.742s, learning 0.091s)
             Mean action noise std: 9.39
          Mean value_function loss: 30.2052
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 28.9114
                       Mean reward: 861.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 172.6547
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.2148
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 171245568
                    Iteration time: 0.83s
                      Time elapsed: 00:29:22
                               ETA: 00:04:22

################################################################################
                     [1m Learning iteration 1742/2000 [0m                     

                       Computation: 110477 steps/s (collection: 0.796s, learning 0.094s)
             Mean action noise std: 9.41
          Mean value_function loss: 40.4281
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 28.9234
                       Mean reward: 873.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7590
     Episode_Reward/lifting_object: 171.6651
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.2143
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 171343872
                    Iteration time: 0.89s
                      Time elapsed: 00:29:23
                               ETA: 00:04:21

################################################################################
                     [1m Learning iteration 1743/2000 [0m                     

                       Computation: 114981 steps/s (collection: 0.760s, learning 0.095s)
             Mean action noise std: 9.42
          Mean value_function loss: 30.9685
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 28.9342
                       Mean reward: 854.70
               Mean episode length: 246.14
    Episode_Reward/reaching_object: 0.7606
     Episode_Reward/lifting_object: 171.2379
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.2134
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 171442176
                    Iteration time: 0.85s
                      Time elapsed: 00:29:24
                               ETA: 00:04:20

################################################################################
                     [1m Learning iteration 1744/2000 [0m                     

                       Computation: 120563 steps/s (collection: 0.724s, learning 0.092s)
             Mean action noise std: 9.43
          Mean value_function loss: 33.9557
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 28.9439
                       Mean reward: 840.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 171.5091
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.2144
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 171540480
                    Iteration time: 0.82s
                      Time elapsed: 00:29:25
                               ETA: 00:04:18

################################################################################
                     [1m Learning iteration 1745/2000 [0m                     

                       Computation: 115112 steps/s (collection: 0.754s, learning 0.100s)
             Mean action noise std: 9.44
          Mean value_function loss: 32.9776
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 28.9525
                       Mean reward: 876.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7566
     Episode_Reward/lifting_object: 171.8916
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.2152
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 171638784
                    Iteration time: 0.85s
                      Time elapsed: 00:29:26
                               ETA: 00:04:17

################################################################################
                     [1m Learning iteration 1746/2000 [0m                     

                       Computation: 113008 steps/s (collection: 0.775s, learning 0.095s)
             Mean action noise std: 9.45
          Mean value_function loss: 31.6904
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 28.9589
                       Mean reward: 858.13
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 171.1571
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.2155
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 171737088
                    Iteration time: 0.87s
                      Time elapsed: 00:29:27
                               ETA: 00:04:16

################################################################################
                     [1m Learning iteration 1747/2000 [0m                     

                       Computation: 116717 steps/s (collection: 0.755s, learning 0.087s)
             Mean action noise std: 9.45
          Mean value_function loss: 31.5225
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 28.9664
                       Mean reward: 855.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7519
     Episode_Reward/lifting_object: 171.2198
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.2169
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 171835392
                    Iteration time: 0.84s
                      Time elapsed: 00:29:27
                               ETA: 00:04:15

################################################################################
                     [1m Learning iteration 1748/2000 [0m                     

                       Computation: 117154 steps/s (collection: 0.755s, learning 0.084s)
             Mean action noise std: 9.45
          Mean value_function loss: 31.6187
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 28.9697
                       Mean reward: 852.09
               Mean episode length: 248.32
    Episode_Reward/reaching_object: 0.7646
     Episode_Reward/lifting_object: 172.0364
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.2168
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 171933696
                    Iteration time: 0.84s
                      Time elapsed: 00:29:28
                               ETA: 00:04:14

################################################################################
                     [1m Learning iteration 1749/2000 [0m                     

                       Computation: 118041 steps/s (collection: 0.738s, learning 0.095s)
             Mean action noise std: 9.46
          Mean value_function loss: 26.8313
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 28.9732
                       Mean reward: 836.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7488
     Episode_Reward/lifting_object: 169.0595
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.2183
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 172032000
                    Iteration time: 0.83s
                      Time elapsed: 00:29:29
                               ETA: 00:04:13

################################################################################
                     [1m Learning iteration 1750/2000 [0m                     

                       Computation: 119332 steps/s (collection: 0.728s, learning 0.096s)
             Mean action noise std: 9.47
          Mean value_function loss: 33.4809
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 28.9791
                       Mean reward: 861.47
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 171.5035
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.2166
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 172130304
                    Iteration time: 0.82s
                      Time elapsed: 00:29:30
                               ETA: 00:04:12

################################################################################
                     [1m Learning iteration 1751/2000 [0m                     

                       Computation: 115114 steps/s (collection: 0.758s, learning 0.096s)
             Mean action noise std: 9.48
          Mean value_function loss: 32.1895
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.9849
                       Mean reward: 867.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7525
     Episode_Reward/lifting_object: 169.9378
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.2178
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 172228608
                    Iteration time: 0.85s
                      Time elapsed: 00:29:31
                               ETA: 00:04:11

################################################################################
                     [1m Learning iteration 1752/2000 [0m                     

                       Computation: 116331 steps/s (collection: 0.746s, learning 0.099s)
             Mean action noise std: 9.49
          Mean value_function loss: 37.8637
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.9921
                       Mean reward: 865.89
               Mean episode length: 249.10
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 173.0166
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.2208
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 172326912
                    Iteration time: 0.85s
                      Time elapsed: 00:29:32
                               ETA: 00:04:10

################################################################################
                     [1m Learning iteration 1753/2000 [0m                     

                       Computation: 111891 steps/s (collection: 0.783s, learning 0.096s)
             Mean action noise std: 9.50
          Mean value_function loss: 35.4873
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 29.0035
                       Mean reward: 867.31
               Mean episode length: 249.84
    Episode_Reward/reaching_object: 0.7610
     Episode_Reward/lifting_object: 171.1653
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.2206
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 172425216
                    Iteration time: 0.88s
                      Time elapsed: 00:29:33
                               ETA: 00:04:09

################################################################################
                     [1m Learning iteration 1754/2000 [0m                     

                       Computation: 115261 steps/s (collection: 0.764s, learning 0.089s)
             Mean action noise std: 9.51
          Mean value_function loss: 36.6510
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 29.0130
                       Mean reward: 865.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 172.8956
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.2203
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 19.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 172523520
                    Iteration time: 0.85s
                      Time elapsed: 00:29:33
                               ETA: 00:04:08

################################################################################
                     [1m Learning iteration 1755/2000 [0m                     

                       Computation: 117231 steps/s (collection: 0.748s, learning 0.091s)
             Mean action noise std: 9.52
          Mean value_function loss: 31.5695
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 29.0219
                       Mean reward: 845.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7585
     Episode_Reward/lifting_object: 171.0977
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.2204
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 172621824
                    Iteration time: 0.84s
                      Time elapsed: 00:29:34
                               ETA: 00:04:07

################################################################################
                     [1m Learning iteration 1756/2000 [0m                     

                       Computation: 115470 steps/s (collection: 0.739s, learning 0.112s)
             Mean action noise std: 9.52
          Mean value_function loss: 28.0354
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 29.0268
                       Mean reward: 858.64
               Mean episode length: 248.55
    Episode_Reward/reaching_object: 0.7639
     Episode_Reward/lifting_object: 172.3113
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.2214
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 172720128
                    Iteration time: 0.85s
                      Time elapsed: 00:29:35
                               ETA: 00:04:06

################################################################################
                     [1m Learning iteration 1757/2000 [0m                     

                       Computation: 118601 steps/s (collection: 0.736s, learning 0.093s)
             Mean action noise std: 9.53
          Mean value_function loss: 30.3529
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 29.0325
                       Mean reward: 851.17
               Mean episode length: 248.79
    Episode_Reward/reaching_object: 0.7641
     Episode_Reward/lifting_object: 171.8521
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.2223
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 172818432
                    Iteration time: 0.83s
                      Time elapsed: 00:29:36
                               ETA: 00:04:05

################################################################################
                     [1m Learning iteration 1758/2000 [0m                     

                       Computation: 113531 steps/s (collection: 0.766s, learning 0.100s)
             Mean action noise std: 9.53
          Mean value_function loss: 32.7309
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 29.0368
                       Mean reward: 863.26
               Mean episode length: 249.89
    Episode_Reward/reaching_object: 0.7736
     Episode_Reward/lifting_object: 172.9919
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.2227
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 172916736
                    Iteration time: 0.87s
                      Time elapsed: 00:29:37
                               ETA: 00:04:04

################################################################################
                     [1m Learning iteration 1759/2000 [0m                     

                       Computation: 114332 steps/s (collection: 0.760s, learning 0.100s)
             Mean action noise std: 9.54
          Mean value_function loss: 33.2225
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 29.0401
                       Mean reward: 860.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7641
     Episode_Reward/lifting_object: 170.8895
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.2235
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173015040
                    Iteration time: 0.86s
                      Time elapsed: 00:29:38
                               ETA: 00:04:03

################################################################################
                     [1m Learning iteration 1760/2000 [0m                     

                       Computation: 116872 steps/s (collection: 0.758s, learning 0.084s)
             Mean action noise std: 9.55
          Mean value_function loss: 34.3600
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 29.0468
                       Mean reward: 874.02
               Mean episode length: 249.65
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 172.4705
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.2227
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 173113344
                    Iteration time: 0.84s
                      Time elapsed: 00:29:38
                               ETA: 00:04:02

################################################################################
                     [1m Learning iteration 1761/2000 [0m                     

                       Computation: 116677 steps/s (collection: 0.757s, learning 0.085s)
             Mean action noise std: 9.56
          Mean value_function loss: 47.7811
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 29.0559
                       Mean reward: 866.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7641
     Episode_Reward/lifting_object: 171.4679
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.2250
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 173211648
                    Iteration time: 0.84s
                      Time elapsed: 00:29:39
                               ETA: 00:04:01

################################################################################
                     [1m Learning iteration 1762/2000 [0m                     

                       Computation: 116631 steps/s (collection: 0.747s, learning 0.096s)
             Mean action noise std: 9.57
          Mean value_function loss: 31.9644
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 29.0631
                       Mean reward: 856.88
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 171.4926
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.2222
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 173309952
                    Iteration time: 0.84s
                      Time elapsed: 00:29:40
                               ETA: 00:04:00

################################################################################
                     [1m Learning iteration 1763/2000 [0m                     

                       Computation: 110853 steps/s (collection: 0.777s, learning 0.110s)
             Mean action noise std: 9.57
          Mean value_function loss: 29.6808
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 29.0667
                       Mean reward: 864.01
               Mean episode length: 249.39
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 172.0656
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.2231
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 173408256
                    Iteration time: 0.89s
                      Time elapsed: 00:29:41
                               ETA: 00:03:59

################################################################################
                     [1m Learning iteration 1764/2000 [0m                     

                       Computation: 115725 steps/s (collection: 0.753s, learning 0.097s)
             Mean action noise std: 9.58
          Mean value_function loss: 22.3712
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 29.0731
                       Mean reward: 854.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7689
     Episode_Reward/lifting_object: 173.1945
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.2245
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173506560
                    Iteration time: 0.85s
                      Time elapsed: 00:29:42
                               ETA: 00:03:58

################################################################################
                     [1m Learning iteration 1765/2000 [0m                     

                       Computation: 116646 steps/s (collection: 0.753s, learning 0.090s)
             Mean action noise std: 9.58
          Mean value_function loss: 29.1047
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 29.0767
                       Mean reward: 857.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 171.6817
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.2240
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173604864
                    Iteration time: 0.84s
                      Time elapsed: 00:29:43
                               ETA: 00:03:57

################################################################################
                     [1m Learning iteration 1766/2000 [0m                     

                       Computation: 112769 steps/s (collection: 0.777s, learning 0.095s)
             Mean action noise std: 9.59
          Mean value_function loss: 28.9045
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 29.0795
                       Mean reward: 866.10
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 171.3300
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.2238
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 173703168
                    Iteration time: 0.87s
                      Time elapsed: 00:29:44
                               ETA: 00:03:56

################################################################################
                     [1m Learning iteration 1767/2000 [0m                     

                       Computation: 114880 steps/s (collection: 0.759s, learning 0.097s)
             Mean action noise std: 9.60
          Mean value_function loss: 31.4940
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 29.0865
                       Mean reward: 859.30
               Mean episode length: 248.59
    Episode_Reward/reaching_object: 0.7673
     Episode_Reward/lifting_object: 172.1445
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.2229
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 173801472
                    Iteration time: 0.86s
                      Time elapsed: 00:29:44
                               ETA: 00:03:55

################################################################################
                     [1m Learning iteration 1768/2000 [0m                     

                       Computation: 113177 steps/s (collection: 0.764s, learning 0.105s)
             Mean action noise std: 9.60
          Mean value_function loss: 24.8165
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 29.0940
                       Mean reward: 869.00
               Mean episode length: 249.57
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 171.5388
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.2222
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 173899776
                    Iteration time: 0.87s
                      Time elapsed: 00:29:45
                               ETA: 00:03:54

################################################################################
                     [1m Learning iteration 1769/2000 [0m                     

                       Computation: 116404 steps/s (collection: 0.751s, learning 0.094s)
             Mean action noise std: 9.60
          Mean value_function loss: 22.4427
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 29.0958
                       Mean reward: 879.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 173.2756
      Episode_Reward/object_height: 0.0615
        Episode_Reward/action_rate: -0.2218
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 173998080
                    Iteration time: 0.84s
                      Time elapsed: 00:29:46
                               ETA: 00:03:53

################################################################################
                     [1m Learning iteration 1770/2000 [0m                     

                       Computation: 113216 steps/s (collection: 0.763s, learning 0.105s)
             Mean action noise std: 9.61
          Mean value_function loss: 26.3423
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 29.0990
                       Mean reward: 860.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7629
     Episode_Reward/lifting_object: 171.6419
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.2237
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 174096384
                    Iteration time: 0.87s
                      Time elapsed: 00:29:47
                               ETA: 00:03:52

################################################################################
                     [1m Learning iteration 1771/2000 [0m                     

                       Computation: 108917 steps/s (collection: 0.807s, learning 0.095s)
             Mean action noise std: 9.61
          Mean value_function loss: 34.3394
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 29.1032
                       Mean reward: 866.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 171.4127
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.2235
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 174194688
                    Iteration time: 0.90s
                      Time elapsed: 00:29:48
                               ETA: 00:03:51

################################################################################
                     [1m Learning iteration 1772/2000 [0m                     

                       Computation: 109121 steps/s (collection: 0.815s, learning 0.086s)
             Mean action noise std: 9.63
          Mean value_function loss: 21.6796
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 29.1133
                       Mean reward: 863.95
               Mean episode length: 247.94
    Episode_Reward/reaching_object: 0.7621
     Episode_Reward/lifting_object: 171.4786
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.2235
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 174292992
                    Iteration time: 0.90s
                      Time elapsed: 00:29:49
                               ETA: 00:03:50

################################################################################
                     [1m Learning iteration 1773/2000 [0m                     

                       Computation: 112681 steps/s (collection: 0.775s, learning 0.098s)
             Mean action noise std: 9.64
          Mean value_function loss: 28.0893
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 29.1221
                       Mean reward: 866.29
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 173.0344
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.2238
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 174391296
                    Iteration time: 0.87s
                      Time elapsed: 00:29:50
                               ETA: 00:03:49

################################################################################
                     [1m Learning iteration 1774/2000 [0m                     

                       Computation: 111800 steps/s (collection: 0.785s, learning 0.095s)
             Mean action noise std: 9.64
          Mean value_function loss: 40.3026
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 29.1295
                       Mean reward: 875.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7694
     Episode_Reward/lifting_object: 172.5629
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.2252
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 174489600
                    Iteration time: 0.88s
                      Time elapsed: 00:29:51
                               ETA: 00:03:48

################################################################################
                     [1m Learning iteration 1775/2000 [0m                     

                       Computation: 112231 steps/s (collection: 0.769s, learning 0.107s)
             Mean action noise std: 9.65
          Mean value_function loss: 32.4360
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 29.1362
                       Mean reward: 855.97
               Mean episode length: 249.10
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 171.6855
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.2257
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 19.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 174587904
                    Iteration time: 0.88s
                      Time elapsed: 00:29:51
                               ETA: 00:03:47

################################################################################
                     [1m Learning iteration 1776/2000 [0m                     

                       Computation: 113423 steps/s (collection: 0.771s, learning 0.095s)
             Mean action noise std: 9.66
          Mean value_function loss: 35.2599
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 29.1453
                       Mean reward: 863.03
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.7615
     Episode_Reward/lifting_object: 171.1680
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.2258
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 174686208
                    Iteration time: 0.87s
                      Time elapsed: 00:29:52
                               ETA: 00:03:45

################################################################################
                     [1m Learning iteration 1777/2000 [0m                     

                       Computation: 114394 steps/s (collection: 0.758s, learning 0.102s)
             Mean action noise std: 9.67
          Mean value_function loss: 33.0013
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 29.1535
                       Mean reward: 874.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7738
     Episode_Reward/lifting_object: 174.0003
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.2274
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 174784512
                    Iteration time: 0.86s
                      Time elapsed: 00:29:53
                               ETA: 00:03:44

################################################################################
                     [1m Learning iteration 1778/2000 [0m                     

                       Computation: 115663 steps/s (collection: 0.753s, learning 0.097s)
             Mean action noise std: 9.68
          Mean value_function loss: 30.8268
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 29.1582
                       Mean reward: 863.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 171.5518
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.2269
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 174882816
                    Iteration time: 0.85s
                      Time elapsed: 00:29:54
                               ETA: 00:03:43

################################################################################
                     [1m Learning iteration 1779/2000 [0m                     

                       Computation: 114337 steps/s (collection: 0.752s, learning 0.108s)
             Mean action noise std: 9.69
          Mean value_function loss: 34.4414
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 29.1657
                       Mean reward: 851.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 172.3116
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.2281
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 174981120
                    Iteration time: 0.86s
                      Time elapsed: 00:29:55
                               ETA: 00:03:42

################################################################################
                     [1m Learning iteration 1780/2000 [0m                     

                       Computation: 112102 steps/s (collection: 0.775s, learning 0.102s)
             Mean action noise std: 9.70
          Mean value_function loss: 26.4894
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 29.1772
                       Mean reward: 864.92
               Mean episode length: 248.49
    Episode_Reward/reaching_object: 0.7704
     Episode_Reward/lifting_object: 172.5763
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.2273
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 175079424
                    Iteration time: 0.88s
                      Time elapsed: 00:29:56
                               ETA: 00:03:41

################################################################################
                     [1m Learning iteration 1781/2000 [0m                     

                       Computation: 114623 steps/s (collection: 0.771s, learning 0.087s)
             Mean action noise std: 9.71
          Mean value_function loss: 41.9174
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 29.1890
                       Mean reward: 859.74
               Mean episode length: 249.04
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 170.7090
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.2272
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 175177728
                    Iteration time: 0.86s
                      Time elapsed: 00:29:57
                               ETA: 00:03:40

################################################################################
                     [1m Learning iteration 1782/2000 [0m                     

                       Computation: 115527 steps/s (collection: 0.766s, learning 0.085s)
             Mean action noise std: 9.72
          Mean value_function loss: 32.5509
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 29.1964
                       Mean reward: 852.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 171.4147
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.2289
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 175276032
                    Iteration time: 0.85s
                      Time elapsed: 00:29:57
                               ETA: 00:03:39

################################################################################
                     [1m Learning iteration 1783/2000 [0m                     

                       Computation: 107678 steps/s (collection: 0.825s, learning 0.088s)
             Mean action noise std: 9.73
          Mean value_function loss: 31.3183
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 29.2044
                       Mean reward: 860.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 170.5108
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.2299
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 175374336
                    Iteration time: 0.91s
                      Time elapsed: 00:29:58
                               ETA: 00:03:38

################################################################################
                     [1m Learning iteration 1784/2000 [0m                     

                       Computation: 113026 steps/s (collection: 0.779s, learning 0.091s)
             Mean action noise std: 9.74
          Mean value_function loss: 24.8839
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 29.2115
                       Mean reward: 852.60
               Mean episode length: 248.66
    Episode_Reward/reaching_object: 0.7652
     Episode_Reward/lifting_object: 172.4510
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.2305
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 175472640
                    Iteration time: 0.87s
                      Time elapsed: 00:29:59
                               ETA: 00:03:37

################################################################################
                     [1m Learning iteration 1785/2000 [0m                     

                       Computation: 109243 steps/s (collection: 0.774s, learning 0.126s)
             Mean action noise std: 9.74
          Mean value_function loss: 35.0465
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 29.2158
                       Mean reward: 859.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7698
     Episode_Reward/lifting_object: 172.9962
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.2325
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.4167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 175570944
                    Iteration time: 0.90s
                      Time elapsed: 00:30:00
                               ETA: 00:03:36

################################################################################
                     [1m Learning iteration 1786/2000 [0m                     

                       Computation: 110411 steps/s (collection: 0.789s, learning 0.102s)
             Mean action noise std: 9.75
          Mean value_function loss: 30.8507
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 29.2206
                       Mean reward: 868.41
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.7627
     Episode_Reward/lifting_object: 172.0978
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.2313
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 175669248
                    Iteration time: 0.89s
                      Time elapsed: 00:30:01
                               ETA: 00:03:35

################################################################################
                     [1m Learning iteration 1787/2000 [0m                     

                       Computation: 111929 steps/s (collection: 0.778s, learning 0.100s)
             Mean action noise std: 9.75
          Mean value_function loss: 34.2858
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 29.2262
                       Mean reward: 866.13
               Mean episode length: 249.04
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 172.0361
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.2325
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 175767552
                    Iteration time: 0.88s
                      Time elapsed: 00:30:02
                               ETA: 00:03:34

################################################################################
                     [1m Learning iteration 1788/2000 [0m                     

                       Computation: 107505 steps/s (collection: 0.801s, learning 0.113s)
             Mean action noise std: 9.76
          Mean value_function loss: 37.5897
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 29.2291
                       Mean reward: 859.22
               Mean episode length: 249.40
    Episode_Reward/reaching_object: 0.7672
     Episode_Reward/lifting_object: 171.8110
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.2327
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 175865856
                    Iteration time: 0.91s
                      Time elapsed: 00:30:03
                               ETA: 00:03:33

################################################################################
                     [1m Learning iteration 1789/2000 [0m                     

                       Computation: 114477 steps/s (collection: 0.762s, learning 0.096s)
             Mean action noise std: 9.77
          Mean value_function loss: 36.2357
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 29.2364
                       Mean reward: 851.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7612
     Episode_Reward/lifting_object: 170.9867
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.2350
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 175964160
                    Iteration time: 0.86s
                      Time elapsed: 00:30:04
                               ETA: 00:03:32

################################################################################
                     [1m Learning iteration 1790/2000 [0m                     

                       Computation: 109644 steps/s (collection: 0.808s, learning 0.088s)
             Mean action noise std: 9.77
          Mean value_function loss: 29.6202
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 29.2437
                       Mean reward: 868.34
               Mean episode length: 249.29
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 172.4913
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.2348
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 176062464
                    Iteration time: 0.90s
                      Time elapsed: 00:30:05
                               ETA: 00:03:31

################################################################################
                     [1m Learning iteration 1791/2000 [0m                     

                       Computation: 114130 steps/s (collection: 0.772s, learning 0.089s)
             Mean action noise std: 9.78
          Mean value_function loss: 30.0670
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 29.2508
                       Mean reward: 871.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 170.8186
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.2357
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 176160768
                    Iteration time: 0.86s
                      Time elapsed: 00:30:05
                               ETA: 00:03:30

################################################################################
                     [1m Learning iteration 1792/2000 [0m                     

                       Computation: 109701 steps/s (collection: 0.806s, learning 0.091s)
             Mean action noise std: 9.79
          Mean value_function loss: 21.9511
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 29.2581
                       Mean reward: 867.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7594
     Episode_Reward/lifting_object: 170.9233
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.2373
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 176259072
                    Iteration time: 0.90s
                      Time elapsed: 00:30:06
                               ETA: 00:03:29

################################################################################
                     [1m Learning iteration 1793/2000 [0m                     

                       Computation: 110342 steps/s (collection: 0.797s, learning 0.094s)
             Mean action noise std: 9.80
          Mean value_function loss: 29.6651
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 29.2673
                       Mean reward: 850.05
               Mean episode length: 249.49
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 171.8540
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.2381
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 176357376
                    Iteration time: 0.89s
                      Time elapsed: 00:30:07
                               ETA: 00:03:28

################################################################################
                     [1m Learning iteration 1794/2000 [0m                     

                       Computation: 111048 steps/s (collection: 0.794s, learning 0.092s)
             Mean action noise std: 9.80
          Mean value_function loss: 24.9030
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 29.2733
                       Mean reward: 853.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 171.1954
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.2382
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 176455680
                    Iteration time: 0.89s
                      Time elapsed: 00:30:08
                               ETA: 00:03:27

################################################################################
                     [1m Learning iteration 1795/2000 [0m                     

                       Computation: 113991 steps/s (collection: 0.749s, learning 0.113s)
             Mean action noise std: 9.81
          Mean value_function loss: 25.8937
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 29.2786
                       Mean reward: 878.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7735
     Episode_Reward/lifting_object: 173.2913
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.2380
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 176553984
                    Iteration time: 0.86s
                      Time elapsed: 00:30:09
                               ETA: 00:03:26

################################################################################
                     [1m Learning iteration 1796/2000 [0m                     

                       Computation: 108441 steps/s (collection: 0.791s, learning 0.116s)
             Mean action noise std: 9.82
          Mean value_function loss: 27.7771
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 29.2851
                       Mean reward: 843.82
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.7597
     Episode_Reward/lifting_object: 171.5112
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.2371
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 19.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 176652288
                    Iteration time: 0.91s
                      Time elapsed: 00:30:10
                               ETA: 00:03:25

################################################################################
                     [1m Learning iteration 1797/2000 [0m                     

                       Computation: 112293 steps/s (collection: 0.774s, learning 0.101s)
             Mean action noise std: 9.83
          Mean value_function loss: 30.2308
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 29.2913
                       Mean reward: 867.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7591
     Episode_Reward/lifting_object: 170.7385
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.2378
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 176750592
                    Iteration time: 0.88s
                      Time elapsed: 00:30:11
                               ETA: 00:03:24

################################################################################
                     [1m Learning iteration 1798/2000 [0m                     

                       Computation: 111444 steps/s (collection: 0.787s, learning 0.095s)
             Mean action noise std: 9.83
          Mean value_function loss: 30.7043
               Mean surrogate loss: 0.0097
                 Mean entropy loss: 29.2988
                       Mean reward: 858.70
               Mean episode length: 249.21
    Episode_Reward/reaching_object: 0.7591
     Episode_Reward/lifting_object: 172.0403
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.2383
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 176848896
                    Iteration time: 0.88s
                      Time elapsed: 00:30:12
                               ETA: 00:03:23

################################################################################
                     [1m Learning iteration 1799/2000 [0m                     

                       Computation: 114967 steps/s (collection: 0.769s, learning 0.086s)
             Mean action noise std: 9.84
          Mean value_function loss: 29.4510
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 29.3002
                       Mean reward: 856.93
               Mean episode length: 247.82
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 171.7428
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.2378
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 176947200
                    Iteration time: 0.86s
                      Time elapsed: 00:30:13
                               ETA: 00:03:22

################################################################################
                     [1m Learning iteration 1800/2000 [0m                     

                       Computation: 110454 steps/s (collection: 0.805s, learning 0.085s)
             Mean action noise std: 9.84
          Mean value_function loss: 25.7474
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 29.3057
                       Mean reward: 872.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7577
     Episode_Reward/lifting_object: 173.1153
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.2378
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 177045504
                    Iteration time: 0.89s
                      Time elapsed: 00:30:13
                               ETA: 00:03:21

################################################################################
                     [1m Learning iteration 1801/2000 [0m                     

                       Computation: 110151 steps/s (collection: 0.801s, learning 0.092s)
             Mean action noise std: 9.86
          Mean value_function loss: 36.7014
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 29.3163
                       Mean reward: 865.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 171.5494
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.2382
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 177143808
                    Iteration time: 0.89s
                      Time elapsed: 00:30:14
                               ETA: 00:03:20

################################################################################
                     [1m Learning iteration 1802/2000 [0m                     

                       Computation: 109560 steps/s (collection: 0.801s, learning 0.097s)
             Mean action noise std: 9.86
          Mean value_function loss: 33.2786
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 29.3232
                       Mean reward: 872.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7597
     Episode_Reward/lifting_object: 172.4473
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.2371
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 177242112
                    Iteration time: 0.90s
                      Time elapsed: 00:30:15
                               ETA: 00:03:19

################################################################################
                     [1m Learning iteration 1803/2000 [0m                     

                       Computation: 111341 steps/s (collection: 0.783s, learning 0.100s)
             Mean action noise std: 9.87
          Mean value_function loss: 29.8254
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 29.3307
                       Mean reward: 860.64
               Mean episode length: 247.14
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 172.8248
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.2385
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 177340416
                    Iteration time: 0.88s
                      Time elapsed: 00:30:16
                               ETA: 00:03:18

################################################################################
                     [1m Learning iteration 1804/2000 [0m                     

                       Computation: 112817 steps/s (collection: 0.776s, learning 0.095s)
             Mean action noise std: 9.88
          Mean value_function loss: 34.6447
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 29.3410
                       Mean reward: 873.37
               Mean episode length: 249.84
    Episode_Reward/reaching_object: 0.7590
     Episode_Reward/lifting_object: 172.0651
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.2393
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 177438720
                    Iteration time: 0.87s
                      Time elapsed: 00:30:17
                               ETA: 00:03:17

################################################################################
                     [1m Learning iteration 1805/2000 [0m                     

                       Computation: 110165 steps/s (collection: 0.785s, learning 0.107s)
             Mean action noise std: 9.89
          Mean value_function loss: 33.2530
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 29.3487
                       Mean reward: 853.69
               Mean episode length: 248.88
    Episode_Reward/reaching_object: 0.7576
     Episode_Reward/lifting_object: 171.7765
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.2424
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 177537024
                    Iteration time: 0.89s
                      Time elapsed: 00:30:18
                               ETA: 00:03:16

################################################################################
                     [1m Learning iteration 1806/2000 [0m                     

                       Computation: 109840 steps/s (collection: 0.799s, learning 0.096s)
             Mean action noise std: 9.90
          Mean value_function loss: 31.0089
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 29.3527
                       Mean reward: 872.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7640
     Episode_Reward/lifting_object: 172.6691
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.2426
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 19.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 177635328
                    Iteration time: 0.89s
                      Time elapsed: 00:30:19
                               ETA: 00:03:15

################################################################################
                     [1m Learning iteration 1807/2000 [0m                     

                       Computation: 111798 steps/s (collection: 0.789s, learning 0.090s)
             Mean action noise std: 9.91
          Mean value_function loss: 30.4753
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 29.3591
                       Mean reward: 862.60
               Mean episode length: 248.92
    Episode_Reward/reaching_object: 0.7652
     Episode_Reward/lifting_object: 172.8075
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.2434
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 177733632
                    Iteration time: 0.88s
                      Time elapsed: 00:30:20
                               ETA: 00:03:14

################################################################################
                     [1m Learning iteration 1808/2000 [0m                     

                       Computation: 110935 steps/s (collection: 0.791s, learning 0.096s)
             Mean action noise std: 9.92
          Mean value_function loss: 40.5134
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 29.3693
                       Mean reward: 855.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 172.4401
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.2464
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 177831936
                    Iteration time: 0.89s
                      Time elapsed: 00:30:21
                               ETA: 00:03:13

################################################################################
                     [1m Learning iteration 1809/2000 [0m                     

                       Computation: 110907 steps/s (collection: 0.793s, learning 0.093s)
             Mean action noise std: 9.93
          Mean value_function loss: 34.1649
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 29.3778
                       Mean reward: 854.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7528
     Episode_Reward/lifting_object: 170.4801
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.2448
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 177930240
                    Iteration time: 0.89s
                      Time elapsed: 00:30:21
                               ETA: 00:03:12

################################################################################
                     [1m Learning iteration 1810/2000 [0m                     

                       Computation: 105638 steps/s (collection: 0.829s, learning 0.101s)
             Mean action noise std: 9.94
          Mean value_function loss: 31.7419
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.3864
                       Mean reward: 864.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7479
     Episode_Reward/lifting_object: 170.4893
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.2476
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 178028544
                    Iteration time: 0.93s
                      Time elapsed: 00:30:22
                               ETA: 00:03:11

################################################################################
                     [1m Learning iteration 1811/2000 [0m                     

                       Computation: 107510 steps/s (collection: 0.795s, learning 0.120s)
             Mean action noise std: 9.95
          Mean value_function loss: 29.8665
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 29.3941
                       Mean reward: 857.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7489
     Episode_Reward/lifting_object: 169.8718
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.2484
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 178126848
                    Iteration time: 0.91s
                      Time elapsed: 00:30:23
                               ETA: 00:03:10

################################################################################
                     [1m Learning iteration 1812/2000 [0m                     

                       Computation: 114485 steps/s (collection: 0.753s, learning 0.106s)
             Mean action noise std: 9.95
          Mean value_function loss: 27.5006
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 29.4011
                       Mean reward: 860.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7544
     Episode_Reward/lifting_object: 170.8549
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.2497
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 178225152
                    Iteration time: 0.86s
                      Time elapsed: 00:30:24
                               ETA: 00:03:09

################################################################################
                     [1m Learning iteration 1813/2000 [0m                     

                       Computation: 109308 steps/s (collection: 0.796s, learning 0.103s)
             Mean action noise std: 9.96
          Mean value_function loss: 31.5500
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 29.4054
                       Mean reward: 844.89
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.7578
     Episode_Reward/lifting_object: 170.2731
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.2498
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 178323456
                    Iteration time: 0.90s
                      Time elapsed: 00:30:25
                               ETA: 00:03:08

################################################################################
                     [1m Learning iteration 1814/2000 [0m                     

                       Computation: 107547 steps/s (collection: 0.799s, learning 0.115s)
             Mean action noise std: 9.97
          Mean value_function loss: 34.1132
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 29.4089
                       Mean reward: 856.11
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.7517
     Episode_Reward/lifting_object: 171.6095
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.2507
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 178421760
                    Iteration time: 0.91s
                      Time elapsed: 00:30:26
                               ETA: 00:03:07

################################################################################
                     [1m Learning iteration 1815/2000 [0m                     

                       Computation: 109116 steps/s (collection: 0.811s, learning 0.090s)
             Mean action noise std: 9.98
          Mean value_function loss: 35.4004
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 29.4185
                       Mean reward: 867.16
               Mean episode length: 249.57
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 172.9220
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.2492
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 178520064
                    Iteration time: 0.90s
                      Time elapsed: 00:30:27
                               ETA: 00:03:06

################################################################################
                     [1m Learning iteration 1816/2000 [0m                     

                       Computation: 109055 steps/s (collection: 0.805s, learning 0.096s)
             Mean action noise std: 9.98
          Mean value_function loss: 34.8317
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 29.4265
                       Mean reward: 869.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7586
     Episode_Reward/lifting_object: 170.9240
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.2494
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 178618368
                    Iteration time: 0.90s
                      Time elapsed: 00:30:28
                               ETA: 00:03:05

################################################################################
                     [1m Learning iteration 1817/2000 [0m                     

                       Computation: 109864 steps/s (collection: 0.801s, learning 0.094s)
             Mean action noise std: 10.00
          Mean value_function loss: 33.9051
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 29.4333
                       Mean reward: 855.22
               Mean episode length: 247.44
    Episode_Reward/reaching_object: 0.7549
     Episode_Reward/lifting_object: 170.9004
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.2488
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 178716672
                    Iteration time: 0.89s
                      Time elapsed: 00:30:29
                               ETA: 00:03:04

################################################################################
                     [1m Learning iteration 1818/2000 [0m                     

                       Computation: 112894 steps/s (collection: 0.785s, learning 0.086s)
             Mean action noise std: 10.00
          Mean value_function loss: 31.0490
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 29.4411
                       Mean reward: 848.88
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.7507
     Episode_Reward/lifting_object: 169.3698
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.2496
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 178814976
                    Iteration time: 0.87s
                      Time elapsed: 00:30:29
                               ETA: 00:03:03

################################################################################
                     [1m Learning iteration 1819/2000 [0m                     

                       Computation: 111075 steps/s (collection: 0.779s, learning 0.106s)
             Mean action noise std: 10.02
          Mean value_function loss: 27.6804
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 29.4489
                       Mean reward: 861.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7606
     Episode_Reward/lifting_object: 171.1812
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.2485
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 178913280
                    Iteration time: 0.89s
                      Time elapsed: 00:30:30
                               ETA: 00:03:02

################################################################################
                     [1m Learning iteration 1820/2000 [0m                     

                       Computation: 110275 steps/s (collection: 0.792s, learning 0.099s)
             Mean action noise std: 10.02
          Mean value_function loss: 26.5217
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 29.4577
                       Mean reward: 870.02
               Mean episode length: 249.44
    Episode_Reward/reaching_object: 0.7652
     Episode_Reward/lifting_object: 172.5044
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.2499
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 179011584
                    Iteration time: 0.89s
                      Time elapsed: 00:30:31
                               ETA: 00:03:01

################################################################################
                     [1m Learning iteration 1821/2000 [0m                     

                       Computation: 115174 steps/s (collection: 0.762s, learning 0.091s)
             Mean action noise std: 10.03
          Mean value_function loss: 30.7488
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 29.4644
                       Mean reward: 872.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7728
     Episode_Reward/lifting_object: 174.2807
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.2506
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 179109888
                    Iteration time: 0.85s
                      Time elapsed: 00:30:32
                               ETA: 00:03:00

################################################################################
                     [1m Learning iteration 1822/2000 [0m                     

                       Computation: 116583 steps/s (collection: 0.753s, learning 0.090s)
             Mean action noise std: 10.04
          Mean value_function loss: 28.4004
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 29.4707
                       Mean reward: 863.81
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 171.2855
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.2495
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 179208192
                    Iteration time: 0.84s
                      Time elapsed: 00:30:33
                               ETA: 00:02:59

################################################################################
                     [1m Learning iteration 1823/2000 [0m                     

                       Computation: 116112 steps/s (collection: 0.757s, learning 0.090s)
             Mean action noise std: 10.05
          Mean value_function loss: 28.5887
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 29.4781
                       Mean reward: 857.44
               Mean episode length: 248.53
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 172.1559
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.2505
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 179306496
                    Iteration time: 0.85s
                      Time elapsed: 00:30:34
                               ETA: 00:02:58

################################################################################
                     [1m Learning iteration 1824/2000 [0m                     

                       Computation: 115466 steps/s (collection: 0.760s, learning 0.092s)
             Mean action noise std: 10.06
          Mean value_function loss: 30.1757
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 29.4859
                       Mean reward: 869.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 172.2428
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.2502
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 179404800
                    Iteration time: 0.85s
                      Time elapsed: 00:30:35
                               ETA: 00:02:56

################################################################################
                     [1m Learning iteration 1825/2000 [0m                     

                       Computation: 113795 steps/s (collection: 0.777s, learning 0.087s)
             Mean action noise std: 10.07
          Mean value_function loss: 30.0433
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 29.4949
                       Mean reward: 862.86
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 170.2812
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.2512
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 179503104
                    Iteration time: 0.86s
                      Time elapsed: 00:30:36
                               ETA: 00:02:55

################################################################################
                     [1m Learning iteration 1826/2000 [0m                     

                       Computation: 112029 steps/s (collection: 0.790s, learning 0.087s)
             Mean action noise std: 10.08
          Mean value_function loss: 26.1537
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 29.5033
                       Mean reward: 856.19
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 171.0314
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.2509
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 179601408
                    Iteration time: 0.88s
                      Time elapsed: 00:30:36
                               ETA: 00:02:54

################################################################################
                     [1m Learning iteration 1827/2000 [0m                     

                       Computation: 110676 steps/s (collection: 0.795s, learning 0.093s)
             Mean action noise std: 10.09
          Mean value_function loss: 31.4543
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 29.5157
                       Mean reward: 848.13
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.7660
     Episode_Reward/lifting_object: 171.2829
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.2505
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 19.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 179699712
                    Iteration time: 0.89s
                      Time elapsed: 00:30:37
                               ETA: 00:02:53

################################################################################
                     [1m Learning iteration 1828/2000 [0m                     

                       Computation: 110735 steps/s (collection: 0.793s, learning 0.095s)
             Mean action noise std: 10.10
          Mean value_function loss: 30.2634
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 29.5236
                       Mean reward: 869.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7698
     Episode_Reward/lifting_object: 173.1865
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.2510
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 179798016
                    Iteration time: 0.89s
                      Time elapsed: 00:30:38
                               ETA: 00:02:52

################################################################################
                     [1m Learning iteration 1829/2000 [0m                     

                       Computation: 112643 steps/s (collection: 0.763s, learning 0.110s)
             Mean action noise std: 10.11
          Mean value_function loss: 38.4866
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 29.5319
                       Mean reward: 860.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7620
     Episode_Reward/lifting_object: 171.0836
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.2510
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 179896320
                    Iteration time: 0.87s
                      Time elapsed: 00:30:39
                               ETA: 00:02:51

################################################################################
                     [1m Learning iteration 1830/2000 [0m                     

                       Computation: 107824 steps/s (collection: 0.798s, learning 0.114s)
             Mean action noise std: 10.12
          Mean value_function loss: 26.7403
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 29.5372
                       Mean reward: 847.46
               Mean episode length: 246.26
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 171.5534
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.2507
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 179994624
                    Iteration time: 0.91s
                      Time elapsed: 00:30:40
                               ETA: 00:02:50

################################################################################
                     [1m Learning iteration 1831/2000 [0m                     

                       Computation: 113184 steps/s (collection: 0.767s, learning 0.102s)
             Mean action noise std: 10.13
          Mean value_function loss: 29.2222
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 29.5453
                       Mean reward: 856.61
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 171.0492
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.2524
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 180092928
                    Iteration time: 0.87s
                      Time elapsed: 00:30:41
                               ETA: 00:02:49

################################################################################
                     [1m Learning iteration 1832/2000 [0m                     

                       Computation: 112144 steps/s (collection: 0.782s, learning 0.095s)
             Mean action noise std: 10.13
          Mean value_function loss: 30.7020
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 29.5512
                       Mean reward: 859.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7572
     Episode_Reward/lifting_object: 171.6988
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.2555
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 180191232
                    Iteration time: 0.88s
                      Time elapsed: 00:30:42
                               ETA: 00:02:48

################################################################################
                     [1m Learning iteration 1833/2000 [0m                     

                       Computation: 114027 steps/s (collection: 0.771s, learning 0.092s)
             Mean action noise std: 10.14
          Mean value_function loss: 32.2884
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 29.5549
                       Mean reward: 838.63
               Mean episode length: 246.56
    Episode_Reward/reaching_object: 0.7523
     Episode_Reward/lifting_object: 170.7811
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.2547
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 180289536
                    Iteration time: 0.86s
                      Time elapsed: 00:30:43
                               ETA: 00:02:47

################################################################################
                     [1m Learning iteration 1834/2000 [0m                     

                       Computation: 111093 steps/s (collection: 0.774s, learning 0.111s)
             Mean action noise std: 10.16
          Mean value_function loss: 35.3205
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 29.5660
                       Mean reward: 870.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 171.0458
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.2550
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 180387840
                    Iteration time: 0.88s
                      Time elapsed: 00:30:43
                               ETA: 00:02:46

################################################################################
                     [1m Learning iteration 1835/2000 [0m                     

                       Computation: 112560 steps/s (collection: 0.770s, learning 0.104s)
             Mean action noise std: 10.17
          Mean value_function loss: 25.6807
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 29.5790
                       Mean reward: 864.71
               Mean episode length: 246.98
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 171.9173
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.2557
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 180486144
                    Iteration time: 0.87s
                      Time elapsed: 00:30:44
                               ETA: 00:02:45

################################################################################
                     [1m Learning iteration 1836/2000 [0m                     

                       Computation: 111510 steps/s (collection: 0.779s, learning 0.102s)
             Mean action noise std: 10.18
          Mean value_function loss: 32.6464
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 29.5843
                       Mean reward: 857.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7694
     Episode_Reward/lifting_object: 172.8446
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.2582
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180584448
                    Iteration time: 0.88s
                      Time elapsed: 00:30:45
                               ETA: 00:02:44

################################################################################
                     [1m Learning iteration 1837/2000 [0m                     

                       Computation: 106599 steps/s (collection: 0.819s, learning 0.104s)
             Mean action noise std: 10.19
          Mean value_function loss: 33.2268
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 29.5938
                       Mean reward: 873.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7588
     Episode_Reward/lifting_object: 170.8109
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.2581
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 180682752
                    Iteration time: 0.92s
                      Time elapsed: 00:30:46
                               ETA: 00:02:43

################################################################################
                     [1m Learning iteration 1838/2000 [0m                     

                       Computation: 111844 steps/s (collection: 0.779s, learning 0.100s)
             Mean action noise std: 10.19
          Mean value_function loss: 38.5603
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 29.6017
                       Mean reward: 868.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 172.2756
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.2601
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 180781056
                    Iteration time: 0.88s
                      Time elapsed: 00:30:47
                               ETA: 00:02:42

################################################################################
                     [1m Learning iteration 1839/2000 [0m                     

                       Computation: 113083 steps/s (collection: 0.775s, learning 0.094s)
             Mean action noise std: 10.19
          Mean value_function loss: 43.7588
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 29.6033
                       Mean reward: 860.59
               Mean episode length: 248.70
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 171.8888
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.2598
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 180879360
                    Iteration time: 0.87s
                      Time elapsed: 00:30:48
                               ETA: 00:02:41

################################################################################
                     [1m Learning iteration 1840/2000 [0m                     

                       Computation: 113148 steps/s (collection: 0.778s, learning 0.091s)
             Mean action noise std: 10.20
          Mean value_function loss: 33.4910
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 29.6077
                       Mean reward: 869.56
               Mean episode length: 249.62
    Episode_Reward/reaching_object: 0.7689
     Episode_Reward/lifting_object: 172.1456
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.2601
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 180977664
                    Iteration time: 0.87s
                      Time elapsed: 00:30:49
                               ETA: 00:02:40

################################################################################
                     [1m Learning iteration 1841/2000 [0m                     

                       Computation: 111573 steps/s (collection: 0.768s, learning 0.113s)
             Mean action noise std: 10.21
          Mean value_function loss: 41.6957
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 29.6142
                       Mean reward: 850.91
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.7557
     Episode_Reward/lifting_object: 170.4131
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.2608
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 181075968
                    Iteration time: 0.88s
                      Time elapsed: 00:30:50
                               ETA: 00:02:39

################################################################################
                     [1m Learning iteration 1842/2000 [0m                     

                       Computation: 111587 steps/s (collection: 0.773s, learning 0.108s)
             Mean action noise std: 10.22
          Mean value_function loss: 28.3554
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 29.6203
                       Mean reward: 865.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7673
     Episode_Reward/lifting_object: 170.9100
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.2619
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 181174272
                    Iteration time: 0.88s
                      Time elapsed: 00:30:51
                               ETA: 00:02:38

################################################################################
                     [1m Learning iteration 1843/2000 [0m                     

                       Computation: 105725 steps/s (collection: 0.833s, learning 0.097s)
             Mean action noise std: 10.23
          Mean value_function loss: 31.7124
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 29.6254
                       Mean reward: 868.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 170.7981
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.2606
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 181272576
                    Iteration time: 0.93s
                      Time elapsed: 00:30:51
                               ETA: 00:02:37

################################################################################
                     [1m Learning iteration 1844/2000 [0m                     

                       Computation: 112263 steps/s (collection: 0.783s, learning 0.093s)
             Mean action noise std: 10.23
          Mean value_function loss: 27.7308
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.6325
                       Mean reward: 836.15
               Mean episode length: 247.65
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 170.3455
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.2618
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 181370880
                    Iteration time: 0.88s
                      Time elapsed: 00:30:52
                               ETA: 00:02:36

################################################################################
                     [1m Learning iteration 1845/2000 [0m                     

                       Computation: 112514 steps/s (collection: 0.784s, learning 0.090s)
             Mean action noise std: 10.24
          Mean value_function loss: 30.6191
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 29.6383
                       Mean reward: 851.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7587
     Episode_Reward/lifting_object: 170.1177
      Episode_Reward/object_height: 0.0543
        Episode_Reward/action_rate: -0.2621
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 181469184
                    Iteration time: 0.87s
                      Time elapsed: 00:30:53
                               ETA: 00:02:35

################################################################################
                     [1m Learning iteration 1846/2000 [0m                     

                       Computation: 113012 steps/s (collection: 0.772s, learning 0.098s)
             Mean action noise std: 10.25
          Mean value_function loss: 26.7038
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 29.6450
                       Mean reward: 837.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 168.8635
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.2642
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 181567488
                    Iteration time: 0.87s
                      Time elapsed: 00:30:54
                               ETA: 00:02:34

################################################################################
                     [1m Learning iteration 1847/2000 [0m                     

                       Computation: 114057 steps/s (collection: 0.767s, learning 0.095s)
             Mean action noise std: 10.25
          Mean value_function loss: 24.7156
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 29.6486
                       Mean reward: 856.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7630
     Episode_Reward/lifting_object: 171.0254
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.2638
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 181665792
                    Iteration time: 0.86s
                      Time elapsed: 00:30:55
                               ETA: 00:02:33

################################################################################
                     [1m Learning iteration 1848/2000 [0m                     

                       Computation: 113627 steps/s (collection: 0.778s, learning 0.087s)
             Mean action noise std: 10.26
          Mean value_function loss: 34.3233
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 29.6523
                       Mean reward: 853.01
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 172.0654
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.2641
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 19.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 181764096
                    Iteration time: 0.87s
                      Time elapsed: 00:30:56
                               ETA: 00:02:32

################################################################################
                     [1m Learning iteration 1849/2000 [0m                     

                       Computation: 111922 steps/s (collection: 0.785s, learning 0.094s)
             Mean action noise std: 10.27
          Mean value_function loss: 33.6782
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 29.6630
                       Mean reward: 847.44
               Mean episode length: 247.47
    Episode_Reward/reaching_object: 0.7562
     Episode_Reward/lifting_object: 171.0818
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.2622
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 181862400
                    Iteration time: 0.88s
                      Time elapsed: 00:30:57
                               ETA: 00:02:31

################################################################################
                     [1m Learning iteration 1850/2000 [0m                     

                       Computation: 110557 steps/s (collection: 0.787s, learning 0.102s)
             Mean action noise std: 10.29
          Mean value_function loss: 31.6995
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 29.6727
                       Mean reward: 856.23
               Mean episode length: 248.17
    Episode_Reward/reaching_object: 0.7507
     Episode_Reward/lifting_object: 170.1703
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.2619
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 181960704
                    Iteration time: 0.89s
                      Time elapsed: 00:30:58
                               ETA: 00:02:30

################################################################################
                     [1m Learning iteration 1851/2000 [0m                     

                       Computation: 106481 steps/s (collection: 0.815s, learning 0.109s)
             Mean action noise std: 10.30
          Mean value_function loss: 24.7777
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 29.6822
                       Mean reward: 860.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7608
     Episode_Reward/lifting_object: 171.2493
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.2634
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 182059008
                    Iteration time: 0.92s
                      Time elapsed: 00:30:58
                               ETA: 00:02:29

################################################################################
                     [1m Learning iteration 1852/2000 [0m                     

                       Computation: 109034 steps/s (collection: 0.796s, learning 0.105s)
             Mean action noise std: 10.31
          Mean value_function loss: 30.7958
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 29.6912
                       Mean reward: 852.28
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.7551
     Episode_Reward/lifting_object: 169.8059
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.2616
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 182157312
                    Iteration time: 0.90s
                      Time elapsed: 00:30:59
                               ETA: 00:02:28

################################################################################
                     [1m Learning iteration 1853/2000 [0m                     

                       Computation: 110043 steps/s (collection: 0.793s, learning 0.100s)
             Mean action noise std: 10.32
          Mean value_function loss: 32.4708
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 29.6971
                       Mean reward: 868.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7636
     Episode_Reward/lifting_object: 173.2720
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.2626
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182255616
                    Iteration time: 0.89s
                      Time elapsed: 00:31:00
                               ETA: 00:02:27

################################################################################
                     [1m Learning iteration 1854/2000 [0m                     

                       Computation: 111735 steps/s (collection: 0.784s, learning 0.096s)
             Mean action noise std: 10.32
          Mean value_function loss: 22.7879
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 29.7019
                       Mean reward: 875.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 172.7308
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.2625
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 182353920
                    Iteration time: 0.88s
                      Time elapsed: 00:31:01
                               ETA: 00:02:26

################################################################################
                     [1m Learning iteration 1855/2000 [0m                     

                       Computation: 109658 steps/s (collection: 0.799s, learning 0.098s)
             Mean action noise std: 10.33
          Mean value_function loss: 29.9798
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 29.7098
                       Mean reward: 871.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7706
     Episode_Reward/lifting_object: 173.0354
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.2646
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182452224
                    Iteration time: 0.90s
                      Time elapsed: 00:31:02
                               ETA: 00:02:25

################################################################################
                     [1m Learning iteration 1856/2000 [0m                     

                       Computation: 111150 steps/s (collection: 0.784s, learning 0.100s)
             Mean action noise std: 10.35
          Mean value_function loss: 23.9347
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 29.7178
                       Mean reward: 856.20
               Mean episode length: 246.92
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 170.3201
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.2629
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 182550528
                    Iteration time: 0.88s
                      Time elapsed: 00:31:03
                               ETA: 00:02:24

################################################################################
                     [1m Learning iteration 1857/2000 [0m                     

                       Computation: 112048 steps/s (collection: 0.789s, learning 0.089s)
             Mean action noise std: 10.36
          Mean value_function loss: 31.1052
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 29.7268
                       Mean reward: 874.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7646
     Episode_Reward/lifting_object: 171.8424
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.2667
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 182648832
                    Iteration time: 0.88s
                      Time elapsed: 00:31:04
                               ETA: 00:02:23

################################################################################
                     [1m Learning iteration 1858/2000 [0m                     

                       Computation: 110401 steps/s (collection: 0.791s, learning 0.099s)
             Mean action noise std: 10.36
          Mean value_function loss: 35.8253
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 29.7322
                       Mean reward: 855.05
               Mean episode length: 249.03
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 170.4075
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.2665
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 182747136
                    Iteration time: 0.89s
                      Time elapsed: 00:31:05
                               ETA: 00:02:22

################################################################################
                     [1m Learning iteration 1859/2000 [0m                     

                       Computation: 110320 steps/s (collection: 0.793s, learning 0.098s)
             Mean action noise std: 10.37
          Mean value_function loss: 29.7868
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 29.7379
                       Mean reward: 860.37
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 171.9120
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.2657
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 182845440
                    Iteration time: 0.89s
                      Time elapsed: 00:31:06
                               ETA: 00:02:21

################################################################################
                     [1m Learning iteration 1860/2000 [0m                     

                       Computation: 110841 steps/s (collection: 0.787s, learning 0.100s)
             Mean action noise std: 10.37
          Mean value_function loss: 22.2290
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 29.7434
                       Mean reward: 871.80
               Mean episode length: 249.65
    Episode_Reward/reaching_object: 0.7610
     Episode_Reward/lifting_object: 172.8859
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.2666
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 182943744
                    Iteration time: 0.89s
                      Time elapsed: 00:31:06
                               ETA: 00:02:20

################################################################################
                     [1m Learning iteration 1861/2000 [0m                     

                       Computation: 110914 steps/s (collection: 0.791s, learning 0.096s)
             Mean action noise std: 10.38
          Mean value_function loss: 29.3051
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 29.7468
                       Mean reward: 856.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7675
     Episode_Reward/lifting_object: 173.0762
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.2693
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 183042048
                    Iteration time: 0.89s
                      Time elapsed: 00:31:07
                               ETA: 00:02:19

################################################################################
                     [1m Learning iteration 1862/2000 [0m                     

                       Computation: 109848 steps/s (collection: 0.799s, learning 0.096s)
             Mean action noise std: 10.39
          Mean value_function loss: 24.1075
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 29.7514
                       Mean reward: 867.40
               Mean episode length: 247.33
    Episode_Reward/reaching_object: 0.7602
     Episode_Reward/lifting_object: 172.7889
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.2682
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 183140352
                    Iteration time: 0.89s
                      Time elapsed: 00:31:08
                               ETA: 00:02:18

################################################################################
                     [1m Learning iteration 1863/2000 [0m                     

                       Computation: 110904 steps/s (collection: 0.788s, learning 0.098s)
             Mean action noise std: 10.40
          Mean value_function loss: 21.9730
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 29.7586
                       Mean reward: 869.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7587
     Episode_Reward/lifting_object: 172.3910
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.2689
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 183238656
                    Iteration time: 0.89s
                      Time elapsed: 00:31:09
                               ETA: 00:02:17

################################################################################
                     [1m Learning iteration 1864/2000 [0m                     

                       Computation: 110936 steps/s (collection: 0.774s, learning 0.112s)
             Mean action noise std: 10.41
          Mean value_function loss: 25.7265
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 29.7689
                       Mean reward: 838.32
               Mean episode length: 244.72
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 171.5823
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.2705
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 183336960
                    Iteration time: 0.89s
                      Time elapsed: 00:31:10
                               ETA: 00:02:16

################################################################################
                     [1m Learning iteration 1865/2000 [0m                     

                       Computation: 108355 steps/s (collection: 0.786s, learning 0.121s)
             Mean action noise std: 10.42
          Mean value_function loss: 37.7815
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 29.7774
                       Mean reward: 871.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7683
     Episode_Reward/lifting_object: 172.7235
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.2725
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 183435264
                    Iteration time: 0.91s
                      Time elapsed: 00:31:11
                               ETA: 00:02:15

################################################################################
                     [1m Learning iteration 1866/2000 [0m                     

                       Computation: 109851 steps/s (collection: 0.781s, learning 0.114s)
             Mean action noise std: 10.43
          Mean value_function loss: 41.6024
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 29.7872
                       Mean reward: 840.66
               Mean episode length: 244.91
    Episode_Reward/reaching_object: 0.7534
     Episode_Reward/lifting_object: 171.1146
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.2713
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 183533568
                    Iteration time: 0.89s
                      Time elapsed: 00:31:12
                               ETA: 00:02:14

################################################################################
                     [1m Learning iteration 1867/2000 [0m                     

                       Computation: 105282 steps/s (collection: 0.815s, learning 0.119s)
             Mean action noise std: 10.44
          Mean value_function loss: 33.3537
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 29.7956
                       Mean reward: 840.22
               Mean episode length: 246.52
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 171.4123
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.2732
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 183631872
                    Iteration time: 0.93s
                      Time elapsed: 00:31:13
                               ETA: 00:02:13

################################################################################
                     [1m Learning iteration 1868/2000 [0m                     

                       Computation: 106165 steps/s (collection: 0.817s, learning 0.109s)
             Mean action noise std: 10.44
          Mean value_function loss: 33.5704
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.7993
                       Mean reward: 859.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7618
     Episode_Reward/lifting_object: 171.9286
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.2723
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 183730176
                    Iteration time: 0.93s
                      Time elapsed: 00:31:14
                               ETA: 00:02:12

################################################################################
                     [1m Learning iteration 1869/2000 [0m                     

                       Computation: 106743 steps/s (collection: 0.806s, learning 0.115s)
             Mean action noise std: 10.45
          Mean value_function loss: 42.2587
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 29.8032
                       Mean reward: 854.80
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 173.0485
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.2766
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 19.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 183828480
                    Iteration time: 0.92s
                      Time elapsed: 00:31:15
                               ETA: 00:02:11

################################################################################
                     [1m Learning iteration 1870/2000 [0m                     

                       Computation: 105899 steps/s (collection: 0.831s, learning 0.098s)
             Mean action noise std: 10.45
          Mean value_function loss: 36.0376
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.8066
                       Mean reward: 857.99
               Mean episode length: 246.99
    Episode_Reward/reaching_object: 0.7610
     Episode_Reward/lifting_object: 171.1796
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.2746
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 183926784
                    Iteration time: 0.93s
                      Time elapsed: 00:31:16
                               ETA: 00:02:10

################################################################################
                     [1m Learning iteration 1871/2000 [0m                     

                       Computation: 108190 steps/s (collection: 0.807s, learning 0.102s)
             Mean action noise std: 10.46
          Mean value_function loss: 48.2808
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 29.8113
                       Mean reward: 860.70
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.7511
     Episode_Reward/lifting_object: 170.8123
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.2759
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 184025088
                    Iteration time: 0.91s
                      Time elapsed: 00:31:16
                               ETA: 00:02:09

################################################################################
                     [1m Learning iteration 1872/2000 [0m                     

                       Computation: 108649 steps/s (collection: 0.796s, learning 0.109s)
             Mean action noise std: 10.47
          Mean value_function loss: 41.1609
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 29.8158
                       Mean reward: 837.20
               Mean episode length: 245.71
    Episode_Reward/reaching_object: 0.7516
     Episode_Reward/lifting_object: 170.5215
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.2753
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 184123392
                    Iteration time: 0.90s
                      Time elapsed: 00:31:17
                               ETA: 00:02:08

################################################################################
                     [1m Learning iteration 1873/2000 [0m                     

                       Computation: 111606 steps/s (collection: 0.779s, learning 0.102s)
             Mean action noise std: 10.48
          Mean value_function loss: 39.9715
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 29.8239
                       Mean reward: 865.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7594
     Episode_Reward/lifting_object: 172.2873
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.2777
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 184221696
                    Iteration time: 0.88s
                      Time elapsed: 00:31:18
                               ETA: 00:02:07

################################################################################
                     [1m Learning iteration 1874/2000 [0m                     

                       Computation: 109746 steps/s (collection: 0.784s, learning 0.111s)
             Mean action noise std: 10.49
          Mean value_function loss: 43.0398
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 29.8299
                       Mean reward: 856.36
               Mean episode length: 247.23
    Episode_Reward/reaching_object: 0.7542
     Episode_Reward/lifting_object: 171.3686
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.2791
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 184320000
                    Iteration time: 0.90s
                      Time elapsed: 00:31:19
                               ETA: 00:02:06

################################################################################
                     [1m Learning iteration 1875/2000 [0m                     

                       Computation: 112534 steps/s (collection: 0.775s, learning 0.099s)
             Mean action noise std: 10.49
          Mean value_function loss: 36.9574
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 29.8363
                       Mean reward: 849.39
               Mean episode length: 246.18
    Episode_Reward/reaching_object: 0.7401
     Episode_Reward/lifting_object: 169.9921
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.2768
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 184418304
                    Iteration time: 0.87s
                      Time elapsed: 00:31:20
                               ETA: 00:02:05

################################################################################
                     [1m Learning iteration 1876/2000 [0m                     

                       Computation: 113025 steps/s (collection: 0.770s, learning 0.100s)
             Mean action noise std: 10.50
          Mean value_function loss: 23.9925
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 29.8400
                       Mean reward: 847.98
               Mean episode length: 248.67
    Episode_Reward/reaching_object: 0.7422
     Episode_Reward/lifting_object: 170.2627
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.2759
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 184516608
                    Iteration time: 0.87s
                      Time elapsed: 00:31:21
                               ETA: 00:02:04

################################################################################
                     [1m Learning iteration 1877/2000 [0m                     

                       Computation: 110009 steps/s (collection: 0.804s, learning 0.090s)
             Mean action noise std: 10.50
          Mean value_function loss: 27.3810
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 29.8454
                       Mean reward: 871.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7575
     Episode_Reward/lifting_object: 172.6458
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.2779
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 184614912
                    Iteration time: 0.89s
                      Time elapsed: 00:31:22
                               ETA: 00:02:03

################################################################################
                     [1m Learning iteration 1878/2000 [0m                     

                       Computation: 114055 steps/s (collection: 0.774s, learning 0.088s)
             Mean action noise std: 10.50
          Mean value_function loss: 21.2513
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 29.8469
                       Mean reward: 870.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7519
     Episode_Reward/lifting_object: 171.2317
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.2778
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 184713216
                    Iteration time: 0.86s
                      Time elapsed: 00:31:23
                               ETA: 00:02:02

################################################################################
                     [1m Learning iteration 1879/2000 [0m                     

                       Computation: 110787 steps/s (collection: 0.796s, learning 0.092s)
             Mean action noise std: 10.52
          Mean value_function loss: 22.0188
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.8526
                       Mean reward: 879.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7562
     Episode_Reward/lifting_object: 171.9133
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.2793
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 19.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 184811520
                    Iteration time: 0.89s
                      Time elapsed: 00:31:24
                               ETA: 00:02:01

################################################################################
                     [1m Learning iteration 1880/2000 [0m                     

                       Computation: 112972 steps/s (collection: 0.778s, learning 0.093s)
             Mean action noise std: 10.53
          Mean value_function loss: 25.0133
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.8633
                       Mean reward: 840.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7534
     Episode_Reward/lifting_object: 169.3020
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.2786
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 184909824
                    Iteration time: 0.87s
                      Time elapsed: 00:31:24
                               ETA: 00:02:00

################################################################################
                     [1m Learning iteration 1881/2000 [0m                     

                       Computation: 110588 steps/s (collection: 0.788s, learning 0.101s)
             Mean action noise std: 10.54
          Mean value_function loss: 21.5192
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 29.8730
                       Mean reward: 872.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7588
     Episode_Reward/lifting_object: 172.4691
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.2780
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 185008128
                    Iteration time: 0.89s
                      Time elapsed: 00:31:25
                               ETA: 00:01:59

################################################################################
                     [1m Learning iteration 1882/2000 [0m                     

                       Computation: 113283 steps/s (collection: 0.778s, learning 0.090s)
             Mean action noise std: 10.56
          Mean value_function loss: 27.4368
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 29.8826
                       Mean reward: 851.18
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.7515
     Episode_Reward/lifting_object: 171.8569
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.2779
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 185106432
                    Iteration time: 0.87s
                      Time elapsed: 00:31:26
                               ETA: 00:01:58

################################################################################
                     [1m Learning iteration 1883/2000 [0m                     

                       Computation: 110378 steps/s (collection: 0.793s, learning 0.098s)
             Mean action noise std: 10.57
          Mean value_function loss: 20.9777
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 29.8931
                       Mean reward: 855.36
               Mean episode length: 248.75
    Episode_Reward/reaching_object: 0.7471
     Episode_Reward/lifting_object: 170.9859
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.2743
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 185204736
                    Iteration time: 0.89s
                      Time elapsed: 00:31:27
                               ETA: 00:01:57

################################################################################
                     [1m Learning iteration 1884/2000 [0m                     

                       Computation: 107441 steps/s (collection: 0.815s, learning 0.100s)
             Mean action noise std: 10.57
          Mean value_function loss: 27.9797
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 29.9005
                       Mean reward: 873.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 173.5502
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.2781
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 185303040
                    Iteration time: 0.91s
                      Time elapsed: 00:31:28
                               ETA: 00:01:56

################################################################################
                     [1m Learning iteration 1885/2000 [0m                     

                       Computation: 113263 steps/s (collection: 0.777s, learning 0.091s)
             Mean action noise std: 10.59
          Mean value_function loss: 34.3413
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.9078
                       Mean reward: 867.49
               Mean episode length: 248.28
    Episode_Reward/reaching_object: 0.7608
     Episode_Reward/lifting_object: 172.3407
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.2775
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 185401344
                    Iteration time: 0.87s
                      Time elapsed: 00:31:29
                               ETA: 00:01:55

################################################################################
                     [1m Learning iteration 1886/2000 [0m                     

                       Computation: 106974 steps/s (collection: 0.821s, learning 0.098s)
             Mean action noise std: 10.60
          Mean value_function loss: 44.5737
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 29.9186
                       Mean reward: 834.38
               Mean episode length: 244.96
    Episode_Reward/reaching_object: 0.7576
     Episode_Reward/lifting_object: 170.9749
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.2765
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 185499648
                    Iteration time: 0.92s
                      Time elapsed: 00:31:30
                               ETA: 00:01:54

################################################################################
                     [1m Learning iteration 1887/2000 [0m                     

                       Computation: 115647 steps/s (collection: 0.764s, learning 0.086s)
             Mean action noise std: 10.61
          Mean value_function loss: 34.4332
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 29.9272
                       Mean reward: 867.51
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7571
     Episode_Reward/lifting_object: 171.1460
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.2780
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 185597952
                    Iteration time: 0.85s
                      Time elapsed: 00:31:31
                               ETA: 00:01:53

################################################################################
                     [1m Learning iteration 1888/2000 [0m                     

                       Computation: 110507 steps/s (collection: 0.791s, learning 0.099s)
             Mean action noise std: 10.62
          Mean value_function loss: 26.4101
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 29.9349
                       Mean reward: 874.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 172.4096
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.2769
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 185696256
                    Iteration time: 0.89s
                      Time elapsed: 00:31:31
                               ETA: 00:01:52

################################################################################
                     [1m Learning iteration 1889/2000 [0m                     

                       Computation: 110066 steps/s (collection: 0.804s, learning 0.089s)
             Mean action noise std: 10.63
          Mean value_function loss: 33.0137
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 29.9434
                       Mean reward: 858.35
               Mean episode length: 249.10
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 172.0317
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.2798
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 185794560
                    Iteration time: 0.89s
                      Time elapsed: 00:31:32
                               ETA: 00:01:51

################################################################################
                     [1m Learning iteration 1890/2000 [0m                     

                       Computation: 111208 steps/s (collection: 0.792s, learning 0.092s)
             Mean action noise std: 10.64
          Mean value_function loss: 30.2749
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 29.9472
                       Mean reward: 867.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 172.3531
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.2804
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 185892864
                    Iteration time: 0.88s
                      Time elapsed: 00:31:33
                               ETA: 00:01:50

################################################################################
                     [1m Learning iteration 1891/2000 [0m                     

                       Computation: 105324 steps/s (collection: 0.832s, learning 0.101s)
             Mean action noise std: 10.65
          Mean value_function loss: 26.9128
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 29.9540
                       Mean reward: 854.90
               Mean episode length: 247.40
    Episode_Reward/reaching_object: 0.7585
     Episode_Reward/lifting_object: 171.6873
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.2807
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 185991168
                    Iteration time: 0.93s
                      Time elapsed: 00:31:34
                               ETA: 00:01:49

################################################################################
                     [1m Learning iteration 1892/2000 [0m                     

                       Computation: 108311 steps/s (collection: 0.812s, learning 0.096s)
             Mean action noise std: 10.66
          Mean value_function loss: 36.4670
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 29.9629
                       Mean reward: 868.36
               Mean episode length: 249.90
    Episode_Reward/reaching_object: 0.7583
     Episode_Reward/lifting_object: 173.3569
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.2823
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 186089472
                    Iteration time: 0.91s
                      Time elapsed: 00:31:35
                               ETA: 00:01:48

################################################################################
                     [1m Learning iteration 1893/2000 [0m                     

                       Computation: 107367 steps/s (collection: 0.811s, learning 0.105s)
             Mean action noise std: 10.67
          Mean value_function loss: 42.5815
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 29.9721
                       Mean reward: 846.77
               Mean episode length: 246.64
    Episode_Reward/reaching_object: 0.7542
     Episode_Reward/lifting_object: 171.3401
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.2834
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 186187776
                    Iteration time: 0.92s
                      Time elapsed: 00:31:36
                               ETA: 00:01:47

################################################################################
                     [1m Learning iteration 1894/2000 [0m                     

                       Computation: 107094 steps/s (collection: 0.820s, learning 0.098s)
             Mean action noise std: 10.68
          Mean value_function loss: 36.1482
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 29.9834
                       Mean reward: 839.61
               Mean episode length: 249.10
    Episode_Reward/reaching_object: 0.7499
     Episode_Reward/lifting_object: 170.0728
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.2858
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 186286080
                    Iteration time: 0.92s
                      Time elapsed: 00:31:37
                               ETA: 00:01:46

################################################################################
                     [1m Learning iteration 1895/2000 [0m                     

                       Computation: 107891 steps/s (collection: 0.812s, learning 0.100s)
             Mean action noise std: 10.69
          Mean value_function loss: 39.8349
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 29.9911
                       Mean reward: 851.32
               Mean episode length: 246.85
    Episode_Reward/reaching_object: 0.7636
     Episode_Reward/lifting_object: 171.4775
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.2867
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 186384384
                    Iteration time: 0.91s
                      Time elapsed: 00:31:38
                               ETA: 00:01:45

################################################################################
                     [1m Learning iteration 1896/2000 [0m                     

                       Computation: 109748 steps/s (collection: 0.798s, learning 0.098s)
             Mean action noise std: 10.69
          Mean value_function loss: 34.1681
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 29.9931
                       Mean reward: 838.07
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.7557
     Episode_Reward/lifting_object: 170.8307
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.2868
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 186482688
                    Iteration time: 0.90s
                      Time elapsed: 00:31:39
                               ETA: 00:01:44

################################################################################
                     [1m Learning iteration 1897/2000 [0m                     

                       Computation: 110960 steps/s (collection: 0.786s, learning 0.100s)
             Mean action noise std: 10.70
          Mean value_function loss: 34.6888
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 29.9970
                       Mean reward: 860.16
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7557
     Episode_Reward/lifting_object: 172.1490
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.2873
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 186580992
                    Iteration time: 0.89s
                      Time elapsed: 00:31:40
                               ETA: 00:01:43

################################################################################
                     [1m Learning iteration 1898/2000 [0m                     

                       Computation: 108613 steps/s (collection: 0.798s, learning 0.107s)
             Mean action noise std: 10.71
          Mean value_function loss: 30.9217
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 30.0051
                       Mean reward: 851.94
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.7572
     Episode_Reward/lifting_object: 171.0144
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.2888
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 186679296
                    Iteration time: 0.91s
                      Time elapsed: 00:31:41
                               ETA: 00:01:42

################################################################################
                     [1m Learning iteration 1899/2000 [0m                     

                       Computation: 110327 steps/s (collection: 0.778s, learning 0.113s)
             Mean action noise std: 10.72
          Mean value_function loss: 34.2304
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 30.0143
                       Mean reward: 841.00
               Mean episode length: 246.15
    Episode_Reward/reaching_object: 0.7602
     Episode_Reward/lifting_object: 171.3468
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.2894
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 186777600
                    Iteration time: 0.89s
                      Time elapsed: 00:31:41
                               ETA: 00:01:41

################################################################################
                     [1m Learning iteration 1900/2000 [0m                     

                       Computation: 110556 steps/s (collection: 0.781s, learning 0.108s)
             Mean action noise std: 10.73
          Mean value_function loss: 36.2116
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 30.0188
                       Mean reward: 846.56
               Mean episode length: 246.67
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 171.0856
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.2918
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 19.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 186875904
                    Iteration time: 0.89s
                      Time elapsed: 00:31:42
                               ETA: 00:01:40

################################################################################
                     [1m Learning iteration 1901/2000 [0m                     

                       Computation: 106785 steps/s (collection: 0.809s, learning 0.112s)
             Mean action noise std: 10.74
          Mean value_function loss: 29.3805
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 30.0289
                       Mean reward: 828.06
               Mean episode length: 244.65
    Episode_Reward/reaching_object: 0.7568
     Episode_Reward/lifting_object: 169.7757
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.2903
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 186974208
                    Iteration time: 0.92s
                      Time elapsed: 00:31:43
                               ETA: 00:01:39

################################################################################
                     [1m Learning iteration 1902/2000 [0m                     

                       Computation: 110796 steps/s (collection: 0.772s, learning 0.115s)
             Mean action noise std: 10.74
          Mean value_function loss: 53.3203
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 30.0324
                       Mean reward: 860.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7588
     Episode_Reward/lifting_object: 172.3213
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.2909
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 187072512
                    Iteration time: 0.89s
                      Time elapsed: 00:31:44
                               ETA: 00:01:38

################################################################################
                     [1m Learning iteration 1903/2000 [0m                     

                       Computation: 111945 steps/s (collection: 0.767s, learning 0.111s)
             Mean action noise std: 10.74
          Mean value_function loss: 46.5986
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 30.0360
                       Mean reward: 860.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7560
     Episode_Reward/lifting_object: 171.5398
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.2928
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 187170816
                    Iteration time: 0.88s
                      Time elapsed: 00:31:45
                               ETA: 00:01:37

################################################################################
                     [1m Learning iteration 1904/2000 [0m                     

                       Computation: 111595 steps/s (collection: 0.773s, learning 0.108s)
             Mean action noise std: 10.75
          Mean value_function loss: 48.1902
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 30.0391
                       Mean reward: 872.73
               Mean episode length: 249.85
    Episode_Reward/reaching_object: 0.7469
     Episode_Reward/lifting_object: 171.0771
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.2926
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 187269120
                    Iteration time: 0.88s
                      Time elapsed: 00:31:46
                               ETA: 00:01:36

################################################################################
                     [1m Learning iteration 1905/2000 [0m                     

                       Computation: 114001 steps/s (collection: 0.766s, learning 0.097s)
             Mean action noise std: 10.76
          Mean value_function loss: 51.3976
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 30.0428
                       Mean reward: 851.18
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.7342
     Episode_Reward/lifting_object: 167.5134
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.2886
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 187367424
                    Iteration time: 0.86s
                      Time elapsed: 00:31:47
                               ETA: 00:01:35

################################################################################
                     [1m Learning iteration 1906/2000 [0m                     

                       Computation: 111924 steps/s (collection: 0.785s, learning 0.094s)
             Mean action noise std: 10.76
          Mean value_function loss: 43.5758
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 30.0490
                       Mean reward: 859.66
               Mean episode length: 247.92
    Episode_Reward/reaching_object: 0.7536
     Episode_Reward/lifting_object: 170.6008
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.2926
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 187465728
                    Iteration time: 0.88s
                      Time elapsed: 00:31:48
                               ETA: 00:01:34

################################################################################
                     [1m Learning iteration 1907/2000 [0m                     

                       Computation: 113262 steps/s (collection: 0.775s, learning 0.093s)
             Mean action noise std: 10.77
          Mean value_function loss: 30.9715
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.0556
                       Mean reward: 855.68
               Mean episode length: 246.81
    Episode_Reward/reaching_object: 0.7493
     Episode_Reward/lifting_object: 170.1511
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.2911
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 187564032
                    Iteration time: 0.87s
                      Time elapsed: 00:31:48
                               ETA: 00:01:33

################################################################################
                     [1m Learning iteration 1908/2000 [0m                     

                       Computation: 111331 steps/s (collection: 0.793s, learning 0.090s)
             Mean action noise std: 10.78
          Mean value_function loss: 36.8321
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 30.0609
                       Mean reward: 870.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7627
     Episode_Reward/lifting_object: 172.5966
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.2946
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 187662336
                    Iteration time: 0.88s
                      Time elapsed: 00:31:49
                               ETA: 00:01:32

################################################################################
                     [1m Learning iteration 1909/2000 [0m                     

                       Computation: 111759 steps/s (collection: 0.790s, learning 0.090s)
             Mean action noise std: 10.79
          Mean value_function loss: 25.5435
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 30.0678
                       Mean reward: 860.23
               Mean episode length: 248.99
    Episode_Reward/reaching_object: 0.7639
     Episode_Reward/lifting_object: 173.2590
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.2928
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 187760640
                    Iteration time: 0.88s
                      Time elapsed: 00:31:50
                               ETA: 00:01:31

################################################################################
                     [1m Learning iteration 1910/2000 [0m                     

                       Computation: 110734 steps/s (collection: 0.790s, learning 0.098s)
             Mean action noise std: 10.81
          Mean value_function loss: 23.2181
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 30.0791
                       Mean reward: 850.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7567
     Episode_Reward/lifting_object: 170.9679
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.2917
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 187858944
                    Iteration time: 0.89s
                      Time elapsed: 00:31:51
                               ETA: 00:01:30

################################################################################
                     [1m Learning iteration 1911/2000 [0m                     

                       Computation: 112519 steps/s (collection: 0.781s, learning 0.093s)
             Mean action noise std: 10.81
          Mean value_function loss: 23.2419
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 30.0864
                       Mean reward: 863.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7542
     Episode_Reward/lifting_object: 171.0139
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.2935
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 187957248
                    Iteration time: 0.87s
                      Time elapsed: 00:31:52
                               ETA: 00:01:29

################################################################################
                     [1m Learning iteration 1912/2000 [0m                     

                       Computation: 114329 steps/s (collection: 0.765s, learning 0.095s)
             Mean action noise std: 10.82
          Mean value_function loss: 31.9793
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 30.0919
                       Mean reward: 872.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7434
     Episode_Reward/lifting_object: 169.6996
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.2932
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 188055552
                    Iteration time: 0.86s
                      Time elapsed: 00:31:53
                               ETA: 00:01:28

################################################################################
                     [1m Learning iteration 1913/2000 [0m                     

                       Computation: 109466 steps/s (collection: 0.799s, learning 0.099s)
             Mean action noise std: 10.83
          Mean value_function loss: 31.6554
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 30.0962
                       Mean reward: 848.76
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7527
     Episode_Reward/lifting_object: 171.1369
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.2922
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 188153856
                    Iteration time: 0.90s
                      Time elapsed: 00:31:54
                               ETA: 00:01:27

################################################################################
                     [1m Learning iteration 1914/2000 [0m                     

                       Computation: 110553 steps/s (collection: 0.793s, learning 0.096s)
             Mean action noise std: 10.84
          Mean value_function loss: 30.0170
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 30.1038
                       Mean reward: 840.71
               Mean episode length: 248.59
    Episode_Reward/reaching_object: 0.7487
     Episode_Reward/lifting_object: 170.2486
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.2930
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 188252160
                    Iteration time: 0.89s
                      Time elapsed: 00:31:55
                               ETA: 00:01:26

################################################################################
                     [1m Learning iteration 1915/2000 [0m                     

                       Computation: 108601 steps/s (collection: 0.815s, learning 0.091s)
             Mean action noise std: 10.85
          Mean value_function loss: 22.3047
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 30.1110
                       Mean reward: 862.61
               Mean episode length: 247.77
    Episode_Reward/reaching_object: 0.7587
     Episode_Reward/lifting_object: 171.1511
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.2931
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 188350464
                    Iteration time: 0.91s
                      Time elapsed: 00:31:56
                               ETA: 00:01:25

################################################################################
                     [1m Learning iteration 1916/2000 [0m                     

                       Computation: 109566 steps/s (collection: 0.800s, learning 0.098s)
             Mean action noise std: 10.85
          Mean value_function loss: 23.7499
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 30.1146
                       Mean reward: 866.94
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: 173.8138
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.2942
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 188448768
                    Iteration time: 0.90s
                      Time elapsed: 00:31:56
                               ETA: 00:01:23

################################################################################
                     [1m Learning iteration 1917/2000 [0m                     

                       Computation: 108684 steps/s (collection: 0.810s, learning 0.095s)
             Mean action noise std: 10.86
          Mean value_function loss: 29.5011
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 30.1190
                       Mean reward: 839.90
               Mean episode length: 247.90
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 170.9499
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.2927
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 188547072
                    Iteration time: 0.90s
                      Time elapsed: 00:31:57
                               ETA: 00:01:22

################################################################################
                     [1m Learning iteration 1918/2000 [0m                     

                       Computation: 108171 steps/s (collection: 0.811s, learning 0.097s)
             Mean action noise std: 10.87
          Mean value_function loss: 26.6467
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 30.1254
                       Mean reward: 861.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7611
     Episode_Reward/lifting_object: 171.9109
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.2929
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 188645376
                    Iteration time: 0.91s
                      Time elapsed: 00:31:58
                               ETA: 00:01:21

################################################################################
                     [1m Learning iteration 1919/2000 [0m                     

                       Computation: 106372 steps/s (collection: 0.815s, learning 0.110s)
             Mean action noise std: 10.87
          Mean value_function loss: 35.6161
               Mean surrogate loss: 0.0091
                 Mean entropy loss: 30.1300
                       Mean reward: 861.21
               Mean episode length: 248.86
    Episode_Reward/reaching_object: 0.7692
     Episode_Reward/lifting_object: 172.7658
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.2911
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 188743680
                    Iteration time: 0.92s
                      Time elapsed: 00:31:59
                               ETA: 00:01:20

################################################################################
                     [1m Learning iteration 1920/2000 [0m                     

                       Computation: 107936 steps/s (collection: 0.785s, learning 0.126s)
             Mean action noise std: 10.87
          Mean value_function loss: 29.2032
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 30.1309
                       Mean reward: 870.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7721
     Episode_Reward/lifting_object: 173.3704
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.2922
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 188841984
                    Iteration time: 0.91s
                      Time elapsed: 00:32:00
                               ETA: 00:01:19

################################################################################
                     [1m Learning iteration 1921/2000 [0m                     

                       Computation: 109922 steps/s (collection: 0.786s, learning 0.109s)
             Mean action noise std: 10.88
          Mean value_function loss: 29.0469
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 30.1346
                       Mean reward: 875.13
               Mean episode length: 249.98
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 170.9865
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.2915
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 19.2083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 188940288
                    Iteration time: 0.89s
                      Time elapsed: 00:32:01
                               ETA: 00:01:18

################################################################################
                     [1m Learning iteration 1922/2000 [0m                     

                       Computation: 111063 steps/s (collection: 0.787s, learning 0.098s)
             Mean action noise std: 10.90
          Mean value_function loss: 31.2436
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 30.1431
                       Mean reward: 877.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7698
     Episode_Reward/lifting_object: 173.0938
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.2928
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 189038592
                    Iteration time: 0.89s
                      Time elapsed: 00:32:02
                               ETA: 00:01:17

################################################################################
                     [1m Learning iteration 1923/2000 [0m                     

                       Computation: 104621 steps/s (collection: 0.838s, learning 0.102s)
             Mean action noise std: 10.90
          Mean value_function loss: 29.6025
               Mean surrogate loss: 0.0143
                 Mean entropy loss: 30.1536
                       Mean reward: 856.64
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 172.8139
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.2910
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 189136896
                    Iteration time: 0.94s
                      Time elapsed: 00:32:03
                               ETA: 00:01:16

################################################################################
                     [1m Learning iteration 1924/2000 [0m                     

                       Computation: 108044 steps/s (collection: 0.806s, learning 0.104s)
             Mean action noise std: 10.90
          Mean value_function loss: 32.7280
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 30.1553
                       Mean reward: 876.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 173.4414
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.2920
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189235200
                    Iteration time: 0.91s
                      Time elapsed: 00:32:04
                               ETA: 00:01:15

################################################################################
                     [1m Learning iteration 1925/2000 [0m                     

                       Computation: 106707 steps/s (collection: 0.822s, learning 0.099s)
             Mean action noise std: 10.91
          Mean value_function loss: 39.2281
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 30.1586
                       Mean reward: 859.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7666
     Episode_Reward/lifting_object: 171.9048
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.2915
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 189333504
                    Iteration time: 0.92s
                      Time elapsed: 00:32:05
                               ETA: 00:01:14

################################################################################
                     [1m Learning iteration 1926/2000 [0m                     

                       Computation: 111110 steps/s (collection: 0.794s, learning 0.091s)
             Mean action noise std: 10.92
          Mean value_function loss: 40.5939
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 30.1651
                       Mean reward: 878.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 172.9468
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.2938
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 189431808
                    Iteration time: 0.88s
                      Time elapsed: 00:32:06
                               ETA: 00:01:13

################################################################################
                     [1m Learning iteration 1927/2000 [0m                     

                       Computation: 109013 steps/s (collection: 0.810s, learning 0.092s)
             Mean action noise std: 10.93
          Mean value_function loss: 47.5972
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 30.1742
                       Mean reward: 862.61
               Mean episode length: 247.80
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 172.1143
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.2916
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 189530112
                    Iteration time: 0.90s
                      Time elapsed: 00:32:06
                               ETA: 00:01:12

################################################################################
                     [1m Learning iteration 1928/2000 [0m                     

                       Computation: 107659 steps/s (collection: 0.815s, learning 0.098s)
             Mean action noise std: 10.94
          Mean value_function loss: 45.0148
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 30.1810
                       Mean reward: 866.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 171.1302
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.2927
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 189628416
                    Iteration time: 0.91s
                      Time elapsed: 00:32:07
                               ETA: 00:01:11

################################################################################
                     [1m Learning iteration 1929/2000 [0m                     

                       Computation: 107979 steps/s (collection: 0.802s, learning 0.108s)
             Mean action noise std: 10.95
          Mean value_function loss: 57.1816
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 30.1885
                       Mean reward: 857.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 172.3747
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.2915
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 189726720
                    Iteration time: 0.91s
                      Time elapsed: 00:32:08
                               ETA: 00:01:10

################################################################################
                     [1m Learning iteration 1930/2000 [0m                     

                       Computation: 110313 steps/s (collection: 0.793s, learning 0.099s)
             Mean action noise std: 10.95
          Mean value_function loss: 46.1156
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 30.1916
                       Mean reward: 860.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 171.0786
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.2925
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 189825024
                    Iteration time: 0.89s
                      Time elapsed: 00:32:09
                               ETA: 00:01:09

################################################################################
                     [1m Learning iteration 1931/2000 [0m                     

                       Computation: 104600 steps/s (collection: 0.832s, learning 0.108s)
             Mean action noise std: 10.96
          Mean value_function loss: 44.6638
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 30.1955
                       Mean reward: 839.12
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 0.7526
     Episode_Reward/lifting_object: 169.8422
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.2928
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 19.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 189923328
                    Iteration time: 0.94s
                      Time elapsed: 00:32:10
                               ETA: 00:01:08

################################################################################
                     [1m Learning iteration 1932/2000 [0m                     

                       Computation: 104347 steps/s (collection: 0.836s, learning 0.106s)
             Mean action noise std: 10.96
          Mean value_function loss: 45.0897
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.1985
                       Mean reward: 868.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7608
     Episode_Reward/lifting_object: 171.1345
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.2927
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 190021632
                    Iteration time: 0.94s
                      Time elapsed: 00:32:11
                               ETA: 00:01:07

################################################################################
                     [1m Learning iteration 1933/2000 [0m                     

                       Computation: 103332 steps/s (collection: 0.842s, learning 0.109s)
             Mean action noise std: 10.97
          Mean value_function loss: 35.1534
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 30.2039
                       Mean reward: 853.06
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 0.7557
     Episode_Reward/lifting_object: 171.0420
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.2913
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 190119936
                    Iteration time: 0.95s
                      Time elapsed: 00:32:12
                               ETA: 00:01:06

################################################################################
                     [1m Learning iteration 1934/2000 [0m                     

                       Computation: 106918 steps/s (collection: 0.799s, learning 0.121s)
             Mean action noise std: 10.98
          Mean value_function loss: 46.6731
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 30.2073
                       Mean reward: 822.70
               Mean episode length: 247.91
    Episode_Reward/reaching_object: 0.7526
     Episode_Reward/lifting_object: 170.3589
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.2911
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 190218240
                    Iteration time: 0.92s
                      Time elapsed: 00:32:13
                               ETA: 00:01:05

################################################################################
                     [1m Learning iteration 1935/2000 [0m                     

                       Computation: 103677 steps/s (collection: 0.835s, learning 0.113s)
             Mean action noise std: 10.98
          Mean value_function loss: 37.6869
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 30.2110
                       Mean reward: 854.95
               Mean episode length: 248.73
    Episode_Reward/reaching_object: 0.7468
     Episode_Reward/lifting_object: 168.7516
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.2908
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 190316544
                    Iteration time: 0.95s
                      Time elapsed: 00:32:14
                               ETA: 00:01:04

################################################################################
                     [1m Learning iteration 1936/2000 [0m                     

                       Computation: 105448 steps/s (collection: 0.824s, learning 0.108s)
             Mean action noise std: 10.98
          Mean value_function loss: 35.5867
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 30.2140
                       Mean reward: 838.98
               Mean episode length: 248.47
    Episode_Reward/reaching_object: 0.7473
     Episode_Reward/lifting_object: 168.8111
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.2910
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 190414848
                    Iteration time: 0.93s
                      Time elapsed: 00:32:15
                               ETA: 00:01:03

################################################################################
                     [1m Learning iteration 1937/2000 [0m                     

                       Computation: 106927 steps/s (collection: 0.813s, learning 0.106s)
             Mean action noise std: 10.99
          Mean value_function loss: 32.7651
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 30.2183
                       Mean reward: 835.63
               Mean episode length: 248.49
    Episode_Reward/reaching_object: 0.7489
     Episode_Reward/lifting_object: 168.2092
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.2919
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 190513152
                    Iteration time: 0.92s
                      Time elapsed: 00:32:16
                               ETA: 00:01:02

################################################################################
                     [1m Learning iteration 1938/2000 [0m                     

                       Computation: 106862 steps/s (collection: 0.808s, learning 0.112s)
             Mean action noise std: 11.00
          Mean value_function loss: 47.8530
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 30.2240
                       Mean reward: 850.35
               Mean episode length: 249.27
    Episode_Reward/reaching_object: 0.7524
     Episode_Reward/lifting_object: 168.8631
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.2934
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 190611456
                    Iteration time: 0.92s
                      Time elapsed: 00:32:17
                               ETA: 00:01:01

################################################################################
                     [1m Learning iteration 1939/2000 [0m                     

                       Computation: 105172 steps/s (collection: 0.825s, learning 0.109s)
             Mean action noise std: 11.01
          Mean value_function loss: 44.5416
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 30.2283
                       Mean reward: 840.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7475
     Episode_Reward/lifting_object: 169.3506
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.2920
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 190709760
                    Iteration time: 0.93s
                      Time elapsed: 00:32:18
                               ETA: 00:01:00

################################################################################
                     [1m Learning iteration 1940/2000 [0m                     

                       Computation: 98954 steps/s (collection: 0.884s, learning 0.109s)
             Mean action noise std: 11.01
          Mean value_function loss: 43.3363
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 30.2325
                       Mean reward: 853.18
               Mean episode length: 248.60
    Episode_Reward/reaching_object: 0.7479
     Episode_Reward/lifting_object: 168.4998
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.2913
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 190808064
                    Iteration time: 0.99s
                      Time elapsed: 00:32:19
                               ETA: 00:00:59

################################################################################
                     [1m Learning iteration 1941/2000 [0m                     

                       Computation: 103954 steps/s (collection: 0.837s, learning 0.109s)
             Mean action noise std: 11.01
          Mean value_function loss: 43.2989
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 30.2351
                       Mean reward: 835.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7554
     Episode_Reward/lifting_object: 170.3247
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.2924
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 190906368
                    Iteration time: 0.95s
                      Time elapsed: 00:32:20
                               ETA: 00:00:58

################################################################################
                     [1m Learning iteration 1942/2000 [0m                     

                       Computation: 110590 steps/s (collection: 0.799s, learning 0.090s)
             Mean action noise std: 11.02
          Mean value_function loss: 39.3634
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 30.2399
                       Mean reward: 859.76
               Mean episode length: 248.88
    Episode_Reward/reaching_object: 0.7489
     Episode_Reward/lifting_object: 169.2445
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.2893
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.7500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 191004672
                    Iteration time: 0.89s
                      Time elapsed: 00:32:20
                               ETA: 00:00:57

################################################################################
                     [1m Learning iteration 1943/2000 [0m                     

                       Computation: 110294 steps/s (collection: 0.797s, learning 0.095s)
             Mean action noise std: 11.03
          Mean value_function loss: 46.3850
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 30.2448
                       Mean reward: 859.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7585
     Episode_Reward/lifting_object: 170.7204
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.2907
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 191102976
                    Iteration time: 0.89s
                      Time elapsed: 00:32:21
                               ETA: 00:00:56

################################################################################
                     [1m Learning iteration 1944/2000 [0m                     

                       Computation: 105242 steps/s (collection: 0.821s, learning 0.114s)
             Mean action noise std: 11.04
          Mean value_function loss: 37.8026
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 30.2523
                       Mean reward: 837.83
               Mean episode length: 245.19
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 170.3133
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.2883
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 191201280
                    Iteration time: 0.93s
                      Time elapsed: 00:32:22
                               ETA: 00:00:55

################################################################################
                     [1m Learning iteration 1945/2000 [0m                     

                       Computation: 107816 steps/s (collection: 0.819s, learning 0.093s)
             Mean action noise std: 11.04
          Mean value_function loss: 40.7561
               Mean surrogate loss: 0.0219
                 Mean entropy loss: 30.2580
                       Mean reward: 860.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7546
     Episode_Reward/lifting_object: 171.0948
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.2924
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 191299584
                    Iteration time: 0.91s
                      Time elapsed: 00:32:23
                               ETA: 00:00:54

################################################################################
                     [1m Learning iteration 1946/2000 [0m                     

                       Computation: 103156 steps/s (collection: 0.848s, learning 0.105s)
             Mean action noise std: 11.04
          Mean value_function loss: 43.6529
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 30.2586
                       Mean reward: 856.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7630
     Episode_Reward/lifting_object: 171.8062
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.2900
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 191397888
                    Iteration time: 0.95s
                      Time elapsed: 00:32:24
                               ETA: 00:00:53

################################################################################
                     [1m Learning iteration 1947/2000 [0m                     

                       Computation: 110096 steps/s (collection: 0.796s, learning 0.097s)
             Mean action noise std: 11.05
          Mean value_function loss: 33.2832
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 30.2625
                       Mean reward: 860.22
               Mean episode length: 248.78
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 169.7144
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.2888
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 191496192
                    Iteration time: 0.89s
                      Time elapsed: 00:32:25
                               ETA: 00:00:52

################################################################################
                     [1m Learning iteration 1948/2000 [0m                     

                       Computation: 110751 steps/s (collection: 0.784s, learning 0.104s)
             Mean action noise std: 11.06
          Mean value_function loss: 45.6645
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 30.2691
                       Mean reward: 865.16
               Mean episode length: 249.73
    Episode_Reward/reaching_object: 0.7504
     Episode_Reward/lifting_object: 170.7395
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.2918
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 191594496
                    Iteration time: 0.89s
                      Time elapsed: 00:32:26
                               ETA: 00:00:51

################################################################################
                     [1m Learning iteration 1949/2000 [0m                     

                       Computation: 108055 steps/s (collection: 0.806s, learning 0.104s)
             Mean action noise std: 11.08
          Mean value_function loss: 37.3404
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 30.2775
                       Mean reward: 863.88
               Mean episode length: 249.59
    Episode_Reward/reaching_object: 0.7521
     Episode_Reward/lifting_object: 169.7103
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.2921
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 191692800
                    Iteration time: 0.91s
                      Time elapsed: 00:32:27
                               ETA: 00:00:50

################################################################################
                     [1m Learning iteration 1950/2000 [0m                     

                       Computation: 107714 steps/s (collection: 0.803s, learning 0.110s)
             Mean action noise std: 11.08
          Mean value_function loss: 42.2393
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 30.2883
                       Mean reward: 831.82
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 0.7590
     Episode_Reward/lifting_object: 170.1697
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.2921
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 191791104
                    Iteration time: 0.91s
                      Time elapsed: 00:32:28
                               ETA: 00:00:49

################################################################################
                     [1m Learning iteration 1951/2000 [0m                     

                       Computation: 108011 steps/s (collection: 0.803s, learning 0.107s)
             Mean action noise std: 11.09
          Mean value_function loss: 36.5128
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 30.2928
                       Mean reward: 861.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7508
     Episode_Reward/lifting_object: 171.3281
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.2928
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 191889408
                    Iteration time: 0.91s
                      Time elapsed: 00:32:29
                               ETA: 00:00:48

################################################################################
                     [1m Learning iteration 1952/2000 [0m                     

                       Computation: 104823 steps/s (collection: 0.820s, learning 0.118s)
             Mean action noise std: 11.10
          Mean value_function loss: 47.9265
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 30.3004
                       Mean reward: 826.67
               Mean episode length: 249.36
    Episode_Reward/reaching_object: 0.7439
     Episode_Reward/lifting_object: 168.6087
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.2936
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 19.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 191987712
                    Iteration time: 0.94s
                      Time elapsed: 00:32:30
                               ETA: 00:00:47

################################################################################
                     [1m Learning iteration 1953/2000 [0m                     

                       Computation: 102041 steps/s (collection: 0.850s, learning 0.114s)
             Mean action noise std: 11.11
          Mean value_function loss: 43.0474
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 30.3085
                       Mean reward: 827.16
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.7459
     Episode_Reward/lifting_object: 167.7847
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.2959
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 192086016
                    Iteration time: 0.96s
                      Time elapsed: 00:32:30
                               ETA: 00:00:46

################################################################################
                     [1m Learning iteration 1954/2000 [0m                     

                       Computation: 99981 steps/s (collection: 0.880s, learning 0.103s)
             Mean action noise std: 11.12
          Mean value_function loss: 41.3710
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 30.3154
                       Mean reward: 869.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7547
     Episode_Reward/lifting_object: 171.7183
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.2931
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 192184320
                    Iteration time: 0.98s
                      Time elapsed: 00:32:31
                               ETA: 00:00:45

################################################################################
                     [1m Learning iteration 1955/2000 [0m                     

                       Computation: 99078 steps/s (collection: 0.896s, learning 0.097s)
             Mean action noise std: 11.13
          Mean value_function loss: 38.0780
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 30.3197
                       Mean reward: 851.74
               Mean episode length: 246.66
    Episode_Reward/reaching_object: 0.7507
     Episode_Reward/lifting_object: 169.0304
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.2937
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 192282624
                    Iteration time: 0.99s
                      Time elapsed: 00:32:32
                               ETA: 00:00:44

################################################################################
                     [1m Learning iteration 1956/2000 [0m                     

                       Computation: 105402 steps/s (collection: 0.837s, learning 0.096s)
             Mean action noise std: 11.14
          Mean value_function loss: 39.3734
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 30.3265
                       Mean reward: 848.92
               Mean episode length: 248.55
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 171.0283
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.2945
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 192380928
                    Iteration time: 0.93s
                      Time elapsed: 00:32:33
                               ETA: 00:00:43

################################################################################
                     [1m Learning iteration 1957/2000 [0m                     

                       Computation: 99874 steps/s (collection: 0.886s, learning 0.098s)
             Mean action noise std: 11.14
          Mean value_function loss: 34.3787
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 30.3331
                       Mean reward: 858.00
               Mean episode length: 249.51
    Episode_Reward/reaching_object: 0.7544
     Episode_Reward/lifting_object: 169.7312
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.2971
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 192479232
                    Iteration time: 0.98s
                      Time elapsed: 00:32:34
                               ETA: 00:00:42

################################################################################
                     [1m Learning iteration 1958/2000 [0m                     

                       Computation: 102053 steps/s (collection: 0.868s, learning 0.095s)
             Mean action noise std: 11.15
          Mean value_function loss: 31.5420
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 30.3384
                       Mean reward: 853.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 170.5751
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.2978
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 192577536
                    Iteration time: 0.96s
                      Time elapsed: 00:32:35
                               ETA: 00:00:41

################################################################################
                     [1m Learning iteration 1959/2000 [0m                     

                       Computation: 103364 steps/s (collection: 0.856s, learning 0.096s)
             Mean action noise std: 11.16
          Mean value_function loss: 44.0344
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 30.3458
                       Mean reward: 850.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 171.2608
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.2988
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 192675840
                    Iteration time: 0.95s
                      Time elapsed: 00:32:36
                               ETA: 00:00:40

################################################################################
                     [1m Learning iteration 1960/2000 [0m                     

                       Computation: 103495 steps/s (collection: 0.852s, learning 0.097s)
             Mean action noise std: 11.18
          Mean value_function loss: 47.6526
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 30.3565
                       Mean reward: 849.39
               Mean episode length: 247.83
    Episode_Reward/reaching_object: 0.7587
     Episode_Reward/lifting_object: 171.7232
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.2993
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 192774144
                    Iteration time: 0.95s
                      Time elapsed: 00:32:37
                               ETA: 00:00:39

################################################################################
                     [1m Learning iteration 1961/2000 [0m                     

                       Computation: 102574 steps/s (collection: 0.861s, learning 0.098s)
             Mean action noise std: 11.19
          Mean value_function loss: 40.5719
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 30.3675
                       Mean reward: 860.53
               Mean episode length: 247.76
    Episode_Reward/reaching_object: 0.7571
     Episode_Reward/lifting_object: 170.4172
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.2991
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 192872448
                    Iteration time: 0.96s
                      Time elapsed: 00:32:38
                               ETA: 00:00:38

################################################################################
                     [1m Learning iteration 1962/2000 [0m                     

                       Computation: 98963 steps/s (collection: 0.881s, learning 0.113s)
             Mean action noise std: 11.20
          Mean value_function loss: 40.5639
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 30.3774
                       Mean reward: 863.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7543
     Episode_Reward/lifting_object: 170.5809
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.2995
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 192970752
                    Iteration time: 0.99s
                      Time elapsed: 00:32:39
                               ETA: 00:00:37

################################################################################
                     [1m Learning iteration 1963/2000 [0m                     

                       Computation: 105153 steps/s (collection: 0.836s, learning 0.099s)
             Mean action noise std: 11.21
          Mean value_function loss: 38.6630
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 30.3833
                       Mean reward: 855.40
               Mean episode length: 249.35
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 171.5540
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.3011
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 193069056
                    Iteration time: 0.93s
                      Time elapsed: 00:32:40
                               ETA: 00:00:36

################################################################################
                     [1m Learning iteration 1964/2000 [0m                     

                       Computation: 101165 steps/s (collection: 0.868s, learning 0.103s)
             Mean action noise std: 11.21
          Mean value_function loss: 39.5560
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 30.3862
                       Mean reward: 857.99
               Mean episode length: 249.05
    Episode_Reward/reaching_object: 0.7548
     Episode_Reward/lifting_object: 171.4449
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.3006
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 193167360
                    Iteration time: 0.97s
                      Time elapsed: 00:32:41
                               ETA: 00:00:35

################################################################################
                     [1m Learning iteration 1965/2000 [0m                     

                       Computation: 105139 steps/s (collection: 0.842s, learning 0.093s)
             Mean action noise std: 11.22
          Mean value_function loss: 28.1222
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 30.3923
                       Mean reward: 830.68
               Mean episode length: 246.42
    Episode_Reward/reaching_object: 0.7495
     Episode_Reward/lifting_object: 169.6407
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.3004
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 193265664
                    Iteration time: 0.93s
                      Time elapsed: 00:32:42
                               ETA: 00:00:34

################################################################################
                     [1m Learning iteration 1966/2000 [0m                     

                       Computation: 99217 steps/s (collection: 0.858s, learning 0.133s)
             Mean action noise std: 11.24
          Mean value_function loss: 27.7266
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 30.3991
                       Mean reward: 859.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7630
     Episode_Reward/lifting_object: 172.7309
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.3007
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 193363968
                    Iteration time: 0.99s
                      Time elapsed: 00:32:43
                               ETA: 00:00:33

################################################################################
                     [1m Learning iteration 1967/2000 [0m                     

                       Computation: 100422 steps/s (collection: 0.874s, learning 0.105s)
             Mean action noise std: 11.25
          Mean value_function loss: 29.9917
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 30.4085
                       Mean reward: 833.69
               Mean episode length: 249.41
    Episode_Reward/reaching_object: 0.7486
     Episode_Reward/lifting_object: 169.5740
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.3031
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 193462272
                    Iteration time: 0.98s
                      Time elapsed: 00:32:44
                               ETA: 00:00:32

################################################################################
                     [1m Learning iteration 1968/2000 [0m                     

                       Computation: 101958 steps/s (collection: 0.848s, learning 0.117s)
             Mean action noise std: 11.26
          Mean value_function loss: 42.5223
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 30.4162
                       Mean reward: 847.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7396
     Episode_Reward/lifting_object: 169.6738
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.3016
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 193560576
                    Iteration time: 0.96s
                      Time elapsed: 00:32:45
                               ETA: 00:00:31

################################################################################
                     [1m Learning iteration 1969/2000 [0m                     

                       Computation: 101414 steps/s (collection: 0.863s, learning 0.106s)
             Mean action noise std: 11.28
          Mean value_function loss: 42.1672
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 30.4293
                       Mean reward: 826.36
               Mean episode length: 249.73
    Episode_Reward/reaching_object: 0.7368
     Episode_Reward/lifting_object: 169.7665
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.3031
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 193658880
                    Iteration time: 0.97s
                      Time elapsed: 00:32:46
                               ETA: 00:00:30

################################################################################
                     [1m Learning iteration 1970/2000 [0m                     

                       Computation: 100875 steps/s (collection: 0.852s, learning 0.123s)
             Mean action noise std: 11.28
          Mean value_function loss: 37.9173
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 30.4372
                       Mean reward: 871.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7442
     Episode_Reward/lifting_object: 170.3772
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.3011
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 193757184
                    Iteration time: 0.97s
                      Time elapsed: 00:32:47
                               ETA: 00:00:29

################################################################################
                     [1m Learning iteration 1971/2000 [0m                     

                       Computation: 99131 steps/s (collection: 0.869s, learning 0.123s)
             Mean action noise std: 11.29
          Mean value_function loss: 30.7194
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 30.4396
                       Mean reward: 864.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7500
     Episode_Reward/lifting_object: 170.1597
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.2993
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 193855488
                    Iteration time: 0.99s
                      Time elapsed: 00:32:48
                               ETA: 00:00:28

################################################################################
                     [1m Learning iteration 1972/2000 [0m                     

                       Computation: 101350 steps/s (collection: 0.839s, learning 0.131s)
             Mean action noise std: 11.29
          Mean value_function loss: 28.4029
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 30.4423
                       Mean reward: 842.74
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7539
     Episode_Reward/lifting_object: 169.0874
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.3022
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 193953792
                    Iteration time: 0.97s
                      Time elapsed: 00:32:49
                               ETA: 00:00:27

################################################################################
                     [1m Learning iteration 1973/2000 [0m                     

                       Computation: 98075 steps/s (collection: 0.851s, learning 0.152s)
             Mean action noise std: 11.30
          Mean value_function loss: 28.8479
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 30.4470
                       Mean reward: 849.81
               Mean episode length: 249.14
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 171.3332
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.3020
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 19.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 194052096
                    Iteration time: 1.00s
                      Time elapsed: 00:32:50
                               ETA: 00:00:26

################################################################################
                     [1m Learning iteration 1974/2000 [0m                     

                       Computation: 99512 steps/s (collection: 0.878s, learning 0.110s)
             Mean action noise std: 11.31
          Mean value_function loss: 38.9540
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 30.4539
                       Mean reward: 845.80
               Mean episode length: 249.12
    Episode_Reward/reaching_object: 0.7535
     Episode_Reward/lifting_object: 170.7540
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.3025
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 194150400
                    Iteration time: 0.99s
                      Time elapsed: 00:32:51
                               ETA: 00:00:25

################################################################################
                     [1m Learning iteration 1975/2000 [0m                     

                       Computation: 101897 steps/s (collection: 0.828s, learning 0.137s)
             Mean action noise std: 11.32
          Mean value_function loss: 30.4059
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 30.4633
                       Mean reward: 871.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 171.8710
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.2989
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 194248704
                    Iteration time: 0.96s
                      Time elapsed: 00:32:52
                               ETA: 00:00:24

################################################################################
                     [1m Learning iteration 1976/2000 [0m                     

                       Computation: 96108 steps/s (collection: 0.910s, learning 0.113s)
             Mean action noise std: 11.33
          Mean value_function loss: 27.5794
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 30.4696
                       Mean reward: 853.52
               Mean episode length: 247.34
    Episode_Reward/reaching_object: 0.7559
     Episode_Reward/lifting_object: 170.6229
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.2981
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 194347008
                    Iteration time: 1.02s
                      Time elapsed: 00:32:53
                               ETA: 00:00:23

################################################################################
                     [1m Learning iteration 1977/2000 [0m                     

                       Computation: 105612 steps/s (collection: 0.818s, learning 0.113s)
             Mean action noise std: 11.33
          Mean value_function loss: 32.5130
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 30.4726
                       Mean reward: 862.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7584
     Episode_Reward/lifting_object: 171.6436
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.3003
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 194445312
                    Iteration time: 0.93s
                      Time elapsed: 00:32:54
                               ETA: 00:00:22

################################################################################
                     [1m Learning iteration 1978/2000 [0m                     

                       Computation: 108209 steps/s (collection: 0.814s, learning 0.095s)
             Mean action noise std: 11.34
          Mean value_function loss: 32.1441
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 30.4791
                       Mean reward: 866.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7427
     Episode_Reward/lifting_object: 169.0304
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.3019
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 194543616
                    Iteration time: 0.91s
                      Time elapsed: 00:32:55
                               ETA: 00:00:21

################################################################################
                     [1m Learning iteration 1979/2000 [0m                     

                       Computation: 110269 steps/s (collection: 0.800s, learning 0.092s)
             Mean action noise std: 11.35
          Mean value_function loss: 42.4242
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 30.4878
                       Mean reward: 868.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 170.6081
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.3001
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 194641920
                    Iteration time: 0.89s
                      Time elapsed: 00:32:56
                               ETA: 00:00:20

################################################################################
                     [1m Learning iteration 1980/2000 [0m                     

                       Computation: 106334 steps/s (collection: 0.829s, learning 0.096s)
             Mean action noise std: 11.36
          Mean value_function loss: 39.0715
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 30.4917
                       Mean reward: 862.37
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.7571
     Episode_Reward/lifting_object: 171.4986
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.3000
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 194740224
                    Iteration time: 0.92s
                      Time elapsed: 00:32:57
                               ETA: 00:00:19

################################################################################
                     [1m Learning iteration 1981/2000 [0m                     

                       Computation: 105320 steps/s (collection: 0.840s, learning 0.093s)
             Mean action noise std: 11.36
          Mean value_function loss: 39.2898
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 30.4953
                       Mean reward: 866.01
               Mean episode length: 248.56
    Episode_Reward/reaching_object: 0.7704
     Episode_Reward/lifting_object: 173.5341
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.3008
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 194838528
                    Iteration time: 0.93s
                      Time elapsed: 00:32:57
                               ETA: 00:00:18

################################################################################
                     [1m Learning iteration 1982/2000 [0m                     

                       Computation: 107512 steps/s (collection: 0.822s, learning 0.093s)
             Mean action noise std: 11.37
          Mean value_function loss: 35.0312
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 30.4983
                       Mean reward: 852.09
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7532
     Episode_Reward/lifting_object: 169.3701
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.3023
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 194936832
                    Iteration time: 0.91s
                      Time elapsed: 00:32:58
                               ETA: 00:00:17

################################################################################
                     [1m Learning iteration 1983/2000 [0m                     

                       Computation: 101617 steps/s (collection: 0.847s, learning 0.121s)
             Mean action noise std: 11.38
          Mean value_function loss: 33.8552
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 30.5033
                       Mean reward: 862.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 171.9423
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.3012
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 195035136
                    Iteration time: 0.97s
                      Time elapsed: 00:32:59
                               ETA: 00:00:16

################################################################################
                     [1m Learning iteration 1984/2000 [0m                     

                       Computation: 105385 steps/s (collection: 0.823s, learning 0.110s)
             Mean action noise std: 11.38
          Mean value_function loss: 33.5400
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 30.5099
                       Mean reward: 860.43
               Mean episode length: 246.97
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 172.1673
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.2993
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 195133440
                    Iteration time: 0.93s
                      Time elapsed: 00:33:00
                               ETA: 00:00:15

################################################################################
                     [1m Learning iteration 1985/2000 [0m                     

                       Computation: 102614 steps/s (collection: 0.857s, learning 0.101s)
             Mean action noise std: 11.40
          Mean value_function loss: 30.0313
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 30.5169
                       Mean reward: 853.33
               Mean episode length: 249.67
    Episode_Reward/reaching_object: 0.7539
     Episode_Reward/lifting_object: 170.1389
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.3000
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 195231744
                    Iteration time: 0.96s
                      Time elapsed: 00:33:01
                               ETA: 00:00:14

################################################################################
                     [1m Learning iteration 1986/2000 [0m                     

                       Computation: 108637 steps/s (collection: 0.803s, learning 0.102s)
             Mean action noise std: 11.41
          Mean value_function loss: 31.9262
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 30.5288
                       Mean reward: 860.38
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.7548
     Episode_Reward/lifting_object: 169.3089
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.3016
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 195330048
                    Iteration time: 0.90s
                      Time elapsed: 00:33:02
                               ETA: 00:00:13

################################################################################
                     [1m Learning iteration 1987/2000 [0m                     

                       Computation: 110340 steps/s (collection: 0.793s, learning 0.098s)
             Mean action noise std: 11.43
          Mean value_function loss: 34.3767
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 30.5392
                       Mean reward: 855.71
               Mean episode length: 249.74
    Episode_Reward/reaching_object: 0.7628
     Episode_Reward/lifting_object: 170.6761
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.3007
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 195428352
                    Iteration time: 0.89s
                      Time elapsed: 00:33:03
                               ETA: 00:00:12

################################################################################
                     [1m Learning iteration 1988/2000 [0m                     

                       Computation: 107375 steps/s (collection: 0.816s, learning 0.099s)
             Mean action noise std: 11.44
          Mean value_function loss: 33.5846
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 30.5483
                       Mean reward: 839.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7564
     Episode_Reward/lifting_object: 170.4193
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.3028
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 195526656
                    Iteration time: 0.92s
                      Time elapsed: 00:33:04
                               ETA: 00:00:11

################################################################################
                     [1m Learning iteration 1989/2000 [0m                     

                       Computation: 109861 steps/s (collection: 0.797s, learning 0.098s)
             Mean action noise std: 11.45
          Mean value_function loss: 32.2095
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 30.5567
                       Mean reward: 867.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 170.7541
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.3015
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 195624960
                    Iteration time: 0.89s
                      Time elapsed: 00:33:05
                               ETA: 00:00:10

################################################################################
                     [1m Learning iteration 1990/2000 [0m                     

                       Computation: 108208 steps/s (collection: 0.814s, learning 0.094s)
             Mean action noise std: 11.46
          Mean value_function loss: 42.5678
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 30.5615
                       Mean reward: 871.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 171.3196
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.3038
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 195723264
                    Iteration time: 0.91s
                      Time elapsed: 00:33:06
                               ETA: 00:00:09

################################################################################
                     [1m Learning iteration 1991/2000 [0m                     

                       Computation: 109349 steps/s (collection: 0.803s, learning 0.096s)
             Mean action noise std: 11.46
          Mean value_function loss: 35.5214
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 30.5675
                       Mean reward: 841.84
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7544
     Episode_Reward/lifting_object: 169.2869
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.3025
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 195821568
                    Iteration time: 0.90s
                      Time elapsed: 00:33:07
                               ETA: 00:00:08

################################################################################
                     [1m Learning iteration 1992/2000 [0m                     

                       Computation: 107577 steps/s (collection: 0.815s, learning 0.099s)
             Mean action noise std: 11.47
          Mean value_function loss: 38.9260
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 30.5721
                       Mean reward: 860.71
               Mean episode length: 247.21
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 171.0696
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.3055
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 195919872
                    Iteration time: 0.91s
                      Time elapsed: 00:33:08
                               ETA: 00:00:07

################################################################################
                     [1m Learning iteration 1993/2000 [0m                     

                       Computation: 110728 steps/s (collection: 0.794s, learning 0.094s)
             Mean action noise std: 11.47
          Mean value_function loss: 41.0096
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.5765
                       Mean reward: 870.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7611
     Episode_Reward/lifting_object: 172.1475
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.3030
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 196018176
                    Iteration time: 0.89s
                      Time elapsed: 00:33:08
                               ETA: 00:00:06

################################################################################
                     [1m Learning iteration 1994/2000 [0m                     

                       Computation: 108071 steps/s (collection: 0.818s, learning 0.092s)
             Mean action noise std: 11.48
          Mean value_function loss: 35.3980
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 30.5801
                       Mean reward: 852.17
               Mean episode length: 247.02
    Episode_Reward/reaching_object: 0.7568
     Episode_Reward/lifting_object: 171.1367
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.3030
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.4583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 196116480
                    Iteration time: 0.91s
                      Time elapsed: 00:33:09
                               ETA: 00:00:05

################################################################################
                     [1m Learning iteration 1995/2000 [0m                     

                       Computation: 108222 steps/s (collection: 0.810s, learning 0.099s)
             Mean action noise std: 11.49
          Mean value_function loss: 34.0450
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 30.5855
                       Mean reward: 852.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7562
     Episode_Reward/lifting_object: 171.9609
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.3057
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 196214784
                    Iteration time: 0.91s
                      Time elapsed: 00:33:10
                               ETA: 00:00:04

################################################################################
                     [1m Learning iteration 1996/2000 [0m                     

                       Computation: 109618 steps/s (collection: 0.804s, learning 0.093s)
             Mean action noise std: 11.50
          Mean value_function loss: 39.1689
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 30.5916
                       Mean reward: 843.56
               Mean episode length: 247.95
    Episode_Reward/reaching_object: 0.7587
     Episode_Reward/lifting_object: 170.5614
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.3059
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 196313088
                    Iteration time: 0.90s
                      Time elapsed: 00:33:11
                               ETA: 00:00:03

################################################################################
                     [1m Learning iteration 1997/2000 [0m                     

                       Computation: 108489 steps/s (collection: 0.806s, learning 0.101s)
             Mean action noise std: 11.51
          Mean value_function loss: 35.3306
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 30.5970
                       Mean reward: 848.02
               Mean episode length: 249.31
    Episode_Reward/reaching_object: 0.7513
     Episode_Reward/lifting_object: 170.0002
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.3052
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 196411392
                    Iteration time: 0.91s
                      Time elapsed: 00:33:12
                               ETA: 00:00:02

################################################################################
                     [1m Learning iteration 1998/2000 [0m                     

                       Computation: 105049 steps/s (collection: 0.826s, learning 0.110s)
             Mean action noise std: 11.51
          Mean value_function loss: 42.3793
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 30.6029
                       Mean reward: 866.13
               Mean episode length: 249.05
    Episode_Reward/reaching_object: 0.7586
     Episode_Reward/lifting_object: 171.9257
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.3084
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 196509696
                    Iteration time: 0.94s
                      Time elapsed: 00:33:13
                               ETA: 00:00:01

################################################################################
                     [1m Learning iteration 1999/2000 [0m                     

                       Computation: 100215 steps/s (collection: 0.857s, learning 0.124s)
             Mean action noise std: 11.52
          Mean value_function loss: 27.5831
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 30.6078
                       Mean reward: 847.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7596
     Episode_Reward/lifting_object: 170.8156
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.3101
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 196608000
                    Iteration time: 0.98s
                      Time elapsed: 00:33:14
                               ETA: 00:00:00

